{
  "processed_papers": {
    "12-bit Delta-Sigma ADC operating at a temperature of up to 250C in Standard 0.18": {
      "full_text": "1\n12-bit Delta-Sigma ADC operating at a temperature\nof up to 250 °C in Standard 0.18 µm SOI CMOS\nChristian Sbrana, Student Member, IEEE, Alessandro Catania, Senior Member, IEEE, Tommaso Toschi,\nSebastiano Strangio, Senior Member, IEEE, Giuseppe Iannaccone, Fellow, IEEE\nAbstract —Some applications require electronic systems to\noperate at extremely high temperature. Extending the operating\ntemperature range of automotive-grade CMOS processes —\nthrough the use of dedicated design techniques — can provide an\nimportant cost-effective advantage. We present a second-order\ndiscrete-time delta-sigma analog-to-digital converter operating\nat a temperature of up to 250 °C, well beyond the 175 °C\nqualification temperature of the automotive-grade CMOS process\nused for its fabrication (XFAB XT018). The analog-to-digital\nconverter incorporates design techniques that are effective in\nmitigating the adverse effects of the high temperature, such\nas increased leakage currents and electromigration. We use\nconfigurations of dummy transistors for leakage compensation,\nclock-boosting methods to limit pass-gate cross-talk, and we\noptimized the circuit architecture to ensure stability and accuracy\nat high temperature. Comprehensive measurements demonstrate\nthat the analog-to-digital converter achieves a signal-to-noise\nratio exceeding 93 dB at 250 °C, with an effective number of bits\nof 12, and a power consumption of only 44 mW. The die area\nof the converter is only 0.065 mm2and the area overhead of the\nhigh-temperature mitigation circuits is only 13.7%. The Schreier\nFigure of Merit is 140 dB at the maximum temperature of\n250 °C, proving the potential of the circuit for reliable operation\nin challenging applications such as gas and oil extraction and\naeronautics.\nIndex Terms —High-temperature electronics, ADC, Delta-\nSigma, SOI CMOS.\nI. I NTRODUCTION\nThis work has been submitted to the IEEE for possible publication.\nCopyright may be transferred without notice, after which this version\nmay no longer be accessible.\nThis research was partially supported by the CHARM project from the\nECSEL Joint Undertaking (Grant Agreement N.876362), by the European\nUnion’s Horizon 2020 research and innovation programme and by the\nItalian Ministry for Economic Development (MISE), and by the ECS4DRES\nproject supported by the Chips Joint Undertaking (grant agreement number\n101139790) and its members, including the top-up funding by Germany, Italy,\nSlovakia, Spain and The Netherlands. It is also partially supported by the\nitalian MUR under the Forelab project of the “Dipartimenti di Eccellenza”\nprogramme.\nChristian Sbrana is with Quantavis s.r.l., Largo Padre Renzo Spadoni,\n56126 Pisa, Italy and with the Dipartimento di Ingegneria dell’Informazione\n(DII), Universit `a di Pisa, 56122 Pisa, Italy; Alessandro Catania is with\nthe Dipartimento di Ingegneria dell’Informazione (DII), Universit `a di Pisa,\n56122 Pisa, Italy; Tommaso Toschi is with Quantavis s.r.l., Largo Padre\nRenzo Spadoni, 56126 Pisa, Italy; Sebastiano Strangio is with the Dipar-\ntimento di Ingegneria dell’Informazione (DII), Universit `a di Pisa, 56122\nPisa, Italy; Giuseppe Iannaccone is with the Dipartimento di Ingegneria\ndell’Informazione (DII), Universit `a di Pisa and with Quantavis s.r.l., Largo\nPadre Renzo Spadoni, 56126 Pisa, Italy (corresponding author: (e-mail:\ngiuseppe.iannaccone@unipi.it)\nManuscript submitted December, 2024.THE push for the digitization of industrial systems, aimed\nat improving control and overall system performance,\noften requires the use of electronics in harsh environments.\nSeveral industrial applications, such as aerospace, aeronautics,\noil and gas drilling, energy generation, motor control, and\nsome types of manufacturing, have much higher operating\ntemperature than conventional electronics, [1]–[3] as illus-\ntrated in Figure 1. Indeed, electronic components and CMOS\ncircuits for consumer and most industrial applications are\nqualified for operation at a maximum temperature of 85-\n125 °C, whereas those for military and automotive applications\nare qualified at up to 150-175 °C.\nThe challenge of high-temperature operation can be ad-\ndressed in three different ways:\ni) by segregating electronics in cooler areas or by\nproper thermal insulation of the electronic subsys-\ntem, paying a price in terms of longer wiring, larger\nweight and volume, and higher cooling costs [4];\nii) by using devices and circuits based on wide bandgap\nmaterials such as SiC and GaN, that are inherently\nsuitable for high-temperature operation up to 600 °C\n[2], [5], and using high-temperature materials also\nfor wires, bonding, packaging and printed circuit\nboard [6], [7];\niii) by extending the temperature range of automotive-\ngrade silicon-on-insulator (SOI) CMOS processes up\nto 250-300 °C, through the use of design techniques\nthat compensate for the degradation mechanisms\noccurring at high temperature [8], [9]; SOI-CMOS\nhas reduced substrate leakage currents due to the\ninsulating layer between the substrate and the active\narea.\nThe first option may suffer from reliability issues caused by the\nassociated deployment complications and may not be suitable\nin some industrial settings where form factors are prohibitive.\nThe second option provides a promising and definitive solution\nto the issue, when reliable and reasonably scaled-down SiC\nor GaN monolithic processes will be available [10] (results\nhave only been presented with SiC CMOS processes with\nminimum feature size of 1 µm [11]–[14]). The third option has\nthe advantage of being based on available and cost-effective\nmonolithic fabrication processes, and is suitable for many\nindustrial applications (Figure 1).\nBasic proof of concepts have been provided up to 300 °C\nfor single transistors [15], [16], focusing also on the analysis\n0000–0000/00$00.00 © 2021 IEEEarXiv:2501.00482v1  [eess.SP]  31 Dec 20242\nof the effects induced by the high temperature on MOSFET\nparameters [17], [18]. Basic digital and analog circuits have\nbeen shown and their potential for high-temperature operations\nhave been investigated [16], [19]–[22], sometimes providing\na useful comparison with other existing technologies [23] or\ngiving important design guidelines for robust circuits realiza-\ntion [24]. SOI CMOS technology has been identified as the\nsolution of choice for integrated microsystems designed to\noperate in high-temperature environments with very different\nrequirements of power consumption and speed [25].\nIn this paper, we present a second-order discrete-time (DT)\ndelta-sigma ( ∆-Σ) analog-to-digital converter (ADC) designed\nfor high-temperature operation up to 250°C. It is realized using\nan automotive-grade CMOS process, the XFAB XT018 [26],\na 180 nm partially depleted SOI CMOS technology with\naluminum metal interconnections. The process is qualified in\nthe range from -40 °C to 175 °C, thus the process design kit\n(PDK) models are calibrated in the same temperature range,\nlikely leading to a poor device behavior prediction above\n175 °C. For this reason, a high-temperature-specific design\napproach is needed to enable proper operation above this limit.\nIn [9], using some basic test structures, we have already in-\nvestigated the possibility of exploiting the same SOI platform\nabove the maximum temperature rating, by characterizing\nthe degradation of device electrical parameters and proposing\ndedicated design countermeasures. The presented ADC is\nbased on the requirements of a high-accuracy conversion of a\nrelatively slow input signal, since the ADC is part of a larger\nIC for application in temperature measurements for the reliable\nmonitoring of thermal processes in industrial ovens.\nIn the technical literature, ∆-ΣADCs have been demon-\nstrated up to 175 °C in a 0.18 µm SOI CMOS [27] and\nup to 250°C in a 0.35 µm High Temperature SOI CMOS\nGeothermal\nenergyBulk \nCMOSSOI\nApplicationsSpace\nexploration  \nAeronautics\nGas & Oil\nextraction \nIndustrial\nelectronics Automotive\napplications600\nExtended SOISiC & GaNTechnologies\n300\n0100200400500T [°C]\nFig. 1. Available ICs manufacturing technologies match the temperature\nratings of different applications. Data extracted from Ref. [1], [3], [6].process [28], however at the expense of large area occupation\nand power consumption. Also SAR converter architectures\nperform well up to 210 °C at high sampling frequency\n[29], [30], but the performance degradation is still signifi-\ncant when approaching 300°C [31]. With respect to results\nfrom the literature, our proposed ADC has a extremely low\narea occupation and energy consumption, for an ENOB in\nexcess of 12 bits up to 250 °C, and a very good Schreier\nFigure of Merit (FoM) of 140 dB at 250 °C [32] defined as\nFoM≡SINAD + 10 ×log(BW/P ), where SINAD is the\nsignal-to-noise-and-distortion ratio, BW is the bandwidth and\nPis the power consumption\nWe discuss the high-temperature ADC architecture and the\ndesign techniques used to mitigate the effects of leakage\ncurrents in Section II. Then we describe the measurement\nsetup (Section III) and the electrical characterization of the\nconverter over the whole temperature range (Section IV),\ncomparing our results with the state of the art. We draw the\nConclusion in Section V .\nII.∆-ΣADC TEMPERATURE -AWARE DESIGN\nA. Converter architecture\nDue to critical design issues emerging at high temperature\n[9], it is recommended to focus on as simple as possible\narchitectures. Considering the specifications, a second-order\nDT∆-ΣADC has been identified as the best solution. In fact,\nin contrast to a first-order modulator architecture, the oversam-\npling ratio (OSR) of a second-order modulator can be tuned\nto optimize the speed-resolution trade-off. The system utilizes\na single-bit cascaded integrator feedback modulator, which\noffers a favourable trade-off among hardware complexity,\npower consumption, and resolution, without imposing strict\nrequirements on the matching characteristics of the digital-to-\nanalog converter (DAC) [32].\nAlthough the ∆-Σarchitecture has been largely adopted\nfor very challenging applications with rigid design constraints\non supply voltage [33], speed [34] and resistance to radiation\n[35], we want to stress that this architecture is not inherently\ndesigned for high-temperature applications, and thus ad-hoc\ndesign optimization becomes necessary.\nFig. 2(a) shows the simplified circuit-level diagram of the\nproposed second-order ∆-Σmodulator. A digital low-pass\nfilter (not shown) has been implemented on the chip to obtain\na full ∆-ΣADC. The two-phase driving clock method is\nemployed to control both the first and the second integrators\nwith precise timing, enabling the capacitors to effectively\nsample and convert the input signals. The result is a single-bit\nbitstream signal (BTS), produced by the comparator based on\nthe differential feedback signal which can assume either V DD\nor GND levels.\nThe integrator stages use a fully differential switched-\ncapacitor topology, based on large gain–bandwidth prod-\nuct (GBW) folded-cascode amplifiers. A low-power dynamic\ncommon-mode feedback (CMFB) circuit is exploited to stabi-\nlize the output common-mode voltage of the folded-cascode\namplifiers.\nIn a two-stage filter design, the first stage must focus\non minimizing input-referred noise and offset. Provided that3\nCF2 CF1\nCF2 CF1CS21CS21CS1\nCS1\nCS22VCM\nVO2 VO1\nVFBVFBVIN\nGND\nGNDBTS\nVREFVO2 VO1VFBVFB\nVIN\n1-bit\nDAC12\n222\n11\n11\n111\n1 11 1\n22\n2\n2\n2\n222\nIntegrator 2\nComparatorIntegrator 1VCM\nVCMVCMCS22\nSR \nlatch\n1-bit\nDAC(a)\nVDD VDD\nVBNVK2VCMFB VBP\nVDDVDD\nGNDVDD VK1\nGND GNDVDD VK1\nn-type\nComp.n-type\nComp.p-type\nComp.p-type\nComp.p-type\nComp.VDD\nVDDGND\nGNDVDDVDDDifferential Input Pair\nCompensationPG VCM\nVBPVCMFB VBNCO1\nCO2CF3\nCF4CF1\nCF2\nVCM\nn-type\nComp.n-type\nComp.n-type\nComp.n-type\nComp.\nGNDx1 x4 x2 x2PG\nPG PG\nPG PG\nCLK1 CLK2CLK CLK\nGND\nGNDCLKVDD\nGND\nComparator(b)\n(c)\nBTS\nsignal\nVREF VREF\nVFBVOUT\nVFBVOUT\nVIN VINVFBVBTS\nVFBVBTS\nVOUT\nVOUT\nFig. 2. ADC architecture schematic overview: (a) 2nd-order DT Delta-Sigma modulator; (b) StrongArm latch comparator for bitstream signal output and\nsingle-bit DAC for feedback; (c) Folded-cascode operational amplifier used in the integrators, with junction leakage compensation method for single PMOS\n(p-type) and NMOS (n-type), and for the differential input pair. The dynamic CMFB circuit is shown in the same subpicture, providing the bias voltage\nVCMFB for the cascode amplifier, based on leakage-compensated pass-gates (PG).\nthe thermal ( kT/C ) noise is a critical concern in switched-\ncapacitor topologies, priority has been assigned to the first\nintegrator by allocating larger capacitors, amplifier area, and\npower consumption (capacitors in the first stage have a capac-\nitance that is ten times larger than that of capacitors in the\nsecond stage)\nFinally, a StrongARM latch architecture has been selected\nfor the comparator [36], reported in Fig. 2(b), offering the ben-\nefits of negligible static power consumption and no hysteresis.\nAt the output, an SR latch provides a robust digital output to\ndrive the DAC with a single-bit topology.\nB. Design techniques for high-temperature operation\nThe main high-temperature challenges in the integrated\ncircuit design are the increased leakage currents in the silicon\nregions and the electromigration in automotive-grade metal\nlines [9]. The leakage currents with the most significant impact\nare the reverse currents of the p-n junctions and the subthresh-\nold channel leakage. Electromigration affects the metal lines,\nwith a progressive increase of the interconnect resistance until\nopen circuit, due to microscopic damage in conditions of high\ncurrent density and high temperature.\nThe reverse saturation current of p-n junctions, from now on\ncalled junction leakage current, has a near-exponential increase\nwith temperature, leading to higher power consumption andpotentially triggering latch-up phenomena [9]. The negative\nleakage effect introduced by drain (and source) to body junc-\ntions can be mitigated using dummy-transistor compensation\n[37]. This strategy is based on the idea of neutralizing the\nleakage currents of both the source-body and drain-body junc-\ntions by injecting an identical but opposite current. This extra\ncontribution is obtained by adding parallel off-state dummy\ntransistors, with the body and source connected and enforcing\na reverse bias on the drain-body junction.\nFig. 2(c) shows this junction leakage compensation method\napplied to the folded-cascode amplifier. A n-type dummy\ntransistor is used to compensate the leakage current of a\nNMOS with source-body connection to the ground, realizing\nthe current cancellation by exploiting its drain-body current,\nwhen the gate is grounded and the supply voltage VDDis\napplied to the drain. Similarly, a p-type dummy transistor is\nused for PMOS compensation with source-body connection to\nthe supply voltage, now applying to the dummy gate VDDand\nGND to the drain.\nThe same method can also be applied to the transistors in the\ncentral positions of the circuit, for example to the differential\ninput pair. However, since these two MOSFETs are generally\nlarge, doubling them to realize the compensation yields an\nincrease of the parasitic capacitance on the common node,\nreducing the speed of the whole circuit. With the presented4\nCLKCLKVSS VSS\nVSS\nVSSVSSVDDVDD\nVDD\nVDD VDDVIN VOUTɸ2 ɸ1\nɸ1 ɸ2VSSVSS\nVSSVREF\nC2\nC1CLK CLK\nCLK_BST\nCLK CLK\nVSS\nVSSVSS VREF\nVDDVDD\n(d) (c)(b) (a)\nFig. 3. Design techniques for leakage current reduction: (a) Pass-gate\ncircuit with four dummy transistors (on the corners) for junction leakage\ncompensation; (b) Two-phases clock boosting generator circuit; (c) Junction\nleakage current dependence on temperature and compensation with dummy\ndevices; (d) Channel leakage current dependence on temperature and clock\nboosting.\ndesign technique, a single dummy device with the same\nsize as one of the transistors in the input pair is enough\nto provide the necessary compensation current, since both\ndrain-body and source-body junctions are used to collect the\ncompensation leakage. Then, this current is mirrored with a\nratio of four and sunk from the source common connection of\nthe input pair, while the separated drain connections receive\ndouble leakage each. The transistors in the current mirror have\nminimum dimensions and are compensated for their leakage\ncurrent, so they realize the compensation mechanism without\nincreasing the parasitic capacitance of the pair common circuit\nnode. However, using a current mirror means introducing\nnon-idealities in the mirrored currents, hardening a perfect\ncancellation of the leakage currents among dummy and active\ntransistors. This method is anyway fundamental to reduce\nthe current variation in the differential input pair from bias\nconditions due to the temperature increase, as its contribution\nis the most significant one given the pair size.\nThe pass-gates used in the ∆-Σmodulator are similarly\ncompensated with dummy transistors, as reported in Fig. 3(a).\nBoth the p-n junctions of a pass-PMOS or a pass-NMOS need\nto be compensated with leakage currents from the dummy\ndevices, so for each pass-gate there are four dummy transistors,\nsourcing and sinking compensation currents on the input and\noutput node. The better the compensation on the currents,\nthe more stable the voltages across the capacitors can be\nduring the DT circuit holding times. The effectiveness of\nthe dummy compensation method appears evident from the\nADC390 μm\n165 μmFig. 4. Chip top view and layout magnification on the ADC block.\nsimulation results reported in Fig. 3(c), where we can observe\nan improvement of more than four decades, also considering\nmismatch. Let us stress that simulations are strictly reliable\nonly up to the qualification temperature (175 °C) but the trend\nis clear also at higher temperature.\nPass-gates are heavily affected by the other leakage mech-\nanism in silicon CMOS, which is the subthreshold channel\ncurrent, showing an exponential dependence on the overdrive\nvoltage [9], [38]. This effect is strictly related to the degra-\ndation of the subthreshold swing as the temperature increases\n[17], [39], reducing the strength of the off-state for a transistor.\nA negative off-state gate voltage shift for the n-MOS switches\nand a positive one for the p-MOS switches can be exploited:\nin this way, the channel leakage current can be drastically\nreduced. Fig. 3(b) reports the charge-pump circuit used to\ngenerate the overdrive voltages for the clock-boosting solution.\nThe charge-pump mechanism is exploited to shift both the high\nand the low levels of the clock signal, obtaining VDD+Vref\nfor the high level and −Vreffor the low one employing C1\nandC2. An additional benefit of this approach is the on-\nresistance reduction for both the n-MOS and the p-MOS of\nthe pass-gates, thus allowing a smaller aspect ratio of the\ntransistors and consequently a lower reverse current of the p-n\njunctions. The analysis of the improvements introduced with\nthe clock-boosting technique has been performed by means of\nsimulations shown in Fig. 3(d), reporting the channel leakage\ncurrent as a function of the temperature and for different\nvalues of the boosting voltage. An absolute boost value of\n200 mV enables a reduction of the leakage current by a factor\nlarger than 20, still being compliant with the transistor gate\nvoltage maximum rating. This value has been used in the\ndesign for channel leakage compensation, generated with a\nresistive partition of the supply voltage since high accuracy is\nnot required.\nThe discussed design techniques for both junction and\nchannel leakage compensations have been implemented in this\nADC design. The clock boosting solution has been used to\ndrive all the minimum size pass-gates, while several dummy\ntransistors have been added to many critical nodes of cir-\ncuits, to either sink or source compensation currents in each\nswitch, bias circuit, op-amp, and comparator. Regarding the\nelectromigration problem, we have carefully sized the width\nof the metal interconnections during the layout phase, so that\nthe current density is a factor ten below the electromigration\nthreshold at 300°C, as extrapolated from available process\nspecifications [9]. This threshold is about 45 µA/µm for the5\ninternal metal layers, rising to 75 µA/µm for the top metal\n(with W= 0.8µm).\nThe integration of the presented simple converter architec-\nture with these temperature-aware design techniques results\nin an on-chip footprint area of 0.065 mm2, and the ADC\nblock is integrated into a more complex IC all optimized for\nhigh-temperature operation, as illustrated in Fig. 4. The total\nfootprint of the additional circuitry required to enable high-\ntemperature operation is 0.0086 mm2, mostly due to the area\nof dummy transistors, corresponding to a total area overhead\nfor the extended temperature range of 13.7%. The increased\nfootprint of the electromigration-resilient metalizations does\nnot lead to a higher area occupation of the ADC.\nIII. C HIP MEASUREMENT SETUP\nThe second-order ∆-ΣADC has been designed to operate\nunder the following nominal conditions: clock frequency fS\nof150 kHz , OSR of 512, reference voltage Vrefof1.8 V and\ninput common-mode voltage Vicof0.9 V.\nThe measurement setup used for the temperature charac-\nterization of the converter is shown in Fig. 5. The IC has\nbeen bonded in a ceramic DIL48 package, able to withstand\ntemperatures as high as 300 °C. The packaged chip has been\nplaced on a two-face ceramic PCB with a custom metal\nsocket, compliant with the explored temperature range. The\nboard is then connected to various measurement instruments\nthrough wires exploiting high-temperature mica insulation.\n(c) (d)(a) (b)\nFig. 5. Measurement set-up for ADC characterization in the temperature\nrange -40°C ÷260°C: (a) Ceramic PCB for chip connection, compatible with\nthe DIL48 package; (b) Connection wires between the ceramic PCB placed\ninside the oven and the measurement instruments outside it, rated for high\ntemperature and with a custom metal shield for noise reduction; (c) Lab\noven for high-temperature measurements up to 260°C, containing the chip\nwith several instruments connected; (d) Climate chamber for low-temperature\nmeasurements up to -40°C.Long wires have been enclosed in a custom metal shield\nconnected to the ground to reduce the electromagnetic noise\nfrom the measurement environment. A laboratory oven and\na climatic chamber have been employed to cover the entire\ntemperature range: G2100 and Genviro 030T, respectively\n[40]. G2100 is a 350°C laboratory oven with internal forced\nventilation, which includes a lateral opening for routing. This\noven can only heat the internal chamber starting from room\ntemperature, and it has been used to cover the higher part of\nthe considered temperature range. The other oven is a climate\nchamber with forced ventilation and a refrigerant circuit,\nused for low-temperature characterization down to -40°C. The\nelectrical characterization test setup included the following\nmeasurement instruments: a power supply (Keihtley 2230-30-\n1) to provide the analog core voltage of 1.8 V , I/O voltage\nof 3.3V and ADC reference voltage; a source measurement\nunit (Keithley 2601B) to provide the bias current to the chip;\ntwo arbitrary waveform generators (Tektronix AFG31102 and\nSiglent SDG2122X) for clock sources (system, CMFB) and\nfor input AC signals; an oscilloscope (Siglent SDS1204XE)\nfor the converter digital output bitstream acquisition. Providing\nall the necessary signals to the ADC from the outside helps\nto isolate the dependence of its performance on the tempera-\nture variations to the contribution introduced by other circuit\nblocks, such as internal bias current generator, clock generator\nand voltage reference.\nSince the XFAB XT018 process is qualified up to 175°C,\nPDK simulations can not be trusted above this limit, and\nonly accurate high-temperature measurements can confirm the\nexpected behaviour of the circuit.\nIV. M EASUREMENT RESULTS\nThe ADC performance has been experimentally assessed\nthrough the SINAD, the signal-to-noise ratio (SNR), and the\neffective number of bits (ENOB) figures of merit measured\nover temperature for 5 different samples.\nThe experimental ADC SNR and SINAD as a function of\nthe operating temperature are shown in Fig. 6 (a) and (b)\nrespectively, indicating the mean with a solid line and the mean\nplus or minus three standard deviations with dashed lines. It\ncan be seen that the SNR is quite stable within the temperature\nrange, reaching a value above 93 dB at 250°C, while it\nfalls down quickly as the operating temperature approaches\n260°C. A similar trend has been noticed for the SINAD,\nwhich however shows a soft worsening from 85 dB at room\ntemperature to 74.5 dB at 250°C, and then collapsing at 260°C.\nAs the performance degradation in the 100 °C-250 °C range\nis observed only from the SINAD point of view, but not in the\nSNR, we can conclude that this is only related to the increasing\nimpact of distortion with rising temperature. In both charts, the\ndramatic performance drop as the temperature exceeds 250°C\nis probably due to a critical temperature-induced alteration\nof the behavior of some internal circuits and fundamental\nparts of the ADC architecture. Even though the main bias\nsignals are provided from the outside, when the temperature\nincreases every p-n junction in the chip is affected by increased\njunction leakage current, therefore even the pad protection6\ndiodes are injecting and withdrawing leakage currents on the\ninput and output connections. For this reason, the measured\nbiasing current cannot be trusted at high temperature, and we\nthus speculate that this could be the main cause of performance\ndegradation. In addition, some nominally-off peripheral circuit\nblocks could turn on unintentionally interfering with signals\nprovided by the characterization setup.\nThe integrative non-linearity (INL) is obtained from the\nDC characterization for the considered temperature range. Fig.\n6(c) shows that the INL increases as the temperature rises, in\nagreement with the degradation of the AC parameters. The\nmeasured value is below 1 mV from -40 °C to 250 °C in\nthe worst case among the tested chips, while it deteriorates at\ntemperatures above the upper limit.\nFig. 6(d) shows the measured supply current adsorbed by\nthe ADC with VDD= 1.8V for five different samples. As\ncan be seen, the power consumption has a relatively small\nvariation as a function of temperature also above the process\nqualification temperature and up to 250 °C. The effects of\nboth noise and non-linearity can also be seen in Fig. 6(e),\nwhere the experimental spectral power density is reported for\none chip at different temperature values. The spectra are quite\nstable in temperature inside the signal bandwidth region, with\nlow flicker noise. When the temperature rises, the distortionpeaks become more visible, and the distortion power related\nto them contributes to reducing the SINAD. The ground noise\nis higher than what is expected from simulations, but in\nhigh-temperature measurements, no capacitors can be placed\nclose to the chip for an effective filtering action, therefore\nhigher noise is captured from the long and unshielded lines.\nThe presence of additional spikes can also be noticed in\nthe reported spectra, related to off-band frequencies. This\nadditional distortion source is due to the amplifier CMFB\ncontrol circuit, since it is based on passive switched capacitor\nnetworks working at fS/512, a frequency that is modulated\nby the ADC input signal one, leading to two peaks very close\nto each other. However, the CMFB frequency is outside the\nsignal bandwidth, and the related distortions can be pushed to\nhigher frequencies by simply increasing the CMFB operating\nfrequency, without loss of performance.\nThe performance extracted from ADC characterization has\nbeen reported in Table I for a comparison with other works in\nthe literature. As already mentioned, the relatively low signal\nbandwidth has been a design choice to optimize the resolution\nat the cost of conversion speed, in line with the ADC target\napplication. The high values of SNR and SINAD at 250 °C,\nidentified as the ADC maximum operating temperature, result\nin an ENOB of 12 bits. These results, together with the\n(a)\n(d)(b) (c)\n(e)\nFig. 6. Results from repeated measurements on five different ICs, performed at fS= 150 kHz ,OSR = 512 ,fCMFB =fS/512,IBIAS = 50 nA,\nVREF = 1.8V: (a) Measured SNR and (b) SINAD when an input AC tone is applied ( VAC= 1.7V,fAC= 25 .177Hz); (c) Worst INL measured\nfrom DC characteristics at each temperature step; (d) ADC current consumption on 1.8-V line; (e) Spectrum reported from AC characterization (AC tone at\nVAC= 1.7V,fAC= 25.177Hz), for a single chip and significant temperature values. The graphs from (a) to (d) report the actual results indicated by\ncircles, the mean trend with a solid line and the +/−3σrange delimited by the dashed lines and coloured in light blue.7\nTABLE I\nSTATE -OF-ARTHIGH-TEMPERATURE ADC C OMPARISON\nReference This work [27] [28] [29] [31]\nTech. [ µm] SOI 0.18 SOI 0.18 SOI 0.35 AD7981 SOI 1.0\nfS[kHz] 150 10000 31.25 600 50\nBW [kHz] 0.146 100 0.488 300 25\nOSR 512 50 64 N/A N/A\nSNR @T MAX 93.4 dB - 84 dB 88 dB 56 dB\nSINAD @T MAX 74.5 dB 68 dB 80 dB 87 dB 43 dB\nENOB @T MAX 12 bit 11 bit 13 bit 14 bit 6.9 bit\nTMAX 250°C 175°C 250°C 210°C 300°C\nPower [mW] 0.044 26.3 - 8 2.17\nArea [mm2] 0.065 2.76 1.75 - 2.5\nArchitecture 2nd∆-Σ 2nd∆-Σ 2nd∆-Σ SAR SAR\nVDD 1.8 V 3.3 V 3.3 V 2.5 V 5 V\nFoM(1)@T MAX 140 134 N/A 163 114\n(1): Schreier FoM defined as SINAD +10×log(BW /P).\nextremely low area occupation and power consumption, have\nbeen used to calculate the Schreier Figure of Merit (FoM)\n[32], [41], reaching a value of 140 dB at 250 °C, and showing\nthe potential of this high-temperature design. Among the\ndata converters for very high temperature, this work performs\nvery well, featuring record Schereier FoM compared to all\ncomparison circuits, except for a commercial SAR converter\ndesigned by Analog Devices (AD7981), which however has a\nmuch lower maximum temperature of 210 °C, since the used\nFoM does not consider the temperature range as a key feature.\nV. C ONCLUSIONS\nWe have presented a second-order discrete-time delta-sigma\nADC designed for high-temperature operation up to 250 °C\nwith a standard automotive-grade SOI CMOS process qualified\nup to 175 °C showcasing impressive performance, using a\nset of design techniques to reduce the negative impact of\nhigh temperature on circuit behavior. The temperature-range\nextension techniques require a total overhead of die area of\nonly 13.7% for a record-low ADC die area of only 0.065 mm2.\nThe ADC exhibits at 250 °C a SINAD of 74.5 dB, a 12-bit\nENOB, and a power consumption of only 44 mW.\nThe achieved results position this high-temperature design\nas a leading contender in the field of data converters for\nextreme environments, with a calculated Schreier Figure of\nMerit of 140 dB at 250 °C, highlighting its potential for\napplications in harsh environments.\nThis work opens up new possibilities for the design of\nelectronic systems capable of reliable operation in challenging\nand high-temperature conditions, using standard automotive-\ngrade processes. We do not see fundamental reasons not\nto further improve these design techniques as to reach the\noperating temperature of 300 °C with SOI CMOS. Additional\ngains can be obtained with temperature-resistant interconnect\nmaterial.\nThe future adoption of SiC CMOS scaled-down technolo-\ngies for monolithic integration will enable to reach higher\ntemperature and higher currents. Until such technology be-\ncomes available and at a reasonable cost, the capability to\ndesign high-temperature circuits in SOI-CMOS is a very cost-\neffective way to obtain components and systems with small\nform factors to enable the digitization of several industrial\nprocesses and applications.",
      "metadata": {
        "filename": "12-bit Delta-Sigma ADC operating at a temperature of up to 250C in Standard 0.18.pdf",
        "hotspot_name": "ECU_Operation",
        "title": "12-bit Delta-Sigma ADC operating at a temperature of up to 250C in\n  Standard 0.18 $μ$m SOI CMOS",
        "published_date": "2024-12-31T15:11:05Z",
        "pdf_link": "http://arxiv.org/pdf/2501.00482v1",
        "query": "automotive electronics low power consumption design optimization techniques"
      }
    },
    "Optimizing RAG Techniques for Automotive Industry PDF Chatbots_ A Case Study wit": {
      "full_text": "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tOptimizing RAG Techniques for Automotive Industry PDF Chatbots: A Case Study with Locally Deployed Ollama Models Optimizing RAG Techniques Based on Locally Deployed Ollama Models A Case Study with Locally Deployed Ollama Models Fei Liu * China\tAutomotive\tTechnology\t&\tResearch\tCenter,\tliufei@catarc.ac.cn\tZejun Kang China\tAutomotive\tTechnology\t&\tResearch\tCenter,\tkangzejun@catarc.ac.cn\tXing Han China\tAutomotive\tTechnology\t&\tResearch\tCenter,\thanxing@catarc.ac.cn\tWith\tthe\tgrowing\tdemand\tfor\toffline\tPDF\tchatbots\tin\tautomotive\tindustrial\tproduction\tenvironments,\toptimizing\tthe\tdeployment\tof\tlarge\tlanguage\tmodels\t(LLMs)\tin\tlocal,\tlow-performance\tsettings\thas\tbecome\tincreasingly\timportant.\tThis\tstudy\tfocuses\ton\tenhancing\t Retrieval-Augmented\t Generation\t (RAG)\t techniques\t for\t processing\t complex\t automotive\t industry\t documents\t using\tlocally\tdeployed\tOllama\tmodels.\tBased\t on\t the\t Langchain\t framework,\t we\t propose\t a\t multi-dimensional\t optimization\t approach\t for\t Ollama's\t local\t RAG\timplementation.\tOur\tmethod\taddresses\tkey\tchallenges\tin\tautomotive\tdocument\tprocessing,\tincluding\tmulti-column\tlayouts\tand\ttechnical\tspecifications.\tWe\tintroduce\timprovements\tin\tPDF\tprocessing,\tretrieval\tmechanisms,\tand\tcontext\tcompression,\ttailored\tto\tthe\tunique\tcharacteristics\tof\tautomotive\tindustry\tdocuments.\tAdditionally,\twe\tdesign\tcustom\tclasses\tsupporting\tembedding\tpipelines\tand\tan\tagent\tsupporting\tself-RAG\tbased\ton\tLangGraph\tbest\tpractices.\tTo\tevaluate\tour\tapproach,\twe\tconstructed\ta\tproprietary\tdataset\tcomprising\ttypical\tautomotive\tindustry\tdocuments,\tincluding\ttechnical\treports\tand\tcorporate\tregulations.\tWe\tcompared\tour\toptimized\tRAG\tmodel\tand\tself-RAG\tagent\tagainst\ta\tnaive\tRAG\tbaseline\tacross\tthree\tdatasets:\tour\tautomotive\tindustry\tdataset,\tQReCC,\tand\tCoQA.\tResults\tdemonstrate\tsignificant\timprovements\tin\tcontext\tprecision,\tcontext\trecall,\tanswer\trelevancy,\tand\tfaithfulness,\twith\tparticularly\tnotable\tperformance\ton\tthe\tautomotive\tindustry\tdataset.\tOur\toptimization\tscheme\tprovides\tan\teffective\tsolution\tfor\tdeploying\tlocal\tRAG\tsystems\tin\tthe\tautomotive\tsector,\taddressing\tthe\tspecific\tneeds\tof\tPDF\tchatbots\tin\tindustrial\tproduction\tenvironments.\tThis\tresearch\thas\timportant\timplications\tfor\tadvancing\tinformation\tprocessing\tand\tintelligent\tproduction\tin\tthe\tautomotive\tindustry.\t *\tPlace\tthe\tfootnote\ttext\tfor\tthe\tauthor\t(if\tapplicable)\there.\t\tCCS\tCONCEPTS\t•\tComputing\tmethodologies\t•\tArtificial\tintelligence\t•\tNatural\tlanguage\tprocessing\t•\tNatural\tlanguage\tgeneration\t\tAdditional\tKeywords\tand\tPhrases:\tAutomotive\tIndustry,\tLangchain,\tself-rag,\tPDF\tProcessing,\tRAG,\tOllama\t1 INTRODUCTION 1.1 Research Background The\tautomotive\tindustry\tis\tundergoing\ta\tsignificant\tdigital\ttransformation,\twith\tan\tincreasing\treliance\ton\tcomplex\ttechnical\t documentation\t for\t various\t processes\t[1].\t This\t shift\t encompasses\t design,\t manufacturing,\t and\t quality\tcontrol,\tall\tof\twhich\tnow\theavily\tdepend\ton\tefficient\tinformation\tmanagement\tsystems\t[2].\tThe\tgrowing\tvolume\tof\ttechnical\t documents,\t often\t in\t PDF\t format,\t has\t created\t a\t pressing\t need\t for\t advanced\t information\t retrieval\t and\tquestion-answering\tcapabilities\tin\tindustrial\tsettings\t[3].\tLarge\tLanguage\tModels\t(LLMs)\thave\temerged\tas\tpowerful\ttools\tin\tnatural\tlanguage\tprocessing,\tdemonstrating\tremarkable\t abilities\t in\t tasks\t such\t as\t document\t understanding\t and\t question\t answering\t[4].\tThese\tmodels\thave\tshown\t potential\t in\t handling\t the\t complex,\t domain-specific\t language\t often\t found\t in\t automotive\t documentation.\tHowever,\tthe\tapplication\tof\tLLMs\tin\tindustrial\tenvironments\tpresents\tunique\tchallenges,\tparticularly\tin\tterms\tof\tcomputational\tresources\tand\tdata\tprivacy\t[5].\tAmong\tthe\tvarious\ttechniques\tdeveloped\tto\tenhance\tLLM\tperformance,\tRetrieval-Augmented\tGeneration\t(RAG)\thas\tgained\tsignificant\tattention\t[6].\tRAG\tcombines\tthe\tgenerative\tcapabilities\tof\tLLMs\twith\texternal\tknowledge\tretrieval,\t allowing\t for\t more\t accurate\t and\t contextually\t relevant\t responses.\t This\t approach,\t initially\t proposed\t by\tLewis\tet\tal.,\thas\tshown\tsuperior\tperformance\tin\tgenerating\tspecific,\tdiverse,\tand\tfactual\tlanguage\tcompared\tto\ttraditional\tmodels\t[7].\tThe\t implementation\t of\t RAG\t techniques\t in\t the\t automotive\t industry,\t however,\t faces\t several\t industry-specific\tchallenges:\tThe\tapplication\tof\tRAG\ttechniques\tin\tthe\tautomotive\tindustry\tpresents\tunique\tchallenges:\t1. Document\t Complexity:\t Automotive\t technical\t documents\t often\t feature\t intricate\t layouts,\t including\t multi-column\t formats\t and\t complex\t tables.\t These\t structural\t elements\t pose\t significant\t challenges\t for\t standard\tdocument\tprocessing\tmethods\t[8].\t2. Data\t Privacy:\t The\t automotive\t industry\t deals\t with\t highly\t confidential\t information\t related\t to\t proprietary\tdesigns\t and\t manufacturing\t processes.\t This\t necessitates\t solutions\t that\t can\t operate\t securely\t within\t the\tcompany's\tinfrastructure,\twithout\trelying\ton\texternal\tcloud\tservices\t[9].\t3. Resource\t Constraints:\t Many\t industrial\t environments\t operate\t with\t limited\t computational\t resources.\t This\tconstraint\t requires\t the\t development\t of\t optimized,\t lightweight\t models\t capable\t of\t running\t efficiently\t on\tstandard\thardware\t[10].\t4. Domain\tSpecificity:\tThe\tautomotive\tsector\temploys\ta\tvast\tarray\tof\tspecialized\tterminology\tand\tconcepts.\tGeneric\tlanguage\tmodels\toften\tlack\tthe\tspecific\tknowledge\trequired\tto\taccurately\tinterpret\tand\trespond\tto\tqueries\tabout\tautomotive\tprocesses\tand\tspecifications\t[11].\t5. Real-time\t Performance:\t In\t fast-paced\t manufacturing\t environments,\t the\t ability\t to\t quickly\t retrieve\t and\tprocess\trelevant\tinformation\tis\tcrucial.\tThis\tnecessitates\thigh-performance\tinformation\tretrieval\tsystems\tcapable\tof\toperating\tunder\ttime\tconstraints\t[12].\tThe\topen-source\tlarge\tlanguage\tmodel\tservice\tframework\tOllama\thas\tgained\tattention\tfor\tits\tability\tto\trapidly\tdeploy\t LLMs\t in\t low-performance\t environments\t[13].\t This\t framework\t offers\t potential\t solutions\t to\t some\t of\t the\tresource\tconstraints\tfaced\tin\tindustrial\tsettings.\tHowever,\tits\tapplication\tin\tthe\tcontext\tof\tRAG\tfor\tautomotive\tdocumentation\tprocessing\tremains\tan\tarea\tripe\tfor\texploration\tand\toptimization.\tAs\tthe\tautomotive\tindustry\tcontinues\tto\tevolve,\tparticularly\twith\tthe\tadvent\tof\telectric\tand\tautonomous\tvehicles,\tthe\t complexity\t and\t volume\t of\t technical\t documentation\t are\t expected\t to\t increase\t further\t[14].\t This\t evolution\tunderscores\tthe\timportance\tof\tdeveloping\trobust,\tefficient,\tand\tsecure\tinformation\tretrieval\tsystems\ttailored\tto\tthe\tspecific\tneeds\tof\tthe\tautomotive\tsector\t[15].\tThe\t intersection\t of\t these\t technological\t advancements\t and\t industry-specific\t challenges\t presents\t a\t unique\topportunity\tfor\tresearch\t[16].\tBy\taddressing\tthe\tparticular\tneeds\tof\tthe\tautomotive\tindustry\tin\tthe\tcontext\tof\tRAG\tand\t local\t LLM\t deployment,\t there\t is\t potential\t to\t significantly\t enhance\t information\t access\t and\t utilization\t in\tautomotive\tengineering\tand\tmanufacturing\tprocesses\t[17].\t1.2 Research Status and Gaps Recent\tadvancements\tin\tRAG\ttechniques\thave\tshown\tpromise\tin\tvarious\tdomains.\tJiang\tet\tal.\tproposed\tFLARE,\twhich\t uses\t predicted\t next-sentence\t content\t to\t proactively\t retrieve\t relevant\t information\t[18].\t Wang\t et\t al.\tintroduced\tFILCO,\ta\tmethod\tfor\tidentifying\tand\tfiltering\tuseful\tcontexts\tto\timprove\tgeneration\tquality\t[19].\tThese\tapproaches\tdemonstrate\tthe\tpotential\tfor\tmore\tcontext-aware\tretrieval\tin\tcomplex\tdocument\tenvironments,\tsuch\tas\tthose\tfound\tin\tautomotive\tengineering.\tThe\tconcept\tof\tself-reflective\tRAG,\tas\texplored\tby\tAsai\tet\tal\t[20],\tintroduces\ta\tnovel\tframework\tdesigned\tto\tenhance\tthe\tquality\tand\tfactual\taccuracy\tof\tLLMs\tthrough\ton-demand\tretrieval\tand\ta\tself-reflection\tmechanism.\tThis\tapproach\tcould\tbe\tespecially\tvaluable\tin\tthe\tautomotive\tcontext,\twhere\tprecision\tand\taccuracy\tin\ttechnical\tinformation\tare\tparamount.\tIn\t the\t domain\t of\t optimizing\t RAG\t for\t specific\t industries,\t Rajpathak\t et\t al\t[21],\tproposed\t a\t domain-adaptive\tretrieval\t method\t that\t is\t particularly\t relevant\t for\t the\t automotive\t sector's\t unique\t terminology\t and\t document\tstructures.\tSimilarly,\tthe\twork\tof\tSiriwardhana\tet\tal\t[22],\ton\timproving\tretrieval\tefficiency\tin\tlarge-scale\tindustrial\tdatasets\toffers\tinsights\tthat\tcould\tbe\tapplied\tto\tthe\tvast\trepositories\tof\ttechnical\tdocumentation\tin\tautomotive\tmanufacturing.\tThe\topen-source\tlarge\tlanguage\tmodel\tservice\tframework\tOllama\thas\tgained\tattention\tfor\tits\tability\tto\trapidly\tdeploy\tLLMs\tin\tlow-performance\tenvironments\t[23],\tBurgan\tet\tal.\tdeveloped\tRamChat,\tan\tAI\tchatbot\taimed\tat\timproving\t accessibility\t[24],\tThese\t developments\t in\t local\t LLM\t deployment\t are\t particularly\t relevant\t to\t the\tautomotive\tindustry's\tneed\tfor\ton-premises,\tresource-efficient\tsolutions.\tRecent\twork\tby\tWang\tet\tal\t[25],\ton\ton-device\tlanguage\tmodels\tfor\tfunction\tcalling\tof\tsoftware\tAPIs\tpresents\tpotential\tapplications\tin\tintegrating\tRAG\tsystems\twith\texisting\tsoftware\tinfrastructure\tin\tautomotive\tproduction\tenvironments.\tThis\tcould\tlead\tto\tmore\tseamless\tintegration\tof\tAI-powered\tinformation\tretrieval\twithin\testablished\tindustrial\tprocesses.\tThe\t challenge\t of\t processing\t complex\t PDF\t documents,\t a\t common\t format\t for\t technical\t specifications\t in\t the\tautomotive\tindustry,\thas\tbeen\taddressed\tby\tseveral\tresearchers.\tLin\tet\tal\t[26],\tproposed\tan\tadvanced\tPDF\tparsing\ttechnique\tthat\tcould\tbe\tadapted\tto\thandle\tthe\tmulti-column\tlayouts\tand\tintricate\ttables\toften\tfound\tin\tautomotive\tdocumentation.\t Furthermore,\t the\t work\t of\t Bensch\t et\t al\t[27],\ton\t information\t extraction\t from\t semi-structured\tdocuments\toffers\tpromising\tapproaches\tfor\tdealing\twith\tthe\tvaried\tformats\tof\tautomotive\ttechnical\tliterature.\tIn\t the\t realm\t of\t domain-specific\t language\t understanding,\t the\t research\t of\t Faysse\t et\t al\t[28],\t on\tfine-tuning\tlanguage\tmodels\tfor\tspecialized\tindustries\tprovides\tvaluable\tinsights\tthat\tcould\tbe\tapplied\tto\ttailoring\tRAG\tsystems\tfor\t automotive\t terminology\t and\t concepts.\t This\t is\t complemented\t by\t the\t work\t of\t Kumar\t et\t al\t[29],\ton\t entity\trecognition\t in\t technical\t documents,\t which\t could\t enhance\t the\t precision\t of\t information\t retrieval\t in\t automotive\tcontexts.\tThe\tintegration\tof\tRAG\twith\tother\tAI\tmethodologies\thas\talso\tshown\tpromise.\tFor\tinstance,\tthe\tcombination\tof\tRAG\twith\treinforcement\tlearning,\tas\texplored\tby\tBelhadj\tet\tal\t[30],\tcould\tlead\tto\tmore\tadaptive\tand\tcontext-aware\tretrieval\t systems\t capable\t of\t handling\t the\t diverse\t query\t types\t encountered\t in\t automotive\t engineering\t and\tproduction.\tPrivacy\tand\tsecurity\tconcerns,\twhich\tare\tparamount\tin\tthe\tautomotive\tindustry,\thave\tbeen\taddressed\tin\tthe\tcontext\tof\tRAG\tby\tresearchers\tsuch\tas\tZeng\tet\tal\t[31],\twho\tproposed\tprivacy-preserving\tretrieval\tmethods\tthat\tcould\tbe\tcrucial\tfor\tprotecting\tproprietary\tautomotive\tdesigns\tand\tprocesses.\tThe\tchallenge\tof\tmaintaining\tcoherence\tin\tlong-form\ttext\tgeneration,\toften\tnecessary\twhen\taddressing\tcomplex\tautomotive\tqueries,\thas\tbeen\ttackled\tby\tresearchers\tlike\tBorgeaud\tet\tal\t[32],\twhose\twork\ton\timproving\tlong-range\tdependencies\tin\tlanguage\tmodels\tcould\tenhance\tthe\tquality\tof\tresponses\tin\tautomotive\tRAG\tapplications.\tRecent\tadvancements\tin\tfew-shot\tlearning,\tas\tdemonstrated\tby\tIzacard\tet\tal\t[33],\twith\tGPT-3,\toffer\tpotential\tfor\trapidly\t adapting\t RAG\t systems\t to\t new\t automotive\t subdomains\t or\t emerging\t technologies\t without\t extensive\tretraining.\tThis\tcould\tbe\tparticularly\tvaluable\tin\tthe\tfast-evolving\tlandscape\tof\tautomotive\ttechnology.\tThe\tapplication\tof\tRAG\tin\tmultilingual\tsettings,\tas\texplored\tby\tAhmad\tet\tal\t[34],\tis\tespecially\trelevant\tfor\tglobal\tautomotive\tcompanies\tdealing\twith\tdocumentation\tin\tmultiple\tlanguages.\tTheir\twork\ton\tcross-lingual\tretrieval\tand\tgeneration\tcould\tfacilitate\tmore\tefficient\tknowledge\tsharing\tacross\tinternational\tteams.\tIn\tthe\tdomain\tof\toptimizing\tretrieval\tmechanisms,\tthe\tresearch\tof\tSu\tet\tal\t[35],\ton\tdense\tretrieval\tmethods\toffers\tpotential\timprovements\tin\tthe\tspeed\tand\taccuracy\tof\tinformation\tlookup,\tcrucial\tfor\treal-time\tquery\tresolution\tin\tfast-paced\tautomotive\tproduction\tenvironments.\tThe\t challenge\t of\t handling\t numerical\t data\t and\t calculations,\t often\t present\t in\t automotive\t specifications\t and\tperformance\tmetrics,\thas\tbeen\taddressed\tby\tresearchers\tlike\tNoorbakhsh\tet\tal.\t[36],\twhose\twork\ton\tintegrating\tsymbolic\tmathematics\twith\tneural\tlanguage\tmodels\tcould\tenhance\tthe\tprecision\tof\tRAG\tsystems\twhen\tdealing\twith\tquantitative\tautomotive\tdata.\tHowever,\tdespite\tthese\tadvancements,\tthere\tremains\ta\tsignificant\tgap\tin\tresearch\tspecifically\taddressing\tthe\tunique\tchallenges\tof\timplementing\tRAG\tsystems\tin\tthe\tautomotive\tindustry,\tparticularly\tin\tresource-constrained,\toffline\tenvironments.\tThe\tsignificance\tof\tthis\tresearch\tlies\tin:\t1. Providing\t an\t effective\t optimization\t scheme\t for\t local\t RAG\t deployment\t of\t Ollama\t in\t automotive\t industrial\tenvironments,\taddressing\tkey\tchallenges\tin\tdocument\tprocessing\tand\tinformation\tretrieval.\t2. Exploring\tthe\tapplication\tof\tself-RAG\tin\toffline,\tindustry-specific\tscenarios,\toffering\tnew\tinsights\tinto\tfunction\tcalling\timplementations\tfor\tdomain-specific\ttasks.\t3. Contributing\t to\t the\t advancement\t of\t intelligent\t information\t processing\t in\t automotive\t manufacturing,\tpotentially\timproving\tefficiency\tand\taccuracy\tin\ttechnical\tdocument\tanalysis\tand\tquery\tresolution.\t1.3 Research Objectives and Significance Given\tthe\tidentified\tchallenges\tand\tresearch\tgaps,\tthis\tstudy\taims\tto\tdevelop\ta\tmulti-dimensional\toptimization\tscheme\tfor\tapplying\tRAG\ttechnology\twith\tOllama\tin\tlocal,\tlow-performance\tautomotive\tindustry\tenvironments.\tOur\tspecific\tresearch\tobjectives\tinclude:\t• Proposing\ta\tPDF\tfile\tprocessing\tmethod\toptimized\tfor\tautomotive\tindustry\tdocuments,\tcapable\tof\thandling\tmulti-column\tlayouts\tand\tcomplex\ttables.\t• Developing\tan\tadvanced\tRAG\tsystem\tbased\ton\tthe\tLangchain\tframework,\tintroducing\treranking\tmodels\tand\tBM25\tretrievers\tto\tbuild\tan\tefficient\tcontext\tcompression\tpipeline.\t• Designing\tan\tintelligent\tagent\tthat\tsupports\tself-RAG\tand\texploring\ta\tfunction\tcalling\tmechanism\tto\tenhance\tOllama's\tresponse\tgeneration\tin\tautomotive-specific\tscenarios.\t• Evaluating\tthe\tproposed\tsystem\tusing\ta\tproprietary\tdataset\tof\tautomotive\tindustry\tdocuments,\talongside\tpublic\tdatasets,\tto\tdemonstrate\tits\teffectiveness\tin\treal-world\tindustrial\tapplications.\tThe\tsignificance\tof\tthis\tresearch\tlies\tin:\t• Providing\t an\t effective\t optimization\t scheme\t for\t local\t RAG\t deployment\t of\t Ollama\t in\t automotive\t industrial\tenvironments,\taddressing\tkey\tchallenges\tin\tdocument\tprocessing\tand\tinformation\tretrieval.\t• Exploring\tthe\tapplication\tof\tself-RAG\tin\toffline,\tindustry-specific\tscenarios,\toffering\tnew\tinsights\tinto\tfunction\tcalling\timplementations\tfor\tdomain-specific\ttasks.\t• Contributing\t to\t the\t advancement\t of\t intelligent\t information\t processing\t in\t automotive\t manufacturing,\tpotentially\timproving\tefficiency\tand\taccuracy\tin\ttechnical\tdocument\tanalysis\tand\tquery\tresolution.\tThis\tresearch\tbuilds\tupon\tand\textends\texisting\twork\tin\tseveral\tkey\tareas:\t• The\t potential\t of\t RAG\t systems\t to\t support\t decision-making\t processes,\t a\t critical\t application\t in\t automotive\tdesign\tand\tmanufacturing,\thas\tbeen\texplored\tby\tresearchers\tsuch\tas\tGamage\tet\tal\t[37].\tTheir\twork\ton\tusing\tRAG\tfor\tfew\tevidences-based\treasoning\tcould\tbe\tadapted\tto\tsupport\tcomplex\tdecision-making\tscenarios\tin\tautomotive\tengineering.\t• Recent\t developments\t in\t efficient\t transformer\t architectures,\t such\t as\t the\t work\t of\t Zhuang\t et\t al\t[38],\ton\tReformer,\toffer\tpotential\tfor\tdeploying\tmore\tpowerful\tRAG\tmodels\twithin\tthe\tcomputational\tconstraints\tof\tindustrial\tenvironments.\t• The\tintegration\t of\t visual\t information\t with\t text-based\t retrieval,\t as\t explored\t by\t Chen\t et\t al\t[39],\tpresents\topportunities\t for\t enhancing\t RAG\t systems\t to\t handle\t technical\t diagrams\t and\t schematics\t common\t in\tautomotive\tdocumentation.\tBy\taddressing\tthese\tresearch\tobjectives\tand\tbuilding\tupon\trecent\tadvancements\tin\tthe\tfield,\tthis\tstudy\taims\tto\tsignificantly\tenhance\tthe\tapplicability\tand\teffectiveness\tof\tRAG\ttechnologies\tin\tthe\tautomotive\tindustry,\tpotentially\ttransforming\t how\t technical\tinformation\t is\t accessed,\t processed,\t and\t utilized\t in\t automotive\t engineering\t and\tmanufacturing\tprocesses.\t2 MATERIALS AND METHODS 2.1 Foundation Our\tresearch\tbuilds\tupon\tthe\tLangchain\tframework\tand\tOllama\tmodel,\tadapting\tthem\tto\tmeet\tthe\tspecific\tneeds\tof\tthe\t automotive\t industry.\t We\t began\t by\t constructing\t a\t preliminary\t retrieval-based\t chatbot\t framework\t using\tLangchain's\tcomponents,\twhich\twe\tthen\toptimized\tfor\tprocessing\tautomotive\ttechnical\tdocuments.\tFigure\t1\tillustrates\tthe\tkey\tcomponents\tof\tthis\tsystem.\tThis\tarchitecture\tintegrates\tadvanced\tdocument\tloading\tcapabilities\tfor\tvarious\tfile\tformats,\tefficient\ttext\tsplitting,\tand\ta\trobust\tretrieval\tmechanism\tusing\tthe\tChroma\tvector\tstore.\tThe\tbasic\tframework\tincludes:\t\n\tFigure 1: Basic RAG Architecture for Automotive Document Processing.  • Document\t Loading:\t Utilizing\t Langchain's\t Loader\t class\t to\t recursively\t load\t PDF\t documents\t from\t specified\tdirectories,\tsimulating\tthe\tdocument\tstructure\tin\tautomotive\tmanufacturing\tenvironments.\t• Text\t Chunking:\t Implementing\t an\t embedded\t tokenization\t model\t to\t split\t documents\t into\t fixed-length\ttext\tchunks,\t optimized\t for\t technical\t specifications\t and\t multi-column\t layouts\t common\t in\t automotive\tdocumentation.\t• Vector\tStorage:\tEncoding\ttext\tchunks\tinto\tsemantic\tvectors\tand\tstoring\tthem\tin\tthe\tChroma\tvector\tdatabase,\tcreating\tan\tindexed\ttext\tretriever\ttailored\tfor\tautomotive\tterminology\tand\tconcepts.\t• Dialogue\t Generation:\t Employing\t Langchain's\tConversationalRetrievalChain\tto\t process\t user\t queries\t and\tretrieved\tcontext,\tgenerating\tresponses\tusing\tthe\tlocally\tdeployed\tOllama\tmodel.\tThis\t foundational\t setup\t serves\t as\t the\t baseline\t for\t our\t subsequent\t optimizations,\t each\t designed\t to\t address\tspecific\tchallenges\tin\tautomotive\tdocument\tprocessing\tand\tinformation\tretrieval.\t2.2 PDF File Processing Optimization To\t address\t the\t unique\t challenges\t posed\t by\t automotive\t industry\t documents,\t we\t developed\t an\t enhanced\t PDF\tprocessing\tmethod\tcombining\tPDFMiner\tand\tTabula\tlibraries.\t2.2.1 Overview of PDFMiner and Tabula\tPDFMiner\tis\ta\tPython\tlibrary\tused\tfor\textracting\tinformation\tfrom\tPDF\tdocuments.\tIt\tis\tcapable\tof\textracting\ttext,\timages,\tmetadata,\tand\tstructural\tinformation\tfrom\tPDF\tdocuments.\tPDFMiner\tprovides\tboth\thigh-level\tand\tlow-level\tAPIs\tto\tsatisfy\tdifferent\tlevels\tof\trequirements.\tAutomotive\ttechnical\tdocuments\toften\tfeature\tmulti-column\tDocument Loader(PDF)Text Splitter(RecursiveCharacterTextSplitter)Embeddings(OllamaEmbeddings)Vector Store(Chroma)RetrieverLLM (Ollama)ConversationalRetrievalChainDocuments\nResponseQuerylayouts\tand\tcomplex\tdiagrams.\tWe\timplemented\ta\tcustom\talgorithm\tusing\tPDFMiner's\textract_pages\tfunction\tto\taccurately\textract\tcontent\twhile\tpreserving\tthe\tlogical\tflow\tof\tinformation:\t1. Page\t Segmentation:\t We\t analyze\t each\t page\t to\t identify\t distinct\t content\t regions,\t including\t text\t columns,\tdiagrams,\tand\ttables.\t2. Content\t Ordering:\t Implementing\t a\t left-to-right,\t top-to-bottom\t reading\t order\t algorithm\t to\t ensure\t proper\tsequencing\tof\textracted\tinformation.\t3. Diagram\tExtraction:\tUtilizing\tPDFMiner's\timage\textraction\tcapabilities\tto\tpreserve\ttechnical\tdiagrams\tcrucial\tfor\tunderstanding\tautomotive\tspecifications.\tTabula\tis\ta\tJava\tlibrary\tused\tfor\textracting\ttabular\tdata\tfrom\tPDF\tfiles.\tIt\tprovides\ta\tPython\twrapper,\tmaking\tit\tmore\tconvenient\tto\tuse\tTabula\tin\tPython.\tTabula\tcan\tautomatically\tdetect\ttable\tboundaries\tand\tconvert\ttabular\tdata\tinto\tDataFrame\tobjects,\tfacilitating\tsubsequent\tdata\tanalysis\tand\tprocessing.\t2.2.2 Multi-column Layout and Table Information Recognition Optimization Tables\tin\tautomotive\tdocuments\toften\tcontain\tcritical\tdata\tsuch\tas\tpart\tspecifications,\ttest\tresults,\tor\tcompliance\tinformation.\tOur\tapproach\tuses\tTabula's\tread_pdf\tfunction\twith\tcustom\tparameters\tlike\tAlgorithm\t1:\tALGORITHM 1: Extract Text and Tables from PDF to Markdown write_2_md(input, output)     pages = extract_pages(input)     open output for writing as out     for page_num, page in enumerate(pages, start=1), do         elements = [e for e in page if isinstance(e, LTTextContainer)]         tables = tabula.read_pdf(input, pages=page_num, multiple_tables=True)         mid_line = (max(e.bbox[2] for e in elements) + min(e.bbox[0] for e in elements)) / 2         left = [e for e in elements if e.bbox[0] < mid_line]         right = [e for e in elements if e.bbox[0] >= mid_line]         sort left by -e.bbox[1]         sort right by -e.bbox[1]         for col in [left, right], do             for e in col, do                 cleaned_text = clean_text(e.get_text())                 write cleaned_text + \"\\n\" to out             end         end         for table in tables, do             write pd.DataFrame(table).to_markdown(index=False) + \"\\n\" to out         end         write \"\\n\\n\" to out     end end 1. Table\tDetection:\t Implementing\t heuristics\t to\t identify\t table\t structures\t within\t automotive\t documents,\tconsidering\tcommon\tformats\tused\tin\tthe\tindustry.\t2. Data\tExtraction:\tConverting\trecognized\ttables\tinto\tstructured\tDataFrame\tobjects,\tpreserving\trelationships\tbetween\tdata\tpoints.\t3. Contextual\t Integration:\t Seamlessly\t integrating\t extracted\t table\t data\t with\t surrounding\t text\t to\t maintain\tdocument\tcoherence.\tWhen\tintegrating\ttext\tand\ttable\tinformation,\twe\tfirst\twrite\tthe\ttext\tinformation\textracted\tby\tthe\tmulti-column\tlayout\trecognition\talgorithm\tinto\tthe\tMarkdown\tfile\tin\tthe\torder\tof\tappearance.\tThen,\tafter\tthe\ttext\tinformation\tof\teach\tpage,\twe\tconvert\tthe\ttable\tinformation\trecognized\ton\tthat\tpage\tinto\tMarkdown\tformat\tand\twrite\tit\tinto\tthe\tfile.\tBy\tdoing\tso,\twe\tcan\tensure\tthat\tthe\tcontent\torder\tin\tthe\tgenerated\tMarkdown\tfile\tis\tconsistent\twith\tthe\toriginal\tPDF\tfile,\tand\tthe\ttable\tinformation\tcan\tbe\tcorrectly\tembedded\tin\tthe\tcorresponding\tpositions.\tTo\t illustrate\t the\t effectiveness\t of\t this\t approach,\t let's\t examine\t two\t figures\t that\t demonstrate\t the\t conversion\tprocess\tfor\ta\tcomplex\tacademic\tpaper\tlayout.\tFigure\t2\tshows\ta\tsample\tpage\tfrom\tthe\toriginal\tPDF\tdocument,\twhich\tfeatures\ta\tchallenging\ttwo-column\tlayout\twith\tembedded\ttables\tand\tvarious\tformatting\telements.\tFigure\tpresents\tthe\tcorresponding\tMarkdown\toutput\tafter\tprocessing\twith\tour\toptimized\tmethod.\t\tFigure 2: Sample page from original PDF document. (https://arxiv.org/pdf/1804.07821) \n\tFigure 3: Corresponding Markdown output. As\twe\tcan\tsee,\tour\tmethod\tsuccessfully\tpreserves\tthe\tstructural\tintegrity\tof\tthe\toriginal\tdocument,\taccurately\tcapturing\tboth\tthe\ttextual\tcontent\tand\ttabular\tdata.\tThe\ttwo-column\tlayout\tis\tseamlessly\tconverted\tinto\ta\tlinear\tMarkdown\tformat,\tmaintaining\tthe\tlogical\tflow\tof\tinformation.\tTables\tare\tproperly\tformatted\tusing\tMarkdown\tsyntax,\tensuring\tthey\tremain\teasily\treadable\tand\tcan\tbe\tfurther\tprocessed\tor\trendered\tas\tneeded.\tThrough\tthis\tapproach,\twe\tcan\teffectively\tcombine\tthe\tfunctionality\tof\tthe\tPDFMiner\tand\tTabula\tlibraries.\tThis\toptimized\tPDF\tprocessing\tmethod\tsignificantly\timproves\tthe\taccuracy\tand\tcompleteness\tof\tinformation\textraction\tfrom\tcomplex\tautomotive\tdocuments,\tproviding\ta\tsolid\tfoundation\tfor\tsubsequent\tRAG\tprocesses.\t2.3 Optimization of Advanced RAG Based on Langchain To\t enhance\t the\t RAG\t system's\t performance\t for\t automotive\t industry\t applications,\t we\t introduced\t several\toptimizations\tto\tthe\tLangchain-based\timplementation.\tWe\tintroduce\tthe\tBGE\treranker\tmodel\tand\tBM25\talgorithm.\t\tBuilding\tupon\tthe\tgroundwork\tlaid\tin\tprevious\tsections,\twe\tfirst\tcombine\tthe\tBM25Retriever\twith\tthe\tdefault\tretriever\tusing\tEnsembleRetriever\tin\tLangchain,\tassigning\tdifferent\tweights\tto\tachieve\tmore\tcomprehensive\tand\taccurate\tretrieval.\tThen,\ta\tcustom\tclass\tis\tdesigned\tto\tintegrate\tthe\treranking\tmodel\tinto\tthe\tcontext\tcompression\tpipeline,\tfurther\tenhancing\tthe\tretrieval\tand\tgeneration\tquality\tof\tthe\tmodel.\t2.3.1 Overview of BGE Reranker Model and BM25 Retriever\tA\treranker\tmodel\tis\ta\tgeneral\tsemantic\tvector\tmodel\tused\tto\toptimize\tthe\tranking\tof\tretrieval\tresults.\tIt\tcan\tadapt\tto\tprioritize\tautomotive-relevant\tinformation\tin\tretrieved\tcontexts.\tIn\tthis\tstudy,\twe\temploy\tBAAI/bge-reranker-large\tas\tthe\treranker\tmodel.\tBGE\t(BAAI\tGeneral\tEmbedding)\tis\ta\treranker\tmodel\tproposed\tby\tthe\tBeijing\tAcademy\tof\tArtificial\tIntelligence\t(BAAI),\tspecifically\toptimized\tfor\tChinese\tqueries\t[40].\t\t\nBM25\t(Best\tMatching\t25)\tis\ta\tclassic\tbag-of-words\tretrieval\talgorithm\tthat\tevaluates\trelevance\tby\tcalculating\tthe\tterm\tfrequency-inverse\tdocument\tfrequency\t(TF-IDF)\tscore\tbetween\tthe\tquery\tand\tdocuments\t[41].\tThe\tBM25\tretriever\tcan\tquickly\tand\tefficiently\tfilter\tout\tthe\tmost\tquery-relevant\tdocuments\tfrom\ta\tlarge-scale\ttext\tcorpus.\tWe\tintroduce\tthis\ttraditional\trelevance\tassessment\tmethod\tin\tcombination\twith\tthe\treranker\tmodel\tto\tfurther\tstrengthen\tthe\teffectiveness\tof\tthe\tRAG\tretriever.\t2.3.2 Building the Context Compression Pipeline and Custom Class Design\tTo\toptimize\tthe\tcontext\tinformation\tprocessing\tof\tthe\tRAG\tmodel,\twe\tconstruct\ta\tcontext\tcompression\tpipeline\tDocumentCompressorPipeline\tthat\t is\t pecifically\t tailored\t for\t automotive\t technical\t content.\t In\t addition\t to\t the\tcollection\tretrievers\tmentioned\tearlier,\twe\talso\tintroduce:\t\t• EmbeddingsRedundantFilter:\t An\t embedding-based\t redundancy\t filter\t to\t remove\t redundant\t information\tfrom\tthe\tretrieval\tresults.\t• LongContextReorder:\tOptimizes\tthe\torder\tof\tcontext\tinformation,\tprioritizing\tkey\tautomotive\tspecifications\tand\tprocedures.\t• BgeRerank:\tA\tcustom\tclass\tthat\tinherits\tfrom\tBaseDocumentCompressor.\tSince\tLangchain's\tofficial\tsupport\tfor\tthe\tBGE\tmodel\tis\trelatively\tlimited,\tthe\tpurpose\tof\tdesigning\tthis\tclass\tis\tto\tseamlessly\tintegrate\tthe\tBGE\treranker\tmodel\tinto\tthe\tpipeline.\tThis\tcustom\tclass\tcan\timprove\trelevance\tscoring\tfor\tautomotive\tqueries.\tThe\tfollowing\tAlgorithm\t2\tis\tthe\tpseudocode\tfor\tthe\tBgeRerank\tclass:\tALGORITHM 2: BgeRerank BgeRerank(documents, query)     initialize model = CrossEncoder(model_name)     doc_list = list(documents)     _docs = [d.page_content for d in doc_list]     model_inputs = [[query, doc] for doc in _docs]     scores = model.predict(model_inputs)     results = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)[:top_n]     final_results = []     for r in results, do         doc = doc_list[r[0]]         doc.metadata[\"relevance_score\"] = r[1]         append doc to final_results     end     return final_results By\toverriding\tthe\tcompress_documents\tmethod,\tthe\tBGE\tmodel\tis\tused\tto\tcalculate\tthe\trelevance\tbetween\tthe\tquery\tand\tdocuments.\tThe\tdocuments\tare\tthen\tsorted\tbased\ton\ttheir\tscores,\tand\tthe\ttop\tN\tdocuments\tare\tselected\tas\t the\t final\t compression\t results.\t This\t custom\t class\t design\t compensates\t for\tthe\t insufficient\t support\t for\t BGE\t in\tLangchain,\tallowing\tus\tto\tflexibly\tincorporate\tit\tinto\tthe\tpipeline\tstructure.\tIn\t summary,\t the\t innovative\t aspects\t of\t this\t section\t include:\t introducing\t the\t BGE\t reranker\t model\t and\t BM25\tretriever,\tbuilding\tthe\tcontext\tcompression\tpipeline,\tand\tseamlessly\tintegrating\tBGE\tinto\tthe\tLangchain\tframework\tthrough\tthe\tcustom\tBgeRerank\tclass.\tThis\tpipeline\tsignificantly\tenhances\tthe\tquality\tand\trelevance\tof\tretrieved\tinformation,\tensuring\tthat\tthe\tmost\tpertinent\tautomotive\ttechnical\tdetails\tare\tpresented\tto\tthe\tlanguage\tmodel.\t2.4 Optimization of Advanced RAG Based on Langchain In\tthe\tprevious\tsection,\twe\tcompleted\tthe\tbasic\tdesign\tof\tthe\tRAG\tsystem.\tHowever,\tintegrating\tLLMs\tinto\tpractical\tapplications\t and\t constructing\t end-to-end\t intelligent\t systems\t still\t present\t numerous\t challenges.\t To\t address\t the\tcomplex,\t multi-step\t problem\t of\tquerying\t PDF\t profiles,\t which\t is\t common\t in\t automotive\t engineering\t and\tmanufacturing\tprocesses,\twe\thave\tdeveloped\tan\tadvanced\tSelf-RAG\tagent\tbased\ton\tthe\tLangGraph\tframework.\t2.4.1 Overview of SELF-RAG\tSelf-Reflective\tRetrieval-Augmented\tGeneration\t(SELF-RAG)\tis\ta\tnovel\tframework\tdesigned\tto\tenhance\tthe\tquality\tand\tfactual\taccuracy\tof\tLLMs\tthrough\ton-demand\tretrieval\tand\ta\tself-reflection\tmechanism\t[42].\tUnlike\ttraditional\tRAG\tmethods,\tSELF-RAG\tendows\tLLMs\twith\tthe\tfollowing\tcapabilities:\t1. On-demand\tRetrieval:\tThe\tLLMs\tautonomously\tdetermines\twhether\tto\tretrieve\trelevant\tinformation\tfrom\tan\texternal\tknowledge\tbase\tbased\ton\tthe\tinput\tit\treceives.\t2. Self-Reflection:\tThe\tLLMs\tevaluates\tand\treflects\tupon\tboth\tthe\tretrieved\tinformation\tand\tits\town\tgenerated\tcontent,\tthereby\timproving\tthe\tquality\tand\treliability\tof\tits\toutput.\tThe\ttraining\tprocess\tof\tSELF-RAG\tconsists\tof\ttwo\tstages:\t1. Offline\tCritic\tModel\tTraining:\tAn\tindependent\tcritic\tmodel\tis\ttrained\tto\tgenerate\t\"reflection\ttokens\".\tThese\ttokens\tare\tinserted\tinto\tthe\tLLMs'\toutput\tto\tguide\tits\tself-reflection\tprocess.\t2. Generative\t Model\t Training:\t The\t LLMs\t is\t fine-tuned\t using\t a\t corpus\t that\t includes\t reflection\t tokens\t and\tretrieved\t documents.\t This\t enables\t the\t LLMs\t to\t understand\t and\t utilize\t these\t tokens,\t incorporating\t self-reflection\tinto\tits\tgeneration\tprocess.\tDuring\tinference,\tthe\tLLMs\tdynamically\tdecides\twhether\tto\tretrieve\tinformation\tbased\ton\tthe\trequirements\tof\tthe\ttask\tat\thand.\tIt\talso\tleverages\tthe\tretrieved\tinformation\tand\tthe\tself-reflection\tmechanism\tto\tgenerate\thigh-quality\toutput.\tFor\tinstance,\tin\ttasks\tdemanding\tfactual\taccuracy,\tthe\tLLMs\tis\tmore\tinclined\tto\tretrieve\tand\tutilize\trelevant\tinformation,\twhile\tin\tmore\topen-ended\ttasks,\tit\tmay\tprioritize\tcreativity\tand\trely\tless\ton\tretrieval.\t2.4.2 Agent Design Supporting Self-RAG\tConsidering\tthis,\tbased\ton\tLangchain,\twe\tdesigned\tan\tintelligent\tAgent\tclass\tcalled\tAgenticRAG,\twhich\taims\tto\tutilize\t Self-RAG\t technology\t to\t answer\t user\t questions.\t AgenticRAG\t combines\t various\t components\t from\t the\tLangGraph\tand\tLangChain\tecosystems\tto\tachieve\ta\tmodular\tand\tscalable\tquestion-answering\tsystem.\tThe\tcore\tof\tthe\tAgenticRAG\tclass\tis\tthe\tcreate_graph\tmethod,\twhich\tdefines\ta\tworkflow\tbased\ton\ta\tdirected\tacyclic\tgraph\t(DAG).\tThe\tworkflow\tconsists\tof\tmultiple\tnodes,\teach\tresponsible\tfor\ta\tspecific\ttask\tin\tthe\tquestion-answering\tprocess.\tThe\tnodes\tare\tconnected\tby\tdirected\tedges,\tforming\ta\tcomplete\tquestion-answering\tflow.\tThe\tmain\tcomponents\tin\tthe\tAgenticRAG\tclass\tare\tas\tfollows:\t• GraphState:\tRepresents\tthe\tcurrent\tstate\tof\tthe\tgraph,\tincluding\tthe\tuser's\tquestion,\tretrieved\tdocuments,\tgenerated\tanswer,\tand\tchat\thistory.\t• Node:\t Represents\t a\t step\t or\t task\t in\t the\t question-answering\t process,\t accepts\t the\t current\t state\t as\t input,\tperforms\tspecific\toperations,\tand\treturns\tthe\tupdated\tstate.\t\tWe\tdesigned\tthe\tAgenticRAG\tclass\tto\thandle\tsophisticated\tautomotive-related\tquestions.\tKey\tcomponents\tas\tFigure\t4:\t\n\tFigure 4: Flowchart of Self-RAG Agent Implementation via LangGraph. 1. Retrieve\tNode:\tOptimized\tto\tfetch\trelevant\tinformation\tfrom\tautomotive\ttechnical\tdocuments,\tconsidering\tindustry-specific\tterminology\tand\tconcepts.\t2. Grade\tDocuments\tNode:\tEvaluates\tretrieved\tdocuments\tbased\ton\ttheir\trelevance\tto\tautomotive\tqueries,\tconsidering\tfactors\tlike\ttechnical\taccuracy\tand\tapplicability\tto\tspecific\tmanufacturing\tprocesses.\t3. Generate\tNode:\t Produces\t responses\t tailored\t to\t automotive\t industry\t needs,\t incorporating\t technical\tspecifications\tand\tindustry\tstandards\tsearched\tfrom\tdocuments.\t4. Transform\tQuery\tNode:\tRefines\tqueries\tto\tbetter\tcapture\tthe\tintent\tbehind\tautomotive-specific\tquestions,\timproving\tretrieval\taccuracy\tin\tsubsequent\titerations.\t\nAgenticRAG\tuses\tconditional\tjudgment\tlogic\tto\tdetermine\tthe\tdirection\tof\tthe\tquestion-answering\tprocess.\tFor\texample,\tbased\ton\tthe\tscoring\tresults\tfrom\tthe\tGrade\tDocuments\tNode,\tit\tdecides\twhether\tto\tgenerate\tan\tanswer\tor\trewrite\tthe\tquestion;\tsimultaneously,\tbased\ton\tthe\tquality\tof\tthe\tanswer\tgenerated\tby\tthe\tGenerate\tNode,\tit\tdetermines\twhether\tto\tregenerate,\trewrite\tthe\tquestion,\tor\toutput\tthe\tfinal\tresult.\tThe\tdesign\tof\tAgenticRAG\tfully\treflects\tmodularity\tand\tscalability.\tBy\tdividing\tthe\tcomplex\tquestion-answering\tprocess\t into\t multiple\t independent\t nodes,\t each\t node\t focuses\t on\t a\t specific\t task,\t making\t the\t system\t easier\t to\tunderstand,\tdebug,\tand\tmaintain.\tAt\tthe\tsame\ttime,\tsince\tthe\tnodes\tpass\tinformation\tthrough\tstates,\tnew\tnodes\tcan\tbe\teasily\tinserted\tinto\tthe\texisting\tworkflow,\tsupporting\tthe\textension\tand\toptimization\tof\tfunctionality.\t2.4.3 Function Calling Design for Optimizing Ollama Output in RAG Scenarios\tFunction\tcalling\tis\ta\tpowerful\ttechnique\tthat\tsignificantly\tenhances\tthe\toutput\tquality\tof\tLLMs\tin\tRAG\tscenarios\t[43].\tBy\tintegrating\texternal\tfunctions\twith\tLLMs,\tspecialized\tlogic\tand\talgorithms\tcan\tbe\temployed\tto\tguide\tthe\tmodel,\t resulting\t in\t more\t accurate,\t coherent,\t and\t informative\t responses.\t This\t approach\t leverages\t the\t few-shot\tlearning\tcapabilities\tof\tLLMs,\tenabling\tthem\tto\tadapt\tto\tnew\ttasks\twhile\tmitigating\ttheir\tinherent\tlimitations\tin\treasoning\tand\tcomputation.\tHowever,\t most\t of\t the\t relevant\t work\t has\t focused\t on\t standard\t transformer-based\t LLMs,\t with\t a\t lack\t of\timplementation\t for\t optimizing\t Ollama\t callbacks,\t especially\t in\t RAG\t scenarios.\t Although\t Langchain\t officially\tprovides\tollamafunction.bind\tto\tcall\tfunctions,\tfurther\tresearch\ton\tthis\tmethod\tis\tcurrently\tscarce.\tTherefore,\tto\tenhance\t the\t agent's\t capability\t in\t handling\t automotive-specific\t tasks,\t we\t designed\t a\t custom\t class\t adapted\t to\tollamafunction.bind,\tcalled\tChatFunction.\tIt\tinherits\tfrom\tthe\tBaseFunction\tclass\tand\toverrides\tthe\t__init__\tand\t__call__\tmethods.\tThe\tmain\tfeatures\tof\tChatFunction\tare\tas\tAlgorithm\t3:\tALGORITHM 3: ChatFunction ChatFunction(persona, lang, retry)     inputProp = Property(type=INPUT, desc=InputDesc())     outputProp = Property(type=OUTPUT, desc=OutputDesc())     props = [inputProp, outputProp]     params = Parameter(props=props, required=[input, output])     initialize BaseFunction(name=ChatFuncName(), desc=ChatFuncDesc(), params) end procedure  call(args)     output = args[output]     input = args[input]     if IsEmpty(output) or IsEmpty(input), do         raise ValueError(MissingArgMessage())     end     detail = GetPromptByRetry(retry)     persona = GetPersonaString(lang)     history = GetHistoryString(input)     current = GetCurrentString(input)     prompt = Format(detail, persona, history, current)     respTool = LookupTool(ChatResponseEnhancer())     return {         tool: respTool,         tool_input: {             response: output,             query: prompt         }     } end procedure  GetPromptByRetry(retry)     if retry = 0, do         return GetBriefPrompt()     else if retry = 1, do         return GetMediumPrompt()     else         return GetDetailedPrompt()     end end function The\t initialization\t accepts\t three\t parameters:\t the\t AI\t assistant's\t personality\t description\tpersonality,\t language\tpreference\tlanguage,\tand\tretry\tcount\tretry_count.\tTwo\tproperties\tare\tdefined:\tquery_input\tand\toutput,\trepresenting\tthe\tcomplete\tinput\t(including\tpersonality\tdescription,\tlanguage\tinstructions,\tand\tconversation\thistory)\tand\tthe\tAI\tassistant's\tresponse,\trespectively.\tThis\tensures\tthat\tthe\tmodel\tis\tprovided\twith\tthe\tnecessary\tbackground\tknowledge\tfor\teach\tretry,\tenabling\tit\tto\tgenerate\tresponses\tbased\ton\tthe\tspecified\trole\tcharacteristics\tand\tlanguage\tstyle.\tThe\t constructor\t of\t the\t parent\t class\tBaseFunction\tis\t called,\t passing\t the\t function\t name,\t description,\t and\tparameter\tlist.\tWhen\tinvoked,\tit\taccepts\tan\targuments\tdictionary\tcontaining\ttwo\trequired\tparameters:\toutput\tand\tquery_input.\tAccording\t to\t the\t retry\t count\tretry_count\tin\t the\t self-rag\t Agent\t in\t 3.3.2,\t the\t level\t of\t detail\t in\t the\t answer\tdetail_prompt\tis\tdynamically\tadjusted\tto\tmodify\tprompts\tbased\ton\tthe\tcomplexity\tof\tautomotive\tqueries\tand\tthe\tdepth\tof\ttechnical\tdetail\trequired.\tThe\tquery_input\tfor\tthe\tnext\tround\tis\tconstructed\tby\tencoding\tthe\tpersonality\tdescription,\tlanguage\tpreference,\tdetail\tlevel\trequirement,\tand\tcurrent\tconversation\thistory.\tTo\tmaintain\tawareness\tof\tthe\tongoing\tconversation\tcontext,\tcrucial\tfor\taddressing\tmulti-step\tautomotive\tprocesses\tor\tcomplex\tdiagnostic\tqueries.\tA\tdictionary\tis\treturned,\tcontaining\tthe\ttool\tfield\t(specifying\tthe\tcallback\tfunction\tname)\tand\tthe\ttool_input\tfield\t(containing\tthe\toptimized\tresponse\tand\tupdated\tquery_input),\tallowing\tthe\tnext\tround\tof\tcallback\trequests\tto\tcontinue\toptimizing\tbased\ton\tthe\tcurrent\tresults\tand\tstate,\tforming\ta\tclosed-loop\tself-improvement\tprocess.\tThe\tinnovation\tof\tChatFunction\tlies\tin\tits\tfull\tutilization\tof\tOllama's\tJSON\tmode\tcallback\tfunction\tpotential,\toptimizing\t the\t response\t effectiveness\t in\t the\t RAG\t scenario.\t This\t customized\t function\t call\t mechanism\t greatly\timproves\tthe\tagent's\tability\tto\thandle\tcomplex\tautomotive\tqueries,\tprovide\ttechnically\taccurate\tinformation,\tand\tcan\tguide\tusers\tthrough\tthe\tefficient\tacquisition\tof\tknowledge\tcommon\tto\tautomotive\tdesign\tand\tmanufacturing.\t3 RESULTS 3.1 Overview of the RAGAS Performance Evaluation Framework RAGAS\t (Retrieval\t Augmented\t Generation\t Assessment\t Suite)\t is\t a\t comprehensive\t evaluation\t framework\t for\tassessing\tthe\tperformance\tof\tRAG\tmodels.\tIt\tprovides\ta\tseries\tof\tmetrics\tto\tquantify\tthe\tquality\tof\tgenerated\tresults,\twith\ta\tparticular\tfocus\ton\tthe\timpact\tof\tinformation\tretrieval\ton\tthe\tgeneration\tprocess.\tThe\tevaluation\tmetrics\tin\tRAGAS\tinclude:\t• Context\tPrecision:\tMeasures\tthe\tprecision\tof\trelevant\tcontextual\tinformation\tcontained\tin\tthe\tgenerated\tresults.\t• Faithfulness:\tEvaluates\tthe\tfaithfulness\tof\tthe\tgenerated\tresults\tto\tthe\toriginal\tcontextual\tinformation,\ti.e.,\tthe\tconsistency\tbetween\tthe\tgenerated\tcontent\tand\tthe\toriginal\tinformation.\t• Answer\t Relevancy:\t Assesses\t the\t relevance\t of\t the\t generated\t answers\t to\t the\t questions,\t i.e.,\t whether\t the\tanswers\tare\ton-topic\tand\tmeet\tthe\trequirements\tof\tthe\tquestions.\t• Context\tRecall:\tMeasures\tthe\tcoverage\tof\trelevant\tcontextual\tinformation\tin\tthe\tgenerated\tresults,\ti.e.,\thow\tmuch\tkey\tinformation\tis\tcaptured.\tWe\tadopt\tRAGAS\tas\tthe\tevaluation\tframework\tto\tcompare\tand\tanalyze\tthe\tperformance\tof\tdifferent\tRAG\tmodels.\tThe\tquantitative\tmetrics\tprovided\tby\tRAGAS\tenable\tus\tto\tobjectively\tevaluate\tthe\tstrengths\tand\tweaknesses\tof\tthe\tmodels\t in\t terms\t of\t contextual\t information\t utilization\t and\t generated\t content\t quality,\t providing\t important\treferences\tfor\tmodel\tselection\tand\toptimization.\t3.2 Experimental Results and Analysis 3.2.1 Experimental Scenario Design\tIn\tthis\tpaper,\twe\tselect\ttwo\tpublic\tdatasets,\tQReCC\t[44],\tand\tCoQA\t[45],\tand\tintroduce\ta\tself-constructed\tdataset\tas\tthe\texperimental\tdata\tsources.\tThe\tQReCC\tdataset\tcontains\t10,000\tquestions,\teach\tcorresponding\tto\ta\tbackground\tknowledge\ttext,\twith\tquestions\tpresented\tin\tthe\tform\tof\tmulti-turn\tdialogues.\tThe\tCoQA\tdataset\tcontains\tover\t8,000\tmulti-domain\tdialogues,\teach\tbased\ton\ta\tgiven\ttext\tand\tconsisting\tof\tmultiple\tquestion-answer\tturns.\t\tThe\tself-constructed\tdataset\tis\ta\tautomotive\tindustry\tproprietary\tdataset\tcompiled\tfrom\tinternal\tdocuments\tof\ta\tleading\tautomotive\tmanufacturer.\tThis\tdataset\tincludes:\t• Technical\tspecifications\tand\tdesign\tdocuments\t• Manufacturing\tprocess\tguidelines\t• Quality\tcontrol\tprocedures\t• Corporate\tregulations\tand\tstandards\tDue\tto\tthe\tconfidential\tnature\tof\tthese\tdocuments,\twe\tcannot\tprovide\tdetailed\tstatistics\tor\texamples.\tHowever,\tthis\t dataset\t represents\t the\t core\t focus\t of\t our\t study,\t reflecting\t real-world\t challenges\t in\t automotive\t document\tprocessing.\tWhile\t the\t self-constructed\t dataset\t serves\t as\t our\t primary\t testbed,\t we\t included\t QReCC\t and\t CoQA\t for\t several\tcrucial\treasons:\t1. Structural\tSimilarity:\tThe\tquestion-answer\tpairs\tin\tQReCC\tand\tCoQA\tshare\ta\tsimilar\tformat\twith\tour\tcustom-designed\ttest\tcases\tfor\tthe\tself-constructed\tdataset.\tThis\tstructural\tconsistency\tallows\tfor\ta\tfair\tcomparison\tof\tour\tmodel's\tperformance\tacross\tdifferent\tdomains.\t2. Conversational\tNature:\tBoth\tQReCC\tand\tCoQA\tfeature\tmulti-turn\tdialogues,\tmirroring\tthe\tcomplex,\tcontext-dependent\tqueries\toften\tencountered\tin\tautomotive\tengineering\tand\tmanufacturing\tprocesses.\t3. Diverse\t Domain\t Coverage:\t These\t datasets\t cover\t a\t wide\t range\t of\t topics,\t helping\t us\t evaluate\t our\t model's\tgeneralization\tcapabilities\tbeyond\tthe\tautomotive\tdomain.\t4. Benchmark\tComparability:\tAs\twidely\tused\tpublic\tdatasets,\tQReCC\tand\tCoQA\tenable\tus\tto\tbenchmark\tour\tsystem\tagainst\tother\tstate-of-the-art\tmodels\tin\ta\treproducible\tmanner.\t5. Confidentiality\tCompliance:\tBy\tusing\tthese\tpublic\tdatasets\talongside\tour\tproprietary\tdata,\twe\tcan\topenly\tdiscuss\tand\tcompare\tresults\twithout\tcompromising\tsensitive\tcorporate\tinformation.\t6. Robustness\tTesting:\tThe\tinclusion\tof\tthese\tdatasets\thelps\tdemonstrate\tthat\tour\toptimizations,\twhile\ttailored\tfor\tautomotive\tapplications,\tdo\tnot\tcompromise\tperformance\ton\tgeneral\tconversational\ttasks.\tConsidering\tthat\tour\tresearch\taims\tat\tthe\trequirements\tof\tPDF\tdialogue\tchatbots\tand\tuses\tthe\tBGE\treordering\tmodel,\t we\t first\t translate\t the\t knowledge\t or\t link\t information\t in\t the\t QReCC\t and\t CoQA\t datasets\t into\t Chinese\t and\tconvert\tit\tto\tPDF\tformat.\tThis\tstep\tensures\tthe\tconsistency\tof\texperimental\tdata\twith\tcomplex\tPDF\tdocuments\tin\treal\t application\t scenarios\t and\t facilitates\t the\t evaluation\t of\t the\t performance\t of\t different\t RAG\t models\t and\t their\toptimization\t schemes\t in\t practice.\t The\t self-constructed\t dataset\t does\t not\t require\t additional\t processing\t as\t it\t is\talready\tin\tPDF\tformat.\tAfter\tdata\tpreparation,\twe\tdesign\tor\tdirectly\tutilize\ta\tnumber\tof\tquestion-answer\tpairs\twith\tcontextual\tmemory\tcontent\tfrom\tthese\tdatasets\tas\ttest\tcases.\tHere,\twe\ttake\ta\tset\tof\tquestion-answer\tpairs\tfrom\tthe\tQReCC\tdataset\tas\tan\texample,\tgiven\ta\tbackground\ttext:\t“John\tis\ta\tboy\twho\tlikes\tto\tplay\toutside.\tAfter\tschool,\the\talways\tgoes\tto\tthe\tpark\tto\tmeet\this\tfriends.”\t\tThe\tfollowing\tis\ta\tset\tof\tmulti-turn\tquestion-answering\tbased\ton\tthis\tbackground\ttext:\t“Q:\tWhat\tdid\tJohn\tdo\tafter\tschool?\tA:\tJohn\twent\tto\tthe\tpark\tafter\tschool.\tQ:\tWho\tdid\the\tmeet\tat\tthe\tpark?\tA:\tHe\tmet\this\tfriends\tat\tthe\tpark.”\t\tWe\tdesigned\tour\texperiments\tto\tevaluate\tthe\tperformance\tof\tfour\tsystems:\t• Naive\tRAG\t(Baseline)\t• Advanced\tRAG\t(Our\tOptimized\tModel)\t• Self-RAG\tAgent\t(Baseline)\t• Self-RAG\tAgent\t(Our\tProposed\tAgent\twith\tCustom\tFunction\tCalling)\tFor\teach\tdataset,\twe\tcreated\ttest\tsets\tcomprising:\t• 500\tquestion-answer\tpairs\tfrom\tthe\tself-constructed\tdataset\t• 500\tpairs\tfrom\tQReCC\t• 500\tpairs\tfrom\tCoQA\tThese\t test\t sets\t were\t carefully\t curated\t to\t ensure\t a\t balance\t of\t simple\t queries,\t multi-turn\t conversations,\t and\tcomplex\ttechnical\tquestions,\tmirroring\treal-world\tusage\tscenarios\tin\tthe\tautomotive\tindustry\tand\tbeyond.\t3.2.2 Optimization Effects of Langchain-based RAG\t\n\tFigure 5: Performance Comparison of Naive RAG vs. Advanced RAG with Custom Context Compression Pipeline across Datasets. Table 1: Comparative Analysis of Naive RAG vs. Advanced RAG with Custom Context Compression Pipeline across Datasets Datasets\tMetrics\tQReCC\tCoQA\tSelf-constructed\tdataset\tNaive\tRAG\t\tAnswer\tRelevancy\t0.782\t0.772\t0.759\tFaithfulness\t0.81\t0.803\t0.803\tContext\tPrecision\t0.845\t0.838\t0.847\tContext\tRecall\t0.831\t0.824\t0.822\tAdvanced\tRAG\tAnswer\tRelevancy\t0.811\t0.829\t0.817\tFaithfulness\t0.847\t0.85\t0.832\tContext\tPrecision\t0.839\t0.841\t0.836\tContext\tRecall\t0.842\t0.811\t0.835\tAccording\t to\tthe\t Figure\t5\tand\t Table\t1,\t experimental\t results\t show\t that\t the\t proposed\t Langchain-based\t RAG\toptimization\tapproach\tachieves\tcertain\timprovements\ton\tsome\tmetrics\tcompared\tto\tthe\tnaive\tRAG\tmodel.\tBy\tintroducing\tthe\tBGE\treordering\tmodel\tand\tBM25\tretriever,\tand\tconstructing\ta\tcontext\tcompression\tpipeline,\tthe\t optimized\t RAG\t model\t improves\t context\t precision\t by\t 0.7%,\t 0.4%,\t and\t 1.3%\ton\t the\t QReCC,\t CoQA,\t and\t self-constructed\t dataset,\t respectively.\t The\t context\t recall\t also\t increases\t by\t 1.3%\tand\t 1.6%\ton\t the\t QReCC\t and\t self-constructed\tdataset,\tbut\tdecreases\tby\t1.6%\ton\tthe\tCoQA\tdataset.\tThese\tresults\tindicate\tthat\twhile\tthe\toptimized\tmodel\tshows\tsome\timprovements\tin\tcapturing\tquestion-relevant\tbackground\tknowledge,\tit\tis\tnot\talways\tthe\tcase.\tHowever,\tthe\toptimized\tmodel\texhibits\t3.7%,\t7.4%,\tand\t7.6%\trelative\timprovements\tin\tanswer\trelevancy,\tas\twell\tas\t4.6%,\t5.9%,\tand\t3.6%\tboosts\tin\tfaithfulness\ton\tQReCC,\tCoQA,\tand\tself-constructed\tdataset,\trespectively.\t\tThese\tresults\tunderscore\tthe\teffectiveness\tof\tour\toptimizations\tin\thandling\tcomplex\tautomotive\tdocumentation\tand\t queries.\t The\t Advanced\t RAG\t model\t also\t showed\t improvements,\t but\t the\t Self-RAG\t Agent's\t performance\t was\tnotably\tsuperior,\tparticularly\tin\tdealing\twith\tmulti-step\ttechnical\tqueries\tcommon\tin\tautomotive\tapplications.\t3.2.3 Optimization Effects of Langgraph-based Self-RAG Agent\t\n\tFigure 6: Performance Comparison of Self-RAG Agent vs. Self-RAG Agent with Custom Function Calling across Datasets. Table 2: Comparative Analysis of Self-RAG Agent vs. Self-RAG Agent with Custom Function Calling across Datasets Datasets\tMetrics\tQReCC\tCoQA\tSelf-constructed\tdataset\tAnswer\tRelevancy\t0.839\t0.821\t0.83\tDatasets\tMetrics\tQReCC\tCoQA\tSelf-constructed\tdataset\tSelf-RAG\tAgent\t\tFaithfulness\t0.847\t0.858\t0.847\tContext\tPrecision\t0.864\t0.866\t0.867\tContext\tRecall\t0.851\t0.841\t0.849\tSelf-RAG\tAgent\twith\tcustom\t function\tcalling\tAnswer\tRelevancy\t0.852\t0.851\t0.86\tFaithfulness\t0.859\t0.86\t0.857\tContext\tPrecision\t0.861\t0.882\t0.872\tContext\tRecall\t0.871\t0.87\t0.86\tAccording\tto\tthe\tFigure\t6\tand\tTable\t2,\tevaluation\tresults\tshow\tthat\tthe\tself-RAG\tagent\tobtains\tmore\tsignificantly\timprovements\tover\tthe\tnaive\tRAG\tmodel\ton\tmost\tmetrics\tacross\tthe\tthree\tdatasets.\tSpecifically,\tit\tsurpasses\tthe\tnaive\tRAG\tby\t7.3%,\t6.3%,\tand\t9.4%\tin\tanswer\trelevancy,\t4.6%,\t6.8%,\tand\t5.5%\tin\tfaithfulness,\t2.2%,\t3.3%,\tand\t2.4%\tin\tcontext\tprecision,\tand\t2.4%,\t2.1%,\tand\t3.3%\tin\tcontext\trecall\ton\tQReCC,\tCoQA,\tand\tself-constructed\tdataset,\trespectively.\tThe\tsuperior\tperformance\ton\tmost\tmetrics\tdemonstrates\tthe\teffectiveness\tof\tthe\tproposed\tself-asking\tand\tself-verification\tmechanism\tin\tenabling\tmore\ttargeted\tand\treliable\tcontext\tretrieval\tand\tanswer\tinference.\tFurthermore,\tby\tintroducing\tthe\tcustom\tfunction\tcalling\tmechanism\tthat\toptimizes\tthe\tOllama\toutput,\tthe\tself-RAG\tagent\tobtains\tadditional\t9.0%,\t10.2%,\tand\t13.3%\tgains\tin\tanswer\trelevancy,\t6.0%,\t7.1%,\tand\t6.7%\tincreases\tin\tfaithfulness,\t1.9%,\t5.3%,\tand\t3.0%\timprovements\tin\tcontext\tprecision,\tas\twell\tas\t4.8%,\t5.6%,\tand\t4.6%\tboosts\tin\tcontext\trecall\ton\tthe\tthree\tdatasets\tcompared\tto\tthe\tnaive\tRAG\tmodel.\tThese\tresults\tvalidate\tthat\tdynamically\tadjusting\tquestion\tand\tcontext\tinputs\tto\tthe\tOllama\tmodel\tbased\ton\tconversation\tstates\tcan\teffectively\tguide\tit\tto\tgenerate\tmore\taccurate,\tinformative,\tand\tcoherent\tresponses\tthat\tbetter\tsatisfy\tuser\tneeds.\tThis\tperformance\tdemonstrates\tthat\tour\toptimizations\tenhance\tgeneral\tconversational\tAI\tcapabilities\twhile\texcelling\t in\t domain-specific\t tasks.\t The\t consistent\t improvement\t across\t datasets\t suggests\t that\t our\t approach\tsuccessfully\tbalances\tdomain-specific\toptimization\twith\tgeneral\tlanguage\tunderstanding.\t4 DISCUSSION Our\tinnovative\tapproach\tto\thandling\tcomplex\tautomotive\tqueries\tis\texemplified\tin\tFigure\t7,\twhich\tdepicts\tthe\tSelf-RAG\tprocess\tflow\twith\tcustom\tfunction\tcalling,\tapplied\tto\tan\tAnti-lock\tBraking\tSystem\t(ABS)\tquery.\tThis\tdiagram\tshowcases\t the\t integration\t of\t our\t custom\tChatFunction\twithin\t the\t Self-RAG\t framework,\t demonstrating\t how\t it\tdynamically\tadjusts\tthe\tdetail\tand\tfocus\tof\tresponses\tbased\ton\tthe\tsystem's\tself-assessment\tand\tretry\tcount.\tThe\tprocess\tilluminates\tthe\tsystem's\tcapability\tto\titeratively\trefine\tits\tanswers,\tensuring\thigh\trelevance\tand\taccuracy\tin\tthe\tcontext\tof\tspecialized\tautomotive\tknowledge.\t\tFigure 7: Optimized Self-RAG Process Flow with Custom Function Calling - examples. When\tcomparing\tperformance\tacross\tdatasets,\twe\tobserved\tthe\tfact\tthat\twhile\tour\tmodels\twere\toptimized\tfor\tautomotive\tapplications,\tthey\talso\tshowed\timprovements\ton\tthe\tQReCC\tand\tCoQA\tdatasets:\t• The\tmost\tsignificant\timprovements\twere\ton\tthe\tAID,\taligning\twith\tour\tfocus\ton\tautomotive\tapplications.\t• Performance\tgains\ton\tQReCC\tand\tCoQA,\twhile\tsmaller,\twere\tstill\tsubstantial,\tindicating\tthe\trobustness\tof\tour\tapproach.\t• The\tSelf-RAG\tAgent\tshowed\tthe\tmost\tconsistent\tperformance\tacross\tall\tdatasets,\thighlighting\tits\tadaptability\tto\tvarious\tquery\ttypes\tand\tdomains.\tThese\tresults\tvalidate\tour\tapproach\tof\tusing\tdiverse\tdatasets\tfor\tevaluation.\tWhile\tour\tprimary\tfocus\tremains\ton\toptimizing\tfor\tautomotive\tapplications,\tthe\timprovements\tseen\tacross\tdatasets\tsuggest\tthat\tour\tmethods\tenhance\tfundamental\taspects\tof\tinformation\tretrieval\tand\tgeneration,\tbenefiting\tboth\tspecialized\tand\tgeneral\tapplications.\t«Example»1. User Query«Example»3. Retrieved Fragments\n«Example»6. Initial Answer\n«Example»9. Optimized AnswerExplain ABS working principleand safety advantagesABS prevents wheel lock...ABS components...\nABS reduces braking distance...ABS prevents wheel lock,improves safety\nABS prevents wheel lock,uses sensors and ECU,reduces braking distanceby 10-20%2. DocumentRetrieval\n«Highlight»4. RelevanceScoring«Highlight»5. GenerateInitial Answer\n«Highlight»7. QualityAssessment«Highlight»8. Self-reflectionand Optimization10. FinalOutputCustom Function Calling (ChatFunction):- Adjusts detail based on retry_count- Integrates personality and language- Optimizes for user needsSelf-RAGLoop5 DISCUSSION This\tstudy\tpresents\ta\tcomprehensive\tapproach\tto\toptimizing\tRAG\ttechniques\tfor\tautomotive\tindustry\tapplications,\tspecifically\tfocusing\ton\tPDF\tchatbots\tdeployed\tin\tlocal,\tlow-performance\tenvironments.\tOur\tresearch\taddresses\tcritical\tchallenges\tin\tprocessing\tcomplex\tautomotive\tdocumentation\tand\tresponding\tto\tindustry-specific\tqueries.\t5.1 Key Contributions • Enhanced\t PDF\t Processing:\t We\t developed\t a\t novel\t method\t combining\t PDFMiner\t and\t Tabula\t to\t effectively\thandle\t multi-column\t layouts\t and\t complex\t tables\t prevalent\t in\t automotive\t technical\t documents.\t This\tsignificantly\timproves\tinformation\textraction\taccuracy\tfrom\tindustry-specific\tPDFs.\t• Advanced\tRAG\tOptimization:\tOur\tLangchain-based\tRAG\tsystem,\tfeaturing\ta\tcustom\tretriever\tensemble\tand\tcontext\t compression\t pipeline,\t demonstrates\t substantial\t improvements\t in\t retrieving\t and\t utilizing\tautomotive-specific\tinformation.\t• Self-RAG\t Agent\t Design:\t The\t proposed\t AgenticRAG,\t enhanced\t with\t a\t custom\t function\t calling\t mechanism,\tshows\tsuperior\tperformance\tin\thandling\tcomplex,\tmulti-step\tqueries\ttypical\tin\tautomotive\tengineering\tand\tmanufacturing\tprocesses.\t• Cross-Domain\t Effectiveness:\t While\t optimized\t for\t automotive\t applications,\t our\t approach\t also\t shows\timprovements\tin\tgeneral\tconversational\tAI\ttasks,\tas\tevidenced\tby\tperformance\tgains\ton\tQReCC\tand\tCoQA\tdatasets.\t5.2 Implications for the Automotive Industry Our\tresearch\thas\tsignificant\timplications\tfor\tthe\tautomotive\tsector:\t1. Improved\t Information\t Access:\t The\t optimized\t PDF\t chatbot\t can\t greatly\t enhance\t access\t to\t technical\tinformation\tfor\tengineers,\ttechnicians,\tand\tother\tstakeholders\tin\tthe\tautomotive\tindustry.\t2. Enhanced\tDecision\tMaking:\tBy\tproviding\tmore\taccurate\tand\tcontextually\trelevant\tinformation,\tour\tsystem\tcan\tsupport\tbetter\tdecision-making\tin\tdesign,\tmanufacturing,\tand\tquality\tcontrol\tprocesses.\t3. Resource\t Efficiency:\t The\t ability\t to\t deploy\t these\t advanced\t capabilities\t in\t low-performance,\t local\tenvironments\taddresses\tthe\tindustry's\tneeds\tfor\tdata\tprivacy\tand\tresource\tconstraints.\t5.3 Limitations and Future Work While\tour\tstudy\tdemonstrates\tsignificant\tadvancements,\tthere\tare\tareas\tfor\tfurther\tresearch:\t1. Expanding\tDomain\tCoverage:\tFuture\twork\tcould\tfocus\ton\tadapting\tthe\tsystem\tto\tcover\ta\tbroader\trange\tof\tautomotive\tsub-domains,\tsuch\tas\telectric\tvehicle\ttechnology\tor\tautonomous\tdriving\tsystems.\t2. Real-Time\t Performance\t Optimization:\t Further\tresearch\t is\t needed\t to\t enhance\t the\t system's\t real-time\tperformance\tin\tresource-constrained\tindustrial\tenvironments.\t3. Multi-Modal\tIntegration:\tIncorporating\tthe\tability\tto\tprocess\tand\trespond\tto\tqueries\tabout\tvisual\telements\tin\ttechnical\tdiagrams\tand\tschematics\tcould\tgreatly\tenhance\tthe\tsystem's\tutility.\t4. Longitudinal\tStudy:\tA\tlong-term\tstudy\tin\treal\tautomotive\tmanufacturing\tsettings\tcould\tprovide\tinsights\tinto\tthe\tsystem's\timpact\ton\toperational\tefficiency\tand\tdecision-making\tprocesses.\t5. Ethical\tand\tPrivacy\tConsiderations:\tAs\tthe\tsystem\tdeals\twith\tproprietary\tinformation,\tfuture\twork\tshould\texplore\tadvanced\tmethods\tfor\tensuring\tdata\tprivacy\tand\tethical\tuse\tof\tAI\tin\tindustrial\tsettings.\tIn\tconclusion,\tthis\tresearch\trepresents\ta\tsignificant\tstep\tforward\tin\tapplying\tadvanced\tnatural\tlanguage\tprocessing\ttechniques\t to\t the\t specific\t needs\t of\t the\t automotive\t industry.\t By\t bridging\t the\t gap\t between\t cutting-edge\t AI\tcapabilities\tand\tthe\tpractical\tconstraints\tof\tindustrial\tenvironments,\tour\twork\tcontributes\tto\tthe\tongoing\tdigital\ttransformation\tof\tthe\tautomotive\tsector.\tThe\tdemonstrated\timprovements\tin\thandling\tcomplex,\tdomain-specific\tinformation\tretrieval\tand\tquery\tresolution\tpave\tthe\tway\tfor\tmore\tintelligent,\tefficient,\tand\tresponsive\tinformation\tsystems\tin\tautomotive\tmanufacturing\tand\tengineering.\tREFERENCES [1] C.\tLlopis-Albert,\tF.\tRubio,\tand\tF.\tValero.\t2021.\tImpact\tOf\tDigital\tTransformation\tOn\tThe\tAutomotive\tIndustry.\tTechnological\tForecasting\tand\tSocial\tChange\t162\t(2021),\t120343.\thttps://doi.org/10.1016/j.techfore.2020.120343\t[2] \tD.\tG.\tSchniederjans,\tC.\tCurado,\tand\tM.\tKhalajhedayati.\t2020.\tSupply\tChain\tDigitisation\tTrends:\tAn\tIntegration\tof\tKnowledge\tManagement.\tInternational\tJournal\tof\tProduction\tEconomics\t220\t(2020),\t107439.\thttps://doi.org/10.1016/j.ijpe.2019.07.001\t[3] A.\tZahra\tand\tM.\tSaeedeh.\t2021.\tText-Based\tQuestion\tAnswering\tFrom\tInformation\tRetrieval\tAnd\tDeep\tNeural\tNetwork\tPerspectives:\tA\tSurvey.\tWiley\tInterdisciplinary\tReviews:\tData\tMining\tand\tKnowledge\tDiscovery\t11,\t6\t(2021),\te1412.\thttps://doi.org/10.1002/widm.1412\t[4] M.\t Shanahan.\t 2024.\t Talking\t about\t Large\t Language\t Models.\t Communications\t of\t the\t ACM\t 67,\t 2\t (2024),\t 68-79.\thttps://doi.org/10.1145/3631338\t[5] M.\tMozes,\tX.\tHe,\tB.\tKleinberg,\tand\tL.\tD.\tGriffin.\t2023.\tUse\tof\tLLMs\tfor\tIllicit\tPurposes:\tThreats,\tPrevention\tMeasures,\tand\tVulnerabilities.\tarXiv\tpreprint\tarXiv:2308.12833\t(2023).\t[6] Y.\tGao,\tY.\tXiong,\tX.\tGao,\tK.\tJia,\tJ.\tPan,\tY.\tBi,\tY.\tDai,\tJ.\tSun,\tM.\tWang,\tand\tH.\tWang.\t2023.\tRetrieval-Augmented\tGeneration\tfor\tLarge\tLanguage\tModels:\tA\tSurvey.\tarXiv\tpreprint\tarXiv:2312.10997\t(2023).\t[7] P.\tLewis,\tE.\tPerez,\tA.\tPiktus,\tF.\tPetroni,\tV.\tKarpukhin,\tN.\tGoyal,\tH.\tKüttler,\tM.\tLewis,\tW.\tYih,\tT.\tRocktäschel,\tS.\tRiedel,\tand\tD.\tKiela.\t2020.\tRetrieval-Augmented\tGeneration\tfor\tKnowledge-Intensive\tNLP\tTasks.\tIn\tAdvances\tin\tNeural\tInformation\tProcessing\tSystems,\tVol.\t33.\t9459-9474.\t[8] S.\tRaja,\tA.\tMondal,\tand\tC.\tV.\tJawahar.\t2022.\tVisual\tUnderstanding\tof\tComplex\tTable\tStructures\tfrom\tDocument\tImages.\tIn\tProceedings\tof\tthe\tIEEE/CVF\tWinter\tConference\ton\tApplications\tof\tComputer\tVision.\t2543-2552.\t[9] M.\t Krichen.\t 2023.\t Formal\t Methods\t and\t Validation\t Techniques\t for\t Ensuring\t Automotive\t Systems\t Security.\t Electronics\t 14,\t 3\t (2023),\t 666.\thttps://doi.org/10.3390/electronics14030666\t[10] H.\tLiu,\tM.\tGalindo,\tH.\tXie,\tL.\tWong,\tH.\tShuai,\tY.\tLi,\tand\tW.\tCheng.\t2024.\tLightweight\tDeep\tLearning\tfor\tResource-Constrained\tEnvironments:\tA\tSurvey.\tarXiv\tpreprint\tarXiv:2404.07236\t(2024).\t[11] V.\tD.\tViellieber\tand\tM.\tAßenmacher.\t2020.\tPre-trained\tlanguage\tmodels\tas\tknowledge\tbases\tfor\tAutomotive\tComplaint\tAnalysis.\tarXiv\tpreprint\tarXiv:2012.02558\t(2020).\t[12] M.\tGhaleb,\tH.\tZolfagharinia,\tand\tS.\tTaghipour.\t2020.\tReal-time\tproduction\tscheduling\tin\tthe\tIndustry-4.0\tcontext:\tAddressing\tuncertainties\tin\tjob\tarrivals\tand\tmachine\tbreakdowns.\tComputers\t&\tIndustrial\tEngineering\t123\t(2020),\t105031.\thttps://doi.org/10.1016/j.cie.2020.105031\t[13] J.\t B.\t Gruber\t and\t M.\t Weber.\t 2024.\t rollama:\t An\t R\t package\t for\t using\t generative\t large\t language\t models\t through\t Ollama.\t arXiv\t preprint\tarXiv:2404.07654\t(2024).\t[14] R.\tHussain\tand\tS.\tZeadally.\t2019.\tAutonomous\tCars:\tResearch\tResults,\tIssues\tand\tFuture\tChallenges.\tIEEE\tCommunications\tSurveys\t&\tTutorials\t21,\t2\t(2019),\t1275-1313.\thttps://doi.org/10.1109/COMST.2018.2869360\t[15] Q.\tAi,\tT.\tBai,\tZ.\tCao,\tY.\tChang,\tJ.\tChen,\tZ.\tChen,\tZ.\tCheng,\tS.\tDong,\tZ.\tDou,\tF.\tFeng,\tS.\tGao,\tJ.\tGuo,\tX.\tHe,\tY.\tLan,\tC.\tLi,\tY.\tLiu,\tZ.\tLyu,\tW.\tMa,\tJ.\tMa,\tand\tZ.\tRen.\t2023.\tInformation\tRetrieval\tmeets\tLarge\tLanguage\tModels:\tA\tstrategic\treport\tfrom\tChinese\tIR\tcommunity.\tFundamental\tResearch\t4\t(2023),\t80-90.\thttps://doi.org/10.1016/j.fmre.2023.06.009\t[16] M.\tKrafft,\tL.\tSajtos,\tand\tM.\tHaenlein.\t2020.\tChallenges\tand\tOpportunities\tfor\tMarketing\tScholars\tin\tTimes\tof\tthe\tFourth\tIndustrial\tRevolution.\tJournal\tof\tInteractive\tMarketing\t51\t(2020),\t1-8.\thttps://doi.org/10.1016/j.intmar.2020.06.001\t[17] D.\tAmalfitano,\tV.\tDe\tSimone,\tR.\tR.\tMaietta,\tS.\tScala,\tand\tA.\tR.\tFasolino.\t2019.\tUsing\ttool\tintegration\tfor\timproving\ttraceability\tmanagement\ttesting\t processes:\t An\t automotive\t industrial\t experience.\t Journal\t of\t Systems\t and\t Software\t 151\t (2019),\t e2171.\thttps://doi.org/10.1002/smr.2171\t[18] Z.\t Jiang,\t F.\t F.\t Xu,\t L.\t Gao,\t Z.\t Sun,\t Q.\t Liu,\t J.\t Dwivedi-Yu,\t Y.\t Yang,\t J.\t Callan,\t and\t G.\t Neubig.\t 2023.\t Active\t Retrieval\t Augmented\t Generation.\t In\tProceedings\tof\tthe\t2023\tConference\ton\tEmpirical\tMethods\tin\tNatural\tLanguage\tProcessing.\t7969-7992.\t[19] Z.\tWang,\tJ.\tAraki,\tZ.\tJiang,\tM.\tR.\tParvez,\tand\tG.\tNeubig.\t2023.\tLearning\tto\tFilter\tContext\tfor\tRetrieval-Augmented\tGeneration.\tarXiv\tpreprint\tarXiv:2311.08377\t(2023).\t[20] A.\tAsai,\tZ.\tWu,\tY.\tWang,\tA.\tSil,\tand\tH.\tHajishirzi.\t2023.\tSelf-RAG:\tLearning\tto\tRetrieve,\tGenerate,\tand\tCritique\tthrough\tSelf-Reflection.\tarXiv\tpreprint\tarXiv:2310.11511\t(2023).\t[21] D.\tRajpathak,\tY.\tXu,\tand\tI.\tGibbs.\t2020.\tAn\tIntegrated\tFramework\tFor\tAutomatic\tOntology\tLearning\tFrom\tUnstructured\tRepair\tText\tData\tFor\tEffective\t Fault\t Detection\t And\t Isolation\t In\t Automotive\t Domain.\t Expert\t Systems\t with\t Applications\t 123\t (2020),\t 103338.\thttps://doi.org/10.1016/j.eswa.2020.103338\t[22] S.\tSiriwardhana,\tR.\tWeerasekera,\tE.\tWen,\tT.\tKaluarachchi,\tR.\tRana,\tand\tS.\tNanayakkara.\t2022.\tImproving\tthe\tDomain\tAdaptation\tof\tRetrieval\tAugmented\t Generation\t (RAG)\t Models\t for\t Open\t Domain\t Question\t Answering.\t Electronics\t 11,\t 20\t (2022),\t 3388.\thttps://doi.org/10.3390/electronics11203388\t[23] S.\tYin,\tC.\tFu,\tS.\tZhao,\tK.\tLi,\tX.\tSun,\tT.\tXu,\tand\tE.\tChen.\t2023.\tA\tSurvey\ton\tMultimodal\tLarge\tLanguage\tModels.\tarXiv\tpreprint\tarXiv:2306.13549\t(2023).\t[24] C.\tBurgan,\tJ.\tKowalski,\tand\tW.\tLiao.\t2024.\tDeveloping\ta\tRetrieval\tAugmented\tGeneration\t(RAG)\tChatbot\tApp\tUsing\tAdaptive\tLarge\tLanguage\tModels\t(LLM)\tand\tLangChain\tFramework.\tIEEE\tAccess\t96\t(2024).\thttps://doi.org/10.1109/ACCESS.2024.3359176\t[25] B.\tWang,\tG.\tLi,\tand\tY.\tLi.\t2023.\tEnabling\tConversational\tInteraction\twith\tMobile\tUI\tusing\tLarge\tLanguage\tModels.\tIn\tProceedings\tof\tthe\t36th\tAnnual\tACM\tSymposium\ton\tUser\tInterface\tSoftware\tand\tTechnology.\tArticle\t432,\t17\tpages.\thttps://doi.org/10.1145/3586183.3606716\t[26] D.\tLin.\t2024.\tRevolutionizing\tRetrieval-Augmented\tGeneration\twith\tEnhanced\tPDF\tStructure\tRecognition.\tarXiv\tpreprint\tarXiv:2401.12599\t(2024).\t[27] O.\tBensch,\tM.\tPopa,\tand\tC.\tSpille.\t2021.\tKey\tInformation\tExtraction\tFrom\tDocuments\t-\tEvaluation\tAnd\tGenerator.\tIn\tProceedings\tof\tthe\t3rd\tInternational\tConference\ton\tDeep\tLearning\tTheory\tand\tApplications.\t47-53.\thttps://doi.org/10.5220/0010517700470053\t[28] M.\tFaysse,\tG.\tViaud,\tC.\tHudelot,\tand\tP.\tColombo.\t2023.\tRevisiting\tInstruction\tFine-tuned\tModel\tEvaluation\tto\tGuide\tIndustrial\tApplications.\tIn\tProceedings\tof\tthe\t2023\tConference\ton\tEmpirical\tMethods\tin\tNatural\tLanguage\tProcessing.\t9033-9048.\t[29] A.\tKumar\tand\tB.\tStarly.\t2021.\tFabNER:\tinformation\textraction\tfrom\tmanufacturing\tprocess\tscience\tdomain\tliterature\tusing\tnamed\tentity\trecognition.\tJournal\tof\tIntelligent\tManufacturing\t33\t(2021),\t2393-2407.\thttps://doi.org/10.1007/s10845-021-01867-z\t[30] D.\tBelhadj,\tA.\tBelaïd,\tand\tY.\tBelaïd.\t2023.\tImproving\tInformation\tExtraction\tfrom\tSemi-structured\tDocuments\tUsing\tAttention\tBased\tSemi-variational\tGraph\tAuto-Encoder.\tIn\tDocument\tAnalysis\tSystems.\t113-129.\thttps://doi.org/10.1007/978-3-031-41682-8_8\t[31] S.\tZeng,\tJ.\tZhang,\tP.\tHe,\tY.\tXing,\tY.\tLiu,\tH.\tXu,\tJ.\tRen,\tS.\tWang,\tD.\tYin,\tY.\tChang,\tand\tJ.\tTang.\t2024.\tThe\tGood\tand\tThe\tBad:\tExploring\tPrivacy\tIssues\tin\tRetrieval-Augmented\tGeneration\t(RAG).\tarXiv\tpreprint\tarXiv:2402.16893\t(2024).\t[32] S.\tBorgeaud,\tA.\tMensch,\tJ.\tHoffmann,\tT.\tCai,\tE.\tRutherford,\tK.\tMillican,\tG.\tvan\tden\tDriessche,\tJ.\tLespiau,\tB.\tDamoc,\tA.\tClark,\tD.\tde\tLas\tCasas,\tA.\tGuy,\tJ.\tMenick,\tR.\tRing,\tT.\tHennigan,\tS.\tHuang,\tL.\tMaggiore,\tC.\tJones,\tA.\tCassirer,\tA.\tBrock,\tM.\tPaganini,\tG.\tIrving,\tO.\tVinyals,\tS.\tOsindero,\tK.\tSimonyan,\t J.\t W.\t Rae,\t E.\t Elsen,\t and\t L.\t Sifre.\t 2022.\t Improving\t Language\t Models\t by\t Retrieving\t from\t Trillions\t of\t Tokens.\t arXiv\t preprint\tarXiv:2112.04426\t(2022).\t[33] G.\tIzacard,\tP.\tLewis,\tM.\tLomeli,\tL.\tHosseini,\tF.\tPetroni,\tT.\tSchick,\tJ.\tDwivedi-Yu,\tA.\tJoulin,\tS.\tRiedel,\tand\tE.\tGrave.\t2023.\tATLAS:\tFew-shot\tLearning\twith\tRetrieval\tAugmented\tLanguage\tModels.\tJournal\tof\tMachine\tLearning\tResearch\t24\t(2023),\t251:1-251:43.\t[34] S.\tR.\tAhmad.\t2024.\tEnhancing\tMultilingual\tInformation\tRetrieval\tin\tMixed\tHuman\tResources\tEnvironments:\tA\tRAG\tModel\tImplementation\tfor\tMulticultural\tEnterprise.\tarXiv\tpreprint\tarXiv:2401.01511\t(2024).\t[35] W.\tSu,\tY.\tTang,\tQ.\tAi,\tZ.\tWu,\tand\tY.\tLiu.\t2024.\tDRAGIN:\tDynamic\tRetrieval\tAugmented\tGeneration\tbased\ton\tthe\tInformation\tNeeds\tof\tLarge\tLanguage\tModels.\tarXiv\tpreprint\tarXiv:2403.10081\t(2024).\t[36] K.\tNoorbakhsh,\tM.\tSulaiman,\tM.\tSharifi,\tK.\tRoy,\tand\tP.\tJamshidi.\t2023.\tPretrained\tLanguage\tModels\tare\tSymbolic\tMathematics\tSolvers\ttoo!.\tarXiv\tpreprint\tarXiv:2110.03501\t(2023).\t[37] G.\tGamage,\tN.\tMills,\tD.\tDe\tSilva,\tM.\tManic,\tH.\tMoraliyage,\tA.\tJennings,\tand\tD.\tAlahakoon.\t2024.\tMulti-Agent\tRAG\tChatbot\tArchitecture\tfor\tDecision\tSupport\tin\tNet-Zero\tEmission\tEnergy\tSystems.\tIn\t2024\tIEEE\tPower\t&\tEnergy\tSociety\tInnovative\tSmart\tGrid\tTechnologies\tConference\t(ISGT).\t1-6.\thttps://doi.org/10.1109/ISGT58387.2024.10471952\t[38] B.\tZhuang,\tJ.\tLiu,\tZ.\tPan,\tH.\tHe,\tY.\tWeng,\tand\tC.\tShen.\t2023.\tA\tsurvey\ton\tefficient\ttraining\tof\ttransformers.\tIn\tProceedings\tof\tthe\tIEEE/CVF\tInternational\tConference\ton\tComputer\tVision.\t6823-6831.\t[39] W.\tChen,\tH.\tHu,\tX.\tChen,\tP.\tVerga,\tand\tW.\tW.\tCohen.\t2022.\tMuRAG:\tMultimodal\tRetrieval-Augmented\tGenerator\tfor\tOpen\tQuestion\tAnswering\tover\tImages\tand\tText.\tIn\tProceedings\tof\tthe\t2022\tConference\ton\tEmpirical\tMethods\tin\tNatural\tLanguage\tProcessing.\t5558-5570.\t[40] S.\t Xiao,\t Z.\t Liu,\t P.\t Zhang,\t and\t N.\t Muennighoff.\t 2023.\t C-Pack:\t Packaged\t Resources\t To\t Advance\t General\t Chinese\t Embedding.\t arXiv\t preprint\tarXiv:2309.07597\t(2023).\thttps://doi.org/10.48550/arXiv.2309.07597\t[41] V.\tKarpukhin,\tB.\tOğuz,\tS.\tMin,\tL.\tWu,\tS.\tEdunov,\tD.\tChen,\tand\tW.\tYih.\t2020.\tDense\tPassage\tRetrieval\tfor\tOpen-Domain\tQuestion\tAnswering.\tIn\tProceedings\t of\t the\t 2020\t Conference\t on\t Empirical\t Methods\t in\t Natural\t Language\t Processing\t (EMNLP).\t 6769-6781.\thttps://doi.org/10.18653/v1/2020.emnlp-main.550\t[42] A.\tAsai,\tZ.\tWu,\tY.\tWang,\tA.\tSil,\tand\tH.\tHajishirzi.\t2023.\tSelf-RAG:\tLearning\tto\tRetrieve,\tGenerate,\tand\tCritique\tthrough\tSelf-Reflection.\tarXiv\tpreprint\tarXiv:2310.11511\t(2023).\thttps://doi.org/10.48550/arXiv.2310.11511\t[43] W.\tChen,\tZ.\tLi,\tand\tM.\tMa.\t2024.\tOctopus:\tOn-device\tlanguage\tmodel\tfor\tfunction\tcalling\tof\tsoftware\tAPIs.\tarXiv\tpreprint\tarXiv:2404.01549\t(2024).\thttps://doi.org/10.48550/arXiv.2404.01549\t[44] R.\tAnantha,\tS.\tVakulenko,\tZ.\tTu,\tS.\tLongpre,\tS.\tPulman,\tand\tS.\tChappidi.\t2021.\tOpen-Domain\tQuestion\tAnswering\tGoes\tConversational\tvia\tQuestion\tRewriting.\tIn\tProceedings\tof\tthe\t2021\tConference\tof\tthe\tNorth\tAmerican\tChapter\tof\tthe\tAssociation\tfor\tComputational\tLinguistics:\tHuman\tLanguage\tTechnologies.\thttps://doi.org/10.18653/v1/2021.naacl-main.44\t[45] S.\t Reddy,\t D.\t Chen,\t and\t C.D.\t Manning.\t 2019.\t CoQA:\t A\t Conversational\t Question\t Answering\t Challenge.\t Transactions\t of\t the\t Association\t of\tComputational\tLinguistics\t7\t(2019),\t249-266.\thttps://doi.org/10.1162/tacl_a_00266\t",
      "metadata": {
        "filename": "Optimizing RAG Techniques for Automotive Industry PDF Chatbots_ A Case Study wit.pdf",
        "hotspot_name": "ECU_Operation",
        "title": "Optimizing RAG Techniques for Automotive Industry PDF Chatbots: A Case\n  Study with Locally Deployed Ollama Models",
        "published_date": "2024-08-12T06:16:37Z",
        "pdf_link": "http://arxiv.org/pdf/2408.05933v1",
        "query": "automotive electronics low power consumption design optimization techniques"
      }
    },
    "Cold Isostatic Pressing to Improve the Mechanical Performance of Additively Manu": {
      "full_text": "1 Cold Isostatic Pressing to improve the mechanical performance of \nadditively manufactured metallic components  \n \n \nI.I. Cuesta (1), E. Martínez -Pañeda (2), A. Díaz (1), J.M. Alegre (1) \n \n(1) Structural Integrity Group, Universidad de Burgos , Avda. Cantabria s/n, 09006 Burgos. SPAIN  \n(2) University of Cambridge, Department of Engineering, Trumpington Street, Cambridge CB2 \n1PZ, UNITED KINGDOM  \n \n \nTelephone: +34 947 258922; e -mail: iicuesta@ubu.es  \n \n \n \nAbstract  \n \nAdditive Manufacturing is becoming a technique with great prospects for the production  of \ncomponents with new designs or shapes  that are difficult to obtain  by conventional manufacturing \nmethods . One of the most promising techniques for printing metallic components  is Binder Jetting , \ndue to its time efficiency  and its ability to generate complex part s. In this process , a liquid binding \nagent  is selectively deposited to adhere  the powder  particles of t he printing material. Once the \nmetallic piece is generated, it undergoes a subsequent process of curing and sintering to increase  its \ndensity  (Hot Isostatic Pressing) . In this work, we propose subjecting the manufactured component \nto an additional post -processing treatment involving the application of a high hydrostatic pressure \n(5000 bar ) at room temperature . This post -processing technique, so -called Cold Isostatic Pres sing \n(CIP), is shown to increase the yield load and the maximum carrying capacity of an additively \nmanufactured AISI 316L stainless steel. The mechanical properties, with and without CIP \nprocessing, are estimated by means of the Small Punch Test,  a suitabl e experimental technique to \nassess the mechanical response of small samples. In addition, we investigate the  porosity and  \nmicrostructure of the material according to the orientations of layer deposition during the \nmanufacturing process. Our observations re veal a homogeneous distribution independent of th ese \norientation s, evidencing thus an isotropic behaviour  of the material.  \n \nKeywords:  Cold isostatic pressure , metal 3D printing , Small punch test , Binder jetting . \n \n 2 1.  Introduction  \n \nAdditive Manufacturing  (AM)  is experiencing  an increasing popularity in both academic  and \nindustrial applications;  see the work by Frazier [1] for a review . Its versatility in manufacturing \nengineering components by metal deposition is making AM a feasible  alternative for the product ion \nof parts and  prototypes in different sectors, such as the biomedicine, aerospace or automotive  [2-4]. \nOne advantage of AM is the reduction in  the time elapsed from the conception of the component to \nits final form , as it does not re quire the design and  manufacturing  of special tools  unlike  other \nproduction processes  like casting or forming . Among the different AM techniques proposed, Binder \nJetting  is gaining particular traction  due to its time efficiency and its ability to generate complex \ncomponents. In this process , an inkjet print head selectively deposits a liquid binding material across \na bed of powder. Thus , the material layers are superimposed to form the desired part  while the print \nnozzle  strategically drops  the bind ing agent into  the powder  surface . Once the metallic piece is \ngenerated, it undergoes a subsequent process of curing and sintering (Hot Isostatic Pressing  - HIP) \nto achieve the desired  density.  The benefits of a HIP post -processing step in improving the \nmechanical properties of addit ively manufactured materials have been documented at large. For \nexample, Dadbakhsh  and Hao  [5] examined the role of HIP in Al composite parts generated by \nselective laser melting (SLM) , finding an increase in density and a decrease in hardness. Similarly, \nSrivastava et al.  [6] achieved a remarkable increase in density by applying HIP to a bulk metallic \nglass cast component. In addition, AlMangour et al.  [7] showed  that HIP is an effecti ve post-\ntreatment  technology  for suppressing  larger  pores and fusion defects from additively manufactured \ncomponents . Important physical insight into the benefits of HIP post -processing has also been \ngained from the numerical perspective. For example, Kim [8] used numerical creep techniques to \nmodel the HIP process in a n AISI 316L  steel by incorporating the numerous diffusion mechanisms \ntaking place into a novel constitutive model. However, the HIP post -process is typically applied in \nisolation , and the ben efits of combining HIP with other post -treatments have been scarcely explored \nin metal additive manufacturing.  \n \nIn this paper , we propose and investigate the effect of applying a high hydrostatic pressure as an \nadditional post -processing technique to further increase material density and mechanical \nperformance. The so -called Cold Isostatic P ressing (CIP) technique is used to subject metallic \nsamples to high pressures at room temper ature.  CIP post -proccesing has been used in combination \nwith HIP in the context of conventional powder metallurgy, see for example the pioneering work 3 by Ng et al.  [9], but its potential in additive manufact uring remains to be explored. We aim at filling \nthis knowledge gap by applying v ery high pressures, up to 6000 bar, by means of a new device, \nrecently patented, which has been developed based on High Pressure Processing (HPP) technology.  \nMore s pecifically,  we evaluate the mechanical properties of additively manufactured AISI 316L \nsteel samples that have been produced by means of Binder Jetting  and subsequently subjected to \nHIP and CIP at 5000 bar.  It is expected that CIP post -processing will compact the sam ples, reducing \nthe number and shape of internal defects. The capabilities of CIP in improving material performance \nare assessed by means of the Small Punch Test (SPT) . The SPT was initially developed by Baik et \nal. [10] to study the influence of radiation on the ductile -to-brittle transition temperature in metallic \nmaterials. Since then , it has been successfully employed to measure both mechanical and fracture \nproperties from small samples; see the works by Saucedo -Muñoz  et al.  [11] and Ju et al.  [12] for \nfracture toughness estimations, and the articles by Alegre et al.  [13] and Hou et al.  [14] for creep \nproperties. The  SPT is particularly suitable for this application , as the pressurised volume inside the \ncylindrical  CIP device is not sufficiently large to accommodate conventional test specimens. In \naddition to investigating the effect of CIP post -processing in AM steels, we analyse porosity and \nmaterial microstructure to assess the influence of layer orientation during manufacturing process.  \n \nThe rem ainder of this manuscript  is structured as follows. Section 2 describes the material employed \nand the microstructure analysis conducted. In Section 3, we provide details of the CIP process and \nthe subsequent small -scale mechanical tests conducted by means of the SPT. The results are \npresented and discussed in Section 4. Finally, concluding remarks are given in Section 5.  \n  4  \n2.  Material  \n \nThe present study is conducted on a stainless steel  studied by Nastac  et al.  [15], AISI 316L , which  \nhas been additively manufactured by means of the Binder Jetting method in an ExOne M-Flex metal \n3D printer. The chemical composition and mechanical properties of the 316L alloy are given in \nTables 1 and 2, respectively.  \nThe microstructure analysis reveals the exis tence of numerous pores. A representative Scanning \nElectron Microscope (SEM) micrograph is shown in Figure 1, in which the size of the pores and the \nporosity distribution can be clearly observed; arrows are used to help the eye. In addition, we \ninvestigate  possible anisotropies by conducting a microstructure analysis along the three \ncharacteristic orientations intrinsic to the Binder Jetting process: L, T, and S. These are defined as \nfollows:  \n L. Orientation defined by the advance direction of the printhead.   \n T. Orientation defined by t he direction perpendicular to the advance of the printhead, i.e. \nnormal to the L orientation.  \n S. Orientation perpendicular to the plane LT and coincident with the vertical movement of \nthe printing bed.  \n \nFigure 2 shows the microstructure f ound for each of the planes defined by the orientations L, T and \nS, i.e. LT, LS and TS. Different sizes and shapes of the pores can be obs erved as well as the grain \npattern; the grain size is found to range between 30 and 70 µm. It can be r eadily seen  that the number \nof pores is similar in every plane and that the distribution is relatively uniform. Hence, an isotropic \nbehaviour can be assumed and one can neglect  the layering effects during Binder Jetting.  \n \n3.  Methodology  \n3.1 Cold Isostatic Pr essing  \nAs described schematically in Figure 3, an HPP-based device is employed to carry out the high \nhydrostatic pressure post-processing at room temperature . This device is based on a conveniently \nmodified high pressure intensifier that is coupled to a universal testing machine. Thus, the device  is \nconnected to a universal testing machine MTS 810 with a load frame capacity of  250 kN by two \njaws, at the  top and the bottom . The device comprises  a bearing tube inside which a high -pressure \ncylin der is housed, through which a stem  passes. A  flange crossed by the stem is arranged  at the 5 lower end of the high -pressure cylinder . At the upper end there is a stopper that closes the assembly , \nleaving an interior space where the miniature specimens that are going to be pressurized are housed . \nThe lower jaw transmits the vertical movement to the stem and pressuri ses the inner chamber of the \ncylinder.  \nThe Cold Isostatic Pressing (CIP) procedure is performed in three steps: first, the hydrostatic \npressure is raised up to 5000 bar, then it is maintained for 3 minutes and, finally, the pressure is \nremoved in a controlled manner. Once the pressure unloading  has finished, the device is \ndisassembled  and the  specimens can be collected and tested.  \n \n3.2 Small Punch Testing  \nThe mechanical assessment of the effect of the CIP process on the material properties is conducted \nby means of the Small Punch Test (SPT). Extensive details of the testing equipment and the \nexperimental procedure are given in the SPT CEN code of practice [16]. As described in Figure 4, \nthe SPT  consists  on punching  a small specimen with  its outer edges embedded  by two dies . Small \nPunch Tests  have been carried out on a universal testing machine MTS Criterion 43  with 10  kN load \ncapacity.  We employ l ubrication to minimize the influence  of friction , see details  in the works by \nCuesta et al.  [17] and Martínez -Pañeda  et al.  [18]. The punch displace ment and the corresponding \napplied load are recorded  during the test , being the resulting load -displacement curve the main \noutcome of the SPT experiment . As proposed by Martínez -Pañeda  et al.  [19], the load -displacement \ncurve can be divided into different zones, each influenced by the characteristic elastic -plastic \nparameters of the material. Thus, from the punch load versus displacement curve, one can obtain a \nnumber of parameters that can then be correlated with the nominal properties of the material. Of \nparticular interest for this investigation are the load at the onset of yield  \nyP, the maximum  value of \nthe load \nmaxP , and the displacement at maximum load  Δ𝑃𝑚𝑎𝑥. \nOne of the most i nfluencing works on SPT correlations for determining the  mechanical properties \nis the one  by Mao  and Takahashi [20]. They established  a relationship between the  yield load from \nthe SPT, 𝑃𝑦, and the material  yield stress , \ny, through the empirical equation : \n (1) \nHere, \nt  is the thickness of the SPT sample (usually, 0.5 mm) , and \n  is a non -dimensional empirical \ncoefficient that is ch aracteristic of each material. In steels, 𝛼=360 . Different fitting strategies have \n2y\nyP\nt6 been proposed by García  [21] for the determination of t he yield load \nyP , i.e. the load delimiting \nzone I and zone II in the SPT typical curve; we choose  to use here the so -called  offset method.  \n \nSPT specimens have been e xtracted from a component printed by the Binder Jetting method, \nobtaining square specimens of \n210 10 mm  in each of the defined planes: LT, LS and TS. The small \nspecimens were polished to  achieve a  uniform thickness of  roughly  0.5 mm . SPT experiments are \ncarried out , at room temperature,  for 316L samples with and without CIP post -processing. The tests \nare quasi -static, with the punch displacement rate being equal to 𝑣=0.5 mm/min. The punch \ndiameter  equals  𝑑𝑝=2.5 mm, whereas the size of the lower die is characterized by a diameter of \n𝐷𝑑=4 mm and a round radius 𝑟=0.5 mm. It is important to emphasize that, due to the specimen \ndimensions, the CIP post -processing does not solely lead to volumetric strains.  \n \nThe resulting force versus displacement curves obtained from the experiment are normalized by the \nmeasured  specimen thickness 𝑡 to account for small differences  in thicknesses  that may have arisen \nafter polishing. With this objective , following  Cuesta  et al.  [22], we determine an effective load \n0.5P  \nfrom the experimentally measured load 𝑃𝑡𝑒𝑠𝑡 and the actual specimen thickness 𝑡. The normalisation \nprocedure is divided on two stages that meet at the inflection point of the curve, where zones II and \nIII of the load -displacement curve intersect. Thus, when 𝑃𝑡𝑒𝑠𝑡 is smaller than the load at the \ninflection point, 𝑃𝐼𝑁𝐹, the ef fective load is given by  \n2\n0.5 20.5test\ntest INFPP P Pt  \n (2) \nWhile the following expression  is employed f or load  normalisation after this point:  \n\n0.5 20.50.5 0.5INF test\ntest INFPt PP P Ptt    \n (3) \n \nTo ensure reproducibility, three experiments have been conducted for every combination of \nspecimen orientation (L, T, S) and post -processing treatment (with and without CIP).  \n \n4.  Results and discussion  \nThe values measured  with the Small Punch Test (SPT) for the load at yield  𝑃𝑦, the maximum value \nof the load\nmaxP , and the displacement at maximum load  Δ𝑃𝑚𝑎𝑥, are listed in Tables 3 and 4. 7 Specifically, Table 3 shows the results obtained for the samples where CIP post -processing has not \nbeen conducted while in Table 4 the results are listed for the samples sub jected to CIP; in both cases, \naverage values are also provided for each characteristic parameter of the SPT. The results reveal \nnoticeable differences between the two scenarios. Namely, applying a CIP treatment at 5000 bar \nleads to an increase in the yield  load, the maximum load and the displacement at 𝑃𝑚𝑎𝑥 of, \nrespectively, 6.5% ( 𝑃𝑦), 3.1% ( 𝑃𝑚𝑎𝑥), and 1.4% ( Δ𝑃𝑚𝑎𝑥). The significant increase in yield stress and \nultimate strength observed suggests that CIP treatments at very high pressures can t ranslate into an \nimprovement in the mechanical properties of additively manufactured steels . The application of a \nhigh hydrostatic pressure introduces plastic deformation in the vicinity of the voids, influencing \ntheir shape and size ; this has been examine d in great detail by Sket et al.  [23]. Sket and co -workers \nobserved an enhancement of about 30.5 MPa in the uniaxial stress over the plastic region  in a Mg \nAZ91 Alloy with initial yield stress of approximately 90 MPa. This larger sensitivity to the effect \nhigh hydrostatic pressures agrees with expectations, due to the different mechanical properties of \nmagnesium alloys and stainless steels  \nThe representative load versus displacement curves obtained from the SPT are shown in  Figure 5 \nfor the two scenarios considered – with and without CIP processing. The curves follow each other \nclosely until the vicinity of the maximum load, where the sample without CIP - which is expected \nto have larger pores  - exhibits a n earlier  failure  and a smaller maximum load carrying capacity . The \nfailure mechanism is the same in both cases: ductile damage characterized by the circumferential \nfracture shown in the SEM image included as inset in Figure 5. We conclude that  the use of CIP \npost-processing  to improve the mechanical performance of AM steels does not bring qualitative \nchanges but renders significant quantitative improvements in enhancing yielding and damage \nresistance.  \nRemarkab ly, two of the tested specimens suffered a premature failure in orientation L, which is \ndefined by the advance direction of the printhead. The two samples, B3 and E3, correspond to the \nLS plane. Figure 6 shows the load -displacement curve of both specimens  including also the \ncorresponding SEM images of the observed fracture. The load versus displacement curves exhibit \nevident differences with the results shown in Figure 5. With the aim of determining the \nmicromechanical origin of this different shape, different scales of the fracture surface  of specimen \nE3 are shown in Figure 7, where a mix between  ductile and intergranular failure mechanisms can \nbe observed for the L orientation. This fracture aspect is substantially different from that observed \nin eve ry other specimen , where  a ductile fracture  surface , populated with numerous dimples, is \ntypically observed; see Figure 8. The fractured area shown in Figure 7 exhibits a longitudinal aspect 8 that appears to coincide with the L direction of advance of the p rinthead. Thus, the anomalous \nbehaviour associated with samples with LS orientation is likely to be intrinsically related with the \nlack of adhesion between  binder jetting  layers.  \n \n5.  Conclusions  \n \nWe propose and assess the use of Cold Isostatic Pressing  (CIP) post-processing techniques to \ncompact samples of AISI 316L steel that have been additively manufactured (AM) by Binder Jetting. \nPressures of 5000 bar are applied by means of a novel device that builds  upon High Pressure \nProcessing (HPP) technology ; the aim is to improve the mechanical response of the AM specimens. \nDue to the limited size of the samples that can be accommodated in this device, the Small Punch \nTest is employed to characterize material performance with and without CIP post -processing.  \nSpecifically, three material parameters are extracted: the yield load 𝑃𝑦, the maximum load carrying \ncapacity 𝑃𝑚𝑎𝑥, and the failure displacement Δ𝑃𝑚𝑎𝑥. In addition, a microstructure analysis is \nconducted to identify potential anisotropies that ma y arise as a consequence of the Binder Jetting \nmanufacturing process.  \n \nOur main findings are  twofold. First, l ittle differences are observed in the microstructure along the \nthree characteristic planes intrinsic to the Binder Jetting Process. However, mech anical testing \nreveals early cracking and cleavage -like features in the LS plane samples. Second, the use of CIP \npost-processing appears to improve the mechanical performance of AM steels. Small Punch Test \nmeasurements show that, when the CIP technique is employed as an additional post -processing of \nthe material, there is a 6.5% increase in the yield resistance , a 3.1% increase in the critical load , and \na 1.4%  increase  in the displacement to failure. In addition, t his enhancement due to CIP appears to \nbe se nsitive to the  characteristic orientations intrinsic to the Binder Jetting process. Future work will \ninvolve gaining further insight into this effect.  \n \n6.  Acknowledgments  \n \nThe authors wish to thank the funding received from the Ministry of Education of the Regional \nGovernment of Castile and Leon under the auspices of the support for the Recognized Research \nGroups of public universities of Castile and Leon started in 2018, Project: BU033G18.  The SEM 9 images  were performed in the Microscopy and Microcomputed To mography laboratory at CENIEH \nfacilities with the collaboration of CENIEH Staff . \n \n \n7.  References  \n \n[1] Frazier , W. E.,  2014.  Metal additive manufacturing: a review. Journal of Materials Engineering \nand Performance 23 1917 -1928.  \n[2] Akhoundi, B., Behravesh, A.H. , 2019.  Effect of Filling Pattern on the Tensile and Flexural \nMechanical Properties of FDM 3D Printed Products . Experimental Mechanics , In Press.  \n[3] O.R. Bilal, A . Foehr, C. Daraio , 2017. Observation of trampoline phenomena in 3D -printed \nmetamaterial plates . Extreme Mechanics Letters  15 103-107. \n[4] Somireddy,  M., Singh, C.V., Czekanski, A., 2019. Analysis of the Material Behavior of 3D \nPrinted Laminates Via FFF . Experimental Mechanics, In Press.  \n[5] Dadbakhsh , S., Hao , L., 2012. Effect of hot isostatic pressing (HIP) on Al composite parts made \nfrom laser consolidated Al/Fe2O3 powder mixtures . Journal of Materials Processing Technology  \n212 2474 -2483 . \n[6] Srivastava , A.P., Tong, M., Ștefanov,  T., Browne,  D.J., 2017 . Elimination of porosity in bulk \nmetallic glass castings using hot isostatic pressing. Journal of Non -Crystalline Solids 468 5 –11. \n[7] AlMan gour, B., Grzesiak , D., Yang , J.M., 2017. Selective laser melting of TiB2/H13 steel \nnanocomposites: Influence of hot isostatic pressing post -treatment . Journal of Materials Processing \nTechnology  244 344-353. \n[8] Kim, H.S., 2002. Densification mechanisms during hot isostatic pressing of stainless steel \npowder compacts . Journal of Materials Processing Technology  123 319-322. \n[9] Ng, L.S., Loh , N.L., Boey  F.Y.C., 1997. Cold -hot isostatic pressing of Mar M200 superalloy \npowders . Journal of Materials Processing Technology  67 143-149. \n[10] Baik , J.M., Kameda , J., Back , O., 1983.  Small Punch Test evaluation of intergranular \nembrittlement of an alloy steel, Scr. Metall. et Mater. 17 1443 -1447.  \n[11] Saucedo -Muñoz , M.L., Liu  S.C., Hashida , T., Takahashi , H., Nakajima , H., 2001.  \nCorrelationship between JIc and equivalent fracture strain determined by small -punch tests in JN1, \nJJ1 and JK2 austenitic stainless steels, Cryogenics 41  713-719. 10 [12] Ju, J-B., Jang, J., Kwon , D.,2003.  Evaluation of fracture toughness by small -punch testing \ntechniques using sharp notched specimens . International Journal of Pressure Vessels and Piping 80 \n221–228. \n[13] Alegre, J.M., Cuesta, I.I., Lorenzo, M.,2014. An Extension of the Monkman -Grant Model for \nthe Prediction of the Creep Rupture Time Using Small Punch Tests . Experimental Mechanics, 54 \n1441 -1451.  \n[14] Hou, F., Xu , H., Wang , Y., Zhang , L.,2013.  Determination of creep property of 1.25Cr0.5Mo \npearlitic steels by small punch test, Eng. Fail. Anal. 28 215 -221. \n[15] Nastac, M., Lucas, R., Klein, A., 2017. Microstructure and mechanical properties comparison \nof 316l parts pr oduced by different additive manufacturing processes, Solid Freeform Fabrication: \nProceedings of the 28th Annual International, pp. 332 -341. \n[16] CEN Workshop Agreement , 2007 , CWA 15627:2007 D/E/F, Small Punch Test Method for \nMetallic M aterials, CEN, Brussels Belgium . \n[17] Cuesta,  I.I., Alegre , J.M., 2011 . Determination of the fracture toughness by applying a \nstructural integrity approach to pre -cracked Small Punch Test specimens . Engineering Fracture \nMechanics  78 289-300. \n[18] Martínez -Pañeda, E., García, T.E., Rodríguez, C., 2016a. Fracture toughness characterization \nthrough notched small punch test specimens. Materials Science and Engineering A 657 422 -430. \n[19] Martínez -Pañeda , E., Cuesta , I.I., Peñuelas , I., Díaz , A., Alegre , J.M., 2016b . Damage modeling \nin small punch test specimens. Theoretical and Applied Fracture Mechanics 86 51 -60. \n[20] Mao, X., Takahashi , H., 1987. Development of a further -miniaturized specimen of 3 mm \ndiameter for tem disk small punch tests, J. Nucl. Mater. 150 42 -52. \n[21] García, T.E., Rodríguez, C., Belzunce, F.J., Suárez, C.,  2014. Estimation of the mechanical \nproperties of metallic materials by means of the small punch test, J. of Alloys and Compounds 582 \n708-717. \n[22] Cuesta, I.I., Alegre, J.M., 2012. Hardening evaluation of stamped aluminium alloy components \nusing the Small Punch Test, Eng. Failure Anal ysis 26 240 –246. \n[23] Sket, F., Fernández, A., Jérusalem, A., Molina -Aldareguía, J.M., Pérez -Prado, M.T., 2015. \nEffect of hydrostatic pressure on the 3D porosity  distribution and mechanical behavior of a high  \npressure die cast Mg AZ91 alloy , Metallurgical and materials transactions A  46A 4056 -4069.  \n \n \nTable 1. Chemical composition of  AISI 316L  steel studied by Nastac et al. [1 5]. 11 Element  Cr Ni Mo Mn Si P C S Fe \nwt. %  16.0-18.0 10.0-14.0 2.0-3.0 Max 2  Max 1  Max 0.04  Max 0.03  Max 0.03  balance  \n \n  12  \nTable 2. Mechanical properties of  AISI 316L  steel studied by Nastac et al. [12].  \nYield strength \n𝜎𝑌 (MPa)  Ultimate tensile \nstrength 𝜎𝑈𝑇𝑆 \n(MPa)  Elongation at \nbreak, %  Hardness (HRB)  Density (g/ 𝑐𝑚3) \n214 517 43 66 7.7 \n \n \n  13  \nTable 3. Characteristic SPT parameters \nyP , \nmaxP  and \nmaxP  for the  SPT specimens without  CIP. \nOrientation  Specimen  \nyP  (kN) \nmaxP  (kN) \nmaxP  (mm)  \nTS A1 0.193  1.749  2.147  \nA2 0.195  1.817  2.073  \nA3 0.206  1.883  2.210  \nA4 0.209  1.878  2.101  \nA5 0.205  1.801  1.985  \nLS B1 0.214  1.804  2.213  \nB2 0.185  1.851  2.226  \nB3 - - - \nLT C1 0.201  1.807  2.257  \nC2 0.204  1.914  2.286  \nC3 0.205  1.914  2.225  \nAverage:  0.202  \n±0.008  1.841 \n±0.055  2.170 \n±0.094  \n \n  14  \nTable 4. Characteristic SPT parameters \nyP , \nmaxP  and \nmaxP  for the  SPT specimens with  CIP. \nOrientation  Specimen  \nyP  (kN) \nmaxP  (kN) \nmaxP  (mm)  \nTS D1 0.212  1.945  2.268  \nD2 0.215  1.828  2.214  \nD3 0.231  1.840  2.159  \nD4 0.233  1.929  2.163  \nLS E1 0.208  1.842  2.156  \nE2 0.203  1.952  2.259  \nE3 - - - \nLT F1 0.209  1.808  2.180  \nF2 0.212  1.943  2.180  \nF3 0.217  2.061  2.316  \nAverage:  0.215 \n±0.010 1.904 \n±0.082 2.210 \n±0.058 \n \n \n \n \n  15  \n \n \nFigur e 1. Porosity distribution in AISI 316L manufactured using Binder Jetting.  \n  \n16     \n                        \n \nFigure 2. Orientations and microstructure in AISI 316L manufactured using Binder Jetting.  \n  \nL\nST\nT\nL\nS\nL\nS\nT17  \nFigure 3. Schematic description of the device to carry out the high hydrostatic pressure post -\nprocessing at room temperature.  \n  \n18   \n \nFigure 4. Typical load -displacement curve and s chematic description of the experimental setup \ninvolved in the Small Punch Test.  \n  \nPunch Displacement ( mm)0.0 0.5 1.0 1.5 2.0 2.5Punch Load ( kN)\n0.00.51.01.52.0\nPy\n\"t/10 offset method\"(\nPmax , Pmax)\nZone I\nZone IIZone IIIZone IV\nZone V\npd\ndD\nr\npunch\nspecimen\nlower die\nupper die19  \n \nFigure 5. SPT load -displacement curves (TS orientation)  for the AISI 316L  with and without CIP.  \n  \n20  \nFigure 6. SPT load -displacement curves for specimens showing premature fracture in L direction : \na) B3 specimen and  b) E3 specimen . \n  \n21   \n \n \nFigure 7. Aspect of premature fracture (ductile -inter granular) in the SPT specimen E3.  \n22  \n \n \nFigure 8. Aspect of typical ductile fracture in SPT specimens.  \n \n \n",
      "metadata": {
        "filename": "Cold Isostatic Pressing to Improve the Mechanical Performance of Additively Manu.pdf",
        "hotspot_name": "EMC_Shield_Production",
        "title": "Cold Isostatic Pressing to Improve the Mechanical Performance of\n  Additively Manufactured Metallic Components",
        "published_date": "2019-08-08T10:28:28Z",
        "pdf_link": "http://arxiv.org/pdf/1908.03003v1",
        "query": "steel punching and bending process optimization sustainable manufacturing"
      }
    },
    "Power consumption prediction for steel industry": {
      "full_text": "Proceedings of the IISE Annual Conference  & Expo 202 3 \nWT Al-shaibani .Tareq Babaqi and Abdulraqeeb Alsarori  \n \nPower consumption prediction for steel industry  \n \nWT Al -shaibani.Tareq Babaqi and Abdulraqeeb Alsarori  (al-shaibani18 @itu.edu.tr )  \n \nAbstract  \n \nThe use of steel is essential in many industries, including infrastructure, transportation, and modern architecture. \nPredicting power consumption in the steel industry is crucial to meet the rising demand for steel and promoting  city \ndevelopment. However, predicting energy consumption in the steel industry is challenging due to several factors , such \nas the type of steel produced, the manufacturing process, and the efficiency of the manufacturing facility. This research \naims  to contribute by creating a predi ctive model that estimates power consumption in the steel industry. The unique \napproach combines linear regression to predict a continuous variable related to power consumption and the KNN \nclustering method to identify the demanding load type. This study's  novelty lies in the development of a model that \naccurately predicts energy consumption in the steel industry, leading to more sustainable and efficient practices. This \nresearch contributes to enabling  industries to anticipate and optimize their energy con sumption, leading to more \nsustainable practices and economic development.  \n \nKeywords  \nMachine learning, Prediction, Classification, Power consumption  \n \n1. Introduction  \nFor many years, accurate prediction of energy consumption in the industrial sector has been a critical issue. Precise \npredictions can help industries manage their energy usage, reduce costs, and increase efficiency. Historically, \nstatistical models were us ed to forecast industrial energy consumption by identifying trends and patterns in historical \ndata. However, these models were often inaccurate as they did not account for changes in production levels or weather \nconditions. In recent years, machine learnin g algorithms have become popular for energy consumption prediction due \nto their ability to analyze large datasets and make more precise predictions. These algorithms can be trained on various \nfactors, such as production levels, weather conditions, and equi pment efficiency, to forecast energy consumption \naccurately . In addition, physics -based models that simulate industrial energy consumption by taking into account \nprinciples of heat transfer, thermodynamics, and fluid dynamics have also been used . Still, th ey require significant \ncomputational resources and expertise to develop and implement. Overall, industrial energy consumption prediction \nhas made significant progress, and advanced technologies such as machine learning are  improving the accuracy and \nreliability of energy consumption predictions.  \nThis research paper aims  to create a predictive model for estimating power consumption in the steel industry. The \nresearch team used linear regression and found that CO2 was the dom inant feature, with lagging current coming in \nsecond, to predict power consumption usage successfully . The research is structured into several sections, including \nan introduction that provides a brief overview and in -depth insight into the research problem . This is followed by \nsections on the data and models used to accomplish the work, with Section 4 discussing the results. An evaluation of \nthe findings is presented before concluding the research.   \n \n2. Data Description   \nThis section discusses the process of g athering and processing datasets used for the machine learning model. The data \nwas obtained by querying the cloud -based information storage system owned and operated by Korea Electric Power \nCorporation [2]. The dataset comprises daily, monthly, and annual data and was used to compile the final dataset. \nStandard practice in machine learning involves dividing the data into two sets: a training set and a test set. The machine \nlearning model is trained on the training set, while the performance of the model aft er training is evaluated using the \ntest set. Table 1 provides an overview of the dataset features.  \nThe first step in data processing was to check for null values. The null values were counted and visualized to ensure \nthey were accurately identified. Furthe rmore, a final check was performed to verify that question marks were correctly \nidentified as null values. Figure 2.1 shows that the dataset features contain no missing values.  \nA normality check was performed to determine whether the dataset was normally d istributed. A histogram was plotted \nto assess the distribution of the data visually . The normal  distribution is essential because many statistical tests and \nmethods assume normally distributed data . If the data is not normally distributed, the results of t hese tests and methods \nmay be inaccurate. Figure 2.2 depicts the histogram plots for the dataset features.   \n Table 2.1:  Dataset feature description  \n \nFigure 2.1: The dataset features contains no missing values.  \n \n \n \nFigure 2.2: Normality check for dataset features  \n \nA heatmap was used to visualize the relationship between different variables in the dataset. Heatmapping is a graphical \nrepresentation of data where the individual values in a matrix are represented as colors. The correlation heatmap \ndisplays the magnitude of a linear relationship that exists between two or more variables. Figure 2.3 shows the \ncorrelation between different variables with respect to power usage. The highest correlation was found between CO2 \nand lagging current features beca use of their relation to power usage.  \n \nDate  Data collected in real time on the first of the  month  \n \nUsage_kWh  Energy Consumption in Industry kWh continuous \nLagging  Current  Reactive energy kVarh  Continuous  \n \nLeading  Current  Reactive energy kVarh  Continuous \nCO2  CO2 Continuous ppm  \n \nNSM  Minutes and seconds since midnight S C ontinuous  \n \nWeek status  Weekday or  Weekend  \n \nDay of week  Sunday, Monday  ..etc \n \nLoad Type  Light Load, Medium Load, Maximum  Load  \n    Feature  Description  \n \n  \nFigure 2.3: The correlation relation between each two features.  \n \n \n3. Models  \nSince the objective of this research is to predict a continuous variable, namely energy consumption, and to identify \nthe type of load, the continuous variable is energy consumption. The best model for this type of problem is typically \nlinear regression, and the KNN model has also been applied to determine the load type [3].  \n3.1. Linear regression  \nLinear regression is a statistical method used to m odel the relationship between a dependent variable and one or more \nindependent variables. It is a type of regression analysis that utilizes a straight line to estimate the relationship between  \na dependent variable and one or more independent variables. On the basis of the values of the independent variables, \nthe equation of the line is used to make predictions about the dependent variable. The dependent variable in linear \nregression is continuous, meaning it can take any value within a range. The independen t variables can be of any type, \nincluding continuous or categorical variables.  \ny≡𝛽0+𝛽1𝑋i (3.1)  \nWhere y is the dependent variable, X i is the in dependent variable s. B0 is the intercept , and B1 is the coefficients or \nslops.  \n  \n3.2. Linear regression with normalization  \nThe process of rescaling data so that it has a mean of 0 and a standard deviation of 1 is known as normalization. If the \nvariables in the dataset have different scales, this can h elp to make the regression results more interpretable. To \nperform linear regression with normalization, the data must first be normalized using the following formula 3.2:  \n𝑥−norm=𝑥−mean⁡(𝑥)\nstd⁡(𝑥) (3.2) \nwhere \"x\" is the original variable, \"mean(x)\" is the variable's mean, and \"std(x)\" is the variable's standard deviation. \nOnce the data has been normalized, we can perform linear regression as usual, with the normalized variables serving \nas the independent and dependent variables. Line equation 3.1 will be the same as in regular linear regression, but it \nwill be a  but normalized data rather than original data.  \n \n3.3. Ridge regression  \nRidge regression is a regression analysis that is used to examine the relationshi p between one or more independent \nvariables and a dependent variable. It is a regularized linear regression in which a penalty term is introduced into the \n \n model to prevent overfitting and improve the findings' interpretability. The purpose of ridge regress ion is to discover \nthe coefficient values, or \"weights\" assigned to each independent variable, that minimize the residual sum of squares, \nor the sum of the squared differences between the observed and predicted values, subject to a limit on the size of the  \ncoefficients. This limitation is known as the \"shrinkage penalty,\" and it is represented by the word \"λ\" in the ridge \nregression equation. Ridge regression coefficients are determined by solving the optimization problem shown in 3.3:  \n𝑚𝑖𝑛[Σ(𝑦−𝑥𝛽)2+𝜆Σ𝛽2] (3.3) \nRidge regression is beneficial because, by putting a penalty on the magnitude of the coefficients, it can help to prevent \noverfitting and enhance model interpretability. This can make the results more stable and dependable  and make the \nlink between the variables in the model clearer to grasp.  \n \n3.4. LASSO  \nThe Least Absolute Shrinkage and Selection Operator is a regularized regression analysis technique used to investigate \nthe relationship between a dependent variable an d one or more independent variables. It is similar to ridge regression \nin that the penalty term is the total of the absolute values of the coefficients rather than the sum of the squared \ncoefficients. The LASSO regression coefficients are determined by sol ving the optimization problem shown in 3.4:  \n \n𝑚𝑖𝑛[∑(𝑦−𝑋𝛽)2+𝜆∑|𝛽|] (3.4) \nLASSO regression is effective in determining which independent variables are most essential in predicting the \ndependent variable. In the optimization problem, the penalty term promotes the coefficients of less relevant variables \nto be decreased to zero, essentially eliminating them from the model. This can help to make the model more \ninterpretable and prevent overfitting.  \n \n3.5. KNN  \nKNN is an abbreviation for \"K -Nearest Neighbors.\" It  is an instance -based or lazy learning method in which the \nfunction is only approximated locally and all computation is postponed until classification. In other words, the \nalgorithm does not explicitly develop a model, but saves the training data and waits  for a prediction request. When a \nprediction is required, the algorithm locates the nearest training data  points (the \"nearest neighbors\") and makes the \nforecast using their labels. It is a straightforward and efficient approach to classification and regre ssion problems.  \n \n4. Results and Discussion  \nThis section shows the outcomes from training the models stated in the Models section. Each model has been trained \nand its outcomes have been recorded, thus this part will exhibit and discuss these results.  \nThe CO2 was shown to be the most important feature in the linear regression. The correlation value from the heatmap \nin figure 2.3 might be used to double -check this conclusion. The feature coefficient values are shown in Table 4.1  \n \nTable 4.1:  Lin ear regression coefficients  \n \nFeature  Coefficient value  \nLagging Current Reactive Power kVarh  0.289401  \nLeading Current Reactive Power kVarh  0.116304  \nCO2(tCO2)  1694.237843  \nLagging Current Power Factor  0.114722  \nLeading Current Power Factor  0.064895  \nNSM  0.000004  \nWeek Status  0.089524  \nLoad Type  0.458500  \nFriday  -0.130716  \nMonday  -0.118452  \nSaturday  -0.102274  \nSunday  0.012750  \nThursday  -0.143821  \nTuesday  0.528630  \nWednesday  -0.046117  \n \nCO2 was also shown to be an important feature in the linear regression with normalization, but with less importance \ncompared to the previous result. The feature coefficient values are shown in Table 4.2. As previously mentioned, ridge \nis a shrinking method , and Figure 4.1 depicts the shrinkage process as the penalty increases and the coefficient  \n decreases towards zero. LASSO, on the other hand, displays more interesting results. It shows that CO2 is the most \nsignificant contributor, followed by lagging curr ent reactive power. Table 4.4 displays the feature coefficient values \nfor LASSO. Figure 4.2 shows the shrinkage process as the penalty increases and the coefficient decreases towards \nzero, as previously mentioned. Table 4.2: Linear regression with normaliza tion coefficients  \nFeature  Coefficient value  \nLagging Current Reactive Power kVarh  0.500906  \nLeading Current Reactive Power kVarh  -0.455309  \nCO2(tCO2)  1413.114801  \nLagging Current Power Factor  0.064763  \nLeading Current Power Factor  -0.035739  \nNSM  -0.007579  \nWeek Status  -1.212324  \nLoad Type  1.324652  \nFriday  -0.164516  \nMonday  -0.285530  \nSaturday  -1.349013  \nSunday  -1.467378  \nThursday  -0.179278  \nTuesday  -0.457353  \nWednesday  -0.125647  \n \nTable 4.3: Ridge regression coefficients  \nFeature  Coefficient value  \nLagging Current Reactive Power kVarh  1.630267  \nLeading Current Reactive Power kVarh  -0.813496  \nCO2(tCO2)  0.015020  \nLagging Current Power Factor  0.140572  \nLeading Current Power Factor  -0.141011  \nNSM  -0.013395  \nWeek Status  -0.111796  \nLoad Type  0.013755  \nFriday  -0.011596  \nMonday  -0.001977  \nSaturday  0.014301  \nSunday  0.099641  \nThursday  -0.074901  \nTuesday  -0.096383  \nWednesday  0.073061  \n \nTable 4. 4: LASSO coefficients  \nFeature  Coefficient value  \nLagging Current Reactive Power kVarh  0.003053  \nLeading Current Reactive Power kVarh  0.000000  \nCO2(tCO2)  0.004998  \nLagging Current Power Factor  0.000000  \nLeading Current Power Factor  0.000000  \nNSM  0.000000  \nWeek Status  0.000000  \nLoad Type  0.000000  \nFriday  0.000000  \nMonday  0.000000  \nSaturday  0.000000  \nSunday  0.000000  \nThursday  0.000000  \nTuesday  0.000000  \nWednesday  0.000000  \n \n  \n   \nFigure 4.1: Ridge shrinkage process.  Figure 4.2: LASSO shrinkage process.  \n \nThe KNN methodology has been applied in this instance. First, the optimal value for k was determined by iterating \nthe problem 40 times and selecting the solution with the least  error. The load type had then been predicted using the \nKNN approach, with the o ptimal founded value of k being utilized in the process. Figure 4.3: illustrates the KNN \nprocess to find the optimal value of k [4 -6]. \n \nFigure 4.3: Illustrates the Error Rate vs. K Value  \n \n5. Evaluation  \nIn the field of machine learning, regression is a tas k that involves predicting a continuous value based on input data, \nand KNN and linear regression are algorithms that are frequently used for this purpose. To assess the efficacy of a \nlinear regression model's predictions, several metrics such as mean absol ute error, mean squared error, and R -squared \ncan be utilized. The mean absolute error (MAE) measures the average magnitude of errors in a model's predictions, \nwithout considering the direction of the errors. It is computed as the average of the absolute di screpancies between \nthe anticipated and actual values.  \nThe mean squared error (MSE) is similar to the MAE, but it takes into account the direction of errors. It is determined \nby taking the average of the squares of the discrepancies between the anticipated  and actual values. The R -squared \nstatistic, also known as the coefficient of determination, evaluates a model's ability to predict future outcomes. It \nrepresents the fraction of the total variance in the true values that can be accounted for by the model.  An R -squared \nvalue of 0 indicates that the model does not explain any of the variation, while a value of 1 indicates that the model \ncorrectly explains all of the variance in the true values.  \nThese measures can be used to compare the performance of differe nt linear regression models or to track the \nperformance of a single model over time as it is trained and improved. Table 1 presents the evaluation of the different \nmodels used in this project. It is clear that the Ridge model has the best performance among  all of the models used.  \n \nTable 4.4: Evaluation metrics for the models used in the project  \n Accuracy Score  Mean Absolute Error  Mean Squared Error  Root Mean Squared Error  \nLR 0.981299  2.529006  20.516540  4.529519  \nRidge  0.992049  0.000143  0.000001  0.001032  \nLASSO  0.978588  0.000367  0.000003  0.001693  \nKNN  0.917333  0.089612  0.103501  0.321715  \n \n6.  Conclusions  \nIn conclusion, our study demonstrates that linear regression is an effective approach for estimating power \nconsumption, and the KNN model has been used to accurately categorize different types of loads. When evaluating \nthe models used in this project, it is clear that Ridge regression had the best performance based on the chosen \nevaluation measures. Our results also indicate that CO2 is the most  important feature for predicting power \nconsumption, followed by lagging current.This study has important implications for the energy sector, as accurate \npower consumption prediction can help improve energy management and reduce costs. Our findings can als o serve as \na basis for further research and development of more accurate and efficient energy forecasting models.  \n \n \n References   \n[1] Korea Electric Power Corporation [available online]  https://home.kepco.co.kr/kepco/EN/main.do  \n[2] Scikit -learn: Machine Learning in P ython, Pedregosa et al., JMLR 12, pp. 2825 -2830,  2011.  \n[3] Sathishkumar  V E, Changsun  Shin,  Youngyun  Cho,  â€œEfficient  energy  consumption  prediction  model  \nfor a data analytic -enabled industry building in a smart cityâ€ , Building Research & Information, Vol. 49. \nno. 1, pp. 127 -143, 2021.  \n[4] Sathishkumar  V E, Myeongbae  Lee, Jonghyun  Lim,  Yubin  Kim,  Changsun  Shin,  Jangwoo  Park,  \nYongyun Cho, â€œAn Energy Consumption Prediction Model for Smart Factory using Data Mining \nAlgorithmsâ€ KIPS Transactions  on Software and Data Engineering, Vol. 9, no. 5, pp. 153 -160, 2020.  \nTransactions on Software and Data Engineering, Vol. 9, no. 5, pp. 153 -160, 2020.  \n[5] Sathishkumar V E, Jonghyun Lim, Myeongbae Lee, Yongyun Cho, Jangwoo Park, Changsun Shin, and \nYongyun Cho, â€œIndustry Energy Consumption Prediction Using Data Mining Techniquesâ€ , \nInternational Journal of Energy Information and Communications, Vol. 11, no. 1, pp. 7 -14, 2020.  ",
      "metadata": {
        "filename": "Power consumption prediction for steel industry.pdf",
        "hotspot_name": "EMC_Shield_Production",
        "title": "Power consumption prediction for steel industry",
        "published_date": "2023-07-14T19:41:53Z",
        "pdf_link": "http://arxiv.org/pdf/2307.07597v1",
        "query": "steel punching and bending process optimization sustainable manufacturing"
      }
    },
    "DRL-Based Injection Molding Process Parameter Optimization for Adaptive and Prof": {
      "full_text": "DRL -Based Injection Molding Process Parameter \nOptimization for Adaptive and Profitable Production  \nJoon -Young Kim 1,2*, Jecheon Yu 1*, Heekyu Kim 1, Seunghwa Ryu 1‡ \n1 Department of Mechanical Engineering, Korea Advanced Institute of Science and Technology \n(KAIST), Daejeon, Republic of Korea  \n2 Industrial Intelligence Research Group, AI/DX Center , Institute for Advanced Engineering (IAE) , \nYongin , Republic of Korea  \n \nAbstract:  Plastic injection molding remains essential to modern manufacturing . However, \noptimizing process parameters to balance product quality and profitability under dynamic \nenvironmental and economic conditions remains a persistent challenge. This study presents a novel \ndeep reinforcement learning (DRL) -based framework for real -time process optimization in \ninjection molding, integrating product quality and profitability into the control objective.  A profit \nfunction was developed to reflect real -world manufacturing costs, incorporating resin, mold wear, \nand electricity prices, incl uding time -of-use variations. Surrogate models were constructed to \npredict product quality and cycle time, enabling efficient offline training of DRL agents using soft \nactor-critic (SAC) and proximal policy optimization (PPO) algorithms. Experimental results \ndemonstrate that the proposed DRL framework can dynamically adapt to seasonal and operational \nvariations, consistently maintaining product quality while maximizing profit.  Compared to \ntraditional optimization methods such as genetic algorithms, the DRL models achieved comparable \neconomic performance with up to 135 × faster inference speeds, making them well -suited for real -\ntime applications. The framework’s scalability and adaptability highlight its potential as a \nfoundation for intelligent, data -driven decision -making in modern manufacturing environments.  \n \nKeywords:  Plastic in jection molding ; Process parameter  optimization ; Profit  optimization ; \nAdaptive production systems ; Deep reinforcement learning  (DRL) ; Proximal policy optimization  \n(PPO) ; Soft actor-critic (SAC)  \n  1    INTRODUCTION  \n1.1    Research Background  \n Plastic injection molding is a cornerstone of modern manufacturing, enabling the mass \nproduction of plastic components with high precision, repeatability, and efficiency [1,2]. Its \napplications span various  industries, including automotive, electronics, medical devices, and \nconsumer goods, where lightweight and durable plastic components have replaced mainly  \ntraditional materials such as  metal and glass. The capacity to produce complex geometries at scale \nwith minimal material waste has further established injection molding as a vital manufacturing \nprocess [3]. \n Despite widespread adoption, the injection molding industry operates under intense cost \npressures due to its high -volume, low -margin nature. Manufacturers must maintain a delicate \nbalance between ensuring product quality and optimizing operational costs [4]. Inefficiencies such \nas excessive material usage, prolonged cycle times, and elevated electricity  consumption can \nsignificantly erode profit margins  [5–8]. This challenge is exacerbated by external factors , \nincluding  fluctuations in raw material prices, electricity costs, and seasonal variations, all of which \ncan impact the overall profitability of injection molding operations [9,10] . \n Traditionally, process parameters in injection molding, such as injection pressure, mold \ntemperature, cooling time, and holding pressure, have been optimized with a primary focus on \nproduct quality.  Numerous studies have investigated the influence of these parameters on defect \nrates, mechanical performance, and surface finish [11]. In recent years, statistical models and \nmachine learning techniques have been increasingly adopted to predict optimal process conditions \nfor consistent product quality  [12–16].  \n However, these methodologies fail to address a pivotal element of dynamic cost fluctuations in \nreal-world manufacturing contexts.  These methodologies  optimize for quality but do not account \nfor the financial impact of changing electricity  prices, equipment operation , and maintenance costs. As a result, manufacturers may achieve high -quality production but fail to maximize profitability, \nleading to potential financial losses despite optimal technical performance. For example, electricity \npricing varies significantly based on time of use, with  peak-hour electricity  costs often being \nconsiderably higher than off -peak rates [17–20]. In regions where electricity pricing follows a \ndynamic pricing  structure, operating injection molding machines during peak hours without \nadjusting process parameters accordingly can lead to unnecessarily high production costs. \nEnvironmental conditions such as ambient temperature and relative humidity can also affect \nmachine performance, influencing cycle times and cooling efficiency [21–26]. When process \nsettings remain static, they cannot respond effectively to these external variations, leading to \nincreased electricity  consumption and reduced cost -effectiveness.  \n To overcome these limitations, developing a decision -making model capable of dynamically \nadjusting process parameters in response to real -time operational and environmental conditions is \nessential . This necessitates transitioning  from conventional static optimization methods to an \nadaptive, real -time decision -making approach.  Reinforcement learning  (RL)  methods present a \npromising solution because they allow systems to learn continuously from operational feedback \nand to improve decision -making strategies based on changing cost and quality considerations  [27–\n29]. \n This study proposes a deep reinforcement learning (DRL) -based framework for optimizing \ninjection molding process parameters with the dual objectives of ensuring product quality and \nmaximizing profitability. Traditional optimization approaches have primarily focused on reducing \ndefects under fixed operating conditions, often overlooking the e conomic impact of dynamic \nvariables such as electricity pricing, resin costs, and mold wear. To address this gap, the proposed \nframework integrates cost -related factors directly into the optimization process, enabling adaptive \ncontrol in variable manufactu ring environments  [30–33].   The framework employs soft actor-critic (SAC)  [34,35]  and proximal policy optimization \n(PPO) [34] algorithms , which are well -suited for continuous control tasks with stability and sample \nefficiency. Surrogate models were developed using production data collected through a Design of \nExperiments (DOE) methodology to predict product quality and cycle time. These models allow \nthe DRL agent to be trained offline in a simulated environment that closely reflects real -world \nprocess dynamics. Once deployed, the agent optimizes process parameters in real time, adjusting \nto fluctuating environmental and cost conditions within sub -second inference time.  \n \nFig. 1. Framework  for real -time process parameter optimization using deep reinforcement \nlearning considering environmental variations in injection molding processes.  \n \n As illustrated in Fig. 1 , the proposed framework achieves profitability levels comparable to \nglobal optimization methods such as genetic algorithms while offering over 100 -fold faster \ndecision -making. This approach not only supports real -time manufacturing control but also \ncontributes to the development of intelligent, cost -aware production systems that are scalable and \napplicable across various industrial contexts. The results demonstrate the feasibility of DRL as a \npractical solution for sustainable, economically optimized manufacturing operations.  \n The structure of this paper is as follows. Section 1.2  presents a comprehensive review of \nprior studies on the application of Deep Learning (DL) and RL for injection molding process \noptimization . Section 2  outlines the problem definition, incorporating the formulation of the \nprofit function and the Markov Decision Process (MDP)  of DRL -based decision -making \nmodel s. It also introduces the surrogate models used for quality classification and cycle time \nprediction, as well as the theoretical foundations of PPO and SAC.  Section 3  describes  the \nexperimental results, including  data acquisition, surrogate model evaluation, and DRL  agent \nfor process parameter optimization. This section also provides a comparative analysis of PPO \nand SAC, benchmarked against a Genetic Algorithm (GA), and presents deployment results of \nthe DRL agents under three different seasonal scenarios i n a virtual production environment.  \nSection 4  discusses the implications, limitations, and future research directions of the proposed \nDRL framework . Finally, Section 5  summarizes key findings and  highlights their implications \nfor the injection molding industry.  \n \n1.2    Related works  \n Injection molding is a complex manufacturing process that requires precise control over \nparameters such as injection pressure, mold temperature, cooling time, and holding pressure to \nensure both product quality and operational efficiency. Traditional optim ization approaches, \nincluding DOE and Response Surface Methodology (RSM), have been extensively employed to identify optimal process settings and improve performance. DOE has been successfully applied in \noptimizing injection molding for various applications such as automotive components, micro -\ninjection processes, and geometrically complex parts [34–36]. These studies have demonstrated \nimprovements in defect reduction, dimensional accuracy, and part stability. DOE has also been \nused to minimize contour distortion in composite components and to enhance structural integrity \nin molded products  [37]. A combined simu lation and experimental approach has validated DOE -\ndriven parameter selection for thin -shell plastic parts, ensuring dimensional consistency and defect \nreduction [38]. RSM, particularly when integrated with advanced optimization techniques, has \nfacilitated fine -tuning of process parameters  [39]. For instance, combining RSM with nonlinear \nprogramming and Artificial Neural Networks (ANNs) has enabled the precise optimization of \nmolding processes for plastic lenses and reinforced polycarbonate composites, resu lting in better \nmechanical properties and reduced variability [40,41]. However, these methods generally require \nextensive experimental trials and may be limited in their ability to capture the nonlinear and \ndynamic nature of modern injection molding systems  [42,43].  \n With the advancement of computational intelligence, machine learning (ML) and deep learning \n(DL) techniques have emerged as powerful tools for modeling complex interactions between \nprocess parameters and product outcomes.  One study applied various DOE methods with artificial \nneural networks (ANN) to assess their impact on modeling efficiency and accuracy  [44]. Another \ndeveloped a regression -based ML system for real -time defect prediction, improving production \nefficiency and reducing waste  [45]. ML has also been used to reveal links between processing \nconditions, structural features, and material properties in polypropylene moldi ng, enabling \nparameter optimization for better mechanical performance [46]. In addition, supervised learning \nhas been employed to predict cooling time, aiding in production planning and cycle time reduction  \n[47]. DL techniques have addressed class imbalance in fault detection, enhancing defect \nclassification and process control [48], while DL -based optimal tracking control methods have been proposed to monitor resin flow in molding machines  [49]. These studies highlight the growing \nrole of deep learning in optimizing injection molding, improving accuracy in defect detection, \nparameter prediction, and process stability, ultimately leading to better product quality and \nmanufacturing efficiency .  \n RL is a method in which an agent interacts with an environment to learn an optimal policy for \nmaximizing rewards without requiring expert knowledge, making it an attractive approach for \nvarious applications. Similarly, DRL has emerged by combining RL with deep neural networks, \nallowing agents to learn effectively in broader and more complex state and action spaces [30,35] . \nAdvances in DRL have gained significant attention by demonstrating impressive performance in \nhighly complex games such as Go [36] and StarCraft II [37]. Recently, DRL has also been actively \napplied in the manufacturing domain. For instance, its applications have been reported in areas \nsuch as system design [38–40], process planning [41,42]  process control and optimization [43–46] \nand quality control [47,48] .  \n In the domain of injection molding, RL has been applied to real -time process parameter \noptimization. One study proposed a decision -making framework that used offline data and a self -\npredictive ANN to dynamically adjust parameters and maintain consistent pr oduct quality [50]. \nHowever, this approach relied on simulation -based surrogate models, which may introduce \nmodeling inaccuracies, and utilized the Deep Deterministic Policy Gradient (DDPG) algorithm, \nwhich is known to suffer from instability and limited exploration in complex, high -dimensional \nspaces. Moreover, most existing studies have primarily focused on quality objectives, such as \ndimensional accuracy or part thickness, without integrating cost or profit metrics into the learning \nprocess.  Other studies have explored various RL strategies for optimizing molding processes. RL \nhas been employed to refine ram velocity and packing pressure profiles  [27], optimize flow \ndistribution in liquid composite molding  [49], and improve trajectory control using Iterative \nLearning Control (ILC) combined with RL  [50]. Approaches such as Deep Q -Networks (DQN) have been applied to achieve intelligent decision -making under complex production conditions  \n[30]. In addition, DRL has been utilized for intelligent temperature control in stretch blow molding \nand for temperature compensation using Actor -Critic framework, surpassing traditional PID and \nGPC controllers in stability and accuracy  [32,51] . Further developments include RL -based self -\nrecovery mechanisms for correcting non -optimal conditions [52], two -dimensional Q -learning for \nunknown system tracking  [53], and fault -tolerant tracking control using off -policy RL for robust \noperation under actuator failures [54].  \n In summary, while deep learning and RL have significantly advanced the optimization of \ninjection molding processes, integrating profitability into these models remains an emerging area. \nIt is imperative to acknowledge the limitations of previous studies and to develop more accurate \nsurrogate m odels, employ robust RL algorithms such as PPO or SAC, and expand the scope of the \nconsidered process parameters. This approach will lead to more adaptive and profitable production \nstrategies.  \n \n2    METHODOLOGY  \n2.1    Data Acquisition  \n A real-world dataset was employed to develop the DRL agent capable of optimizing process \nparameters in injection molding . The data were acquired from a fully instrumented injection \nmolding testbed, which has been described in detail in our prior study  [55]. The experimental setup \nincluded the installation of environmental sensors to monitor factory  and machine -level \ntemperature and relative humidity , integrating a Central Monitoring System (CMS) for capturing \nprocess parameters, and deploying  an automated vision inspection system for product quality \nassessment.  \n The collected  dataset comprises  process parameters such as injection speed, pressure, position, \nand hold time, along with external environmental variables including temperature and relative humidity , both within the factory and at the machine level. A total of 2,794 samples were collected. \nEach data sample comprises  10 controllable process parameters, 4 external environmental \nvariables, and 1 binary quality label (good or defective), which was automatically assigned through \nvisual  inspection. The target product for the experiment was a circular cosmetic container cap made \nfrom acrylonitrile butadiene styrene  (ABS ), a thermoplastic material commonly used in industrial \napplications.  \nTable 1. Statistical analysis results of the dataset.  \n Variable name  Mean  Std \nDev Min 25% 50% 75% Max Unit \nProcess \nparameter  Injection speed 1  29.5 6.5 20.0 25.0 30.0 35.0 40.0 % \nInjection speed 2  30.1 6.5 20.0 25.0 30.0 35.0 40.0 % \nInjection speed 3  20.1 6.5 10.0 15.0 20.0 25.0 30.0 % \nInjection pressure 1  129.9  6.5 120.0  125.0  130.0  135.0  140.0  bar \nInjection pressure 2  130.1  6.5 120.0  125.0  130.0  135.0  140.0  bar \nInjection pressure 3  140.1  6.5 130.0  135.0  140.0  145.0  150.0  bar \nInjection position 1  46.0 1.3 44.0 45.0 46.0 47.0 48.0 mm \nInjection position 2  37.8 3.9 32.0 35.0 38.0 41.0 44.0 mm \nInjection position 3  30.0 1.3 28.0 29.0 30.0 31.0 32.0 mm \nHold time  1.2 0.8 0.0 0.6 1.2 1.8 2.4 sec \nEnvironmental \nvariables  Machine temperature  15.4 4.2 5.5 13.3 14.5 19.8 21.8 ℃ \nMachine humidity  42.1 9.8 23.6 33.0 44.7 51.0 62.4 % \nFactory temperature  15.3 4.4 5.6 12.1 14.3 19.9 22.8 ℃ \nFactory humidity  42.6 10.6 23.0 33.2 47.1 50.8 63.6 % \n \n The environmental conditions were recorded continuously throughout the production process \nand naturally fluctuated over time. Conversely, process parameters were systematically varied \naccording to a DOE scheme, specifically utilizing an L81 orthogonal arra y. This methodological \ndesign ensured broad parameter space coverage  and improved the dataset's representativeness \nconcerning  the complex, nonlinear interactions present in real -world injection molding \nenvironments.  For a detailed description of the experi mental configuration, the testbed architecture , \nand quality labeling methodology, readers are directed to our previous work. A statistical summary \nof the dataset is presented in  Table 1 . 2.2    Problem definition and MDP formulation of DRL -based decision -making model  \n The primary  objective of this research is to develop a DRL -based optimization framework \nthat maximizes profitability in the injection molding process. Conventional  approaches have \nprimarily focused  on optimizing process parameters to enhance product quality, often without \nincorporating critical economic factors such as electricity  consumption, mold costs, and overall \nproduction efficiency. To address this limitation, a profit function is formulated that integrates both \nquality -related and cost -related components. This formulation supports a holistic and financ ially \nsustainable optimization strategy.  \n The profit function, which represents profit per a single production cycle , is formulated as \nfollows:  \n𝑓(𝑥)=𝑝∑𝑦𝑖 𝐶𝑣\n𝑖=1− ∑(𝑐𝑖resin+𝑐𝑖mold(𝑃max)+𝑐𝑖elect(𝑃max))𝐶𝑣\n𝑖=1(1) \nWhere,   \n● 𝐶𝑣 represents  the number of cavities per cycle (in this case, 𝐶𝑣=4). \n● 𝑝 represents the unit price per cavity (in this case, 𝑝 = 0.2$/cavity).  \n● 𝑦𝑖 represents  a binary variable that indicates whether the product from the 𝑖-th cavity is \ngood  (1) or defective (0).  \n𝑦𝑖= { 1,       if  𝑖–th cavity  is good  cavity         \n0,       if  𝑖–th cavity  is defective  cavity(2) \n● 𝑐𝑖resin represents  the resin cost per cavity (in this case, 𝑐𝑖resin = 0.04$/cavity).  \n● 𝑐𝑖mold(𝑃max) represents the mold cost per cavity ($/cavity), which is determined based \non the maximum injection pressure. This cost is derived from a method utilized by the \nfactory in this study for cost analysis, where the estimated mold wears  per cycle and \nremaining usable cycles are calculated based on maximum injection pressure. The cost per cavity is obtained by distributing the total mold replacement cost over 4 cavities per \ncycle.  \n𝑐𝑖mold(𝑃max)= {0.025 ,          if  𝑃max <140  bar \n 0.02775 ,     if  140  bar ≤ 𝑃max(3) \n● 𝑐𝑖elect(𝑃max) represents the electricity cost per cavity ($/cavity), which depends on the \nmaximum injection pressure. The electricity consumption per cycle is 0.8 kWh for  \n𝑃max <135  bar, 1.0 kWh for 135  bar ≤𝑃max <145  bar, and 1.1 kWh for \n145 bar ≤ 𝑃max. Dividing by 4 cavities per cycle gives per -cavity values of 0.2 kWh, \n0.25 kWh, and 0.275 kWh. The electricity cost per cavity is then determined by \napplying seasonal and time -dependent electricity pricing.  Tables 2 and 3 summarize \nthe time -of-use electricity prices and their seasonal classifications. Fig. 2 illustrates the \ncorresponding hourly price variations across seasons.  \n𝑐𝑖elect(𝑃max)= {0.2    ×electricity  price ,if  𝑃max <135 bar                        \n 0.25  ×electricity  price ,if  135  bar ≤𝑃max <145  bar   \n 0.275 ×electricity  price ,if  145  bar ≤ 𝑃max                       (4) \n By maximizing the profit  function, the DRL agent can learn an optimal control policy that \nbalances product quality, operational efficiency, and cost minimization in dynamic production \nenvironments. The proposed DRL -based framework is designed to adjust process parameters \nautonomously in real time . This allow s it to respond effectively to fluctuating external conditions, \nthereby enabling adaptive and data -driven decision -making.  Several key factors are considered in \nthe problem definition . Environmental conditions, including ambient temperature and humidity, \nsignificantly influence resin behavior and affect flow dynamics, necessitating real -time parameter \nadjustments to maintain acceptable product quality.  Another essential  factor  is cycle time, which \nmust be mini mized to enhance productivity while still ensuring that products meet quality standards . \nIn addition , electricity  cost variability plays a role, as electricity prices fluctuate depending on time \nand season, requiring optimized scheduling to reduce costs. The m old cost is also crucial because frequent high -pressure cycles accelerate wear, shortening  mold lifespan. Therefore, the agent must \nbalance production speed with the long -term health of the mold.  \n \nTable 2. Time -of-use electricity prices in South Korea in USD/kWh.  \n(The exchange rate is 1,000 KRW to a USD)  \n Spring/Fall  Summer  Winter  \nOff-peak hours  0.0995  0.0995  0.1065  \nMid-peak hours  0.1220  0.1524  0.1526  \nOn-peak hours  0.1527  0.2345  0.2101  \n \nTable 3. Time -of-use classification by season in South Korea.  \n Spring/Fall  Summer  Winter  \nOff-peak hours  22:00 - 08:00  22:00 - 08:00  22:00 - 08:00  \nMid-peak hours  08:00 - 11:00  \n12:00 - 13:00  \n18:00 - 22:00  08:00 - 11:00  \n12:00 - 13:00  \n18:00 - 22:00  08:00 - 09:00  \n12:00 - 16:00  \n19:00 - 22:00  \nOn-peak hours  11:00 - 12:00  \n13:00 - 18:00  11:00 - 12:00  \n13:00 - 18:00  09:00 - 12:00  \n16:00 - 19:00  \n \nTable 4. Ranges of process parameters including upper -lower bounds, small -large step size.  \nProcess parameter  Lower bound  Upper bound  Small step size  Large step size  \nInjection speed 1  20 40 0.1 5.0 \nInjection speed 2  20 40 0.1 5.0 \nInjection speed 3  10 30 0.1 5.0 \nInjection pressure 1  120 140 1.0 20 \nInjection pressure 2  120 140 1.0 20 \nInjection pressure 3  130 150 1.0 20 \nInjection position 1  44 48 0.1 1.0 \nInjection position 2  32 44 0.1 1.0 \nInjection position 3  28 32 0.1 1.0 \nHold time  0 2.4 0.1 1.0  \nFig. 2. Seasonal time -of-use electricity price profiles by hour for spring/fall, summer, and \nwinter.  \n \nTo enable the DRL agent to make optimal decisions in dynamic production environments, the \ndecision -making problem is formulated as MDP . The MDP framework provides a mathematical \nfoundation for sequential decision -making, aligning well with the challenges of injection molding , \nwhere external conditions and operational parameters continuously evolve.  An MDP is \ncharacterized by a tuple  (𝑆,𝐴,𝑅,𝑇), representing the state space, action space, reward function, \nand transition probability, respectivel y [56,57] . This study defines these components  to reflect the \nspecific requirements of injection molding optimization.  \nFirst, a state space 𝑆 represents the set of all possible states that the environment can assume. \nA state 𝑠𝑡∈𝑆 is defined as a vector that  incorporates  not only the controllable process parameters \nbut also external environmental variables and electricity price variations, all of which critically \naffect product quality, cycle time, and production  costs:  \n𝑠𝑡=[𝑝𝑡,𝑒𝑡,𝑐𝑡]∈𝑆 (5) \nwhere 𝑝𝑡 denotes the vector of controllable process parameters, 𝑒𝑡 represents the \nenvironmental variables, and 𝑐𝑡 corresponds to the electricity pricing information at time 𝑡. \nThe process parameter vector 𝑝𝑡 is defined as:  \n𝑝𝑡=[𝑣𝑡1,𝑣𝑡2,𝑣𝑡3,𝑃𝑡1,𝑃𝑡2,𝑃𝑡3,𝑥𝑡1,𝑥𝑡2,𝑥𝑡3,ℎ𝑡] (6)  \nwhere 𝑣𝑡𝑘, 𝑃𝑡𝑘, and 𝑥𝑡𝑘 represent the injection speed, pressure, and position at three distinct \nstage s (𝑘=1,2,3) of the injection process, and ℎ𝑡 denotes hold time.  The environmental \nvariable vector 𝑒𝑡 is defined as:  \n𝑒𝑡=[𝑇𝑡machine,𝑇𝑡factory,𝐻𝑡machine,𝐻𝑡factory] (7) \nrepresenting the temperature  and relative humidity conditions  at both the machine and factory.  \nThe electricity price 𝑐𝑡 is encoded as a 9 -dimensional one -hot vector, reflecting seasonal and \ntime-dependent price categories outlined in Tables 2 and 3. To stabilize the learning  process, \nthe continuous elements of the state vector are normalized to the range [−1,1], using the upper \nand lower bounds listed in Table 4. \n Second, an action space 𝐴 denotes the set of possible adjustments that the agent can make to \nthe process parameters. An action  𝑎𝑡∈𝐴 is defined  as a continuous adjustment vector, allowing \nthe agent to control the process parameters under varying environmental conditions:  \n𝑎𝑡=[Δ𝑣𝑡1,Δ𝑣𝑡2,Δ𝑣𝑡3,Δ𝑃𝑡1,Δ𝑃𝑡2,Δ𝑃𝑡3,Δ𝑥𝑡1,Δ𝑥𝑡2,Δ𝑥𝑡3,Δℎ𝑡]∈𝐴 (8) \nwhere each  Δ component represents  the adjustment to the corresponding controllable process \nparameter: injection  speed (𝑣𝑡𝑘), pressure (𝑃𝑡𝑘), position (𝑥𝑡𝑘), and hold time (ℎ𝑡) at different \nstages (𝑘=1,2,3). The evolution of process parameters over time follows a cumulative \nadjustment mechanism formulated as:  \n𝑝𝑡=𝑝0+∑𝑎𝑖𝑡−1\n𝑖=0(9)  \nwhere the current process parameters 𝑝𝑡 are determined by adding the initial process parameter \nsetting 𝑝0 and the cumulative sum of the past adjustment actions up to timestep 𝑡−1. To stabilize \nthe learning process and maintain consistency with the normalized state representation, the DRL agent outputs actions normalized within the range [−1,1]. In the environment, these normalized \nactions are rescaled using predefined adjustment step sizes specific to each process  parameter , as \nlisted in  Table 5 .  Two types of step sizes are defined: (i) a small step size, corresponding to one \nfourth of the difference between the upper and lower bounds of each process parameters , and (ii) a \nlarger step size, corresponding to one half of that difference. Furthermore, to ensure operational \nfeasibility, any action  resulting  in a combination of process parameter s exceeding its allowed \nbounds, as specified in  Table 4, is clipped accordingly.  \n \nTable 5  Adjustment step sizes for normalized actions used in the environment.  \nProcess parameter  Small step size  Large step size  \nInjection speed 1  5.0 10.0 \nInjection speed 2  5.0 10.0 \nInjection speed 3  5.0 10.0 \nInjection pressure 1  5.0 10.0 \nInjection pressure 2  5.0 10.0 \nInjection pressure 3  5.0 10.0 \nInjection position 1  1.0 2.0 \nInjection position 2  3.0 6.0 \nInjection position 3  1.0 2.0 \nHold time  0.6 1.2 \n \n Third, the reward function 𝑅 assigns a reward 𝑟𝑡=𝑅(𝑠𝑡,𝑎𝑡) based on the action 𝑎𝑡 taken in \nstate 𝑠𝑡. In the defined  problem, the agent generates newly adjusted process parameters, which are \nthen applied to the injection molding machine. After executing the injection process with these \nparameters, the reward is computed as the profit over a 10 -minute production interval, formulated \nbased on the profit function defined in Eq. 1  and expressed as:  \n𝑟𝑡=600\n𝑇×[   𝑝∑𝑦𝑖𝐶𝑣\n𝑖=1 – ∑(𝑐𝑖resin+𝑐𝑖mold(𝑃max)+𝑐𝑖elect(𝑃max))𝐶𝑣\n𝑖=1   ] (10) \nwhere 𝑇 represents the cycle time  in seconds , 𝐶𝑣 is the number of cavities per cycle, 𝑦𝑖 indicates \nthe quality  of i-th cavity . 𝑐𝑖resin, 𝑐𝑖mold, and 𝑐𝑖elect denote the resin cost, mold cost, and electricity cost per cavity  within a single production cycle , respectively. The choice to formulate the reward \nbased on a 10 -minute interval is made to match the setting where each timestep within an episode \ncorresponds to a 10 -minute production period during training  phase . Both the cycle time ( 𝑇) and \nthe cavity quality labels (𝑦𝑖) are functions of the process parameters  and environmental variables. \nThese quantities are inferred during training using the trained surrogate model s embedded within \nthe environment, enabling rapid reward computation without real -world experiments . The \nsurrogate models are described in detail in  Section 2.3 . \n Fourth, the transition probability 𝑃 represents the likelihood of transitioning to the next state \n𝑠𝑡+1 when an action 𝑎𝑡 is taken in the current state 𝑠𝑡, expressed as 𝑝(𝑠𝑡+1|𝑠𝑡,𝑎𝑡). It defines the \nstate transition dynamics within the MDP  framework . The agent does not explicitly model or \nestimate this transition function in model -free RL . Instead, it learns through trial -and-error \ninteractions with the environment, implicitly capturing the underlying transition dynamics from \nsampled transitions . Despite the absence of an explicit transition model, agents can successfully \nlearn state value functions, state -action value functions, and optimal policies by leveraging \ntemporal difference methods  [56]. \n \n2.3    Surrogate model s for quality classification and cycle time regression  \n A reliable simulation environment was required to enable efficient offline training of the DRL \nagent . This was achieved  by developing two surrogate models . One model was designed to predict \nproduct quality, while the other was developed to estimate cycle time.  These models serve as key \ncomponents of the virtual environment where the DRL agent can simulate the outcomes of different \nprocess parameters without directly interacting  with the physical injection molding machine .  \n The first surrogate model performs quality classification  by predicting whether a product \nmanufactured under a given set of process parameters is acceptable or defective.  This model was \ninitially  developed and validated in our previous study on diffusion -based quality estimation and process parameter inference in real manufacturing environments  [55]. It was trained using \nproduction data labeled with quality outcomes, along with controllable process parameters and \nenvironmental variables. The model replicates quality behavior observed in the real -world testbed \nand outputs a binary label representing the predicted quality classification.  \n In contrast, the surrogate model for cycle time regression was newly developed in this study to \nsupport the novel objective of profitability -oriented process optimization. This regression model \nestimates production cycle time based on given process paramet ers. To identify the most effective  \nregression algorithm , 18 ML models  were tested  using the Py Caret package  [58], including \nRandom Forest, Gradient Boosting, Light Gradient Boosting Machine  (LightGBM ), Decision Tree, \nand others. The dataset was split into training and testing sets in an 80:20 ratio, with 10% of the \ntraining data further allocated to a validation set.  Model evaluation was conducted through 10 -fold \ncross -validation, and the average performance scores across folds were compared.  Although \nRMSE (Root Mean Squared Error) and MAE (Mean Absolute Error) are widely used  metrics  in \nregression evaluation, this study adopted R²  (coefficient of determination) as the performance \nmetric. RMSE can disp roportionately penalize outliers due to the squaring of errors . While  MAE \noffers a straightforward average error, it does not capture the model’s ability to explain the variance \nof the target variable. Both are also scale -dependent and thus less suitable for comparing models \nacross varying process settings. In contrast, R²  offers a normalized, interpretable measure of how \nmuch variance in the dependent variable is accounted for by the model, making it more suitable for \nevaluating predictive performance in c omplex, multivariable environments like injection molding. \nUsing 10 -fold cross -validation, LightGBM was identified as the best -performing model among the \n18 tested, achieving the highest average R²  score. Its excellent predictive accuracy, fast training \ntime, and strong generalization capability made it an ideal choice for modeling the nonlinear and \ndynamic nature of cycle time in real manufacturing scenarios. The results are presented in  Table \n6. Further details  of surrogate models  are provided in  Section 1 of the Supplementary Materials . Table 6. The performance results from 10 -fold cross -validation on 18 ML models for cycle \ntime regression.  \nModel  RMSE  MAE  R² \nLight Gradient Boosting Machine  0.1468  0.0632  0.9743  \nRandom Forest Regressor  0.1537  0.0583  0.9719  \nExtra Trees Regressor  0.1559  0.0451  0.971 0 \nDecision Tree Regressor  0.1674  0.0475  0.9666  \nGradient Boosting Regressor  0.1749  0.0986  0.9645  \nK Neighbors Regressor  0.3776  0.2285  0.8365  \nAdaBoost Regressor  0.3916  0.3123  0.8246  \nRidge Regression  0.7239  0.627 0 0.4011  \nBayesian Ridge  0.7239  0.6271  0.4011  \nLinear Regression  0.7239  0.627 0 0.4011  \nElastic Net  0.9142  0.7613  0.0467  \nOrthogonal Matching Pursuit  0.9333  0.7768  0.0064  \nLasso Regression  0.9388  0.783 0 -0.0052  \nLasso Least Angle Regression  0.9388  0.783 0 -0.0052  \nDummy Regressor  0.9391  0.7832  -0.0058  \nHuber Regressor  1.1059  0.8215  -0.4008  \nLeast Angle Regression  1.1678  0.7724  -0.5819  \nPassive Aggressive Regressor  1.3215  1.0765  -1.0907  \n \n2.4    DRL algorithms  \n Model -free DRL algorithms can be categorized into value -based and policy -based methods. In \nvalue -based methods, the agent first learns a value function and subsequently derives a policy based \non it. However, value -based approaches are generally restricted to discrete ac tion spaces and are \nnot readily applicable to continuous action domains. To overcome  this limitation, policy -based \nmethods learn a policy directly, enabling the handling of continuous action spaces  [59]. \nNevertheless , policy -based approaches often  suffer from high gradient variance, negatively \nimpacting  learning stability and overall  performance. A framework that integrates both approaches \nis the actor -critic method, where in the actor  is responsible for learning the policy, and the critic \nestimates the value function to guide and stabilize policy updates  [60].   Additionally, DRL algorithms can be classified into on -policy and off -policy methods \ndepending on whether the policy used for training matches the policy that determines  the actions. \nIn on -policy methods, the policy being updated is identical to the one that determined the action. \nBy contrast , off-policy methods enable  learning from samples collected  by a different policy. \nConsequently , on-policy methods cannot reuse sample s generated by previous policies and must \nrely on newly collected trajectories  for each training iteration . In contrast , off-policy methods can \nleverage  past experiences by storing them in an experience replay buffer and incorporating them \ninto training. On-policy methods offer  more  stable learning and exhibit better convergence \nproperties. However, they are susceptible to getting trapped in local optima and suffer from high \nsample complexity. Off -policy methods are potentially less stable but achieve greater sample \nefficiency by ext ensively reusing prior experiences  [56,61,62] . \n On-policy DRL algorithms collect data using the current policy and update the policy based on \nthose collected samples. Trusted region policy optimization (TRPO)  [63] ensures stable policy \nimprovement by restricting updates within a trust region. However, TRPO  suffers from high \ncomputational complexity and implementation difficulty. PPO  [34] addresses these limitations by \nintroducing a clipped surrogate objective 𝐿CLIP(𝜃), as shown in  Eq. 1 1, thereby  replacing the \ncomplex trust region constraint in TRPO. The actor network is trained to maximize this  surrogate  \nobjective. To further enhance the stability of policy updates, PPO employs Generalized Advantage \nestimation (GAE) 𝐴̂𝑡GAE, which is  defined in  Eq. 1 2 and Eq. 1 3. The actor network is trained to \nmaximize 𝐿actor defined in  Eq. 1 4, which incorporates an entropy bonus term 𝑆[𝜋𝜃](𝑠𝑡) to \nencourage sufficient exploration. The critic network estimates the state value function 𝑉𝜙(𝑠𝑡), \nwhich is u tilized in  comput ing the GAE and guid ing updates of the actor network . The critic is \ntrained by minimizing the loss function 𝐿critic(𝜙) given in  Eq. 1 5. \n𝐿CLIP(𝜃)=𝐸̂𝑡[𝜋𝜃(𝑠𝑡)\n𝜋𝜃𝑜𝑙𝑑(𝑠𝑡)𝐴̂𝑡GAE,clip (𝜋𝜃(𝑠𝑡)\n𝜋𝜃𝑜𝑙𝑑(𝑠𝑡),1−𝜖,1+𝜖)𝐴̂𝑡GAE ] (11) 𝐴̂𝑡=𝛿𝑡+(𝛾𝜆)𝛿𝑡+1+⋯+⋯+(𝛾𝜆)𝑇−𝑡+1 (12)  \n𝑤ℎ𝑒𝑟𝑒 𝛿𝑡=𝑟𝑡+𝛾𝑉𝜙(𝑠𝑡+1)−𝑉𝜙(𝑠𝑡) (13) \n𝐿actor=𝐸̂𝑡[𝐿𝑡CLIP(𝜃)+𝑐𝑆[𝜋𝜃](𝑠𝑡)] (14) \n𝐿critic(𝜙)=𝐸̂𝑡[(𝑉𝜙(𝑠𝑡)−𝑅̂𝑡)2] (15) \nwhere 𝜃 and 𝜙 denote the parameters of the actor and critic network, respectively . The term \n𝜋𝜃(𝑠𝑡)\n𝜋𝜃𝑜𝑙𝑑(𝑠𝑡) represents the probability ratio between the current policy and the previous policy. The \nhyperparameter 𝜖,𝛾, and 𝜆 represent the clipping parameter, the discount factor, and GAE \nsmoothing parameter, respectively. 𝛿𝑡 denotes the temporal difference error. 𝑆[𝜋𝜃](𝑠𝑡) denotes \nthe entropy of the policy 𝜋𝜃 at state 𝑠𝑡, with 𝑐 as its weighting coefficient. The estimated return 𝑅̂𝑡 \nis computed as the discounted sum of future rewards.  \n Various off -policy algorithms have been developed to achieve high sample efficiency by \nreusing samples from experience replay to address challenges associated with continuous state and \naction spaces . A representative approach is Deep Deterministic Policy Gradient (DDPG)  [35], \nwhich optimizes a deterministic policy.  However, DDPG relies on external noise for exploration \nrather than incorporating stochasticity into the policy itself, making it sensitive to hyperparameter \nsettings and prone to unstable training.  In contrast , SAC  [64,65]  adopts a stochastic policy and \nintroduces entropy regularization by incorporating the expected policy entropy 𝐻(𝜋(𝑠𝑡)) in Eq. \n16. This framework enables the policy to explicitly balance  exploration and exploitation, \nfacilitating broad action sampling  during the early training phase and mitigating premature  \nconvergence to local optima. The policy  objective  𝐽̂𝜋(𝜃) is formulated in  Eq. 1 7 using the \nminimum of two soft Q -value functions, implementing a clipped double Q -learning to reduce \noverestimation bias.  The critic consists of four networks: two Q - networks for learning and two \ncorresponding target networks.  The Q -networks are trained by minimizing the loss defined in  Eq. 18, where the target value is computed according to Eq. 19 . The target networks are updated \nvia a soft update as defined in Eq. 20 . \n𝐽(𝜋)=∑𝐸𝑡[𝑟(𝑠𝑡,𝑎𝑡)+𝛼𝐻(𝜋(𝑠𝑡))]  𝑇\n𝑡=0(16) \n𝐽̂\n𝜋(𝜃)=𝐸̂𝑡[𝛼𝑙𝑜𝑔 𝑙𝑜𝑔  𝜋𝜃(𝑠𝑡) −𝑄𝜙𝑗(𝑠𝑡,𝑎𝑡) ]  (17) \n𝐽̂𝑄(𝜙𝑖)=𝐸𝑡[(𝑦−𝑄𝜙𝑖(𝑠𝑡,𝑎𝑡))2\n] (18)  \n𝑦=𝑟𝑡+1+𝛾(𝑄𝜙targ ,𝑗(𝑠𝑡,𝑎𝑡) −𝛼 𝑙𝑜𝑔  𝜋𝜃(𝑠𝑡+1) ) (19) \n𝜙𝑡𝑎𝑟𝑔 ,𝑖=𝜏𝜙𝑖+(1−𝜏)𝜙targ ,𝑖 (20) \nwhere 𝜃,𝜙, and 𝜙targ are parameters of the actor network, the Q -network, and the corresponding \ntarget network, respectively.  𝐻(𝜋(𝑠𝑡)) denotes the entropy  of the policy, and 𝛼 is the entropy \ntemperature coefficient. 𝑄(𝑠𝑡,𝑎𝑡) represents the soft Q -value function, which estimates the \nexpected return given state 𝑠𝑡 and action 𝑎𝑡. 𝜏∈(0,1) is the Polyak averaging coefficient used for \ntarget network updates.  \n \nTable 7. Summary of the hyperparameters used for the PPO and SAC algorithms  \nHyperparameters  PPO SAC  \nPolicy learning rate  3×10−4  3×10−4  \nValue learning rate  1×10−3  3×10−4  \nTotal timesteps  180,000  180,000  \nBuffer size (Experience replay)  720 25,000  \nDiscount factor 𝛾 0.99 0.99 \nClipping parameter 𝜖 0.2 N/A \nGAE smoothing parameter 𝜆 0.95 N/A \nEntropy coefficient 𝑐 0.005  N/A \nLearning starts (steps)  N/A 2016  \nTarget update rate 𝜏 N/A 0.005  \nTraining frequency (steps)  N/A 72  \nEntropy temperature coefficient 𝛼  N/A 0.1 \nHidden layer sizes  [256, 256]  [256, 256]  \nActivation function  Tanh  ReLU  \nOptimizer  Adam; betas=(0.9, 0.999)  Adam; betas=(0.9, 0.999)  \n The optimal policy was trained using PPO and SAC with hyperparameters summarized in  \nTable 7.  Each episode correspond ed to a 24 -hour period comprising 144 steps with 10 -minute \nintervals. The training process was conducted over 1,250 episodes, resulting in a total of 180,000 \nsteps. All training procedures were executed on an Intel Core i9 -13900K CPU, and the DRL \nframework  was implemented based on the OpenAI Gym library  [66]. \n \n2.5    Training and deployment process in DRL   \n The DRL agent receives a state input that includes the current process parameters and \nenvironmental conditions such as the temperature, relative humidity, and electricity prices.  Based \non this state, the agent selects an action corresponding to adjusting  the process parameters , which \nis applied to the environment. However, conducting real -world experiments  to train the agent using \nprocess parameters derived from virtual environmental scenarios would be prohibitively expensive. \nThe surrogate models introduced in Section 2.3  simulate the outcomes of different process settings \nto address this challenge . These models enable the agent to interact with a virtual environment in \nan offline setting and learn to optimize the reward function defined in Eq. 10 . This reward is \ndesigned to encourage both profitability and product quality under variable operating conditions.  \n Fig. 3 illustrates the framework of the proposed decision -making model for optimizing \ninjection molding process parameters.  The framework consists of an offline training phase and an \nonline deployment phase. In the offline training phase , the DRL  agent is trained under a virtual \nenvironmental scenario rather than using real -time environmental variables from the actual \nmachine. The virtual environmental scenario simulated variations in temperature and relative \nhumidity  within the machine and factory over 144 timesteps, corresponding to a total duration of \n24 hours , and is treated as one training episode.  The magnitude of environmental variations is \nrestricted to 10% of the range between the maximum and minimum environmental values designed \nbased on  the DOE as shown in Table 1. Additionally, electricity price fluctuations are introduced randomly over time  to reflect dynamic cost conditions. The DRL agent is trained across a series of \nrandomly generated virtual environmental scenarios, aiming to develop a policy that maximizes \nprofit under varying conditions  and enhances robustness against environmental variability. During \ntraining , the agent adopts a stochastic policy modeled by a normal distribution, enabling the \nsampling of random actions and facilitating effective exploration.  \n \n \nFig. 3. Training phase and deployment phase of the DRL -based  decision -making  model for \ninjection molding process parameter optimization  \n \n In the online deployment phase, the DRL agent operates within a real -world production \nenvironment, receiving inputs such as sensor -measured temperature and relative humidity  as well \nas electricity prices that vary by season and time.  Leveraging the policy trained during the offline \nphase, the agent optimizes process parameters within approximately 0.5 seconds under the given \nenvironmental conditions.  During deployment, a deterministic policy is adopted by selecting the \nmeans of the normal distribution, ensu ring that identical states consistently yield identical actions. \nThis approach enhances the reliability of the decision -making process. It should be noted that only \nthe actor network is utilized during deployment, whereas both the actor and critic networks  are \nrequired during training.  \n \n3    RESULTS  \n3.1    Performance evaluation of the developed  DRL -based  decision -making models  \n3.1.1    Training and evaluation results of developed  DRL  models  \n In the training phase, each episode corresponds to one of the 1,250 distinct predefined virtual \nscenarios, and the initial process parameters are randomly initialized to enhance the robustness of \nthe learned policy. A higher reward indicates that the trained policy effectively adjusts the process \nparameters to maximi ze profit under given process conditions, environmental variables, and \nelectricity prices.  \n Figs.  4a and 4b illustrate the change in reward over 180,000 training steps, equivalent to 1,250 \nepisodes, for two DRL algorithms. The curve represents the moving average of 50 episodes, and \nthe shaded region denotes one standard deviation around the mean. Since each episode is associated \nwith a distinct scenario with randomly initialized process parameters, the maximum achievable \nreward varies across episodes, which leads to a certain degree of variability in the reward curves in \nthe training phase. Nevertheless, both al gorithms exhibit convergence of the average reward toward \napproximately 6.3. In addition, SAC exhibits slightly faster convergence compared to PPO. This is attributed to its higher sample efficiency as an off -policy algorithm, whereas PPO relies on recently \ncollected data for training, necessitating  a larger number of samples. Figs.  4c and 4d show the \nnumber of defective cavities produced during training, approaching  zero as the learning process . \nHowever, some defects still occur even after the policy has sufficiently converged, particularly \nwhen the given initial process parameters inherently lead to the production of defective cavities.  \nSAC also exhibits a faster reduction in defective cavities compared to PPO.  \n \nFig. 4. Training curves of the average reward for  models trained with  (a) SAC and (b) PPO. \nNumber of defective cavities during training of models using (c) SAC and (d) PPO. All curves are \nsmoothed over 50 episodes using a moving average.  \nFigs.  5a and 5b show the total accumulated profit obtained by testing the trained policy \nunder a specific environmental variation scenario. In this scenario, the temperature and relative \nhumidity variations correspond to the Scenario 1  curves in Figs. 6a  and 6b, respectively, while \nthe seasonal electricity price followed the spring price fluctuation pattern shown in Fig. 2 . The \ntrained policy makes a one -time adjustment to the process parameters in response to external \nenvironmental variations, aiming to maximize profit.  In the test phase, the action was chosen \ndeterministically as the mean of the policy distribution, in contrast to the training phase , where \nthe action was stochastically sampled from the policy. When evaluating the average \nperformance after 60,000 timesteps , where the policy had sufficiently converged, the average \nprofits were 953.76 for SAC and 958.99 for PPO.  The difference can be attributed to the \ninherent characteristics of the two algorithms, which lead to different timesteps required to \nreach the optimal solution with SAC requiring more timesteps than PPO.  \n \nFig. 5. Total profit over a single episode evaluated on a predefined scenario using policies \ntrained with (a) SAC and (b) PPO. The scenario corresponds to \"Scenario 1\" in Fig. 6 , which \nassumes seasonal electricity pricing based on spring/fall rates. Profit is evaluated using a \ndeterministic version of the policy, where the mean action is selected instead of sampling from \nthe stochastic distribution.  \n \nFig. 6. Seasonal variations in environmental conditions for injection molding processes. (a) \nTemperature profiles for spring, summer, and winter. (b) Relative h umidity profiles for spring, \nsummer, and winter.  \n \n3.1.2    Comparison of  the developed  DRL models under specific environmental conditions  \n The performance of the DRL  models trained in Section 3. 1.1 was evaluated to determine \nwhether they c ould adjust process parameters from diverse initial process parameter settings to \nmaximize profit.  To this end, 9 initial process parameter cases  were selected under a fixed \nenvironmental condition of 14℃  temperature and 45% relative humidity for both the machine and \nfactory  with spring off -peak electricity pricing, as listed in Table 8 . Six cases  produced \nnondefective  cavities with varying profit levels, whereas three resulted in defective cavities.  Each \ninitial process parameter case underwent a sequence of 10 consecutive adjustments, with each \nadjustment corresponding to one time -step. A deterministic policy was followed by taking the \nmean s of the policy distribution at every step , ensuring that identical inputs consistently yield \nidentical actions.  Parameters were  updated cumulatively without reinitialization, so that the result \nof each adjustment served as the next state, forming a continuous trajectory of parameter evolution.  \n \nTable 8. Initial process parameter cases and corresponding profit and defective cavity count \nunder spring off -peak conditions ( temperature = 14℃, relative h umidity = 45%)  \nProcess Parameter  Case  \n1 2 3 4 5 6 7 8 9 \nInjection speed 1  32.5 38.8 26.2 36.0 44.4 30.5 37.6 44.3 38.2 \nInjection speed 2  32.5 38.8 26.2 21.7 23.2 43.1 32.2 31.3 34.3 \nInjection speed 3  20 25 15 12.8 12.8 25.6 25.4 12.0 12.4 \nInjection pressure 1  130 135 125 131.5  136.2  120.7  121.14  133.5  124.7  \nInjection pressure 2  130 135 125 133.4  127.1  120.8  132.17  122.5  125.0  \nInjection pressure 3  140 145 135 132.2  144.3  133.0  140.36  134.7  137.8  \nInjection position 1  46 47 45 45.6 45.2 45.4 46.0 47.4 44.4 \nInjection position 2  38 41 35 40.0 34.7 40.7 40.3 39.4 41.1 \nInjection position 3  30 31 29 30.7 29.6 30.5 28.3 31.3 29.9 \nHold time  1.2 1.8 0.6 0.36 2.30 2.23 0.13 0.03 0.10 \nProfit  6.224  6.021  6.383  6.674  6.082  6.548  3.356  0.881  -2.323  \nDefective cavity count  0 0 0 0 0 0 1 2 3 \n \nTable 9. Performance evaluation of SAC -base model : Profit variation by step across 9 initial \nprocess parameter cases under spring off -peak conditions ( temperature = 14℃, relative \nhumidity = 45%)  \nStep Profit by case  \n1 2 3 4 5 6 7 8 9 \n0(initial parameter)  6.224  6.021  6.383  6.674  6.082  6.548  3.356  0.881  -2.323  \n1 6.639  6.101  6.717  6.656  6.154  6.654  6.376  6.730  6.680  \n2 6.737  6.102  6.746  6.712  6.668  6.773  6.762  6.733  6.722  \n3 6.763  6.104  6.776  6.727  6.717  6.799  6.768  6.742  6.730  \n4 6.768  6.295  6.788  6.752  6.750  6.800  6.799  6.753  6.788  \n5 6.774  6.691  6.799  6.788  6.752  6.800  6.800  6.788  6.799  \n6 6.799  6.747  6.800  6.799  6.788  6.800  6.800  6.791  6.799  \n7 6.799  6.774  6.800  6.800  6.799  6.800  6.800  6.800  6.800  \n8 6.800  6.775  6.800  6.800  6.800  6.800  6.800  6.800  6.800  \n9 6.800  6.774  6.800  6.800  6.800  6.800  6.800  6.800  6.800  \n10 6.800  6.799  6.800  6.800  6.800  6.800  6.800  6.800  6.800  \nMaximum Profit  6.800  6.799  6.800  6.800  6.800  6.800  6.800  6.800  6.800  \n \n \n Table 10. Performance evaluation of PP O-based model : Profit variation by step across 9 initial \nprocess parameter cases under spring off -peak conditions ( temperature = 14℃, relative \nhumidity = 45%)  \nStep Profit by case  \n1 2 3 4 5 6 7 8 9 \n0(initial parameter)  6.224  6.021  6.383  6.674  6.082  6.548  3.356  0.881  -2.323  \n1 6.758  6.396  6.698  6.718  6.364  6.587  6.481  6.726  6.728  \n2 6.773  6.777  6.793  6.797  6.772  6.769  6.791  6.796  6.762  \n3 6.799  6.773  6.794  6.793  6.773  6.772  6.795  6.793  6.769  \n4 6.799  6.799  6.795  6.795  6.799  6.799  6.795  6.795  6.795  \n5 6.799  6.799  6.795  6.795  6.799  6.799  6.795  6.799  6.799  \n6 6.799  6.799  6.795  6.799  6.799  6.799  6.799  6.799  6.799  \n7 6.799  6.799  6.799  6.799  6.799  6.799  6.799  6.799  6.799  \n8 6.799  6.799  6.799  6.799  6.799  6.799  6.799  6.799  6.799  \n9 6.799  6.799  6.799  6.799  6.799  6.799  6.799  6.799  6.799  \n10 6.799  6.799  6.799  6.799  6.799  6.799  6.799  6.799  6.799  \nMaximum Profit  6.799  6.799  6.799  6.799  6.799  6.799  6.799  6.799  6.799  \n \n Both SAC - based and PPO -based  models  converged  to profit -maximizing process parameters \nwithin 10 adjustment steps , regardless of the initial process parameter settings, and the respective \nresults are presented in Tables  9 and 10. SAC-based model  reached an average profit of 6.800 in \n10 steps, whereas PPO -based model  achieved 6.799 in 7 steps.  The changes in average profit over  \nadjustment  steps for SAC -based  and PPO -based model  are illustrated in Fig. 7 . In Section 3. 1.1, \nPPO-based model  yielded higher profit than SAC -based model  when evaluated on the virtual \nscenario with a single adjustment from the DRL agent, which can be attributed to the characteristic \nof PPO to converge in fewer steps. Specifically, the mean first -step profit across all  nine initial \nsettings  in Tables 9 and 10 was 6.523 for the SAC -based model and 6.606 for the  PPO-based \nmodel , which is consistent with the results presented in the previous section.   \nFig. 7. Average profit by step for DRL models  trained with SAC and PPO . \n \n The results obtained using the DRL models were compared with those produced by a GA \nmethod utilizing the same surrogate model s. GA is a population -based optimization method \ninspired by natural selection and genetics principles . It is one of the most widely adopted static \napproaches for solving complex global optimization problems  and has been applied to injection \nmolding studies  [13,67] . This study used GA  to search for process parameters that maximize the \nsame profit function defined in Eq. 1 , consistent with the DRL models . The optimization was \nperformed under identical environmental conditions , including 14°C temperature, 45% relative \nhumidity, and the spring off -peak electricity price . Using the DEAP library, we implemented  the \nGA with simulated binary crossover and polynomial mutation, which are widely used in continuous, \nbounded optimization problems  [68].Parent selection was performed using tournament selection , \nand the population size and number of generations were set to 40 and 2 5, respectively. The detailed \nhyperparameter settings for  the GA are provided in Section 2 of the Supplementary Materials . \n Fig. 8 shows the change in average profit for 9 initial populations over 25 generations. The \nresults are compared with the profits obtained from the optimal parameters identified by SAC -\nbased  and PPO -based model . The GA converged to an average profit of 6.799 after 20 generations \n(800 profit evaluations) with profit ranging from 6.784 to 6.805 and a standard deviation of 0.00613. \nFor the DRL models , SAC -based  and PPO -based model  achieved profits of 6.799 and 6.800, \nrespectively. Despite being trained in a stochastic manner, the model produced consistent results \nacross trials in the deployment phase due to the use of a deterministic policy. This highlights the \nhigh robustness of the DRL model under the given environmental conditions, while still achieving \nperformance comparable to  the GA. \n \nFig. 8. Evolution of the average best profit  during genetic algorithm optimization, performed \nwith 9 random initial populations  under spring off -peak conditions (temperature = 14℃, \nrelative humidity = 45%) . The shaded region represents  the range of fitness variation across \npopulations. Results are compared with the average profits achieved  by SAC -based  and PPO -\nbased model  after 10 and 7 adjustment steps, respectively . \n \nTable 11. Comparison of DRL models and GA \nMethod  Average profit  Standard deviation  Computational time  Training time  \nDRL – SAC  6.800  – 0.421s (10 steps)  206 min \nDRL – PPO 6.799  – 0.287s (7 steps)  182 min \nGA 6.799  0.00613  21.20 1s (20 gens)  – \n \nTable 11  presents the optimization and training times as well as the average profit and standard \ndeviation for the two DRL models  and the GA method . As shown in the previous results, SAC -\nbased  and PPO -based model  required 10 and 7 profit evaluations respectively to reach convergence, \nwhile  the GA required 20 generations, corresponding to a total of 800 evaluations. The total training \ntimes were 206 minutes for SAC -based model  and 182 minutes for PPO -based model , while  the \nGA required no prior training. However, it is important to note that the training process for the \nDRL models can be conducted offline, before  deployment, and does not need to occur within the \nproduction environment. As such, the training time is not  a critical constraint in practical \napplications.  In contrast, the time required for online optimization is a key constraint in injection \nmolding processes, where optimization must be completed within the cycle time. In this regard, the \nDRL -based approaches demonstrated a significant advantage by performing optimization tens of \ntimes faster than  the GA, achieving inference times of 0.421 seconds for SAC -based model  and \n0.287 seconds for PPO -based model , compared to 21.201 seconds for  the GA. Furthermore,  the \nGA searches for optimal process parameters under fixed environmental conditions, whereas DRL \nincorporates environmental variables into the state representation, enabling dynamic optimization \nthat can adapt to changing conditions. To properly assess the effect iveness of the trained DRL \nmodels, it is important to verify their ability to perform reliably under fixed environmental \nconditions and  across a diverse range of scenarios, which is evaluated in  Section 3. 1.3. \n \n3.1.3    Evaluation of the developed DRL models under varying  environmental conditions  \n   To assess whether the trained DRL models can track optimal process parameters under \ndynamic conditions, we performed a 24-hour virtual deployment in three seasonally \nrepresentative scenarios  including spring, summer, and winter. In South Korea, spring and \nautumn electricity pricing and environmental characteristics are nearly identical. Accordingly, \nspring, summer, and winter were selected as representative seasons for evaluating the effects \nof seasonal variations.  It is important to note that the scenarios representing seasonal variations \ncan be adapted according to country -specific environmental characteristics and conditions.  \nEach scenario followed  the corresponding temperature –humidity trajectory shown in Fig. 6 , \nunder the simplifying assumption that the machine and factory shared identical seasonal \nconditions at every time step. These profiles were selected to cover various  seasonal conditions while remaining within the experimental bounds listed in Table 1 , ensuring that the trained \npolicy could adapt to practical operating environments.  \nThe cumulative profit defined in Eq. 1 over a 24 -hour operation of a single injection molding \nmachine was compared with DRL  models and GA method  under the three seasonal scenarios. \nThe machine was assumed to operate continuously for 24 hours, with production occurring \nimmediately  after each cycle. Process parameters were optimized for all methods based on the \ntemperature and relative humidity  measured at the beginning of each cycle.  For the DRL \nmodels, t he final performance was consistent across different  initial process parameters  \n(Section 3. 1.2); thus , the initial condition for optimization  was set to the midpoint (50%) of the \nrange listed in Table 1, corresponding to a normalized state 𝒔0=𝟎. The number of steps \nrequired for convergence was determined to be 10 steps for SAC -based  model  and 7 steps for \nPPO-based model , based on the results in Tables 9 and 10, and the process parameters obtained \nat these steps were adopted as the optimized parameters. The same hyperparameter settings \nused in Section 3.1.2  were applied for the GA , and optimization was conducted at each cycle \nbased on the updated environmental conditions.  \nFigs. 9a, 9b,  and 9c show the cumulative profit changes over 24 hours for the DRL models \ntrained with SAC and PPO, and for the GA method, under three seasonal scenarios. For all methods, \nthe cumulative profit consistently followed the order of spring, winter, and summer, corresponding \nto the decreasing electricity prices shown in Fig. 2 . Table 12  summarizes the total profit, the \nnumber of cavities produced over 24 hours, and the computational time measured for the spring \nscenario. SAC -based model  achieved profits of $958.8 8, $915. 63, and $930. 85 in the spring, \nsummer, and winter scenarios, respectively, while PPO -based model  achieved profits of $958.33, \n$914.68, and $929.85 under  the same scenarios.  Meanwhile, SAC -based model  produced 8,644, \n8,744, and 8,824 cavities , whereas PPO -based model  produced 8,640, 8,736, and 8,816 cavities  under  the same scenarios . The G A recorded profits of $959.69, $915.87, and $932.66 and produced \n8,652, 8,748, and 8,844 cavities in the spring, summer, and winter scenarios, respectively.  \nAlthough the GA  achieved the highest profits and production volumes across all scenarios, the \nDRL models exhibited highly comparable performance  in both economic and operational terms, \ndemonstrating their practical competitiveness under varying environmental conditions.  \n \nFig. 9. Cumulative profit over a 24 -hour period under varying environmental conditions is \npresented for 2 DRL  models trained using (a) SAC and (b) PPO, and (c) the GA method . All \nmethods were deployed under dynamic environmental conditions, incorporating the \ntemperature and relative humidity  profiles shown in Fig. 6  and the electricity price variations \nillustrated in Fig. 2 . Across all approaches, the seasonal ranking of cumulative profit remains \nconsistent ( spring > winter > summer ). \n \n While the DRL models and the GA  method  exhibited highly comparable performance levels , \ntheir computational efficiency differed markedly. In this study, computational time refers to \ninference time  for the trained DRL models and full optimization time for the GA method.  Inference \ntime refers to the time required for a trained DRL model to produce process parameter adjusetment \ndecision via multiple forward passes of the policy network.  The SAC -based  and PPO -based  models \nrequired only 15.5 minutes and 10.4 minutes, respectively, primarily due to differences in the \nnumber of steps required to reach convergence. In contrast, the GA method required a substantially \nlonger time of 781.0 minutes. Importantly, once traine d to accommodate dynamic environmental \nvariations, the DRL models can infer optimal process parameters through a single forward pass of \nthe policy network, incurring minimal computational overhead during operation. Conversely, the \nGA method must perform a complete optimization routine whenever environmental conditions \nchange, resulting in significant and persistent computational costs duri ng real -time deployment.  \nTable 1 2 Comparison of total profit and number of cavities produced (in parentheses)  over 24 \nhours by season for DRL model s (trained using SAC and PPO ) and the GA method . \nComputational time  is measured based on the spring scenario . \n SAC  PPO GA \nSpring  $958.8 8 (8,644)  $958.33 (8,640)  $959.69 (8,652)  \nSummer  $915. 63 (8,744)  $914.68 (8,736)  $915.87 (8,748)  \nWinter  $930. 85 (8,824)  $929.85 (8,816)  $932.66 (8,844)  \nComputational time  15.5 min  10.4 min  781.0 min  \n \n3.2    Comparison of the DRL models under different adjustment  step size settings  \n The impact of different adjustment step sizes , as presented in Table 5 , on the performance of \nthe DRL models was systematically analyzed. Section 3. 1.3 evaluated the models using a large \nadjustment step size across 3 seasonal scenarios. This section examines a smaller adjustment step \nsize for comparative analysis.  Two cases were considered under the small adjustment condition: \n(1) maintaining the number of steps identical to the large adjustment case (10 steps for SAC, 7 \nsteps for PPO), and (2) doubling the number of steps (20 steps for SAC, 14 steps for PPO).  \n Table s 13 and 14 provide quantitative comparisons of cumulative profit and the number of \ncavities produced over 24 hours  for SAC -based and PPO -based models under different step size \nand step count conditions. When the smaller adjustment step size was employed with the same \nnumber of steps (10 for SAC, 7 for PPO), the SAC -based model experienced a decrease in both \nprofit and  production, particularly during summer and winter. Similarly, the PPO -based model \nshowed decreased performance in spring and summer scenarios, although  a slight profit increase \noccurred in winter. Increasing the number of steps to 20 for SAC and 14 for PPO improved \nperformance for both models. Specifically, SAC achieved near -equivalent performance to the large adjustment scenario, whereas PPO showed improved but slightly lower outcomes. Computational \ntimes for both models increased proportionally with the number of steps.  Fig.10 shows the total \nprofit the total profit and number of cavities over 24 hours for SAC - and PPO -based models across \nthree seasonal senarios  and varying step conditions , summarizing the results presented in Tables \n12, 13, and 14. \n \nTable 13. Comparison of total profit and number of cavities produced (in parentheses) over 24 \nhours for SAC -based  model operated  with different step sizes and numbers of steps.  \nComputational time  is measured based on the spring scenario . \n SAC  \n(large step size, 10 steps)  SAC  \n(small step size, 10 steps)  SAC  \n(small step size, 20 steps)  \nSpring  $958.88 (8,644)  $958.79(8,644)  $958.79(8,644)  \nSummer  $915.63 (8,744)  $912.91(8,720)  $914.29 (8,732)  \nWinter  $930.85 (8,824)  $928.64 (8,804)  $929.09 (8,808)  \nComputational time  15.5 min  15.3 min  30.5 min  \n \nTable 1 4. Comparison of total profit and number of cavities produced (in parentheses) over 24 \nhours for PPO -based  model operated with different step sizes and numbers of steps.  \nComputational time  is measured based on the spring scenario . \n PPO  \n(large step size, 7 steps)  PPO \n(small step size, 7 steps)  PPO \n(small step size, 14 steps)  \nSpring  $958.33 (8,640)  $952.81 (8,592 ) $956.04(8,620)  \nSummer  $914.68 (8,736)  $909.70 (8,688)  $911.54  (8,704)  \nWinter  $929.85 (8,816)  $930.76  (8,824)  $930.76  (8,824)  \nComputational time  10.4 min  10.3 min  20.5 min  \n \n The observed slight profit increase for the PPO -based model in the winter scenario under the \nsmaller adjustment setting (7 steps) is not considered decisive. Evaluating DRL models must \nemphasize their capability to consistently identify optimal parameters across diverse environmental \nconditions, rather than isolated improvements in specific scenarios. Moreover, while doubling the \nstep count improved performance, this enhancement was accompanied by significantly increased \ncomputational time and operational c osts. Thus, a smaller adjustment step size demands greater computational resources for convergence without guaranteeing a substantial or consistent \nadvantage. Consequently, a larger adjustment step size emerges as the preferable choice, offering \na robust and computationally efficient strategy for real -time process  optimization.  \n \nFig. 10. Comparison of total profit and number of cavities produced over 24 hours for SAC -\nbased and PPO -based mod el operated  with different step sizes and numbers of steps  under three \nseasonal  scenarios  (spring, summer,  and winter)   \n \n4    DISCUSSION  \n4.1    Performance Discussion  of DRL-based  decision -making  models  \n We developed decision -making models based on two DRL algorithms, SAC and PPO, to \nidentify  optimal process parameters  under dynamic environmental conditions reflecting temporal \nand seasonal variation s in temperature , relative humidity , and electricity prices.  Under the large \nadjustment step size condition, comparisons across three virtual seasonal scenarios showed that \nSAC -based model  achieved slightly better cumulative profit and production volume compared to \nPPO-based model . However, due to the characteristics of its maximum entropy framework, SAC \nmaintains higher policy stochasticity to explore a broader  action space, requiring more steps to \nconverge than PPO .  \n Under the small adjustment step size condition, optimizati on required a gr eater number of steps ; \nnevertheless, SAC -based model  was able to maintain performance comparable to that under the \nlarge step size setting , while PPO-based model  showed a stronger tendency to converge to a locally \noptimal policy . The inefficiency and extended convergence time resulting from smaller adjustment \nsteps have also been observed in previous research that applied DDPG for optimizing injection \nmolding processes  [44]. Off-policy algorithms such as SAC and DDPG can reuse previously \ncollected data, enabling broader exploration of the state-action  space  [35,65] . In contrast, the on -\npolicy PPO algorithm relies solely on newly collected trajectories, which limits data diversity and \nnarrows the exploration range  [34]. This algorithmic difference was reflected in our small \nadjustment step size experiments, as SAC could approach near -global optima, whereas PPO more \nfrequently settled into locally optimal solutions . \n The optimization performance of the developed DRL -based  models was evaluated against a \nstatic method, GA, under diverse environmental conditions.  The SAC -based model could achieve \nresults comparable to  the GA, with profit differences of $0.81, $0.24, and $1.81 over 24 hours for \nthe spring, summer, and winter scenarios, respectively . In the deployment phase,  SAC -based model  required 15.5 minutes to optimize a single scenario, while  the GA took 781 minutes, demonstrating \nthat DRL  approach  is much more efficient in terms of optimization cost. The faster inference speed \nof the DRL models was achieved by training them not only with process parameters  but also with \nenvironmental variables and electricity prices included in the state  representation , allowing them \nto adapt  to dynamic environmental changes. In contrast,  the GA optimizes process parameters \nunder fixed conditions and must be rerun  whenever the environment changes.  While this can yield \nmore precise results in static settings, it is inefficient for real -time applications under continuously \nvarying conditions.  \n This difference  becomes even more critical as the number of process param eters increases since \nthe opt imization time of the GA scales with the input dimensionality  [69], whereas the inference \ntime of the DRL -based models remains effectively constant.  In particular, the  previous study \nidentified 10 key process parameters  among 43 candidates based on their practical adjustability by  \nfield experts , and this study builds upon that selection.  If additional process paramters must be \nconsidered , the computational cost of  the GA rises sharply, making real -time deployment even \nmore challenging.  In contrast, although the DRL models  becomes more complicated to train with \na greater number of variables, it can still achieve rapid inference speed once training is completed, \nmaking it more suitable for real -time adaptation under continuously changing conditions.  \n \n4.2    Expectation of DRL -based decision -making models  \n The proposed DRL -based decision -making models , utilizing SAC and PPO, dynamically adapt \nto seasonal and time -dependent fluctuations while optimizing process parameters to enhance \nproduction quality and profitability. Leveraging DRL supports adaptive and profitable production, \nenabling real -time decision -making in evolving manufacturing environments.  \n A key strength of these  models  is their ability to generalize across diverse environmental \nconditions. Once trained, the DRL models  can efficiently determine optimal process parameters that ensure consistent profitability, even when fluctuations in electricity pricing, environmental \nfactors, and production constraints occur . Unlike static optimization methods, DRL -based dynamic \noptimization  enables real -time adaptation, maintaining both cost -efficiency and production \nperformance. Comparative experiments with the GA , a widely used global optimization method, \nconfirm that the DRL -based models achieve comparable profitability while reducing optimization \ntime by up to 135 times. This computat ional efficiency is a technical advantage and  a critical factor \nfor practical deployment in real -world manufacturing environments.  \n Field experts apply a recommended process parameter setting in actual manufacturing \nenvironments, especially in injection molding . These experts need enough time to review the \nsuggested parameters, assess their plausibility, and input them into the molding machine before the \nnext cycle begins. Given that the target product's cycle time in this study is approximately 39 \nseconds, the GA’s optimization time of 21 seconds alone consumes more than half of the available \ncycle time. This severely limits the remai ning time available for expert verification and manual \nsetup, making GA impractical for real -time field deployment.   \n Although  the GA may achieve slightly higher profitability under some seasonal settings, with \ngains ranging from $0.86 to $2.81 per 24 -hour operation compared to SAC -based  or PPO -based \nmodels , realizing such improvements would require significantly higher computational \nspecifications, which in turn leads to increased investment capital. In the injection molding industry, \nwhich typically operates under low -margin, cost -sensitive conditions, inv esting in high -\nperformance computing infrastructure solely to support GA -based  optimization is economically \ninefficient and unrealistic. Factories in this sector  are often reluctant to invest in computational \nupgrades, prioritizing low operational costs over marginal gains in theoretical optimization \nperformance.  In contrast, the DRL  models offer a cost -effective and scalable solution. Their rapid \ninference time aligns well with the actual production cycle, allowing field experts sufficient time \nfor evaluation and implementation without disrupting production flow.   Importantly, the generality of the proposed models is largely driven by their profit function \nformulation. Since the profit function can be customized to reflect the economic goals and \nconstraints of different manufacturing contexts, the DRL -based  dynamic  optimization  framework \nare not limited to plastic injection molding. Instead, they provide a scalable and transferable \nsolution for real -time process optimization in other industries, such as semiconductor fabrication, \nmetal forming, and food processing, w here dynamic operating conditions and cost variables \ninfluence profitability . By combining data -driven insights with reinforcement learning, this study \npresents a flexible decision -making framework capable of adapting to a wide range of production \nsystems that require rapid, cost -effective, and stable operation.  \n \n4.3    Limitation of DRL -based decision -making models  \n One of the principal limitations of this research is the dependence of DRL performance on the \naccuracy of the surrogate model, which serves as the offline training environment for the DRL  \nagent. The success of DRL -based optimization critically hinges on the surrogate model’s ability to \napproximate the real injection molding process faithfully . Suppose the surrogate model lacks \nsufficient precision due to measurement errors, environmental uncertainties, or inherent modeling \nlimitations. In that case,  the DRL  agent may learn suboptimal policies that fail to generalize to \nactual production environments. Significant discrepancies between the surrogate model and the \nreal-world process can result in policies that perform poorly when deployed, leading to unexpected  \ndeclines in product quality or profitability and ultimately compromising the effectiveness of the \nDRL -based optimization.  If the surrogate model lacks sufficient training data, its predictive \naccuracy remains limited, ultimately affecting the optimization  quality of the DRL -based  model . \nThus, securing extensive datasets is crucial for improving overall system performance.  Ensuring \nhigh surrogate model fidelity necessitates using  a sufficiently large and representative dataset for \ntraining. However, in industrial environments, data collection is often constrained by restricted access to production systems, variability in operating conditions, and high acquisition costs. \nTraining the surrogate model on limited or biased datasets can degrade its predictive accuracy, \nthereby diminishing the optimization quality and robustness of th e DRL model . Therefore, securing \nextensive and diverse datasets is critical to enhancing surrogate model fidelity, enabling the DRL \nagent to develop reliable and transferable policies  to real production systems.  \n Another limitation arises as the number of process parameters increases or interdependencies \namong parameters become more complex. Under such conditions, the single -agent DRL approach \nemployed in this study may struggle to efficiently explore the solution space and identify globally \noptimal process parameters. The increased complexity can cause the agent to converge to local \noptima, thus hindering the achievement of the most effective production settings. To overcome this \nchallenge, multi -agent DRL algorit hms should be considered. The m ulti-agent algorithms  would \nallow individual agents to specialize in optimizing subsets of process parameters while \ncollaborating to achieve a globally optimized solution. This approach could enhance adaptability \nand scalability in highly complex manufacturing environments, wh ere single -agent DRL  \nalgorithms  may encounter performance limitations.  \n \n4.4    Future work  \n While the proposed DRL -based decision -making models have demonstrated their effectiveness \nin dynamically adapting to changing manufacturing environments, further advancements are \nnecessary to address certain limitations and enhance practical applicability.  One key area for future \nresearch is improving the surrogate model by expanding the available dataset. Since the accuracy \nof the surrogate model directly impacts the performance of the DRL model,  securing a sufficient \nand diverse dataset is essential. However, operational limitations often constrain industrial data \ncollection , which necessitates  alternative approaches to improve model robustness.  A promising \ndirection is the generation of synthetic data to supplement real -world datasets. This can be achieved by leveraging multi -fidelity simulation techniques, which integrate high - and low -fidelity models \nto create realistic training environments for the DRL agent. Additionally, transfer learning and few -\nshot learning can be explored to enhance the model’s adap tability when only limited real -world \ndata are available. By incorporating these techniques, future studies can alleviate data scarcity \nissues and improve the overall accuracy and reliability of the surrogate model.  \n Furthermore, as the number of process parameters increases or the interdependencies among \nvariables become more complex,  single -agent DRL algorithms may struggle to optimize all \nparameters efficiently. In such scenarios, multi -agent DRL algorithms should be considered to \novercome local optima and improve scalability. A multi -agent framework allows individual agents \nto optimi ze specific process variables while collaborating to achieve a globally optimal solution. \nThis approach can significantly enhance the flexibility and robustness of DRL -based optimization, \nmaking it more suitable for large -scale and complex manufacturing processes.  \n By refining the surrogate model and investigating multi -agent reinforcement learning, future \nresearch can further improve the adaptability, efficiency, and scalability of DRL -based \noptimization for injection molding and broader industrial applications. These advancements will \nsupport the development of more intelligent, data -efficient, and real -time adaptive manufacturing \nsystems, ensuring higher reliability in highly dynamic production environments.  \n \n5    CONCLUSION  \n This study presented a Deep Reinforcement Learning (DRL) -based framework and models for \noptimizing injection molding process parameters with the dual objectives of improving product \nquality and maximizing profitability. The proposed framework addressed a c ritical limitation of \nconventional optimization approaches by formulating a profit -driven objective function that \ncomprehensively integrates material costs, mold maintenance costs, electricity consumption, and \nreal-time electricity pricing. To enable effic ient offline training, surrogate machine learning models were developed for quality classification and cycle time prediction, allowing DRL agents to be \ntrained without direct interaction with physical production systems.  \n Experimental results demonstrated that the developed DRL -based decision -making models, \nutilizing SAC and PPO algorithms, effectively adapted process parameters to fluctuating \nenvironmental conditions. The models achieved profitability comparable to a genetic algorithm \nwhile significantly reducing optimization time, offering a clear computational advantage. The rapid \ninference capability  enables parameter optimization within seconds, making  the framework highly \nsuitable for real -time deployment in manufactu ring environments where timely and adaptive \ndecision -making is essential.  \n The key contributions of this research are summarized as follows. First, a profit -driven \noptimization strategy was proposed that directly links operational decisions to economic outcomes, \naddressing a gap often overlooked in conventional quality -focused a pproaches. Second, a DRL -\nbased real -time decision -making framework was developed, demonstrating stable and efficient \noptimization performance under dynamic environmental and economic conditions. Third, the \nproposed models exhibited robust adaptability acro ss diverse seasonal scenarios, validating their \ngeneralizability in real -world manufacturing environments. Fourth, the framework was designed \nto be scalable and transferable to other industrial domains, highlighting its potential applicability \nbeyond injec tion molding. Finally, by identifying limitations related to surrogate modeling and \noptimization complexity, this study lays the foundation for future research to improve  system \nscalability through enhanced data -driven modeling and multi -agent reinforcemen t learning \nstrategies.  \n Overall, this research underscores the transformative potential of DRL -based optimization in \nsmart manufacturing systems. Future work will focus on expanding datasets to improve surrogate \nmodel fidelity and investigating multi -agent reinforcement learning  approaches to enhance \nscalability, robustness, and adaptability in increasingly complex industrial environments.   \nAcknowledgement s: This work was supported by the Technology Innovation Program (RS -\n2023 -00284506) funded by the Ministry of Trade, Industry and Energy (MOTIE, Korea), the \nTechnology Development Program (S3207585) funded by the Ministry of SMEs and Startups \n(MSS, Korea), the National Research Foundation of Korea (NR F) grant funded by the Ministry \nof Science and ICT (RS -2023 -00222166), and a grant from the Ministry of Food and Drug \nSafety (RS -2023 -00215667).  \n \nData Availability Statement:  The data used  are not publicly available.  \n \nConflicts of Interest:  The authors declare no conflict of interest.  \n \n \n \n  REFERENCE   \n[1] EL Ghadoui M, Mouchtachi A, Majdoul R. Exploring and optimizing deep neural networks for precision \ndefect detection system in injection molding process. J Intell Manuf 2024:1 –18. \nhttps://doi.org/10.1007/S10845 -024-02394 -3/TABLES/9.  \n[2] Selvaraj SK, Raj A, Rishikesh Mahadevan R, Chadha U, Paramasivam V. A Review on Machine Learning \nModels in Injection Molding Machines. Advances in Materials Science and Engineering \n2022;2022:1949061. https://doi.org/10.1155/2022/1949061.  \n[3] Fernandes C, Pontes AJ, Viana JC, Gaspar -Cunha A. Modeling and Optimization of the Injection -\nMolding Process: A Review. Advances in Polymer Technology 2018;37:429 –49. \nhttps://doi.org/10.1002/ADV.21683.  \n[4] Breaking Down the Cost Factors in Plastic Manufacturing n.d. \nhttps://www.goldengatemolders.com/post/breaking -down -the-cost-factors -in-plastic -manufacturing \n(accessed March 3, 2025).  \n[5] Wang HS, Wang YN, Wang YC. Cost estimation of plastic injection molding parts through integration of \nPSO and BP neural network. Expert Syst Appl 2013;40:418 –28. \nhttps://doi.org/10.1016/J.ESWA.2012.01.166.  \n[6] Wang HS. Application of BPN with feature -based models on cost estimation of plastic injection products. \nComput Ind Eng 2007;53:79 –94. https://doi.org/10.1016/J.CIE.2007.04.005.  \n[7] Mcmurtrey A. ENERGY SAVING STRATEGIES FOR PLASTICS INJECTION MOLDING: \nLUBRICATION ADAM MCMURTREY, EXXONMOBIL, SPRING, TX - Google 검색  n.d. \nhttps://www.ilsag.info/wp -content/uploads/Exxon -Mobil -EE-Idea-Attachments_April -2020.pdf \n(accessed March 3, 2025).  \n[8] Optimizing an Injection Mold Process - Turner Group n.d. https://turnergroup.net/optimizing -an-\ninjection -mold -process/ (accessed March 3, 2025).  \n[9] Thiriez A, Gutowski T. An environmental analysis of injection molding. IEEE International Symposium \non Electronics and the Environment 2006;2006:195 –200. https://doi.org/10.1109/ISEE.2006.1650060.  \n[10] Huang J, Li Y, Li X, Ding Y, Hong F, Peng S. Energy Consumption Prediction of Injection Molding \nProcess Based on Rolling Learning Informer Model. Polymers 2024, Vol 16, Page 3097 2024;16:3097. \nhttps://doi.org/10.3390/POLYM16213097.  \n[11] AYadav R, Scholar R, SVJoshi P, NKKamble A. Recent Methods for Optimization of Plastic Injection \nMolding Process -A Literature Review. Int J Sci Eng Res 2012;3.  \n[12] Hernández -Vega JI, Reynoso -Guajardo LA, Gallardo -Morales MC, Macias -Arias ME, Hernández A, de \nla Cruz N, et al. Plastic Injection Molding Process Analysis: Data Integration and Modeling for Improved \nProduction Efficiency. Applied Sciences 2024, Vol 14, Pa ge 10279 2024;14:10279. \nhttps://doi.org/10.3390/APP142210279.  \n[13] EL Ghadoui M, Mouchtachi A, Majdoul R. A hybrid optimization approach for intelligent manufacturing \nin plastic injection molding by using artificial neural network and genetic algorithm. Scientific Reports \n2023 13:1 2023;13:1 –15. https://doi.org/10.1038/s 41598 -023-48679 -0. \n[14] Kariminejad M, Tormey D, Ryan C, O’Hara C, Weinert A, McAfee M. Single and multi -objective real -\ntime optimisation of an industrial injection moulding process via a Bayesian adaptive design of \nexperiment approach. Scientific Reports 2024 14:1 2024;14:1 –19. https://doi.org/10.1038/s41598 -024-\n80405 -2. \n[15] Shi H, Gao Y, Wang X. Optimization of injection molding process parameters using integrated artificial \nneural network model and expected improvement function method. International Journal of Advanced Manufacturing Technology 2010;48:955 –62. https://doi.org/10.1007/S00170 -009-2346 -7/FIGURES/10.  \n[16] Uglov A, Nikolaev S, Belov S, Padalitsa D, Greenkina T, Biagio MS, et al. Surrogate modeling for \ninjection molding processes using deep learning. Structural and Multidisciplinary Optimization \n2022;65:1 –13. https://doi.org/10.1007/S00158 -022-03380 -0/TABLES/10.  \n[17] Rivers N. Leveraging the Smart Grid: The Effect of Real -Time Information on Consumer Decisions. vol. \n127. 2018. https://doi.org/10.1787/6AD4D5E3 -EN. \n[18] KEPCO Electric Rates Table 2023. \nhttps://home.kepco.co.kr/kepco/EN/F/htmlView/ENFBHP00109.do?menuCd=EN060201 (accessed \nMarch 5, 2025).  \n[19] Muniain P, Ziel F. Probabilistic forecasting in day -ahead electricity markets: Simulating peak and off -\npeak prices. Int J Forecast 2020;36:1193 –210. https://doi.org/10.1016/J.IJFORECAST.2019.11.006.  \n[20] Brännlund R, Vesterberg M. Peak and off -peak demand for electricity: Is there a potential for load shifting? \nEnergy Econ 2021;102:105466. https://doi.org/10.1016/J.ENECO.2021.105466.  \n[21] Wu Y, Feng Y, Peng S, Mao Z, Chen B. Generative machine learning -based multi -objective process \nparameter optimization towards energy and quality of injection molding. Environmental Science and \nPollution Research 2023;30:51518 –30. https://doi.org/10.1007/S 11356 -023-26007 -3/FIGURES/15.  \n[22] Lovrec D, Tic V, Tasner T. Dynamic behaviour of different hydraulic drive concepts - comparison and \nlimits. International Journal of Simulation Modelling 2017;16:448 –57. \nhttps://doi.org/10.2507/IJSIMM16(3)7.389.  \n[23] Madan J, Mani M, Lyons KW. Characterizing Energy Consumption of the Injection Molding Process. \nASME 2013 International Manufacturing Science and Engineering Conference Collocated with the 41st \nNorth American Manufacturing Research Conference, MSEC 2013 20 13;2. \nhttps://doi.org/10.1115/MSEC2013 -1222.  \n[24] Liu H, Zhang X, Quan L, Zhang H. Research on energy consumption of injection molding machine driven \nby five different types of electro -hydraulic power units. J Clean Prod 2020;242:118355. \nhttps://doi.org/10.1016/J.JCLEPRO.2019.118355.  \n[25] Meekers I, Refalo P, Rochman A. Analysis of Process Parameters affecting Energy Consumption in \nPlastic Injection Moulding. Procedia CIRP 2018;69:342 –7. \nhttps://doi.org/10.1016/J.PROCIR.2017.11.042.  \n[26] Li W, Kara S, Qureshi F. Characterising energy and eco -efficiency of injection moulding processes. \nInternational Journal of Sustainable Engineering 2015;8:55 –65. \nhttps://doi.org/10.1080/19397038.2014.895067.  \n[27] Wang F, Dong S, Danai K, Kazmer DO. Input Profiling for Injection Molding by Reinforcement Learning. \nASME International Mechanical Engineering Congress and Exposition, Proceedings, vol. 2, American \nSociety of Mechanical Engineers Digital Collection; 2021,  p. 701 –8. \nhttps://doi.org/10.1115/IMECE2001/DSC -24587.  \n[28] Nievas N, Pagès -Bernaus A, Bonada F, Echeverria L, Domingo X. Reinforcement Learning for \nAutonomous Process Control in Industry 4.0: Advantages and Challenges. Applied Artificial Intelligence \n2024;38. https://doi.org/10.1080/08839514.2024.2383101.  \n[29] Párizs RD, Török D. An experimental study on the application of reinforcement learning in injection \nmolding in the spirit of Industry 4.0. Appl Soft Comput 2024;167:112236. \nhttps://doi.org/10.1016/J.ASOC.2024.112236.  \n[30] Mnih V, Kavukcuoglu K, Silver D, Rusu AA, Veness J, Bellemare MG, et al. Human -level control \nthrough deep reinforcement learning. Nature 2015;518:529 –33. https://doi.org/10.1038/NATURE14236.  [31] Lee S, Cho Y, Lee YH. Injection Mold Production Sustainable Scheduling Using Deep Reinforcement \nLearning. Sustainability 2020, Vol 12, Page 8718 2020;12:8718. https://doi.org/10.3390/SU12208718.  \n[32] Hsieh PC. Intelligent Temperature Control of a Stretch Blow Molding Machine Using Deep \nReinforcement Learning. Processes 2023, Vol 11, Page 1872 2023;11:1872. \nhttps://doi.org/10.3390/PR11071872.  \n[33] Ren Z, Tang P, Zheng W, Zhang B. A Deep Reinforcement Learning Approach to Injection Speed Control \nin Injection Molding Machines with Servomotor -Driven Constant Pump Hydraulic System. Actuators \n2024, Vol 13, Page 376 2024;13:376. https://doi.org/10.3390/A CT13090376.  \n[34] Schulman J, Wolski F, Dhariwal P, Radford A, Openai OK. Proximal Policy Optimization Algorithms. \nArXiv Preprint ArXiv:170706347 2017.  \n[35] Lillicrap TP, Hunt JJ, Pritzel A, Heess N, Erez T, Tassa Y, et al. Continuous control with deep \nreinforcement learning. 4th International Conference on Learning Representations, ICLR 2016 - \nConference Track Proceedings 2015.  \n[36] Siilver D, Huang A, Maddison CJ, Guez A, Sifre L, van den Driessche G, et al. Mastering the game of \nGo with deep neural networks and tree search. Nature 2016;529:484 –9. \nhttps://doi.org/10.1038/nature16961.  \n[37] Vinyals O, Babuschkin I, Czarnecki W, Mathieu M, Dudzik A, Chung J, et al. Grandmaster level in \nStarCraft II using multi -agent reinforcement learning. Nature 2019;575:350 –4. \nhttps://doi.org/10.1038/s41586 -019-1724 -z. \n[38] Zhang Y, Qiao J, Zhang G, Tian H, Li L. Artificial Intelligence -Assisted Repair System for Structural and \nElectrical Restoration Using 3D Printing. Advanced Intelligent Systems 2022;4:2200162. \nhttps://doi.org/10.1002/AISY.202200162.  \n[39] Huang J, Su J, Chang Q. Graph neural network and multi -agent reinforcement learning for machine -\nprocess -system integrated control to optimize production yield. J Manuf Syst 2022;64:81 –93. \nhttps://doi.org/10.1016/J.JMSY.2022.05.018.  \n[40] Li C, Chang Q. Hybrid feedback and reinforcement learning -based control of machine cycle time for a \nmulti -stage production system. J Manuf Syst 2022;65:351 –61. \nhttps://doi.org/10.1016/J.JMSY.2022.09.020.  \n[41] Wu W, Huang Z, Zeng J, Fan K. A fast decision -making method for process planning with dynamic \nmachining resources via deep reinforcement learning. J Manuf Syst 2021;58:392 –411. \nhttps://doi.org/10.1016/J.JMSY.2020.12.015.  \n[42] Wu W, Huang Z, Zeng J, Fan K. A decision -making method for assembly sequence planning with \ndynamic resources. Int J Prod Res 2022;60:4797 –816. https://doi.org/10.1080/00207543.2021.1937748.  \n[43] Ogoke F, Farimani AB. Thermal control of laser powder bed fusion using deep reinforcement learning. \nAddit Manuf 2021;46:102033. https://doi.org/10.1016/J.ADDMA.2021.102033.  \n[44] Guo F, Zhou X, Liu J, Zhang Y, Li D, Zhou H. A reinforcement learning decision model for online process \nparameters optimization from offline data in injection molding. Appl Soft Comput 2019;85:105828. \nhttps://doi.org/10.1016/J.ASOC.2019.105828.  \n[45] He Z, Tran KP, Thomassey S, Zeng X, Xu J, Yi C. Multi -objective optimization of the textile \nmanufacturing process using deep -Q-network based multi -agent reinforcement learning. J Manuf Syst \n2022;62:939 –49. https://doi.org/10.1016/J.JMSY.2021.03.017.  \n[46] Mattera G, Caggiano A, Nele L. Optimal data -driven control of manufacturing processes using \nreinforcement learning: an application to wire arc additive manufacturing. J Intell Manuf 2024;36:1291 –\n310. https://doi.org/10.1007/S10845 -023-02307 -W/FIGURES/18.  [47] Cheng CK, Tsai HY. Enhanced detection of diverse defects by developing lighting strategies using \nmultiple light sources based on reinforcement learning. J Intell Manuf 2022;33:2357 –69. \nhttps://doi.org/10.1007/S10845 -021-01800 -4/TABLES/3.  \n[48] Paraschos PD, Koulinas GK, Koulouriotis DE. Reinforcement learning for combined production -\nmaintenance and quality control of a manufacturing system with deterioration failures. J Manuf Syst \n2020;56:470 –83. https://doi.org/10.1016/J.JMSY.2020.07.004.  \n[49] Szarski M, Chauhan S. Instant flow distribution network optimization in liquid composite molding using \ndeep reinforcement learning. J Intell Manuf 2023;34:197 –218. https://doi.org/10.1007/S10845 -022-\n01990 -5/FIGURES/22.  \n[50] Ruan Y, Zhang Y, Mao T, Zhou X, Li D, Zhou H. Trajectory optimization and positioning control for \nbatch process using learning control. Control Eng Pract 2019;85:1 –10. \nhttps://doi.org/10.1016/J.CONENGPRAC.2019.01.004.  \n[51] Ruan Y, Gao H, Li D. Improving the Consistency of Injection Molding Products by Intelligent \nTemperature Compensation Control. Advances in Polymer Technology 2019;2019:1591204. \nhttps://doi.org/10.1155/2019/1591204.  \n[52] Qin Y, Zhao C, Gao F. An intelligent non -optimality self -recovery method based on reinforcement \nlearning with small data in big data era. Chemometrics and Intelligent Laboratory Systems 2018;176:89 –\n100. https://doi.org/10.1016/J.CHEMOLAB.2018.03.010.  \n[53] Wen X, Shi H, Su C, Jiang X, Li P, Yu J. Novel data -driven two -dimensional Q -learning for optimal \ntracking control of batch process with unknown dynamics. ISA Trans 2022;125:10 –21. \nhttps://doi.org/10.1016/J.ISATRA.2021.06.007.  \n[54] Li X, Luo Q, Wang L, Zhang R, Gao F. Off -policy reinforcement learning -based novel model -free \nminmax fault -tolerant tracking control for industrial processes. J Process Control 2022;115:145 –56. \nhttps://doi.org/10.1016/J.JPROCONT.2022.05.006.  \n[55] Kim JY, Kim H, Nam K, Kang D, Ryu S. Development of an injection molding production condition \ninference system based on diffusion model. J Manuf Syst 2025;79:162 –78. \nhttps://doi.org/10.1016/J.JMSY.2025.01.008.  \n[56] Sutton R, Barto A. Reinforcement learning: An introduction. 2018.  \n[57] Bellman R. A Markovian decision process. Journal of Mathematics and Mechanics 1957;6:679 –84. \n[58] Ali M. PyCaret: An open source, low -code machine learning library in Python 2020.  \n[59] Williams RJ. Simple statistical gradient -following algorithms for connectionist reinforcement learning. \nMach Learn 1992;8:229 –56. https://doi.org/10.1007/BF00992696.  \n[60] Shakya AK, Pillai G, Chakrabarty S. Reinforcement learning algorithms: A brief survey. Expert Syst Appl \n2023;231:120495. https://doi.org/10.1016/J.ESWA.2023.120495.  \n[61] Vincent François -Lavet, Peter Henderson, Riashat Islam, Marc G. Bellemare, Joelle Pineau. An \nintroduction to deep reinforcement learning. Foundations and Trends®  in Machine Learning \n2018;11:219 –354. https://doi.org/10.1561/2200000071.  \n[62] Kang H, Jung S, Kim H, Jeoung J, Hong T. Reinforcement learning -based optimal scheduling model of \nbattery energy storage system at the building level. Renewable and Sustainable Energy Reviews \n2024;190:114054. https://doi.org/10.1016/J.RSER.2023.114054.  \n[63] Schulman J, Levine S, Abbeel P, Jordan M, Moritz P. Trust Region Policy Optimization. Proceedings of \nthe 32nd International Conference on Machine Learning, PMLR; 2015, p. 1889 –97. \n[64] Haarnoja T, Zhou A, Hartikainen K, Tucker G, Ha S, Tan J, et al. Soft Actor -Critic Algorithms and \nApplications. ArXiv Preprint ArXiv:181205905 2018.  [65] Haarnoja T, Zhou A, Abbeel P, Levine S. Soft Actor -Critic: Off -Policy Maximum Entropy Deep \nReinforcement Learning with a Stochastic Actor, PMLR; 2018, p. 1861 –70. \n[66] Brockman G, Cheung V, Pettersson L, Schneider J, Schulman J, Tang J, et al. OpenAI Gym. ArXiv \nPreprint ArXiv:160601540 2016.  \n[67] Tsai K -M, Luo H -J. An inverse model for injection molding of optical lens using artificial neural network \ncoupled with genetic algorithm. J Intell Manuf 2017;28:473 –87. https://doi.org/10.1007/S10845 -014-\n0999 -Z. \n[68] Félix -Antoine Fortin, François -Michel De Rainville, Marc -André Gardner Gardner, Marc Parizeau, \nChristian Gagné. DEAP: Evolutionary Algorithms Made Easy. The Journal of Machine Learning \nResearch 2012;13:2171 –5. \n[69] Goldberg D. Genetic Algorithms in Search, Optimization and Machine Learning. 1st ed. New Delhi: \nPearson Education India; 2013.  \n  ",
      "metadata": {
        "filename": "DRL-Based Injection Molding Process Parameter Optimization for Adaptive and Prof.pdf",
        "hotspot_name": "Housing_Production",
        "title": "DRL-Based Injection Molding Process Parameter Optimization for Adaptive\n  and Profitable Production",
        "published_date": "2025-05-16T08:35:31Z",
        "pdf_link": "http://arxiv.org/pdf/2505.10988v1",
        "query": "injection molding process optimization energy efficiency plastic manufacturing"
      }
    },
    "Enhancing the Product Quality of the Injection Process Using eXplainable Artific": {
      "full_text": "   \n \n Research Article  \nEnhancing the Product Quality of the Injection Process Using \neXplainable Artificial Intelligence  \nJisoo Hong  1, Yongmin Hong  1, Jung -Woo Baek  2 and Sung -Woo Kang  1,* \n1 Department of Industrial Engineering, Inha University , Incheon 22212 , Republic of Korea ;  \nwltnghd5182@inha.edu(J.H.) ; hym9771@gmail.com(Y.H) ;   \n2 Department of Industrial & Systems Engineering, Dongguk University , Seoul 04620, Republic of Korea ; \njwbaek@dongguk.edu  \n* Correspondence: kangsungwoo@inha.ac.kr  \nAbstract: The injection  molding  process is a traditional technique for making products in \nvarious industries such as electronics and automobiles via solidifying liquid resin into \ncertain molds. Although the process is not related to creating the main part of engines or \nsemiconductors, this manufacturing methodology sets the final form of the products. Re-\ncently, research has continued to reduce the defect rate of the injection molding  process. \nThis study proposes an optimal injection molding  process control system to reduce the \ndefect rate of injection molding  products with XAI (eXplainable Artificial Intelligence) ap-\nproaches. Boosting algorithms (XGBoost and LightGBM) are used as tree -based classifiers \nfor predicting whether each produ ct is normal or defective. The main features to control \nthe process for improving the product are extracted by SHapley Additive exPlanations , \nwhile the individual conditional expectation analyzes the optimal control range of these \nextracted features. To va lidate the methodology presented in this work, the actual injec-\ntion molding AI manufacturing dataset provided by KAMP (Korea AI Manufacturing \nPlatform) is employed for the case study. The results reveal that the defect rate decreases \nfrom 1.00% (Original d efect rate) to 0.21% with XGBoost and 0.13% with LightGBM, re-\nspectively . \nKeywords: XAI; Manufacturing Process ; Injection Molding ; SHAP ; ICE; \n \n1. Introduction  \nDuring the injection molding process, liquid raw materials are injected into a mold \nand hardened to produce a product. It is widely used as an effective technique to mass -\nproduce large core components and small parts, such as automobiles, displays, and sem-\niconductors. The injection molding process maintains a relatively high quality and has \nbeen improved over time.  \nInjection molding manufacturers have recently employed machine learning, deep \nlearning, and artificial intelligence to the injection molding process [1 -4]. However, ma-\nchine learning and deep learning often lack transparency and inte rpretability, making \nthem unfamiliar to field operators.  \nThe injection molding process has been continuously improved hereby reaching a \nhigh yield rate.(over 90%) However, achieving a process yield close to 100% from an al-\nready high -yield state requires f ine-tuning of process variables. This paper aims to reduce \nthe defect rate of injection -molded products, by employing eXplainable Artificial Intelli-\ngence (XAI) algorithm to fine -tune the process variables.    \n Traditional machine learning techniques that exhi bit black -box characteristics, lack \nthe ability to provide explanations for their predictions, thereby demonstrating limited \nreliability. This shortcoming poses significant challenges to their practical implementation \nin real -world processes. However, XAI methods provide clear reasons and justifications \nfor the model's outcomes. This feature makes XAI a suitable approach for fine -tuning pro-\ncess variables to improve the defect rate in injection molding processes. This paper aims \nto enhance the reliability of  the process and achieve even higher yield rates by employing \nXAI. Also, XAI enables field experts to more easily understand AI predictions by provid-\ning evidence for model learning.  \nSHAP (SHapley Additive exPlanations) extracts the main features affecting  product \ndefects. Tree -based algorithms, such as XGBoost and LightGBM, are used as training \nmodels for feature extraction. The optimal control range of features identified through \nSHAP is determined using the ICE (Individual Conditional Expectation) algori thm.  \nThe remainder of this paper is organized as follows. Section 1 introduces the motiva-\ntion and purpose of this study. Section 2 describes previous studies. Section 3 presents a \nmethodology that explains the process management method presented in this p aper. Sec-\ntion 4 presents the experimental results using actual injection molding process data. Sec-\ntion 5 discusses the conclusions and future work . \n2. Related Studies  \n2.1. Injection Process  \nThe injection molding process involves plastic molding. The struct ure of injection \nmolding process is shown in Figure 1.  \n \n \nFigure 1. Structure of Injection  Molding  Process  \nThe injection process involves plastic molding. This process is performed by injecting \na dissolved thermoplastic resin into a mold and cooling it[5] . \n \n \nFigure 2. Injection Process  \n \n The injection molding process has six stages, as shown in Figure 2: plasticization, clamp-\ning, filling, packing, cooling, demolding, and ejection [6].  \n1. Plasticization stage: The screw moves forward, and the plastic resin is dissolved by a \nheated barrel.  \n2. Clamping stage: The oil pressure system enables the plastic resin  to fit the fixed and \nmovable parts of the mold closely.  \n3. Filling stage: The mold is filled with dissolved plastic resin from the nozzle.  \n4. Packing stage: To prevent the volume from shrinking, pressure was applied before \nthe plastic resin hardens comple tely.  \n5. Cooling stage: The dissolved plastic resin is cooled and hardened.  \n6. Demolding and ejection stage: When the mold is opened, the resin shrinks, and the \nproduct is ejected.  \nThe injection molding products are processed by repeating the clamping, dem olding, \nand ejection stages. Because the injection molding process produces finished products, a \nhigh quality must be maintained. Therefore, the optimal management of variables, such \nas temperature and pressure, which are the major variables that determine  product qual-\nity, is very important for improving the process product yield.  \nControlling the parameters of the injection molding process is important for optimiza-\ntion in various fields. In the field of injection molding process control for internal combus-\ntion engines, numerical analysis of the injection molding process is performed by model-\ning and computer simulations based on multiple fuel injections[7]. The AVL Boost simu-\nlation application is used to monitor engine functionality. However, the simulation u sed \nonly three monitoring conditions. This study uses continuous feature conditions to pro-\npose the control range of main features. In the medical field, research on injection molding \nprocess optimization is also being conducted. A polycaprolactone parts de velopment sys-\ntem is proposed for future implants through several injection molding parameter im-\nprovements, including the melting temperature, injection time, and injection pressure[8]. \nThe results of this system demonstrate the potential of using simulatio ns as tools to opti-\nmize the injection -molding process. However, the data used in this study are artificial data \ngenerated from the literature. Therefore, it is necessary to consider its application in actual \nprocesses.  \nInjection molding process has low def ect rate. Therefore, failure data is extremely lower \nthan the normal product data. Consequently, when applying artificial intelligence to in-\njection molding process data, an imbalance between normal and defective data is inherent. \nVarious studies have been conducted to address this issue [9 -11]. SMOTE(Synthetic Mi-\nnority Over -sampling TechniquE) is appropriate for addressing data imbalance in manu-\nfacturing processes because it generates new data points between existing variable val-\nues[9]. This study employs t he SMOTE technique to augment defective data, thereby re-\nsolving the imbalance problem . \n2.2. eXplainable Artificial Intelligence(XAI)  \nUnlike existing AI, explainable XAI is a  algorithm that increases reliability by pre-\nsenting validity and grounds for machine learning[12]. Original AI has the “black box” \ncharacteristic that does not provide grounds for prediction results. In 2017, the Defense \nAdvanced Research Projects Agency s uggested using XAI to address the limitations of AI,  \n as shown in Figure 3 [13]. Because of these characteristics of XAI, field experts can easily \nunderstand the prediction results . \n \n \nFigure 3. eXplainable Artificial Intelligence(XAI)  \nRecently, research in to yield improvement processes based on these factors has pro-\ngressed. Zhang proposed a fault -diagnosis system for oil -immersed transformers [14]. The \nsystem used the SHAP for feature selection and achieved a recall value of 0.96 for the fault \nsamples[15]. However, no additional measures were conducted for the selected features. \nThis study employs ICE algorithm to provide the optimal control range of each selected \nfeatures to the field experts.  \nTo improve manufacturing quality, rule -based explanations are pe rformed based on \nensemble machine learning[16]. Feature importance is used to obtain the most significant \nprocess conditions, and PDP(Partial Dependence Plot) and ICE plots are used to provide \na visual overview. However, the feature importance does not con sider the correlation of \neach feature. The SHAP algorithm creates a subset of each feature to extract the main fea-\ntures by calculating the correlations. In addition, this study uses the PDP and ICE plots to \ndetermine the optimal control range of the main f eatures . \n3. Methodology  \nThe injection  molding  process is a traditional manufacturing method with high pro-\nduction yield. This process is the final step in creating the surface of a product. Therefore, \nit is directly related to product defects, and strict yield management is required. Recently, \nXAI has  become a state -of-the-art methodology for improving manufacturing processes. \nThis paper presents a pilot study for implementing XAI to increase the injection molding  \nprocess yield. This study aims to improve the injection molding  process based on artifici al \nintelligence, and the methodology of the study is shown  in Figure 4. \n \n \nFigure 4. Flowchart of the Methodology  \n \n The injection process shows a data imbalance between normal and defect data owing \nto the high yield of its own nature. To resolve the data imbalance, the SMOTE technique \nis employed in the data preprocessing stage. (Section 3.1) Then, the tree -based classi fier \n(Section 3.2) trains a model for predicting the product's defect. The SHAP Algorithm (Sec-\ntion 3.3) extracts major features that critically affect defect prediction. Finally, the control \nrange of the major features is determined using the ICE algorithm  (Section 3.4). \n3.1. Data Preprocessing for Injection Process  \nThis study uses the injection molding  process data collected by sensors from a mold \nand machine[ 17]. The DataFrame is constructed by selecting controllable features such as \ntemperature and pressure. The injection molding  process has a high yield; therefore, the \nnumbers of normal data and defect data are imbalanced, which results in a biased analysis. \nTherefore, oversampling is performed to balance the data used in the study. To solve this \nproblem, this study employs the SMOTE algorithm for oversampling. SMOTE is a k -\nnearest neighbor (KNN) -based oversampling algorithm[ 18]. Figure  5 shows the operating \nprinciple of SMOTE . \n \nFigure 5. Operating Principle of the SMOTE Algorithm  \nFirst, one selects one of the data points of the minority class; in this case, the defect is \na minority class, such as the red squares ( 𝑥𝑖) in Figure  5. The squares represent defect  data \nfor the injection molding  process. One of the K nearest data points of the corresponding \ndata is randomly selected, and the difference between the two selected data points is \nmultiplied by the weight to generate new data, such as the green squares in  Figure 5(𝑥𝑛𝑒𝑤). \nIn this case, the weight is randomly generated between zero and one. The imbalance in \nthe data is resolved by repeating this process until a sufficient amount of data is generated. \nIn this study, defective data  are oversampled to equal  the amount of normal data. Because \nthe injection  molding  process data is distributed within a similar range owing to the \ncharacteristics of the process, the SMOTE algorithm is employed to generate virtual defect \ndatasets close to the original data . \n3.2. Tree Based Classifier(XGBoost, LightGBM)  \nThis  study uses a tree -based classifier to learn and predict whether products are de-\nfective. The tree -based classifiers used in this study are XGBoost and LightGBM. XGBoost \nis a gradient -boosting -based algorithm t hat combines several weak decision trees to build \na robust model[1 9,20]. XGBoost is widely used in many ways because of its parallel learn-\ning, fast calculation speed, and excellent performance. The learning process for XGBoost \nis shown in Table  1. \nTable 1 . XGBoost Algorithm  \nXGBoost (eXtreme Gradient Boosting)  \n \n Input:  \nInstance set of current node; feature dimension;  \nProcedure :  \n𝐽(𝑃)=0 \n𝐺= ∑𝑖∈𝐼 𝑔𝑖,𝐻= ∑𝑖∈𝐼 ℎ𝑖 \n𝑓𝑜𝑟 𝑘=1 𝑡𝑜 𝑛 𝑑𝑜 \n𝐺𝐿=0,𝐻𝐿=0  \n𝑓𝑜𝑟 𝑗 𝑖𝑛 𝑠𝑜𝑟𝑡𝑒𝑑  𝑑𝑜 \n𝐺𝐿=𝐺𝐿+ 𝑔𝑗,𝐻𝐿=𝐻𝐿+ 𝐻𝑗 \n𝐺𝑅=𝐺− 𝐺𝐿,𝐻𝑅=𝐻𝐿− 𝐻𝐿 \n𝑠𝑐𝑜𝑟𝑒 =max  (𝑠𝑐𝑜𝑟𝑒 ,𝐽(𝑃)) \n𝑒𝑛𝑑 \n𝑒𝑛𝑑 \nOutput:  Split with max score  \n \nLightGBM is a gradient -boost -based algorithm, like XGB oost[ 21, 22]. The primary \ntechnology used is gradient -based one -sided sampling ( GOSS), which applies multiplier \nconstants to low -weight objects. LightGBM uses memory more efficiently by dividing the \ntree leafwise rather than levelwise; therefore, it exhibits good speed and performance. A \nlevelwise tree requires additional operations to balance it. However, a leafwise tree is \nmore efficient, because it divides and calculates the node with the largest delta loss. The \nLightGBM learning process is shown in Table  2. \nTable 2. Ligh tGBM Algorithm  \nLightGBM  (Light G radient Boosting  Machine ) \nInput:  \n𝑇𝑟𝑎𝑖𝑛𝑖𝑛𝑔  𝑑𝑎𝑡𝑎 : \n𝑫={(𝑥1,𝑦1),(𝑥2,𝑦2),…,(𝑥𝑁,𝑦𝑁)}, \n 𝑥𝑖∈𝑥,𝑥⊆𝑅,𝑦𝑖∈ −1,+1; \n𝐿𝑜𝑠𝑠  𝑓𝑢𝑛𝑐𝑡𝑖𝑜𝑛 : 𝐿(𝑦,𝜃(𝑥)) \n \nIterations:  \n𝑴;Big gradient  data  sampling  ratio :a; \nslight  gradient  data  sampling  ratio :b; \n1.𝐶𝑜𝑚𝑏𝑖𝑛𝑒  𝑓𝑒𝑎𝑡𝑢𝑟𝑒𝑠  𝑡ℎ𝑎𝑡 𝑎𝑟𝑒 𝑚𝑢𝑡𝑢𝑎𝑙𝑙𝑦   \n𝑒𝑥𝑐𝑙𝑢𝑠𝑖𝑣𝑒 (𝑖.𝑒.,𝑓𝑒𝑎𝑡𝑢𝑟𝑒𝑠  𝑛𝑒𝑣𝑒𝑟  𝑠𝑖𝑚𝑢𝑙𝑡𝑎𝑛𝑒𝑜𝑢𝑠𝑙𝑦   \n𝑎𝑐𝑐𝑒𝑝𝑡  𝑛𝑜𝑛𝑧𝑒𝑟𝑜  𝑣𝑎𝑙𝑢𝑒𝑠 ) 𝑜𝑓 𝑥𝑖,𝑖={1,…,𝑁} 𝑏𝑦  \n𝑡ℎ𝑒 𝑒𝑥𝑐𝑙𝑢𝑠𝑖𝑐𝑒  𝑓𝑒𝑎𝑡𝑢𝑟𝑒  𝑏𝑢𝑛𝑑𝑙𝑖𝑛𝑔  (𝐸𝐹𝐵 ) 𝑡𝑒𝑐ℎ𝑛𝑖𝑞𝑢𝑒 ;  \n2.𝑆𝑒𝑡 𝜃0(𝑥)=𝑎𝑟𝑔𝑚𝑖𝑛 𝑐∑𝐿(𝑦𝑖,𝑐);𝑁\n𝑖  \n 3.𝑓𝑜𝑟 𝑚=1 𝑡𝑜 𝑀 𝑑𝑜 \n4.𝐶𝑎𝑙𝑐𝑢𝑙𝑎𝑡𝑒  𝑔𝑟𝑎𝑑𝑖𝑒𝑛𝑡  𝑎𝑏𝑠𝑜𝑙𝑢𝑡𝑒  𝑣𝑎𝑙𝑢𝑒𝑠 ; \n𝑟𝑖=|𝜕𝐿(𝑦𝑖,𝜃(𝑥𝑖))/𝜕𝜃(𝑥𝑖)|𝜃(𝑥)=𝜃𝑚−1(𝑥), 𝑖={1,…,𝑁} \n5.𝑅𝑒𝑠𝑎𝑚𝑝𝑙𝑒  𝑑𝑎𝑡𝑎  𝑠𝑒𝑡 𝑢𝑠𝑖𝑛𝑔  𝑔𝑟𝑎𝑑𝑖𝑒𝑛𝑡  𝑏𝑎𝑠𝑒𝑑  𝑜𝑛𝑒 \n 𝑠𝑖𝑑𝑒  𝑠𝑎𝑚𝑝𝑙𝑖𝑛𝑔  (𝐺𝑂𝑆𝑆 ) 𝑝𝑟𝑜𝑐𝑒𝑠𝑠 ;  \n𝑡𝑜𝑝𝑁 =𝑎 ×𝑙𝑒𝑛(𝐷);𝑟𝑎𝑛𝑑𝑁 =𝑏 ×𝑙𝑒𝑛(𝐷); \n𝑆𝑜𝑟𝑡𝑒𝑑 =𝐺𝑒𝑡𝑆𝑜𝑟𝑡𝑒𝑑𝐼𝑛𝑑𝑖𝑐𝑒𝑠 (𝑎𝑏𝑠(𝑟)); \n𝐴=𝑠𝑜𝑟𝑡𝑒𝑑 [1:𝑡𝑜𝑝𝑁 ]; \n𝐵=𝑅𝑎𝑛𝑑𝑜𝑚𝑃𝑖𝑐𝑘 (𝑠𝑜𝑟𝑡𝑒𝑑 [𝑡𝑜𝑝𝑁 :𝑙𝑒𝑛(𝐷)],𝑟𝑎𝑛𝑑𝑁 ); \n𝐷́=𝐴+𝐵; \n6.𝐶𝑎𝑙𝑐𝑢𝑙𝑎𝑡𝑒  𝑖𝑛𝑓𝑜𝑟𝑚𝑎𝑡𝑖 𝑜𝑛 𝑔𝑎𝑖𝑛𝑠 ; \n𝑉𝑗(𝑑)= ((∑ 𝑟𝑖+((1−𝑎)/𝑏)∑ 𝑟𝑖\n𝑥𝑖∈𝐵𝑙 𝑥𝑖∈𝐴𝑙)2\n/ 𝑛𝑙𝑗(𝑑)\n+ (∑ 𝑟𝑖+((1−𝑎) / 𝑏)∑ 𝑟𝑖\n𝑥𝑖∈𝐵𝑟 𝑥𝑖∈𝐴𝑟)2\n/𝑛𝑟𝑗(𝑑))/𝑛 \n7.𝐷𝑒𝑣𝑒𝑙𝑜𝑝  𝑎 𝑛𝑒𝑤  𝑑𝑒𝑐𝑖𝑠𝑖𝑜𝑛  𝑡𝑟𝑒𝑒  𝜃𝑚(𝑥)′ 𝑜𝑛 𝑠𝑒𝑡 𝐷′ \n8.𝑈𝑝𝑑𝑎𝑡𝑒  𝜃𝑚(𝑥)= 𝜃𝑚−1(𝑥)+ 𝜃𝑚(𝑥) \n9.𝐸𝑛𝑑 \nOutput:  Return 𝜃̃(𝑥)= 𝜃𝑀(𝑥) \n3.3. Shapley Additive exPlanations (SHAP)  \nThe SHAP algorithm extracts the main features of the injection molding  process by \nexploring the impact of each feature on product quality. The algorithm is based on Shap-\nley's game theory, which examines how individuals make decisions when faced with in-\nterdependent circumstances. This algorithm regards each manufacturing feat ure as an in-\ndividual in game theory. The impact on feature i is analyzed using the process described \nin Figure  6. \n \nFigure 6. Procedure for Obtaining the Shapley Value  \n \n 𝑣(𝑆)= ∫𝑓̂(𝑥1,…,𝑥𝑛)𝑑𝑃𝑥∉𝑆−𝐸𝑥(𝑓̂(𝑋)) (1) \n𝜙𝑖(𝑣)= ∑|𝑆|!(𝑛−|𝑆|−1)! \n𝑛!(𝑣(𝑆⋃{𝑖})−𝑣(𝑆))\n𝑆⊆1,…,𝑛 {𝑖} (2) \n𝜙𝑖:𝑆ℎ𝑎𝑝𝑙𝑒𝑦  𝑉𝑎𝑙𝑢𝑒  𝑓𝑜𝑟 𝑚𝑎𝑛𝑢𝑓𝑎𝑐𝑡𝑢𝑟𝑖𝑛𝑔  𝑓𝑒𝑎𝑡𝑢𝑟𝑒  𝑖 \n𝑛:𝑇𝑜𝑡𝑎𝑙  𝑛𝑢𝑚𝑏𝑒𝑟  𝑜𝑓 𝑚𝑎𝑛𝑢𝑓𝑎𝑐𝑡𝑢𝑟𝑖𝑛𝑔  𝑓𝑒𝑎𝑡𝑢𝑟𝑒𝑠  \n𝑆:𝑆𝑢𝑏𝑠𝑒𝑡  𝑡ℎ𝑎𝑡 𝑑𝑜𝑒𝑠  𝑛𝑜𝑡 𝑐𝑜𝑛𝑡𝑎𝑖𝑛  𝑚𝑎𝑛𝑢𝑓𝑎𝑐𝑡𝑢𝑟𝑖 𝑛𝑔 𝑓𝑒𝑎𝑡𝑢𝑟𝑒  𝑖 \n𝑣(𝑆)∶𝐶𝑜𝑛𝑡𝑟𝑖𝑏𝑢𝑡𝑖𝑜𝑛  𝑜𝑓 𝑎 𝑠𝑢𝑏𝑠𝑒𝑡  𝑆 \n𝑣(𝑆⋃𝑖):𝐶𝑜𝑛𝑡𝑟𝑖𝑏𝑢𝑡𝑖𝑜𝑛  𝑜𝑓 𝑎 𝑠𝑢𝑏𝑠𝑒𝑡  (𝑆⋃𝑖)  \nThe SHAP algorithm generates every possible subset of each manufacturing feature. \nTo examine the influence of a manufacturing feature, one subtracts the algorithm subsets \nthe contribution of a subset which does not contain features from the contribution of  a \nsubset; the contribution of the subset is calculated as shown in (1). To check the im-\nportance of the feature, as shown in (2), a value called the Shapley value is calculated. In \nthis study, the Shapley values are used to select the main features. The me an absolute \nShapley Value is used to consider both the negative and positive influences on the product. \nFigure 7 shows the Shapley Value for each instance and expresses the mean of the absolute \nShapley Value. The SHAP algorithm addresses the limitations of  traditional variable im-\nportance methods (e.g., Feature Importance) by accounting for both negative and positive \ninteractions between variables . \nThe injection features are sorted in descending order of importance. The main  fea-\ntures of the process  are selec ted based on the line in which the cumulative importance of \nthe features is 70% of the total importance . \n \nFigure 7. Representative Plots of the SHAP Value  \n3.4. ICE and PDP  \nTo explore the conditions for improving the injection quality, both the ICE and PDP \nalgorithms are proposed to determine the control range of the main features. The ICE pre-\ndicts the target value of an instance according to the changes in the feature values of the \nmanufacturing process. In the injection  molding  process, the target valu e is predicted by \nfixing other features (temperature and RPM) and changing a particular feature (pressure) \nto propose a control pressure range. The ICE process is presented in Table  3. \nTable 3. Procedure Used by the ICE Algorithm to Predict the Control Range in the Injection Process  \n \n ICE algorithm to predict the control range in injection molding process  \nInput:  \n𝑋𝑖∶𝐴 𝑠𝑝𝑒𝑐𝑖𝑓𝑖𝑐  𝑚𝑎𝑛𝑢𝑓𝑎𝑐𝑡𝑢𝑟𝑖𝑛𝑔  𝑓𝑒𝑎𝑡𝑢𝑟𝑒  𝑓𝑜𝑟  \n𝑝𝑟𝑒𝑠𝑒𝑛𝑡𝑖𝑛𝑔  𝑡ℎ𝑒 𝑐𝑜𝑛𝑡𝑟𝑜𝑙  𝑟𝑎𝑛𝑔𝑒  \n𝑋𝑖′∶𝐴𝑙𝑙 𝑚𝑎𝑛𝑢𝑓𝑎𝑐𝑡𝑢𝑟𝑖𝑛𝑔  𝑓𝑒𝑎𝑡𝑢𝑟𝑒𝑠  𝑒𝑥𝑐𝑒𝑝𝑡  𝑋𝑖 \n𝑁∶𝑁𝑢𝑚𝑏𝑒𝑟  𝑜𝑓 𝑖𝑛𝑠𝑡𝑎𝑛𝑐𝑒  \n𝑝,𝑞∶𝐸𝑎𝑐ℎ 𝑖𝑛𝑠𝑡𝑎𝑛𝑐𝑒  \nProcedure:  \n1.𝐼𝑛𝑖𝑡𝑖𝑎𝑙𝑖𝑧𝑒  𝑚𝑜𝑑𝑒𝑙  𝑤𝑖𝑡ℎ 𝑎 𝑐𝑜𝑛𝑠𝑡𝑎𝑛𝑡  𝑣𝑎𝑙𝑢𝑒   \n𝑓̂(𝑋𝑖(𝑝),𝑋𝑖(𝑞)′)𝑝,𝑞=1𝑁 \n2.𝑓𝑜𝑟 𝑞=1 𝑡𝑜 𝑁: \n𝑓𝑜𝑟 𝑝= 1 𝑡𝑜 𝑁: \n𝑋𝑖𝑝=𝑇ℎ𝑒 𝑣𝑎𝑙𝑢𝑒  𝑜𝑓 𝑋𝑖 𝑖𝑛 𝑖𝑛𝑑𝑒𝑥  𝑝   \n𝑋𝑖𝑞′=𝑇ℎ𝑒 𝑣𝑎𝑙𝑢𝑒  𝑜𝑓 𝑋𝑖′ 𝑖𝑛 𝑖𝑛𝑑𝑒 𝑥 𝑞   \n𝑃𝑙𝑜𝑡𝑡𝑖𝑛𝑔  𝑓̂(𝑋𝑖(𝑝),𝑋𝑖(𝑞)′) \nOutput:  ICE & PDP plot  \n \n4. Experimental Results  \nThis paper aims to present a process yield improvement methodology using XAI -based algo-\nrithms. The main features are derived using SHAP, and their control range is determined using ICE . \n4.1. Collection and Preprocessing for the Injection Process  \nThis study uses automobile windshield side molding injection  molding  process data \ncollected from October 16th, 2020 to November 19th, 2020. The total number of colle cted \ndata points is 7,99 0, and the number of features is 45. Total dataframe is shown in Table \nⅣ. The target value is “PassOrFail,” and it is expressed as  1 for normal products and 0 for \ndefective products . \nTable 4. Example of Injection Process Dataset . \nPassOFail  Average_  \nScrew_RPM  Max_  \nScrew_RPM  Barrel_  \nTemperature_1  … Max_  \nInjection_Pressure  \n1 292.5  30.7 276.5  ∙∙∙ 141.8  \n1 292.4  30.8 276.2  ∙∙∙ 141.7  \n1 292.5  30.8 276.2  ∙∙∙ 141.7  \n1 292.6  31.0 276.5  ∙∙∙ 141.5  \n1 292.6  30.8 276.8  ∙∙∙ 142.5  \n0 292.5  30.9 276.3  ∙∙∙ 142.6  \n1 292.5  31.0 275.5  ∙∙∙ 142.5  \n… … … … … … \n0 290.5  30.9 286.1  ∙∙∙ 142.6  \n  \n The preprocessing is performed in three steps. A dataframe is constructed by select-\ning 16 controllable features such as temperature, pressure, and RPM from the collected \nprocess features. Time features such as 'Filling_Time', 'Ejection_Time' and position fea-\ntures are excluded due to uncontrollability.  Also, p roducts with different process indices \nare excluded as they violate the control variables. Subseq uently, a process is conducted to \ncheck for missing values or outliers.  An example of the selected process features is pre-\nsented in Table 5. \nTable 5. Independent Variables of the Injection Molding Process Data . \nIndependent Variable  \n(Unit)  Description  \nMax_Screw_RPM  \n(mm/s)  Maximum speed of screw for injection  \nAverage_Screw_RPM  \n(mm/s)  Average speed of screw for injection  \nMax_Injection_Pressure  \n(MPa)  Maximum pressure applied to the molten resin flowing into \nthe mold  \nMax_Switch_Over_Pressure  \n(MPa)  Pressure converted from injection to packing pressure  \nAverage_Back_Pressure  \n(MPa)  Average pressure to prevent the screw from being pushed \nout \nBarrel_Temperature_1~7  \n(°C) Temperature of the barrel  \nHopper_Temperature  \n(°C) Temperature of the hopper  \nMold_Temperature_3, 4  \n(°C) Temperature of the mold  \n \nTraining and validation are performed using train –test splits. The training and test \ndatasets are split in a 5:5 ratio, and each split dataset is list ed in Table 6. \nTable 6. Result of the Train -Test Split  \n Normal  Defective  \nTrain Dataset  3,964  31 \nTest Dataset  3,955  40 \n \nThe SMOTE algorithm is used to balance the ratios of normal and defective data. The \nresults of the oversampling are listed in Table 7. \nTable 7. Oversampling Results  \n Normal  Defective  \nTrain Dataset  3,964  3,964  \nTest Dataset  3,955  40 \n4.2. Model Training for Injection Process  \nThis study uses a tree -based classifier, XGBoost, and LightGBM to train and predict \nwhether injection molding  process products are defective . The training dataset (Normal \nData: 3964 / Defective Data: 3964) is used for trainin g, and the Test Dataset (Normal Data: \n3955 / Defective Data: 40) is used to check the accuracy of the model. Additionally, cross -\nvalidation is performed to check the model's performance. During the cross -validation \nprocess, the number of subsets is set to three. For XGBoost, the accuracy of each cross - \n validation is 0.9947, 0.9977, and 0.9981, with a CV average accuracy of 0.9968. For \nLightGBM, the respective accuracies are 0.9924, 0.9955, and 0.9977, with a CV average \naccuracy of 0.9952.  The results of XGBo ost and LightGBM are presented in Table  8. \nTable 8. Model Training Results  \n Actual  \nNormal Data  Actual  \nDefective \nData  Accuracy  CV Average \nAccuracy  \nXGBoost  Predicted  \nNormal Data  3,941  25 \n99.02  0.9968  Predicted  \nDefective Data  14 15 \nLightGBM  Predicted  \nNormal Data  3,941  25 \n99.02  0.9952  Predicted  \nDefective Data  14 15 \n4.3. SHAP(S hapley Additive exPlanations)  \nTo verify the importance of features in the injection molding  process, the main fea-\ntures are extracted by using the SHAP algorithm. Figure 8 shows the mean absolute Shap-\nley value of each manufacturing feature for XGBoost and LightGBM . \n \nFigure 8. Shapley Value of Manufacturing Features (Left: XGBoost, Right: LightGB M) \nEach graph shows the importance of manufacturing features in descending order. \nFeatures with cumulative importance corresponding to 70% of the total are selected as the \nmain features. In the case of XGBoost, the main features are “Max Injection Pressure ,” \n“Average Back Pressure,” “Max Switch Over Pressure,” “Barrel Temperature 5,” “Max \nScrew RPM,” “Average Screw RPM,” and “Barrel Temperature 1.”  \nIn the case of LightGBM, the main features are “Max Injection Pressure,” “Max \nSwitch Over Pressure,” “Barrel T emperature 5,” “Average Back Pressure,” “Barrel Tem-\nperature 3,” and “Mold Temperature 4.” The selected main features and mean absolute \nShapley values are listed in Table 9. \nTable 9. Selected Main Features and Mean of the Absolute Shapley Value  \n XGBoost  Cumulative Ratio  Feature Name  Value  \n1 Max_Injection_Pressure  1.74 0.15 \n2 Average_Back_Pressure  1.52 0.28 \n \n 3 Max_Switch_Over_Pressure  1.21 0.38 \n4 Barrel_Temperature_5  0.93 0.46 \n5 Max_Screw_RPM  0.80 0.53 \n6 Average_Screw_RPM  0.77 0.59 \n7 Barrel_Temperature_1  0.75 0.66 \n LightGBM  Cumulative Ratio   Feature Name  Value  \n1 Max_Injection_Pressure  2.05 0.17 \n2 Max_Switch_Over_Pressure  1.92 0.34 \n3 Barrel_Temperature_5  1.06 0.43 \n4 Average_Back_Pressure  1.04 0.51 \n5 Barrel_Temperature_3  0.94 0.59 \n6 Mold_Temperature_4  0.87 0.67 \n4.4. ICE and PDP  \nThe ICE algorithm extracts the control range of the main features to reduce the pro-\ncess-defect rate. The ICE plots of the main features selected in Section  4.3 by each XGBoost \nand LightGBM, are given in Figure 9 and 10, respectively . \nEach control range of the main features is presented according to the algorithm de-\nscribed in Section 3.4. The PDP is the average of the ICE experimental results, which are \nrepresented by orange dotted lines in Figures 9 and 10. The minimum and maximum PDP \nvalues of each main feature are indicated by red lines in Figures 9 and 10.  \nFor example, in the case of  Figure 10 (b), the maximum PDP value is 0.73, and the \nminimum value is 0.26. Both values are calculated according to the change in the x value \nMax_Switch_Over_Pressure. Tables  10 and 11 show the control ranges of the main fea-\ntures  for alpha values of 0.05 , 0.1, and 0.2 based on the y -axis maximum values . \nTable 1 0. Control Range of the Main Features for Three Alpha Values (XGBoost Results)  \nα \nVariable  0.05 0.1 0.2 \nMax_Injection_Pressure  [141.60, 142.40]  [141.20, 183.20]  [141.20, 183.20]  \nAverage_Back_Pressure  [13.30, 90.80]  [13.30, 90.80]  [13.30, 90.80]  \nMax_Switch_Over_Pressure  [115.60, 136.50]  [115.60, 136.52]  [115.60, 136.52]  \nBarrel_Temperature_5  [236.30, 255.00]  [236.30, 266.40]  [236.30, 266.40]  \nMax_Screw_RPM  [30.30, 31.20]  [30.30, 31.20]  [30.30, 31.20]  \nAverage_Screw_RPM  [29.00, 293.40]  [29.00, 293.40]  [29.00, 293.40]  \nBarrel_Temperature_1  [244.70, 287.10]  [244.70, 287.10]  [244.70, 287.10]  \nTable 1 1. Control Range of the Main Features for Three Alpha Values (LightGBM  Results)  \nα \nVariable  0.05 0.1 0.2 \nMax_Injection_Pressure  [141.50, 142.20]  [141.20, 183.20]  [141.20, 183.20]  \nMax_Switch_Over_Pressure  [115.60, 119.00]  [115.60, 119.55]  [115.60, 136.80]  \nBarrel_Temperature_5  [236.30, 254.90]  [236.30, 255.00]  [236.30, 266.40]  \nAverage_Back_Pressure  [13.30, 60.00]  [13.30, 60.00]  [13.30, 60.00]  \nBarrel_Temperature_ 3 [285.50, 285.80]  [245.00, 285.40]  [245.00, 285.40]  \nBarrel_Temperature_ 4 [20.60, 22.60]  [20.60, 22.69]  [20.60, 27.70]   \n  \nFigure 9. ICE Plots of XGBoost  \n \n  \nFigure 10. ICE Plots of LightGBM  \nTo validate the methodology, the test dataset presented in Table 12 is utilized. The \ntest dataset is not oversampled to reflect the low defect rate of the actual process. Subse-\nquently, the optimal con trol range specified in Tables 10 and 11 is applied, and only the \nproducts produced within this range are selected.  The defect rate from the test data set is \ncompared with the original defect rate to determine whether the process has improved. \nThe validati on results are presented in Table 12. \nTable 12. Validation Results  \n XGBoost  Defect rate (%)  Normal  Defect  \nα = 0.05  969 2 0.21 \nα = 0.1 2284 20 0.88 \nα = 0.2 2284 20 0.88 \nOriginal Data  3995 40 1.00 \n LightGBM  Defect rate (%)   Normal  Defect  \nα = 0.05  N/A N/A N/A \nα = 0.1 N/A N/A N/A \n \n α = 0.2 2314 3 0.13 \nOriginal Data  3995 40 1.00 \n \nWhen the alpha value decreases, the defect rate also decreases because of the tight \ncontrol range of the process features. In the case of LightGBM, for alpha values of 0.05 and \n0.1, the defect rate cannot be calculated because no data exist in this range. This also indi-\ncates that defective products are not produced. For all six experiments, the defect rate was \nlower than the original defect rate of 1.00%. Based on the validation, LightGBM is better \nfor controlling the injection molding process than XGBoost.  However, both algorithms \nrequires less than a minute to process the data . \n5. Conclusion  \nThis paper proposes an optimal injection molding process control model to minimize the defect \nrate during the injection molding process. The methodology proposed in this study selects the main \nfeatures of the injection molding process and presents the control range of the main features by using \nXAI. To predict whether the products are defective, tree -based classifier models (XGBoost and \nLightGBM) are used.  The main features affecting the product defectivity are selected using the SHAP \nalgorithm. The control range of the selected main features is presented by using ICE algorithm.  \nA test dataset was used to verify the defect rate reduction for validation. The  original dataset \nconsisted 3,995 of normal data values and 40 defect data values. The defect rate in the original dataset \nwas 1.00%. Using XGBoost, the improved dataset comprised 969 normal data values and 2 defect \ndata values. The defect rate in the impr oved dataset was 0.21%. Using LightGBM, the improved \ndataset consisted of 2,314 normal data values and three defect data values. The defect rate of the \nimproved dataset was 0.13%. The defect rates were 0.79% and 0.87%, respectively.  \nThis study proposes an optimal model for improving product yield using injection molding pro-\ncess data. Compared with traditional AI approaches, XAI allows injection domain experts who may \nlack expertise in AI to understand the results of the methodology. As the injection molding  process is \nnot performed automatically in this study, it could help support injection engineers in improving the \nyield rate by providing the main features with control ranges. The study authors collaborated with LG \nElectronics to decrease the defect rate in the injection molding process.  \nThis study focuses on the controllable variables in the injection molding process. The field ex-\nperts from LG Electronics identified the 16 features, and excluded 29 features including time and \nposition features. Therefore,  the significance of this study lies in its ability to improve process yield \nby adjusting the values of the main features identified in the methodology. Also, it enables field ex-\nperts to more easily understand AI predictions by providing evidence for model  learning by using \nXAI.   \nThrough the collaborating research projects with industries, the methodology presented in this \npaper is extended to the practice level. Also, process datasets other than injection molding process \ndatasets should be conducted to ex pand the model to various manufacturing areas. In addition, the \napplication of neural -network -based classification models or reinforcement learning techniques \nshould be analyzed for automated manufacturing processes . \nAbbreviations  \nThe following abbreviatio ns are used in this manuscript:  \nSHAP  Shapley Additive exPlanations  \nICE Individual Conditional Expectation  \nPDP  Partial Dependence Plot  \nXAI eXplainable Artificial Intelligence  ",
      "metadata": {
        "filename": "Enhancing the Product Quality of the Injection Process Using eXplainable Artific.pdf",
        "hotspot_name": "Housing_Production",
        "title": "Enhancing the Product Quality of the Injection Process Using eXplainable\n  Artificial Intelligence",
        "published_date": "2025-03-04T06:59:01Z",
        "pdf_link": "http://arxiv.org/pdf/2503.02338v1",
        "query": "injection molding process optimization energy efficiency plastic manufacturing"
      }
    },
    "Impact of high-pressure torsion on hydrogen production from photodegradation of": {
      "full_text": "1 \n International Journal of Hydrogen Energy, vol. 81, pp. 411-417, 2024  \nhttps://doi.org/10.1016/j.ijhydene.2024.07.306  \n \n \n \nImpact of high -pressure torsion on hydrogen production from \nphoto degradation of polypropylene plastic wastes  \n \nThanh Tam Nguyena,b and Kaveh Edalatia,b,* \n \na WPI, International Institute for Carbon Neutral Energy Research (WPI -I2CNER), Kyushu \nUniversity, Fukuoka 819 -0395, Japan  \nb Mitsui Chemicals, Inc . -.Carbon Neutral Research Center (MCI -CNRC), Kyushu University, \nFukuoka 819 -0395, Japan  \n \nPlastic waste entering the environment through landfilling or improper disposal poses substantial \nrisks to ecosystems and human health. Photoreforming is emerging as a clean photocatalytic \ntechnology that degrades plastic waste to organic compounds while s imultaneously producing \nhydrogen fuel. This study introduces high -pressure torsion (HPT), a severe plastic deformation \n(SPD) method, as an innovative technique to enhance the photoreforming of polypropylene (PP) \nplastic mixed with a brookite TiO 2 photo catalyst. Hydrogen production systematically increases \nwith the number of HPT turns, accompanied by the formation of valuable small organic molecules. \nThe enhancement in photocatalytic activity is attributed to strain -induced defect formation in both \ncatal ysts and plastics, as well as the creation of catalyst/plastic interphases that enhance charge \ncarrier transport between inorganic and organic phases . These findings reveal a new functional \napplication for SPD in energy conversion and sustainability.  \n \nKeywords:  Photocatalysis; Hydrogen generation , Homopolymer polypropylene ( h-PP); \nNanostructured ceramics; Gas chromatography  - mass spectrometry (GC -MS) \n \n \n*Corresponding author (E -mail: kaveh.edalati@kyudai.jp; Tel: +80 -92-802-6744)  \n \n  2 \n 1. Introduction  \nThe global consumption of plastics continuous ly rises due to extensive human activities, \nbecoming inevitable as plastics  are integral to nearly every economic sector  [1]. Annually, millions \nof tons of thermoplastic polymers such as polypropylene (PP), polyethylene (PE), polyvinyl \nchloride (PVC) and polyethylene terephthalate (PET) are manufactured [2-5]. Despite their \nadvantageous mechanical properties, thermoplastics have notably short useful lifespans,  and their \ndisposal poses significant environmental ch allenges due to their low degradation rates  [6,7] . \nAmong thermoplastics, PP, which  is favored for its low production cost, excellent thermal and \nchemical resistance, high tensile strength, and relatively low densi ty [8,9], is utilized in producing \nvarious items including bottles, toys, automotive parts, medical devices, and fabrics [10-12]. \nHowever, PP as a non -biodegrada ble thermoplastic  polymer exhibits  a slow degradation rate , \nleading to its accumulation in landfills worldwide without being recycled  [9]. \nSeveral methods have been investigated to mitigate plastic waste, including thermal \ndegradation  [13], incineration  [14], landfill ing [15] and ozonation  [16]. However, these approaches \nare often energy -intensive and costly. Recent investigations have focused on alternative techniques \nsuch as biodegradation [17] and photocatalysis  [18]. Biodegradation involves microbes producing \nenzymes that break down macromolecules into small fragments, potentially leading to the \ncomplete mineralization of plastic wastes  [19]. Photocatalysis  is a promising and efficient process  \nthat uses only sunlight to degrade a wide range of organic pollutants into CO 2 and H 2O [20]. A \nmodern advancement in photocatalysis is photoreforming, schematically shown in Fig. 1a , which \ninvolves the simultaneous reduction of water and the oxidation of organic materials [21-23]. The \nphotoreforming process can particularly provide a clean pathway for upcycling of plastic wastes \n[24-26]. This process  uses plastic waste as a feedstock for clean hydrogen production  and small \norganic molecule  generation  under mild processing conditions  [27-32]. \nTitanium dioxide (TiO 2) is recognized for its exceptional semiconducting, optical, and \ncatalytic properties, making it an effective and stable photocatalyst  [33-35]. TiO 2 has three natural \ncrystalline polymorphs including anatase (tetragonal), brookite (orthorhombic), and rutile \n(tetragonal)  [36]. A previous study demonstrated that brookite exhibits the highest activity for the \nphotoreforming  of PET plastic due to easy hydroxyl radical formation, moderate depth of electron -\ntrap states, slow decay rate, and efficient charge transfer kinetics  [37]. Despite this report , there \nhave been no attempts to use brookite in the photoreforming of hard -to-degrade plastics such as \nPP. \nIn this study, brookite TiO 2 is used for simultaneous PP plastic degradation and hydrogen \nproduction . Due to the low efficiency of photoreforming for PP,  severe plastic deformation (SPD) \nvia the high-pressure torsion (HPT) method  [38,39] is used  to mix PP and brookite and successfully \nenhance the activity.  HPT is a relatively new dopant -free approach in the field of catalysis that is \nbased on mechanical processing [39]. In contrast to HPT, most other dopant -free approaches \napplied to TiO 2 photocatalysts, such as the introduction of titanium interstitials [40], enrichment \nof oxygen [41], reduction of oxygen to create black titania [42], and polymorphic transformations \nlike two -dimensional phase production [43], are based on chemical processing.  \n \n2. Experimental Procedure  \nHomopolymer PP (h -PP) powder with spherical particles and sizes of less than 63 µm was \nsupplied by Mitsui Chemicals, Inc., Japan, while brookite with a purity of 99.99% was obtained \nfrom Kojundo Chemical Company, Japan. Examination of brookite powder by sca nning electron \nmicroscopy (SEM, as shown in Fig. 1b ) indicated an average particle size of 2.6 µm, and the 3 \n presence of numerous crystals within these microp articles  was confirmed by transmission electron \nmicroscopy (TEM, as shown in Fig. 1c ). \n \n \nFigure 1.  (a) Schematics of photoreforming process for PP plastic, (b) morphology of brookite \nexamined by SEM, and (c) examination of crystals in brookite by TEM.  \n \nA mixture of 50 wt% brookite and 50 wt% PP plastic was prepared in acetone using a \nmortar and pestle for 30 min. This blended powder was then pressed into a disc with a 10 mm \ndiameter under a pressure of 380 MPa. The disc underwent mechanical processing vi a the HPT \nmethod at a pressure of P = 6 GPa at ambient temperature, with a rotation speed of 1 rpm for N = \n4 \n 1 (moderate  external strain) and 3 turns  (strong external strain) . After HPT processing, the brookite \nand PP mixture, which was in the form of a disc, was manually crushed using a mortar and pestle.  \nThe microstructure of the HPT -processed samples was examined by SEM coupled with \nenergy -dispersive X -ray spectroscopy (EDS) under an acceleration voltage of 5 keV . The \ncrystalline structures were analyzed by X -ray diffraction (XRD) with Cu K α irradiation. \nAttenuated total reflectance Fourier transform infrared spectroscopy (FTIR) within the 600 -4000 \ncm-1 wavenumber range was used to investigate the molecular composition and structure of the PP \nplastic. Steady -state photoluminescence (PL) measurements with a  325 nm laser source were \nperformed to assess the radiative electron -hole recombination.  \nFor photoreforming experiments, 100 mg of powder mixtures (containing 50 mg of \nbrookite and 50 mg of PP) before and after HPT processing were added to 3 mL of 10 M NaOH  \nsolution . Subsequently, 250 µL of 0.01 M Pt(NH 3)4(NO 3)2 was added, providing platinum as a co -\ncatalyst at 1 wt % of the brookite catalyst. This mixture was sonicated for 5 min, air -evacuated with \nargon for 30 min, and then irradiated under continuous stirring with the full arc of a 300 W xenon \nlamp at an intensity of 18 kW/m², maintaining a constant temperature of 25 °C. The amount of \nproduced hydrogen was measured using gas chromatography (GC) equipped with a thermal \nconductivity detector.  \nThe degradation products of PP plastic were identified using a gas chromatography - mass \nspectrometry (GC -MS QP2010, Shimadzu, Japan) device equipped with a fused silica capillary \ncolumn coated with CP -SIL 8 CB. Before GC -MS analysis, liquid samples after the photocatalytic \ntests were pre -treated using the solid phase extraction (SPE) technique with crosslinked \npoly(styrene divinylbenzene) (ENV+) columns. The activation process for SPE columns involved \nusing 1 mL of methanol followed by 1 mL of deionized wa ter. Then, 20 mL of the post -\nphotocatalysis sample was slowly loaded through the SPE column. Next, 10 mL of methanol was \nused to elute the extracted substances. The collected samples were evaporated under argon gas, \nreducing the volume from 10 to 2 mL. The  GC oven was initially held at 80  °C for 5 min, then \nheated to 270  °C at a rate of 50  °C/min, and maintained at 270  °C for 3 min. The transfer line and \nMS ion source temperatures were set to 280  °C and 230  °C, respectively. An aliquot of 1 µL of the \nextrac ted sample was injected into the GC injector, maintained at 250  °C, and MS -EI was \nemployed to screen the degradation products over a mass -to-charge ( m/z) range of 30 -550, with \nhelium serving as the carrier gas for the analysis.  \n \n3. Results  \nFig. 2 shows the SEM and SEM -EDS images of brookite and PP plastic mixed using HPT \nfor N = 3 turns. As shown in Fig. 2a , using SEM back -scatter electron mode, and in Fig. 2b -d, \nusing EDS elemental mapping, PP and brookite are significantly mixed. Neither PP nor brookite \nretain their initial shapes, indicating the occurrence of deformation during the process. In addition \nto mechanical mixing, which results in the formation of br ookite -PP interphases, a large number \nof cracks are observed in PP, as shown in h igher magnification in Fig. 2e . Moreover, a large amount \nof nano -sized brookite is detected on PP in Fig. 2e . This indicates that the brookite micropowders \nare partly fragmented during HPT processing. This observation is inconsistent with earlier \npublications on the application of HPT to different ceramics [38,39], including brookite  [44], \nwhich reported powder consolidation rather than fragmentation. This difference is due to the \npresence of PP, which forms a composite with brookite. It is known that components in composites \ncan be fragmented by HPT processing due to constraints in their co-deformation [45]. The  5 \n formation of nano -sized brookite from the initial brookite micropowder  along with the large \ncontact area between brookite and PP, are beneficial for photoreforming.  \n \n \nFigure 2.  (a) SEM image in secondary electron mode and (b -d) corresponding EDS mappings, and \n(e) SEM image in backscatter electron mode for brookite -PP powder mixture processed by HPT \nfor N = 3 turns.  \n \nFig. 3a shows the XRD profiles of the starting brookite without HPT processing and the \nbrookite -PP mixture processed by HPT with N = 1 and 3 turns. Brookite exhibits the orthorhombic \nstructure with the Pbca space group in all samples. The lattice parameters are a = 9.1740 nm, b = \nc = 5.4490 nm, and α = β = γ = 90°, which are in good agreement with the JCPDF 01 -076-1936 \ncard for brookite. No phase transformation is detected after HPT processing, but peak broadening \n6 \n occurs, indicating lattice strain caused by the formation of linear and planar defects by HPT  \n[38,39,44,45 ]. Such defects can act as active sites for enhancing photocatalytic activity.  \nThe FTIR spectrum of the initial PP plastic without HPT processing and the brookite -PP \nmixture processed by HPT with N = 1 and 3 turns are shown in Fig. 3b . The FTIR peaks of PP \nplastic include isotactic at around 800 -1300 cm-1, -CH 3 bend at 1377 cm-1, -CH 2 bend at 1458 cm-\n1, and C -H stretch at 2700 -3000 cm-1, which are consistent with previous reports  [46-48]. After \nmixing with brookite u sing HPT, the peak intensities of PP decrease, and the TiO 2 bands  [49] \nappear in the FTIR spectra. The FTIR spectra suggest that HPT does not lead to detectable \nmolecular structure changes in PP.  \n \n \nFigure 3.  (a) XRD spectra of initial brookite without HPT and brookite -PP mixture after N = 1 and \n3 HPT turns, (b) FTIR spectra of initial PP plastic without HPT and brookite -PP mixture after N = \n1 and 3 HPT turns, (c) PL spectra of initial brookite without HPT and brookite -PP mixture after N \n= 1 and 3 HPT turns, and (d) PL spectra of brookite and brookite -PP mixture after HPT processing \nfor N = 1 turns.  \n \nThe radiative recombination of photo -induced electrons and holes was examined by steady -\nstate PL ( Fig. 3c and 3d). In the brookite PL spectrum ( Fig. 3c ), a PL peak at around 410 nm \ncorresponds to band -to-band electron -hole recombination, and another peak at a higher wavelength \naround 520 nm corresponds to recombination on defects  [37]. The PL intensities in the mixture \nsamples of PP and brookite processed by  HPT decrease significantly compared to the initial \nbrookite powder. It can be concluded that the ele ctron -hole recombination in the HPT -processed \nmixture is less than in the initial brookite. SPD processes such as HPT generate  various defects in \nthe sample structure, including vacancies, dislocation s and grain boundaries  [46,50], and these \ndefects have been proven to suppress the recombination of electrons and holes after HPT \n7 \n processing in brookite  [44]. Earlier studies using X -ray photoelectron spectroscopy (XPS) \nconfirmed that while the overall valences of titanium and oxygen do not change through HPT \nprocessing of TiO 2, some oxygen vacancies are formed [44,51] . In addition to XPS, which shows \nthe formation of vacancies on the surface, oxygen vacancies in the subsurface and bulk of HPT -\nprocessed brookite were also confirmed earlier by Raman spectroscopy and electron spin \nresonance, respectively [44]. It should be noted that a high HPT processing temperature can \nenhance the formation of vacancies; however, high temperatures were avoided in this study due to \nthe presence of PP plastics. Instead, a larger external strain was applied in this study by increasing \nthe number of turns to N = 3, which is known to be effective in enhancing the fraction of defects \n[38,39] . To further elucidate whether mixing with PP has any extra effect on electron -hole \nrecombination, both pure brookite and brookite -PP mixtures were processed by HPT for 1 turn, \nand their photoluminescence spectra were compared. As shown in Fig. 3d , under similar HPT \nconditions, the PL intensity decreases from 200 cps to 50 cps with the addition of PP. This \nreduction indicates a significant decrease in electron -hole recombination in the mixture of brookite \nand PP plastic due to the transfer of charg e carriers between brookite and PP. Similar to organic -\ninorganic composite photocatalysts, PP (organic molecule) is likely excited, and its excited \nelectrons are injected into the conduction band of brookite (inorganic semiconductor), leading to \nlonger charge separat ion and slower recombination  [52-55]. These PL measurements suggest that \nbrookite -PP mixed by HPT has a higher potential for photoreforming.  \nA comparison of the hydrogen production using the brookite -PP mixture with and without \nHPT, along with the blank test, is presented in Fig. 4 . The blank tests, either without catalyst under \nirradiation (curve from 0 to 20 h) or with catalyst in the absence of light (datum at time zero), show \nno hydrogen production without photoreforming. For the sample without HPT processing ( N = 0), \nthe amount of hydrogen production increases with the increase of irradiation time until 4 h and \nsubsequently stops at a level  of 0.5 mmol g-1. The amount of hydrogen production increases to 2.0 \nmmol g-1 and 2. 7 mmol g-1 after HPT processing for N = 1 and 3 turns, respectively ( 4-5 times \nenhancement).  It should be noted that the standard deviation of gas amount measurements by gas \nchromatography for three different tests was less than 10%, confirming the reliability of the gas \nchromatograph system. Moreover, three independent tests for PP mixed with brookite without HPT \nprocessing, shown in Fig. 4 , indicate an average error of less than 10%, confirming the \nreproducibility of the photoreforming data. The increase in hydroge n production from N = 1 to N \n= 3 is due to the enhancement of the externally applied shear strain because the shear strain is \nproportional to the number of HPT turns ( γ = 2πrN/h; γ: external applied shear strain; r: distance \nfrom the rotation center; N: number of rotations; h: height of the sample) [56,57] . It is well \nestablished that the fraction of HPT -induced defects increases and phase mixing improves with \nincreasing shear strain [56,57] , which should be responsible for the higher activity of the sample \nprocessed with N = 3. A larger number of turns beyond N = 3 is expected to be even more effective, \nalthough a saturation of lattice defects usually occurs at high shear strains where no change in the \nmicrostructure occurs with further increases in the number of turns [57]. Increasing the number of \nturns can be technically challenging due to possible damage to the HPT anvils by hard brookite \nceramics; thus, rotations were limited to N = 3 in  this study. It is noted that after a certain reaction \ntime (4 h, 22 h, and 24 h for N = 0, N = 1, and N = 3, respectively), the production of hydrogen \nstops due to the consumption of PP plastics in contact with the catalyst. Since a larger number of \nturns mixes a more significant amount of PP and catalyst, the hydrogen production at the plateau \nregion is the most enhanced for N = 3. The e nhancement in photoreforming activity without phase \ntransformation or chemical treatment is promising for TiO 2 photocata lysis [33-37]. 8 \n  \n \nFigure 4.  Hydrogen production from photoreforming of PP plastic versus irradiation time for \nbrookite -PP mixtures without HPT ( N = 0) and with HPT processing for N = 1 and 3 turns, \nincluding blank test. Three independent tests were conducted for N = 0 to examine the reliability \nof the photoreforming system and the reproducibility of data.  \n \nSince the photoreforming process not only generates hydrogen from water in the reduction \nhalf-reaction but also produces various organic materials from PP in the oxidation half -reaction  \n(Fig. 1a ), GC -MS was used to identify the organic products. It should be noted that the \nidentification of products was conducted using standard peaks provided by the NIST (National \nInstitute of Standards and Technology) mass spectral search program available  in the authors’ GC -\nMS machine. The possible chemical compounds and the degradation pathway detected by GC -MS \nare summarized in Table 1 . The oxidized products in Table 1  are considered to have low toxicity \nto human health and the aquatic environment according to the Hazardous Substances Data Bank \nof the International Agency for Research on Cancer and the National Institute of Health  [58]. \nMoreover, since they are small molecules, they can be used as initial materials in various chemical \nreactions. Taken together, simultaneous photocatalytic hydrogen production and PP plastic \ndegradation is achievable using brookite as a catalyst and HPT a s a pre -catalysis treatment.  Such \na process applies not only to PP but also potentially to a wide range of thermoplastics that have \napplications in various economic sectors [1-12]. Despite these promising results, future studies are \nneeded to verify  the long -term applicability of the current catalytic system  because cycling \nperformance is considered a main requirement for commercializing photocatalysts such as TiO 2 \n[33,34] . \n \n4. Discussion  \nHere, two critical points warrant further discussion: (i) the underlying mechanism behind \nthe simultaneous photocatalytic hydrogen production and PP plastic degradation and (ii) the \nreasons for the superior photocatalytic performance of the mixture of brookite and PP after HPT \nprocessing.  \nRegarding the first issue, during the photoreforming process under light irradiation, \nelectrons in the photocatalyst are excited to the conduction band (e-CB), where they act as reducing \nagents to reduce  water into hydrogen. Meanwhile, PP plastic serves as a sacrificial electron donor. \nThe photogenerated holes in the valence band (h+VB) function as oxidants, promoting the oxidation \nof PP plastics into various organic molecules. The reactions involved can be summarized as \nfollows [21-23]: \n9 \n Table 1.  Organic compounds produced from PP plastic and possible degradation pathway for \nbrookite -PP mixture processed by HPT for N = 3, subjected to photoreforming and analyzed by \nGC-MS. Arrows show the degradation pathway from plastic waste to small molecules.  \nRetention \nTime (min)  Compound Name  m/z for \nMain Ions  Formula  Possible Structure  \n                         h-PP  \n \n \n     \n2.465  3-Oxapentanol -1, 4-\n[(2,3 -dimethyl)phynyl]  59, 75, 105 C12H18O2 \n \n2.465  2-(2-\nMethoxyethoxy)ethyl \nBenzoate  59, 75, 105 C12H16O4 \n \n2.585  Methoxyacetic acid, \nbenzyl ester  45, 61, 191 C10H12O3 \n \n2.585  2-O-Benzyl -d-arabinose  45, 61, 191 C12H16O5 \n \n2.585  Ethanol, 2 -[2-\nphenylmethoxy]ethoxy]  45, 61, 191 C11H16O3 \n \n2.585  Spiro[2,4]hepta -4,6-\ndiene  45, 61, 191 C7H8 \n \n2.585  2-Benzyloxy -3-methyl -\n1,4-butanediol  45, 61, 191 C12H18O3 \n \n \n \n2.115  Acetic acid, \nbutoxyhydroxy -, butyl \nester  41, 73, 86  C10H20O4 \n \n8.865  Tridecyl acrylate  73, 97, 135 C16H30O2 \n \n8.865  4-Dodecanol  73, 97, 135 C12H26O \n \n8.865  2-Methyl -1-Pentadecene  73, 97, 135 C16H32 \n \n \n10 \n Brookite +ℎ𝜐→h VB++e CB− (1) \n2H++2e−→H2 (2) \nh VB++OH−→•OH (3) \nh VB+/ •OH + PP → Organic Products  (4) \nSince all these reactions occur only by renewable solar energy [24-32], the photoreforming process \nis considered a cleaner technology compared to other plastic degradation methods [13-21]. \nAlthough these reactions suggest that both holes and hydroxyl radicals are involved in the \ntransformation of PP, future quenching tests using scavengers are needed to determine which \nspecies contribute more significantly to the reactions [59]. \nRegarding the second issue, several factors contribute to the efficient photoreforming of \nthe mixture of brookite and PP plastic after HPT processing. First, as reported in a previous study, \nHPT produces defects, such as oxygen vacancies, Ti3+ radicals, dislocations, and nanograin \nboundaries in brookite [44]. These defects serve as active sites for the reaction and facilitate the \nseparation and migration of electron -hole pairs, thereby enhancing photocatalytic activity [60-62]. \nSecond, the HPT process creat es defects such as cracks in PP, forms nano -sized brookite, and \nincreases the contact area between brookite, PP plastic, and the aqueous solution. Third, HPT \ntransforms the brookite -PP mixture into a composite that can function as an inorganic -organic \ncomp osite photocatalyst. Charge carrier transfer at the interphases of such photocatalysts enhances \ncharge separation and diminishes recombination, as evidenced by the PL data. These factors \ncollectively contribute to the superior photocatalytic activity of th e HPT -processed brookite and \nPP plastic mixture , suggesting a new functional application for SPD  processing  [45,63] which \nextends the earlier reported photocatalytic application s of severely deformed materials [44,51,64 ]. \n \n5. Conclusions  \nThis study introduces a new route for the photoreforming of PP plastics and shows the \nimpact of the HPT method as the pre -catalysis stage of this photoreforming process. Simultaneous \nwater splitting to hydrogen and PP plastic degradation to a variety of organic compounds are \nachieved . The photoreforming performance systematically improved with an increasing number \nof HPT turns, due to the formation of defects in both plastics and catalysts and the formation of \nabundant inorganic -organic interphases . These findings suggest a new potential application of SPD \nprocessing for plastic waste managemen t. \n \nAcknowledgments  \nThis study is supported partly by Mitsui Chemicals, Inc., Japan, and partly through a Grant -\nin-Aid from the Japan Society for the Promotion of Science (JP22K18737).  \n ",
      "metadata": {
        "filename": "Impact of high-pressure torsion on hydrogen production from photodegradation of.pdf",
        "hotspot_name": "Incineration_of_Plastics",
        "title": "Impact of high-pressure torsion on hydrogen production from\n  photodegradation of polypropylene plastic wastes",
        "published_date": "2024-08-20T06:37:01Z",
        "pdf_link": "http://arxiv.org/pdf/2408.10579v1",
        "query": "plastic waste management sustainable disposal methods energy recovery techniques"
      }
    },
    "Detecting Manufacturing Defects in PCBs via Data-Centric Machine Learning on Sol": {
      "full_text": "Detecting Manufacturing Defects in PCBs via Data-Centric Machine\nLearning on Solder Paste Inspection Features\nJubilee Prasad Rao1, Roohollah Heidary1, and Jesse Williams1\n1Global Technology Connection, Inc, Atlanta, GA, 30339, USA\njrao,jwilliams,rheidary@globaltechinc.com\nABSTRACT\nAutomated detection of defects in Printed Circuit Board (PCB)\nmanufacturing using Solder Paste Inspection (SPI) and Auto-\nmated Optical Inspection (AOI) machines can help improve\noperational efficiency and significantly reduce the need for\nmanual intervention. In this paper, using SPI-extracted fea-\ntures of 6 million pins, we demonstrate a data-centric ap-\nproach to train Machine Learning (ML) models to detect PCB\ndefects at three stages of PCB manufacturing. The 6 million\nPCB pins correspond to 2 million components that belong to\n15,387 PCBs. Using a base extreme gradient boosting (XG-\nBoost) ML model, we iterate on the data pre-processing step\nto improve detection performance. Combining pin-level SPI\nfeatures using component and PCB IDs, we developed train-\ning instances also at the component and PCB level. This al-\nlows the ML model to capture any inter-pin, inter-component,\nor spatial effects that may not be apparent at the pin level.\nModels are trained at the pin, component, and PCB levels,\nand the detection results from the different models are com-\nbined to identify defective components.\n1. I NTRODUCTION\nPrinted Circuit Boards (PCBs) are an essential part of most\nelectronic devices. Automated manufacturing of PCBs pri-\nmarily includes solder paste printing, surface mount device\nplacement, and a re-flow oven stage. It has been shown that\nabout 50-70% of PCB manufacturing defects are associated\nwith solder paste printing process [1]. A Solder Paste Inspec-\ntion (SPI) machine can be utilized after solder paste printing\nto capture information about the amount and location of sol-\nder paste deposited at each of the PCB pins. It was found that\na good correlation existed between SPI-extracted features and\ndefects detected by an Automatic Optical Inspection (AOI)\nstation located after the re-flow oven [2]. Several defect de-\ntection algorithms at SPI and AOI have been proposed in re-\ncent years that involve machine vision tools, image process-\ning, and/or machine learning (ML) techniques to detect faults\n[3, 4, 5, 6]. These works include models trained on a small\nnumber of PCB images ranging from 29 to 1500 PCBs, withand without defects. Since each PCB design may be different\nfrom another, these ML models may not be readily transferred\nover.\nRapid automated identification of faulty pins on a PCB at an\nearly stage of PCB manufacturing, like SPI, could have sig-\nnificant cost savings to PCB manufacturers and consumers. A\ndefective PCB could be reworked at one-tenth and one-fiftieth\ncost at the SPI stage compared to after the re-flow oven and\nafter an in-circuit test respectively [1]. Hence, utilization of\nSPI stations along with algorithms that can ingest SPI features\nto accurately predict PCB defects can significantly improve\nmanufacturing operations including reducing rework costs.\nMachine Learning algorithms tend to perform well in learn-\ning complex interactions between features to perform a vari-\nety of classification and regression tasks. In the Industry 4.0\nera, ML algorithms are becoming vital in performing func-\ntions such as fault detection, prediction, and prevention [7].\nIn order to continue and improve operations, advanced ML\ntechniques need to be widely adopted, as they are developed.\nIn addition, a new paradigm in ML that shifts the focus from\nfine-tuning of ML models through extensive hyperparameter\ntuning to ensuring data quality is being considered [8]. In this\npaper, we demonstrate the utilization of advanced ML algo-\nrithms and a data-centric approach to defect detection in PCB\nmanufacturing.\nThis paper is structured as follows. In Sec. 2, we describe the\ndatasets and the objectives of the data challenge for which this\nwork was conducted. We then outline our approach and pro-\nvide details on data processing, feature and model selection,\nand evaluation steps in Sec. 3. Finally, an analysis of results\nfrom different trained models is given in Sec. 4 followed by\nconclusions in Sec. 5.\n2. D ATASETS AND PROBLEM DESCRIPTION\nPublicly available SPI feature dataset of 6 million PCB pins\nfrom a real-industrial production line are utilized in this ef-\nfort. The production line is integrated with automated inspec-\ntion machines as shown in Fig. 1. The dataset also includes\n1arXiv:2309.03113v1  [cs.LG]  6 Sep 2023Figure 1. PCB production line\nTable 1. Number of pins per component\nNo. of components No. of pins\n108 2\n1 3\n3 5\n7 6\n8 8\n1 49\nan AOI machine generated label and human operators given\noperator and repair labels.\nThe provided datasets contained information pertaining to 15,387\nidentical PCBs with 8 PCBs per panel totalling 1,924 pan-\nels. A manufactured PCB had 128 components with different\nnumbers of pins as listed in Table 1. The data provided fea-\ntures, extracted by the SPI machine, at the pin level for the\nnearly 6 million pins. The list of IDs and features generated\nby the SPI machine is shown in Table 2. The SPI features\ninclude information about the amount and location of solder\npaste printed at for each pin on a PCB. The SPI machine also\ngenerated a “Result” label that classified a pin as good or by\nthe type of fault it predicted, such as insufficient solder, exces-\nsive solder, and others. Ninety-eight percent of the defects as\ndetected by the SPI machine were labeled as “W.Insufficient”,\nand “E.shape” and “E.Position” were the labels for 0.8% each\nof the SPI machine inspected pins.\nAnother file consisted only of pins found to be defective by\nthe AOI machine. This dataset included labels given by the\nAOI machine as well as two human operators (“Operator La-\nbel”, and “Repair Label”). After the components are placed\non the PCB and go through the re-flow oven, AOI machines\nutilize machine vision techniques to detect defects such as\nlean soldering, misalignment, and others. The distribution of\nthe types of faults detected by the AOI machine are shown\nin Fig. 2. Each of the AOI-identified defective pins is then\nvisually inspected by a human operator who gives an “Oper-\nator Label” as “Good” (not defective) or “Bad” (defective).\nFurther, another operator inspects the pins labeled as “Bad”Table 2. Features extracted by the SPI machine\nFeature Description Type\nPanelID Panel ID ID\nFigureID Figure number in panel ID\nDate Date of mfg. Date\nTime Time of mfg. Time\nComponentID Component name Categorical\nPinNumber Component pin number Categorical\nPadID Panel pad number Categorical\nPadType Type of pad (0 or 1) Categorical\nV olume(%) V olume of solder paste (SP) Numerical\nHeight(um) Height of SP Numerical\nArea(%) Area of SP Numerical\nOffsetX(%) Offset X Numerical\nOffsetY(%) Offset Y Numerical\nSizeX Size X Numerical\nSizeY Size Y Numerical\nV olume(um3) V olume in um3 Numerical\nArea(um2) Area in um2 Numerical\nShape(um) Shape in um Numerical\nPosX(mm) Position X in mm Numerical\nPosY(mm) Position Y in mm Numerical\nResult SPI Result Categorical\nFigure 2. Distribution of faults by type, labelled by AOI ma-\nchines\nunder a microscope, and adds a “Repair Label” indicating\nwhether it is “False Scrap” (no defect) or “Not Possible to\nRepair”. In addition to the labels given by the AOI machine\nand the two human operators, this dataset also includes pin-\nidentifying information (panel, figure, component, and pin\nIDs) needed to associate features of each pin in the AOI table\nto SPI features.\nWe have selected three labels as shown in Fig. 4 to be pre-\ndicted using ML techniques. The three objectives and the\nbenefit each provides is listed below.\n1. Detect the presence of AOI defects using SPI features -\nTo improve on current SPI fault detection algorithm\n2. Predict operator label using SPI features and AOI label -\nTo automate operator label generation\n3. Predict repair label using SPI features and AOI and op-\n2Figure 3. Distribution of faults by component ID as labeled\nby AOI machines\nFigure 4. Data availability for the three objectives\nerator label - To automate repair label generation\nTo evaluate the performance of the three algorithms, F1 score,\ngiven by Eq. (1), for the first prediction and macro average F1\nscore for the second and third predictions are utilized.\nREVIEW ABOVE SENTENCE OR SHOULD I JUST USE\nF1 SCORE FOR ALL PREDICTIONS OR I CAN CHANGE\nTO MCC\nF1 =2∗TP\n2∗TP+FP +FN(1)\nThe ultimate goal of this classification system is to improve\nthe operational efficiency of the PCB manufacturing process.\nA well-performing classification model can accurately iden-\ntify the components to be reworked/eliminated after the SPI\nstep, reducing the number of PCBs an operator has to inspect.\nFigure 5. The proposed data-centric ML approach\n3. A PPROACH\nWe utilized a data-centric approach (Fig. 5), where instead of\nusing sophisticated ML algorithms, like for e.g., deep neural\nnetworks, and performing extensive hyperparameter tuning,\nwe use relatively simple decision trees (XGBoost models)\nand concentrate on data pre-processing steps to achieve im-\nproved results. We developed models at three levels: at the 1.\npin, 2. component, and 3. PCB levels. Pin-level models as-\nsume that all causes of a defective pin can be identified from\nits features alone. For component levels models, we expect\nthat there may exist interactions between pins of a component\nthat lead to a defective component. For the PCB level model,\nthe interactions between components on a PCB are also con-\nsidered. If the variation in solder quantity or impact of a pin\ndefect is such that it almost never affects an adjacent pin or\ncomponent, then the component and PCB level models may\nnot provide additional value to the classification tasks. Before\npreparing training and test datasets for the three objectives,\nwe performed cleaning of the data to rectify data formats and\nremove nan value rows, of which there were very few. Ad-\nditional data processing and model training procedures are\nexplained below.\nFeatures at the Pin, Component, and PCB Level: As men-\ntioned before, the data was collected at the pin level with\neach pin’s features stored as a row. Hence, to generate train-\ning datasets for the pin-level models, additional data process-\ning was not needed. To generate component-level features to\ntrain the corresponding models, data was filtered according\nto the component ID. Then, if the component had two pins,\nthe second-pin features were appended to the first-pin fea-\ntures in the same row. For components with additional pins,\nthe process was repeated until all features for all the pins for\nthat component were added to the row. This process was con-\nducted for all components and resulted in 128 datasets, one\nfor each component. Each of these datasets had 15,387 in-\nstances, equal to the number of PCB IDs available. The num-\nber of features in these datasets ranged from ≈40 in com-\nponents with two pins to ≈1,000 features for the component\nwith the most number of pins (49). Once the component-level\n3features datasets are generated for all components, their cor-\nresponding models for the three objectives can be trained by\nusing the pertinent target labels.\nTo generate PCB-level datasets, a similar process was fol-\nlowed, and all the component-level datasets were merged us-\ning the panel, figure, and PCB IDs. For the PCB level data,\neach row consisted of features for all its pins and for all com-\nponents in a PCB. Here too, the total number of instances is\nequal to the number of PCB IDs but resulted in having just\none dataset. This dataset had more about 5,000 features. Be-\nlow, we describe the process of merging the features datasets\nwith their corresponding labels for the three binary classifica-\ntion problems.\nClassification 1: For the first classification problem, we merged\nthe SPI data with the AOI data using the panel, figure, com-\nponent, and pin number IDs. Some of the instances (found to\nbe defective by the AOI machine) in the AOI data were miss-\ning pin numbers, and since each component had more than\none pin, this created ambiguity about which pin SPI features\nto associate such instances with. To avoid wrong associa-\ntions, such AOI instances are not utilized while training the\npin-level models by performing a left merge. This ambigu-\nity does not exist at the component and the PCB level as pin\nnumber is not required to perform those merges. Since the\nobjective is to detect the presence of a defect in a test com-\nponent, all pins, components, or PCBs (based on the level)\npresent in the AOI data were given a target of 1 versus 0 for\nall others.\nClassification 2: The objective of the second classification is\nto determine if the fault detection, achieved by the AOI ma-\nchine is accurate as determined visually by a human operator.\nHence an inner merge is used, and only the pins, components,\nor PCBs depending of the data level that fail the AOI test are\nused as inputs for these models. The AOI label (lean sol-\ndering, translated, misaligned, or others) is added as an addi-\ntional feature to the SPI features and the operator label (good\n- 0 or bad - 1) is used as the target label.\nClassification 3: For the third classification, the goal is to\npredict the repair label for those instances that have a “bad”\noperator label given by a human visual inspector. The opera-\ntor labels are added to the SPI plus AOI label dataset as fea-\ntures, and the repair labels are replaced with 1 (not repairable)\nor 0 (no defect).\nModel Training and Evaluation Once the datasets at the\nthree levels were generated, we used a five fold split and\ntrained several XGBoost models. For the pin level models,\none dataset captured all component defects. For the com-\nponent level models, there was an option to train either one\nmodel for each component or one model for all the compo-\nnents combined. We trained models using both approaches.\nFor the PCB level models, each set of features and target la-bels can only capture defects for one component at a time.\nThis is because the targets are available at the component\nlevel. When component wise model training was needed, it\nwas achieved using an iterative loop on the 128 components.\nWhen multiple models are used for classification, based on\nthe assumption that higher-level models can capture inter-\npin or inter-component interactions, a defective classification\nfrom even one model is reported as defective. Other approaches\ncould be to use a majority voting approach or using the aver-\nage of defect probabilities. In addition to F1 scores, Receiver\nOperator Curves (ROC) were also generated to visualize and\ncompare model performance.\n4. R ESULTS\nBy following the approach described in Sec. 3, we trained\nnearly 400 classification models. The number of defects was\nvery small compared to the total number of instances avail-\nable. Only nearly 0.4% of the about 6 million pin instances\nin SPI data were labeled as defective. Out of these only about\n4.0% of the instances were considered as defective by the vi-\nsual inspector. Among these 80.5% of the components were\nclassified by the repair label as not possible to be repaired.\nThese percentages are indicated in Fig. 4. Due to the very\nlow number ( ≈21,500for classification 1) of defective com-\nponents, XGBoost models with a very small ( ≈10) num-\nber of features performed the best without over-fitting. The\nPCB-level models did not seem to improve classification per-\nformance. This may indicate that the deviation in the amount\nand location of printed solder paste from its designed values is\nnot significant enough to cause inter-component defects. For\nclassification problems 2 and 3, the number of components in\neach class was very small (including ≈150) for PCB level\nML models to learn from. It is observed that only 4.0% of the\nnumber of SPI detected pin defects are classified as faulty by\nthe AOI, and further nearly 20% of the components labeled\nby AOI as defective were found to not be defective when in-\nspected by an operator under the microscope (as given by the\n“repair label”). This shows that there is significant room for\nimprovement in these algorithms using machine learning ap-\nproaches like the ones proposed here. The F1 scores for the\ndifferent models evaluated on test data, for the three classifi-\ncation problems are tabulated in Table 3. The depth of trees,\nwhich was found to be the most important factor, was only\nmanually tuned. For most models trained, the depth of trees\nwas set to 12 or less, and increasing it for the models did not\nresult in and improvement of classification performance. This\nindicates that a defect at each pin or component is influenced\nby a very small number of extracted features given in Table 2.\nROC curves and Area Under the Curve (AUC) at the pin and\ncomponent level models for the three challenges are shown\nin Figs. 7 to 12. The lower performance of the component\nlevel model compared to the pin level model in classification\nproblem 3 is also observed in Fig. 12. This may be due to the\n4Table 3. F1 scores evaluated on test data\nCh. Model Level Components F1 Scores\n1 Pin level All 0.49\n1 Component level All 0.55\n1 PCB Top 35 0.42\n2 Pin level All 0.80\n2 Component level All 0.80\n2 PCB level - -\n3 Pin level All 0.95\n3 Component level All 0.76\n3 PCB level - -\nFigure 6. Top ten important features to predict repair label as\nlearned by a random forest ML model\nsmall number of training instances at the PCB level.\n5. C ONCLUSIONS\nA data-centric machine learning approach is utilized to de-\ntect PCB manufacturing faults at different stages of process-\ning by considering features from each pin, component, and\nPCB at a time. This involved focusing more and improving\nthe quality of the training data rather than training very com-\nplex models combined with extensive hyperparameter tuning.\nXGBoost algorithm is selected as the base and models are\ntrained on the iteratively processed real production line data\nfrom 15,387 PCBs. The results show that improvements over\ncurrently utilized algorithms are possible and that significant\nreductions in manual inspections and rework costs are readily\nachievable using these models. F1 scores of 0.45, 0.8, and\n0.73 are achieved to predict AOI defects, operator label, and\nrepair label respectively. Further improvements through hy-\nperparameter tuning of the machine learning models can be\nachieved. The data utilized to train the models is from only\n9 days, and increasing the data size is expected to improve\ndetection results.\nFigure 7. ROC for pin level model to predict the presence of\nAOI pin defects\nFigure 8. ROC for the component level model to predict the\npresence of AOI component defects\nACKNOWLEDGMENT\nThe author would like to thank PHME society and Bitron Spa\nfor publishing the utilized datasets.\nNOMENCLATURE\nPCB printed circuit board\nSPI solder paste inspection\nAOI automated optical inspection\nXGBoost extreme gradient boosting\nML machine learnng\nTP true positives\nTN true negatives\nFP false positives\nFN false negatives\nROC receiver operator characteristic\nAUC area under the curve\n5Figure 9. ROC for pin level model to predict operator label\nFigure 10. ROC for the component level model to predict\noperator label",
      "metadata": {
        "filename": "Detecting Manufacturing Defects in PCBs via Data-Centric Machine Learning on Sol.pdf",
        "hotspot_name": "PCBA_Production",
        "title": "Detecting Manufacturing Defects in PCBs via Data-Centric Machine\n  Learning on Solder Paste Inspection Features",
        "published_date": "2023-09-06T15:52:55Z",
        "pdf_link": "http://arxiv.org/pdf/2309.03113v1",
        "query": "printed circuit board soldering energy efficient electronics manufacturing"
      }
    },
    "SolderNet_ Towards Trustworthy Visual Inspection of Solder Joints in Electronics": {
      "full_text": "SolderNet: Towards Trustworthy Visual Inspection of Solder Joints in Electronics\nManufacturing Using Explainable Artiﬁcial Intelligence\nHayden Gunraj1,2, Paul Guerrier3, Sheldon Fernandez2, and Alexander Wong1,2\n1Vision and Image Processing Research Group, University of Waterloo, Waterloo, Canada\n2DarwinAI Corp., Waterloo, Canada\n3Moog Inc., New York, USA\nfhayden.gunraj, a28wong g@uwaterloo.ca\nAbstract\nIn electronics manufacturing, solder joint defects are a com-\nmon problem affecting a variety of printed circuit board com-\nponents. To identify and correct solder joint defects, the sol-\nder joints on a circuit board are typically inspected man-\nually by trained human inspectors, which is a very time-\nconsuming and error-prone process. To improve both inspec-\ntion efﬁciency and accuracy, in this work we describe an ex-\nplainable deep learning-based visual quality inspection sys-\ntem tailored for visual inspection of solder joints in elec-\ntronics manufacturing environments. At the core of this sys-\ntem is an explainable solder joint defect identiﬁcation system\ncalled SolderNet which we design and implement with trust\nand transparency in mind. While several challenges remain\nbefore the full system can be developed and deployed, this\nstudy presents important progress towards trustworthy visual\ninspection of solder joints in electronics manufacturing.1\nIntroduction\nIn electronics manufacturing, solder joint defects are a com-\nmon problem affecting both surface-mounted and through-\nhole components of printed circuit boards (PCBs) (see Fig-\nure 1 for example solder joint defects). Defects introduced in\nthe soldering process can lead to electrical issues and faulty\nparts, especially if not caught early in the process. This is\nparticularly concerning in critical applications such as the\naerospace and medical industries, where defective PCBs can\ncause catastrophic failures in critical systems. If defects are\ncaught early, the solder can be reworked to minimize po-\ntential issues later in the manufacturing process and avoid\nunnecessary electronic waste.\nTo identify and correct solder joint defects, the solder\njoints on a PCB are often visually inspected by trained in-\nspectors. However, human inspectors are estimated to make\nvisual inspection errors in 20-30% of cases, and the perfor-\nmance of human inspectors varies considerably depending\non experience, mental fatigue, defect type, frequency of de-\nfect occurrence, and a variety of other factors (See et al.\n2017; Klamklay and Bishu 1998). Manual inspection is also\nCopyright © 2023, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\n1This paper consists of general capabilities information that is\nnot deﬁned as controlled technical data under ITAR Part 120.10 or\nEAR Part 772.\nFigure 1: Example images of solder joint defects from the\ndataset examined in this study: (A) fractured joint, (B) cold\njoint, (C) burns, (D) ﬂux residue, (E) poor wetting, and (F)\ndisturbed solder.\nexpensive and time-consuming, often requiring multiple in-\nspectors to achieve reasonable throughput.\nIn contrast to manual inspection, automated visual in-\nspection of solder joints offers many beneﬁts, such as high\nthroughput, high performance, and zero fatigue. However,\nautomating this task is technically challenging due to the\nsmall size of the joints, the wide variety of possible joint\nand defect types, limited computing resources, limited in-\nspection time, and the need for high performance. Deep neu-\nral networks offer an attractive solution to this problem, as\nthey have been successfully applied to detection and inspec-\ntion tasks in a variety of manufacturing scenarios (Kim et al.\n2021; Zhang et al. 2022; Westphal and Seitz 2021; Bhatt\net al. 2021; Yang et al. 2020). While deep neural networks\nare capable of achieving human-level performance in vi-\nsual inspection tasks, they have several drawbacks in critical\nmanufacturing scenarios:\n1.Computational complexity : high-performance neural\nnetworks are often compute-hungry and require special-\nized hardware for fast inference. When computing re-\nsources are limited (as in many manufacturing scenar-\nios), such networks can be slow and reduce throughput.arXiv:2211.10274v1  [cs.CV]  18 Nov 2022Figure 2: Overview of the proposed solder joint inspection system. This study focuses on developing the core defect identiﬁca-\ntion system (SolderNet), which comprises the defect identiﬁcation and XAI modules.\n2.Lack of explanations : neural networks typically operate\nas black boxes which provide a prediction but cannot ex-\nplain how the prediction was made. In critical inspection\nscenarios, determining why a network made a particular\nprediction falls to the human inspector, which reduces the\npotential throughput beneﬁts offered by these networks.\n3.Unclear reliability : the development of neural networks\nusually involves testing using data not seen by the net-\nwork. Despite this, a common challenge when deploying\ndeep neural networks is understanding when a model’s\npredictions can be trusted and which scenarios a model\nis likely to fail in.\nIn this work, we outline a trustworthy, explainable deep\nlearning-driven solder joint inspection system for electron-\nics manufacturing. The proposed system is capable of han-\ndling both through-hole and surface-mounted components\nand can provide explanations of its decision in order to facil-\nitate manual review when necessary. We perform a number\nof experiments to implement and test the core functional-\nity of this system with a focus on practical considerations\nfor manufacturing settings. Moreover, we leverage a mix\nof quantitative explainability and trust quantiﬁcation tech-\nniques to further analyze the behaviour and trustworthiness\nof the system, identify gaps in the model development pro-\ncess, and provide insights during the inspection process.\nMethods\nThe proposed system consists of several stages which are\ndesigned to provide high performance, high throughput and\nhigh reliability. Figure 2 illustrates the inspection system,\nwhich proceeds via the following steps:\n1. Images of a PCB are captured by an imaging system.\n2. PCB images are passed through a solder joint detector\nwhich identiﬁes and extracts solder joint images.3. Solder joint images are passed through a defect identi-\nﬁcation network which categorizes solder joints as de-\nfective, possibly defective, or non-defective and activates\nthe appropriate indicator.\n4. For boards with possibly defective joints, an XAI algo-\nrithm is used to generate explanations of the network’s\npredictions.\n5. A human operator reviews the possibly defective joints\nand their XAI visualizations to determine which are truly\ndefective and which are not defective.\n6. All defective solder joints are reworked.\nIn this study, we focus on defect identiﬁcation, explain-\nability, and trustworthiness. These three aspects of the sys-\ntem are described in detail in the following subsections.\nSolder Joint Defect Identiﬁcation\nWe focus on convolutional neural networks (CNNs) to per-\nform the task of solder joint defect identiﬁcation. CNNs are\na ubiquitous technology in image identiﬁcation tasks, and\nrecent advances in network design have enabled the creation\nof high-performance networks which remain computation-\nally efﬁcient. We examined both efﬁciency-focused archi-\ntectures and performance-focused architectures to examine\ntheir respective trade-offs between network complexity and\nnetwork performance. Speciﬁcally, we tested Attend-NeXt\n(Small and Large) (Wong et al. 2022), MobileNet (V2 and\nV3 Small)(Sandler et al. 2018; Howard et al. 2019), Shuf-\nﬂeNetV2 (Ma et al. 2018), and ConvNeXt (Tiny, Small, and\nBase) (Liu et al. 2022). Table 1 shows the number of pa-\nrameters and ﬂoating-point operations (FLOPs) for each of\nthe network architectures examined in this work. We train\neach of the aforementioned architectures as binary classi-\nﬁers which provide a conﬁdence score on the interval [0,\n1] indicating the network’s conﬁdence that an input imagerepresents a defective solder joint. In deployment, this al-\nlows us to deﬁne conﬁdence regions which correspond to\ndeﬁnite defects (requiring repair), possible defects (requir-\ning review), and deﬁnite non-defects (no action required).\nQuantitative Explainability\nFollowing defect identiﬁcation, as can been seen in Figure 2\nsome solder joints may require review by a human opera-\ntor given a level of uncertainty during the solder joint de-\nfect identiﬁcation process with respect to whether that joint\nis indeed defective. In such scenarios, while the identiﬁca-\ntion network’s prediction and conﬁdence provide useful in-\nformation to the operator, the operator must still identify the\nparticular defects (or lack thereof) in the solder joint images\nto determine if it is a false defect detection (in which case\ninspection is complete) or if it is indeed a true defect de-\ntection (in which case the solder needs to be reworked). To\nmake this task easier and faster for operators, we propose a\nquantitative explainability module which identiﬁes the crit-\nical factors in an image which led to the neural network’s\ndecision in a quantitative manner, allowing the operator to\nrapidly identify the locations of potential defects and vali-\ndate the model’s predictions.\nTo this end, we introduce an extended form of GSIn-\nquire (Lin et al. 2019) to provide visual explanations of\na neural network’s decision-making process. We choose\nGSInquire as the core approach to extend upon because\nit identiﬁes speciﬁc critical factors that quantitatively im-\npact the decisions made by the deep neural network, in\ncontrast to other explainability methods such as Grad-\nCAM (Selvaraju et al. 2017), Expected Gradients (Erion\net al. 2021), LIME (Ribeiro, Singh, and Guestrin 2016), and\nSHAP (Lundberg and Lee 2017) that generate only qualita-\ntive heatmaps depicting relative importance.\nIn its original form, GSInquire examines a network’s ac-\ntivation signals in response to an input image and uses them\nto identify critical factors within the image which impact the\nnetwork’s decision in a quantitatively signiﬁcant way. These\ncritical factors may then be projected into the same space as\nthe image to produce a visual interpretation. Building upon\nGSInquire, we introduce an extension which determines the\nrelative importance of different aspects within the critical\nfactors of a given image. This allows for more ﬁne-grained\ninterpretation of the critical factors, as we can now see not\nonly which critical factors contribute the most to the neural\nnetwork’s decision-making process but also which aspects\nof a particular critical factor are most important. We present\nthe relative importance of different aspects within the critical\nfactors as regional heatmap overlays within the boundaries\nof their corresponding critical factors, as shown in Figure 3.\nSecond-order Explainability\nIn addition to providing visual explanations in deployment\nsettings, visual explainability is also a valuable model val-\nidation tool during development. While quantitative met-\nrics such as accuracy provide important measures of a deep\nneural network’s performance, they do not provide informa-\ntion regarding how decisions are made. To address this gap\nin performance analysis, visual XAI enables auditing of aArchitecture Parameters (M) FLOPs (G)\nAttend-NeXt Small 1.632 0.423\nAttend-NeXt Large 3.897 0.861\nMobileNetV2 2.225 0.409\nMobileNetV3 Small 1.519 0.076\nShufﬂeNetV2 1.255 0.193\nConvNeXt Tiny 27.821 5.832\nConvNeXt Small 49.455 11.364\nConvNeXt Base 87.567 20.084\nTable 1: Comparison of architectural and computational\ncomplexity for each of the tested network architectures.\nFLOPs were measured with a 256 \u0002256 3-channel input and\na batch size of 1.\nmodel’s decisions during development to ensure that they\nare based on relevant visual indicators and elucidate poten-\ntial biases in the training data, which may then be used to\nguide improvements to the training framework. However, re-\nviewing visual explanations manually is a time-consuming\ntask, particularly for large-scale image datasets with many\nclasses or high intra-class variability. Manual review may\nalso be inﬂuenced by human biases, and it can be challeng-\ning if not intractable to mentally conceptualize key trends\nand patterns based on the individual explanations at hand.\nTo facilitate auditing of the model and dataset during\ndevelopment, we introduce the concept of second-order\nexplainable artiﬁcial intelligence (SOXAI) which extends\nthe concept of XAI from the sample level to the dataset\nlevel. Rather than manually reviewing visual explanations\nto explore patterns in a model’s decision-making behaviour,\nSOXAI aims to reveal these patterns automatically through\nanalysis of the relationships between quantitative explana-\ntions. This allows for rapid identiﬁcation of the most com-\nmon visual concepts leveraged by a model when making\nits decisions, and can reveal obvious model and dataset bi-\nases. This can also increase transparency by helping users\nunderstand what a model has learned and what it has not.\nIn essence, SOXAI enables us to ”explain explainability” by\nproviding higher-level interpretations of the behaviours of\ndeep neural networks.\nIn this study, we formulate SOXAI as an embedding prob-\nlem: given an image Iand corresponding quantitative ex-\nplanation\u000b, we deﬁne an embedding f: (I;\u000b)!RN\nwhich embeds the regions of Iindicated by \u000b. Performing\nthis embedding for all images in a dataset allows for simi-\nlar embeddings to be grouped together with secondary algo-\nrithms. To generate the visualizations presented in this work,\nwe leverage t-distributed stochastic neighbour embedding (t-\nSNE) (van der Maaten and Hinton 2008) to group the em-\nbeddings and map them to a 2-dimensional space.\nTrust Quantiﬁcation\nIn industrial applications, it is important to understand how\ntrustworthy a model is in order for it to be deployed as an\nautomation tool. Quantifying trust allows for models to be\ncompared in terms of their trustworthiness during the model\ndevelopment process, and may help guide decisions as toArchitecture Latency (s) Accuracy (%) Overkill (%) Escape (%) NetTrustScore\nAttend-NeXt Small 0.275 86.6 8.4 5.0 0.864\nAttend-NeXt Large 0.430 91.1 5.0 3.9 0.904\nMobileNetV2 4.082 88.5 7.8 3.7 0.878\nMobileNetV3 Small 1.647 88.3 7.1 4.6 0.883\nShufﬂeNetV2 1.775 87.0 7.1 5.9 0.867\nConvNeXt Tiny 5.975 88.8 7.2 3.9 0.887\nConvNeXt Small 12.027 89.6 5.8 4.6 0.890\nConvNeXt Base 17.575 88.8 6.3 4.8 0.889\nTable 2: Comparison of quantitative performance metrics for each of the network architectures on the solder joint test dataset.\nLatency was measured on an ARM Cortex-A72 with a 256 \u0002256 3-channel input and a batch size of 1, and is reported as the\naverage of 100 measurements taken after 20 warm-up runs. Accuracy, overkill, and escape were calculated at a conﬁdence\nthreshold of 0.5. Best results highlighted in bold .\nthe level of human supervision and review required once a\nmodel is deployed. Recent studies by Wong et al. (Wong,\nHryniowski, and Wang 2020; Wong, Wang, and Hryniowski\n2020) and Hyrniowski et al. (Hryniowski, Wong, and Wang\n2021) introduced several human-interpretable approaches to\ntrust quantiﬁcation which allow for a model’s overall trust-\nworthiness to be analyzed. These techniques are based on\nthe notion of question-answer trust, in which a question x\n(i.e., ”is this solder joint defective?”) is answered by both\na modelMand an oracle O. The model’s answer and con-\nﬁdence in its answer are compared to the oracle’s answer\nto compute a question-answer trust score which reﬂects the\ntrustworthiness of the model’s answer.\nTo evaluate SolderNet’s trustworthiness, we make use of\ntwo key trust metrics which summarize a model’s overall\nquestion-answer trust:\n1.Trust matrix : a matrix of expected question-answer\ntrusts for each possible model-oracle answer pair, and\n2.NetTrustScore : a scalar metric summarizing the overall\ntrustworthiness of a deep neural network across all pos-\nsible predictions.\nExperimental Setup\nDataset To build and evaluate the proposed solder joint\ndefect identiﬁcation and explanation framework, we lever-\nage a dataset comprising 2690 images of both through-hole\nand surface-mount joints acquired by Moog Inc. Images\nwere acquired using a microscope camera and were labelled\nas either defective joints (1644 images) or non-defective\njoints (1046 images) by an experienced inspector. Notably,\nlabelling was performed off-line without time constraints,\nthereby minimizing label noise in the resulting data. The\ndataset includes a variety of solder defects such as fractured\nsolder joints, cold joints, burns, ﬂux residue, poor wetting,\nand disturbed solder as illustrated in Figure 1. We used a\n60%/20%/20% split stratiﬁed by class to divide this dataset\ninto training, validation, and test sets, respectively.\nPreprocessing Images are preprocessed by resizing them\nto 256\u0002256 pixels and mapping their original unsigned 8-\nbit integer pixel values to the range [0, 1] through division\nby 255. Additionally, the following data augmentations were\nused during training, each applied with a 50% probability:random rotation in the range [0\u000e, 45\u000e], random horizontal\nand vertical ﬂipping, random translation of \u000610% in each\ndirection, random brightness jitter of \u000620%, and random\ncontrast jitter of\u000620%.\nTraining All models examined in this work were pre-\ntrained on ImageNet-1k (Deng et al. 2009). Following (Ku-\nmar et al. 2022), each model’s fully-connected layers were\ntrained for 100 epochs followed by full-model ﬁne-tuning\nfor 1000 epochs. Binary cross-entropy loss and an AdamW\noptimizer (Loshchilov and Hutter 2019) with (\f1;\f2) =\n(0:9;0:999) and weight decay of 1 \u000210\u00004were used in all\nexperiments. We used a batch size of 128 and a learning rate\nof 1\u000210\u00003to train the fully-connected layers, followed by\nfull-network ﬁne-tuning with a batch size of 128, an initial\nlearning rate of 5\u000210\u00004, and cosine learning rate decay.\nEvaluation To evaluate performance, we report accuracy,\noverkill rate (number of false-positives divided by number of\nsamples), and escape rate (number of false-negatives divided\nby number of samples) on the the holdout test set. We chose\nthese metrics due to the manufacturing context of this work;\nmanufacturers care more about the absolute rates of false-\npositives and false-negatives rather than the proportional\nrates. Additionally, we report inference latency on an ARM\nCortex-A72 processor, as well as NetTrustScore (Wong,\nWang, and Hryniowski 2020) to evaluate the trustworthiness\nof each model. Lastly, visual explanations and trust quantiﬁ-\ncation plots were qualitatively evaluated.\nResults\nQuantitative Results\nQuantitative performance metrics for each of the tested net-\nwork architectures are shown in Table 2. All tested archi-\ntectures achieve high performance, with Attend-NeXt Large\nachieving the highest accuracy (91.1%) and lowest overkill\nrate (5.0%), MobileNetV2 achieving the lowest escape\nrate (3.7%), and Attend-NeXt Small achieving the lowest\nlatency (0.275 s).\nAn intuitive interpretation of these metrics can be ob-\ntained by considering a scenario where 100 solder joints\nare to be inspected in a deployment setting. Considering the\nmetrics of Attend-NeXt Large, we see that it would takeFigure 3: Images of solder joint defects (left) and corre-\nsponding visual explanations (right) from the dataset exam-\nined in this study: (A) poor wetting, (B) solder splash, (C)\nforeign object in solder, and (D) pad and board damage.\n43 s to inspect the set of joints and about 91/100 joints\nwould be correctly classiﬁed. Of the remaining 9 misclas-\nsiﬁed joints, about 5 would be misclassiﬁed as defective and\nabout 4 would be misclassiﬁed as non-defective. However, it\nis important to note that this interpretation is only meaning-\nful if the proportions of defective and non-defective joints\nin the test data are representative of the true proportions in\ndeployment. In this study, defective solder joints are over-\nrepresented in the test data, and as such we might expect\nhigher overkill rates and reduced escape rates in practice.\nExplainability Results\nIn this section we present and describe defect explanation\nand localization results obtained through the proposed ex-\nplainability module. Figure 3 illustrates defective solder\njoints and the corresponding explanations of an Attend-\nNeXt Large model’s predictions for these images. In each\ncase, the XAI algorithm identiﬁed the critical factors that\nquantitatively drove the resulting decision (white outline)\nand the relative importance of aspects within each critical\nfactor (semi-transparent regional heatmaps) indicating the\ncritical factors used by the model to make its prediction. In\nall ﬁve cases, we observe that GSInquire identiﬁes the key\nareas of interest driving the network’s decision-making pro-\ncess and further localizes speciﬁc features via the regional\nheatmaps. These cases are described in more detail below.Case A This image illustrates poor wetting, where a glob-\nule of solder which has not adequately joined with the solder\npad can be seen. Additionally, the solder pad itself appears\nlumpy and discoloured which indicates possible residue or\ncontamination. Examining the visual explanation, we see\nthat the model correctly focuses on the non-wetted regions\nof the joint when making its prediction.\nCase B In this example, the soldering process has intro-\nduced a solder splash which can be seen in the top-right\ncorner of the image. Examining the visual explanation, we\nsee that the model correctly identiﬁed this solder splash as\nthe defect and effectively ignored the solder joint (which is\nwell-formed) when making its decision.\nCase C In this image, a foreign object (a piece of ﬁbre)\nwas inadvertently embedded in the solder when the joint was\ncreated. This foreign object is captured in its entirety by vi-\nsual explanation, with the ﬁbre itself being highlighted as\nthe most important aspect in the explanation.\nCase D Extensive damage to the pad and board are shown\nin this example, where a large piece has chipped off of\nthe solder pad and extensive damage to the board’s surface\naround the joint can be seen. While both of these aspects\nwould constitute a defective joint, we see in the visual ex-\nplanation that the model focuses on the damage to the solder\npad while still including the surface damage.\nSecond-order Explainability Results\nSOXAI visualizations are produced by placing the quantita-\ntive explanations produced by GSInquire across the wealth\nof data at hand at their corresponding 2-dimensional embed-\nding locations following t-SNE. The resulting image illus-\ntrates groups of similar quantitative explanations which can\nmore easily be examined for semantic groupings and com-\nmon trends and patterns.\nFigure 4 illustrates a SOXAI visualization for an Attend-\nNeXt Large model. As shown in the ﬁgure, SOXAI auto-\nmatically groups explanations with similar characteristics,\nmaking it easier to ﬁnd trends in the visual explanations.\nFor example, in Figure 4 (A), we see that a homogeneous\nset of overhang defects has been tightly grouped. The rela-\ntively large size of this group indicates that this particular\ndefect is well-recognized by the model but may be over-\nrepresented. In contrast, Figure 4 (B) shows a group of\nlifted leads which exhibit greater diversity but may be under-\nrepresented. In the neighbourhood of Figure 4 (C) we ob-\nserve a large variety of through-hole defects, with (C) high-\nlighting a group of wetting defects. The large size of the\nthrough-hole group paired with the intra-group variability\nindicates that through-hole joints and their defects are well-\nrepresented in the dataset and well-recognized by the model.\nTrust Analysis\nThe NetTrustScores for the models examined in this work\nare shown in the rightmost column of Table 2. These scores\ngive an overall measure of how trustworthy each model’s\npredictions are. As shown, there is little disparity in trustFigure 4: Second-order visual explainability illustrating various types of solder joint defects as viewed by the network: (A) side\noverhang, (B) lifted/unsoldered leads, and (C) wetting defects.\nFigure 5: Trust matrix of Attend-NeXt Large.\namongst the models examined in this work, with Attend-\nNeXt Large achieving the highest score of 0.904 and Attend-\nNeXt Small having the lowest score of 0.864.\nTo explore trust in more detail, Figure 5 shows the trust\nmatrix for Attend-NeXt Large. Notably, we show this ma-\ntrix as an illustrative example since the trust matrices for the\nother architectures have a similar pattern. To interpret the\ntrust matrix, consider that each entry indicates the expected\nquestion-answer trust for the given ground-truth/prediction\npair. As such, higher values are better in all cells. Examin-\ning Figure 5, we see that the diagonal entries (i.e., correct\nprediction scenarios) exhibit high trust. However, the off-\ndiagonal entries (i.e., incorrect prediction scenarios) exhibit\nextremely low trust, indicating that the model is overconﬁ-\ndent when it makes incorrect predictions. This is problem-\natic in deployment scenarios, as it makes it more difﬁcult to\nidentify uncertain model predictions in order to ﬂag them for\nmanual review. To alleviate this problem, techniques such as\nlabel smoothing and mixup regularization (Carratino et al.\n2020) could be used to soften the image labels during train-\ning and encourage intermediate conﬁdence scores.Discussion\nIn this work, we described a design for a trustworthy, ex-\nplainable solder joint inspection system for use in electron-\nics manufacturing. While this system has yet to be fully\nimplemented, we present important progress towards trust-\nworthy, explainable solder joint inspection which forms the\ncore of the proposed system. Moreover, we discuss practi-\ncal considerations for building and evaluating such a system\nand show how trust quantiﬁcation, quantitative explainabil-\nity, and second-order explainability can be leveraged to ana-\nlyze the trustworthiness of the system and identify biases or\ngaps in the data and model development process as well as\nprovide insights during inspection.\nThe image data analyzed in this study varies considerably\nin terms of camera viewpoint, magniﬁcation, and resolu-\ntion. In practice, a standardized imaging system would be\nrequired, however implementing an adequate imaging sys-\ntem is technically challenging due to the fact that different\ntypes of solder joints may need to be imaged at different an-\ngles, exposures, or resolutions in order to capture the major-\nity of possible solder defects. In this study, we have assumed\nthat such a system canbe designed, but the speciﬁc details\nof how to do so are left to future work.\nWhen deploying a system such as the one described in\nthis study, it is important to monitor and validate the sys-\ntem’s performance in the ﬁeld in order to identify and correct\nany issues that arise. No amount of ofﬂine testing can fully\nsimulate a system’s real-world performance, and so collect-\ning prediction and performance data in the ﬁeld is critical\nto evaluation. Additionally, false predictions observed in the\nﬁeld can be collected, curated, and used to ﬁne-tune the sys-\ntem in order to reduce overkill and escape rates. Such con-\ntinuous monitoring also helps to identify and mitigate drift\nin the system (for example, due to drift in camera calibration\nor other imaging parameters).Acknowledgments\nWe would like to thank Saeejith Nair for measuring the in-\nference latencies of the architectures examined in this work.",
      "metadata": {
        "filename": "SolderNet_ Towards Trustworthy Visual Inspection of Solder Joints in Electronics.pdf",
        "hotspot_name": "PCBA_Production",
        "title": "SolderNet: Towards Trustworthy Visual Inspection of Solder Joints in\n  Electronics Manufacturing Using Explainable Artificial Intelligence",
        "published_date": "2022-11-18T15:02:59Z",
        "pdf_link": "http://arxiv.org/pdf/2211.10274v1",
        "query": "printed circuit board soldering energy efficient electronics manufacturing"
      }
    },
    "A process planning system with feature based neural network search strategy for": {
      "full_text": " \n AIJSTPME Vol. 1, No. 1, January-June 2008  \n \n \n \nA Process Planning System with Feature Based Neural Network Search Strategy for \nAluminum Extrusio n Die Manufacturing  \n \n \nS. Butdee1, C. Noomtong1, S. Tichkiewitch2  \n1 IMSRC, Department of Production Engineering, Faculty of Engineering, KMUTNB, Bangkok, Thailand \n2 G-SCOP Laboratory, Grenoble Institute of Technology, Grenoble, France  \n \n \nAbstract \nAluminum extrusion die manufacturing is a critical ta sk for productive improvement and increasing potential \nof competition in aluminum extrusion industry. It causes  to meet the efficiency not only consistent quality but \nalso time and production cost reduction. Die manufacturing consists first of die design and process planning \nin order to make a die for extruding the customer’s re quirement products. The efficiency of die design and \nprocess planning are based on the knowledge and experience of die design and die manufacturer experts. This knowledge has been formulated into a computer system called the knowledge-based system. It can be reused to support a new die design and process planning. Such knowledge can be extracted directly from die geometry \nwhich is composed of die features. These features are stored in die feature library to be prepared for \nproducing a new die manufacturing. Die geometry is defined according to the characteristics of the profile so we can reuse die features from the previous similar profile design cases. This paper presents the CaseXpert \nProcess Planning System for die manufacturing based on feature based neural network technique. Die \nmanufacturing cases in the case library would be retrieved with se arching and learning method by neural \nnetwork for reusing or revising it to build a die design and process planning when a new case is similar with \nthe previous die manufacturing cases. The results of the system are dies design and machining process. The \nsystem has been successfully tested, it has been proved that the system can reduce planning time and respond high consistent plans. \n \n \nKeywords :  \nProcess Planning, Feature-Based Neural Network, Aluminum Extrusion Die Manufacturing. \n \n \n1 INTRODUCTION\nAluminum extrusion is a hot deformation process \nused to produce long, straight, semi finished metal \nproducts such as bars, solid and hollow sections, tubes and many shapes products. The hot aluminum \nextrusion process is pressed under high pressure and \ntemperature in a specific machine. A billet is squeezed from close container to be pushed through a \ndie to reduce its section [1]. A profile shape is \ndeformed in order to be adapted to the die orifice shape. The significant machine used to press \naluminum is called an aluminum extrusion press. The \nmain tooling of aluminum extrusion process is a die. It is used to form aluminum profile shapes. Hence, \ndie must be efficiently constructed to support \naluminum extrusion process to ensure obtaining a good extruded profile. Aluminum extrusion die manufacturing is the work significant activity in \naluminum extrusion industry in order to obtain an \nefficiency die. Die manufacturing is proceed of two main phases, including the die design and the process \nplanning for machining to make a die. Die design is \nbased on the skill of a die designer who has accumulated the working experiences for many \nyears. Die designer gives the die design concept and \ndetails to create the die geometry in CAD system. In addition, process planning for die machining is also \nbased on the experience of  a die process planner \nexpert. The knowledge base of die design and \nprocess planning should be captured from the \nexperts. It would be formulated, managed and stored \nin a computer system in order to use it and revise with new situation die design. At the present, \n \n27 \nS. Butdee  et al. \n \nintelligence system has been discussed in knowledge \nmanagement for industrial fields, such as design and \nmanufacturing. Intelligent systems can be knowledge based system, genetic algorithm, rule-based and \nframe-based expert system , fuzzy logic, artificial \nneural networks which are general tools using in \norder to solve the complex or specific problem in \nengineering works [2]. For aluminum extrusion die \ndesign and process planning of die manufacturing, we propose an artificial intelligent system to aid \nmanagement of the knowledge of design and process \nplanning with the feature based neural networks \ntechnique to search the si milar previous die design \ncase, in order to reuse the previous cases in the new \ncase design and manufacturing. Die geometry can be \nconstructed by the combination of geometrical \nfeatures. The principle features are holes, edges, grooves, pockets etc. These features are given by die \ndesigner experts. Therefore die features should be organized in feature library for reusing at a new die design and to decrease die design lead time. The \nknowledge of die design is translated from the human \nskill to the knowledge based system via feature \ndefinition. Hence, the knowledge base of die design \ncan be directly based on from die geometry design. \nThe geometrical features data of a die contains \nfeature shape, dimensions and so on. Moreover, each \nfeature of die geometry is employed to define the \npossibility of machining processes to fabric a die. The knowledge based system of designing of \nextrusion die is organized with frame-based and rule-\nbased system. This paper consists of five sections. \nFirstly, is to explain the fundamental of aluminum \nextrusion process and extrusion tools. The second section describes the main tool used for extrusion \nprocess (aluminum extrusion die). The third section \npresents the principle methods of die design process, the knowledge base of die design, and die geometry. \nThe fourth section addresses the process planning of \ndie manufacturing, including the knowledge base of die process planning.  The fifth section presents \nfeature based neural network for aluminum extrusion \ndie manufacturing.  Case study is illustrated in the sixth section. Finally, the summary section concludes \nthe application of the proposed system to support \naluminum extrusion die manufacturing. \n  \n2 ALUMINUM EXTRUSION \n2.1 Extrusion process \nAn extrusion press is used to extrude many materials \nsuch as lead, aluminum, copper, zinc, brass, etc. There are four characteristic differences among the various methods of extrusion and press used: \n• The movement of the extrusion relative to \nthe stem - direct and indirect process. \n• The position of the press axis – horizontal or vertical press. \n• Type of drive – hydraulic (water or oil) or mechanical press. \n• Method of load application – conventional \nor hydrostatic extrusion. \nNormally, the extrusion proce ss can be classified into \ntwo basic methods, direct and indirect extrusion. This paper we only discuss on the direct extrusion process. \nThere is the simplest production of aluminum extrusion process, which can be carried out without \nlubricant. The most important and principle method \nused in extrusion is the direct process as shown in Figure 1, which follows the step sequence as given \nbelow [3]:  \n• Loading the billet and piston into the press \n• Extrusion of the billet, press billet through \ndie \n• Decompression of the press and opening of \nthe container to expose the discard and the \npiston \n• Shearing the discard or backend or input a \nnew billet \n• Returning the shear, container and ram to \nthe loading position \n2.2 Equipment and tooling in aluminum \nextrusion process \nThe principle equipment and tooling in hot aluminum extrusion process are explained in [4], including: \n2.2.1  Press machine \nAn aluminum extrusion press is the main machine to extrude aluminum ingot (Billet) through a die; pressure from machine is generated by hydraulic \nsystem. Almost horizontal and hydraulic extrusion \npress is widely using in an aluminum extrusion industrial. Hydraulic extrusion press is illustrated in \nFigure 2. \n2.2.2  Run out table and puller \nThe extrusion emerges from the press onto the run out table, which supports the extrusion. A puller or \npullers guide the extrusion and keep it under constant tension. Run out table and puller equipments are \nshown in Figure 3.  \n2.2.3  Die oven \nDie oven is an equipment to preheat die temperature on appropriate state for extrusion. In hot forming \nmaterial tooling must be heated to increase\n \n \n28  \nCaseXPert A Process Planning System with Feature Based Neural Network Search Strategy for Aluminum \nExtrusion Die Manufacturing  \n \n \nFigure 1: Direct extrusion process \n \n \nFigure 2: Press machine and schematic of direct extrusion press \n \nFigure 3: Run out table and puller \n A1 P \nA1   P \nA2  P AC Main Ra m PistonContaine rDie \nStack Platen \nExtrusre \nSide Cion High \nPressu\nOil \nylinde r\n \n29 \nS. Butdee  et al. \n \nmechanical properties and to control temperature \nenvironment. Normally, an aluminum extrusion die is \nheated nearest billet temperature about 425-450°C. \nAnother, the significant parameter is die preheat \ntime. To prevent under heating or over heating of die, \ndie preheat time also needs to  be strictly controlled. \nThe recommended die preheating practices are as \nfollow: \n• Minimum soaking time: 1hr/inch of the die \nand backer. • Maximum allowance time after a die \nreaches specified temperature \n  300° C - 24 hours \n 370 °C - 10 hours \n 420 °C - 8 hours \n 480 °C - 2 hours \nIndeed, die preheat time ba sed on the efficiency of \ndie oven and any parameter, each company should \nhas its standard time. Die oven is illustrated in Figure 4. \n \n  \nFigure 4: Die oven \n \n2.2.4  Billet Oven \nBillet oven is used to preheat billet temperature 420- 450 \n°C. Most common are gas or induction ovens. \nBillet oven is shown in Figure 5. High billet \ntemperatures (Temperature more than 480 °C) will \nreduce the extrusion pressure, but decrease extrusion \nspeed of the extrusion in order to avoid surface \ndefects. Globally, the productivity of the press is \nreduced.  \n \n \n \nFigure 5: Billet oven \n 2.2.5  Stretcher \nStretcher is an equipment to extend aluminum profiles longitudinal direction. The purpose is to relieve residual stress in extruded profiles. Normally, \nstretch straight extrusion length about 0.5% and \ndecreases the cross-sectional dimensions correspondingly, over stretching can result in \ndistortion or dimensions out of tolerance. Stretching \nof 2% or more can lead to orange peel defects on the extrude surface. A high level of stretching is required \nfor straightening of distorted sections. A better \napproach is to control metal flow through the die to \nreduce distortion rather than to apply high levels of \nstretching. Aluminum extruded stretcher is shown in \nFigure 6. \n2.2.6  Extrusion cut-off saws \nA cut-off saw is an equipment to cut the aluminum profiles to preset length. Circular blades for cutting \naluminum extrusions are 16 to 20 inches and are \nmade from carbide. Modern saws have a self-contained hydraulic system and chip collector to trap \ncontinuously. Figure 7 illustrates extrusion cut-off \nsaws  \n \n \n30  \nCaseXPert A Process Planning System with Feature Based Neural Network Search Strategy for Aluminum \nExtrusion Die Manufacturing  \n \n \n \nFigure 6: Stretcher \n \n \n \nFigure 7: Extrusion cut-off saws \n \n2.2.7  Aging Oven   \nThe aging oven is given a heat treatment the extruded \nprofiles to improve the mechanical qualities of the \nproduct. The extruded profiles are heated to a set \ntemperature for an extended period ranging from 4-10 hours depending on mechanical properties \nrequired. Not all extrusions are artificially aged at the \nplant. Aging oven is demonstrated in Figure 8. \n \n \n \nFigure 8:  Aging oven  \n3 ALUMINUM EXTRUSION DIE  \nIn aluminum extrusion process, a die is an important \ntool to deform aluminum ingot (Billet) to get straight \naluminum profiles. An aluminum profile is formed \nby die orifice (die hole). In addition, the quality of products and extrusion productivity often depend on \ndie performance. An aluminum extrusion die can be \nclassified into three basic types:  solid, semi-hollow and hollow die. Its can produce aluminum profiles \nsolid, semi-hollow and hollow section respectively. \nThe normal type of die for solid and open semi-\nhollow shapes can be used for all metals that are \nextruded. Hollow sections in  a variety of shapes and \nsizes are reserved for alum inum alloys apart from the \nsimple hollow sections that can be produced in heavy metals or steel with a mandrel. The development of die design for simple and complicated aluminum \nsections is determined first of all because the \nextruded products have significant position in the world market. Aluminum extrusion tooling includes \ndie set, die holder, bolster/sub-bolster or pressing \nring, etc. The typical alum inum extrusion dies are \nillustrated in Figure 9. Die set of solid and hollow die \nare presented as in Figure 10 and 11 respectively. In \naddition, the table 1 explains the fundamental \nfunction of each die part component to play an \nimportant role in aluminum extrusion process. \n \n \n \nFigure 9: Aluminum extrusion dies \n \n31 \nS. Butdee  et al. \n \n \n \nFigure 10: Solid die components\n \n  \nFigure 11: Hollow die components\n \nTable 1: The functions of extrusion tooling \n \nDie    Form the section. A solid die \ntype is called die plate and for \nhollow die, we found its die cap \nand mandrel. \nBacker Supports the tongue of die to \nprevent collapse or distortion.   \nThe shape of backer orifice is often closely related to die orifice. Feeder \nplate  Balance aluminum flow through die orifice. \nBolster Supports extrusion load is \ntransmitted from die and backer. \nDie holder Holds the die and, to some \ndegree, the die backer. \nDie carrier Holds the die set in the press. \nBridge Divided metal flows and \nsupports the mandrel.    Typical solid die parts \nFeeder plate \nDie plate \nBacker plate \nBacker plate Solid die assembly view Product solid shape \nProduct \nhollow shape \nMandrel \nDie cap \nHolow die assemble view Bolster \n \n32  \nCaseXPert A Process Planning System with Feature Based Neural Network Search Strategy for Aluminum \nExtrusion Die Manufacturing  \n \nThe difficulty to obtain a good die is on the equal re petition of the velocity in each point of the final \nsection. It is the condition to obtain a good straight \nbar (see 4.1). A die for an aluminum extrusion needs properties follow as: \n• Accurate dimensions and product shape, to avoid the need for any corrective work and to decrease cost repair die. \n• Maximum possible die life, increase productivity and decrease extrusion cost per \nunit. \n• Maximum length of the extruded section, \nunless it is determined by calculating \nextrusion yields and selecting a proper billet \nsizes. \n• High extrusion speed, depend on die design \nand extruded operation. \n• A good-quality surface finish product. \n• Low manufacturing cost.  \nThese requirements are usually fulfilled with rod and \nsimple shapes. However, as the die more complex, it \nbecomes increasingly more difficulty to comply six \nrequirements. Many factors have to be considered in the design and construction of a die, including the \nflow pattern, maximum specific pressure, \ngeometrical shape of the s ection, wall thickness and \ntongue sizes, shape of bearing surface, and tolerance \nof the section.  Extrusion dies are essentially thick, \ncircular steel disks containi ng one or more orifices of \nthe desired profile. A die type and die geometry \ndepend on the characteristic s of each profile to be \nextruded. There are normal ly fabricated from hot \nworking steel, as H-13 (American Standard) and heated to the desired condition. Such difficult section \nof a bar served as example of what it formed to do \nfor forging exhibition. In a typical extrusion, the \nextrusion die will be placed  in the extrusion press \nalong with several supporting tools. These tools, also made from hardened tool steel, are known as backers, \nbolsters and sub-bolster. Its can provide support for the die during the extrusio n process to prevent die \nbroken and to extend the die life. In addition die \nsupport tools contribute to improved tolerance controls and extrusion speed.  A tool stack for a \nhollow die is similar to that used for a solid die. A \nhollow die is a two-piece construction, one piece \nforming the inside of the hollow profile and the other \npiece forming the outside of  the profile. It likewise \nrequires the use of additional support tools.  \n3.1 Solid dies \nSolid dies are used to produce profiles that do not \ncontain any voids. Various styles of solid dies are \nused, depending on the equipment and manufacturing \nphilosophy of the extruder. Some prefer to use recessed pocket or weld-plate style dies. A pocket die \nhas a cavity slightly larger than the profile itself, \napproximately to deep. This cavity helps control the metal flow and allows the billets to be welded \ntogether to facilitate the use of a puller. Both pocket \nand weld plate type dies provide for additional metal \nflow control, compared to that of the flat-face type \ndie. A weld plate is a steel disk that is placed (often pinned and/or bolted) in front of a solid die. It has an opening that controls the flow of metal to the orifice. \nWeld or feeder plates serve to control contour, and/or \nspread the aluminum. It is also important that the \nlayout of a multi-hole die is arranged to prevent \nextrusions from rubbing together or running on top of \neach other as they leave the press. A flat surface and \nnot the edge of a leg or rib should run along the run-\nout table to prevent small part of profiles to be bent. In Figure 12 illustrates typical solid dies one and four \nprofiles for a die plate. \n \nFigure 12: Typical solid dies\n \n33 \nS. Butdee  et al. \n \n3.2 Hollow dies \nA hollow die produces profiles with one or more \nvoids such as tube products. The profiles could be as simple as a tube with one void or as complex as a \nprofile with many detailed voids. The most common \ntype of hollow die is the porthole die, which consists of a mandrel and cap section; it may or may not have \na backer or back plate. The mandrel, also known as \nthe core, generates the internal features of the profile. The mandrel has two or more ports, it based on the \nshape of profiles or die design method. The \naluminum billet will be separated into each port and \nrejoins in the weld chamber prior to entering the \nbearing area and die orifice. Webs, also known as legs, which support the core or mandrel section, \nseparate the ports. The cap creates the external \nfeatures of the profiles. It is assembled with the mandrel. The die cap has a pocket or welding \nchamber, which is rejoin metal flow from porthole \nthrough die orifice. It is assembled with the mandrel. The backer or back plate, when used, provides \ncritical tool support and it is  immediately adjacent to, \nand in direct contact with the exit side of the cap. In \ncase the profiles have critical point a backer will necessary be supported to avoid die broken and to \nextend die life. Typical hollow dies are presented in \nFigure 13. \n \n \n \nFigure 13: Typical hollow dies \n \n3.3 Semi-hollow dies \nA semi-hollow die is used to produce profiles having \nsemi-hollow characteristics as defined in Aluminum Standard and Data (published by the Aluminum \nAssociation, Inc.). Semi-hollow dies have port holes, \nlegs, bridges as same as hollow die but without cores to make a void of section. It gives the extruded solid \nprofile shape with high tongue ratio. In practice, \nsemi-hollow profile, tongue ratio is grater than 5. \nThe semi-hollow classification derives from a mathematical comparison between the area of the \npartially enclosed void and the mathematical square \nof the size of the gap. This ratio (area/gap²) is called \nthe tongue ratio. Depending on the tongue ratio, semi-hollow dies can be constructed as flat, recessed-\npocket, weld-plate, or porthole design. Porthole dies \nare more prevalent in the production of semi-hollow profiles.  \n 4 ALUMINUM EXTRUSION DIE DESIGN \n4.1 Die design process \nDie design is one of crucial task in aluminum extrusion process. The traditional die design is based \non skills and experiences of a die designer. He/she \nlearns and accumulates knowledge of die design from the previous die design cases. The success and failure \ncases are studied and analyzed in order to use their \ninformation from these cases for improving a new die \ndesign to avoid failure case. In practice, a die may be \ntested many times before the extrusion profile is \nsatisfactory [5]. The first die test points out the \nefficiency of die design. If  the extruded profile shape \nis not perfect, the tester will modify die geometry \nsuch as bearing length to balance extrusion speed of \nthe section for obtaining profile shape perfectly.  \nThe quality factors of die design are: decrease the \nnumber of tests, increase die life, produce a good \nproduct and provide high productivity. This part \ndescribes the fundamental of aluminum extrusion \n \n34  \nCaseXPert A Process Planning System with Feature Based Neural Network Search Strategy for Aluminum \nExtrusion Die Manufacturing  \n \ndie design process. In general, die design process cons ists of laying out, selecting type of die and size, \ncalculating shrinkage all point on profile section, \ndetermining feed or porthole shapes (will be defined just after) and sizes for controlling aluminum flow, \ndetermining dimensional die orifice and tongue \ndeflection, calculating the bearing lengths, designing tooling support. Furthermore, a die designer has to \nconsider the size of the containers of the machines \nand the number of extruded profiles graved in the same die for each size of c ontainer. Each die design \nprocess should be focused on the parameters that have effect to die efficien cy such as shape factor, \nextrude ability, extrusion ratio, etc. In fact, die design or die geometry will be modified following the profile characteristics, esp ecially, profile shape and \ndimensions. To obtain high extrusion productivity, die design must be optimized to increase the capability of the extrusion process and to overcome \ndie fail. In addition, a tester should have a through \nunderstanding of the different  functions of each die features (i.e. leg, sink in, porthole, bearing etc.). These features will be modified to interact with the process parameters and each of the product \ncharacteristics. The complex flow of material in the \nextrusion die or container often creates different \ndeformation conditions in various regions of an \nextruded product. Die design has to imagine the \nmetal flow pattern in die and container in order to make decisions for the defi nition of each die feature. \nEspecially, die orifice a nd bearing length are the \nimportant features, which have directly effect with \nmetal flow velocity. In theo ry, flow velocity of each \npoint on the section should be balanced to avoid the \nextruded profile twist. Figure 14 illustrates the \ntypical profile, which is incompletely deformed \nshape because die bearing lengths are unbalance then \nthe velocities of two sides of the section are large \ndifferent. \n \n \n \nFigure 14: Typical profiles unbalance shape \n \nAs already described, bearing length is used to \ncontrol the extruded profile shape and extrusion speed. The suitable given bearing length is required \nin aluminum extrusion die design. If bearing length is \ntoo short, the profile shape may be twist or wave. On the other hand, great bearing length will reduce \nextrusion speed and productivity. In addition, the \nextruded surface quality is not satisfactory. \n4.2 The knowledge of die design \nIn general, die design is based on skills and experiences from a die designer, who accumulates the knowledge of die design for using this knowledge \nto solve the die design problems. The problems in die \ndesign work are: the selection of the suitable press, \nthe type of die, the choice of material, the laying out \nthe profile(s) on the die in order to optimize the extrusion yield, the different of the contraction \ncooling of each sections, the unbalance velocity of \nthe metal flow on each point of the sections, the consolidation of the tooling to withstand in extrusion process, etc. These problems have to use the \nknowledge base of die design in order to find the best \nsolution for solving each problem and to approach a \ngood die. \nFigure 15 shows the basic flat die for solid sections, \nconsisting of feeder plate, die plate, backer plate, \nbolster, and sub-bolster. Naturally, the metal flows from the container into a die by passing feeder, die orifice, back opening, bolster, and sub-bolster \nrespectively. The feeder pl ate contains one or more \ncavity to be used to weld the metal flow together \nbefore through into die orifice and to balance the \nmetal flow to the die. Sometime, flat die design is \nwithout feeder plate; if we can sink the face of the die \naround the cavity in the die plate. \nHowever, the feeder plat e can permit to enable \nsuccessive billets to weld together and to enable \nwider sections to be extruded than is normally \npossible from any given container size. The die plate is used to deform the material in order to take the \n \n35 \nS. Butdee  et al. \n \nshape of die orifice (see Figure 15). The backer provides the immediate support to the die to reduce \ndie deflection and to minimize the die stresses. In \ncase of no long tongues, the simply sections are usually extruded by using one of a standard range of \nbackers. If the shape contai ns critical sections that need good support, then a custom backer with an \naperture closet the die orifice can be used for \nsupporting the tongue necessary. \n   \nMetal flow  \nFeed \nOrific e \nBack \n \nFigure 15: Solid die geometry and features \n \nFurthermore, bolster and sub-bolster are used to \nsupport consolidation of a die. The bolster supports \nthe backer and usually sufficient support can be \nachieved by using standard bolster.  \nPorthole dies are primarily used to produce hollow \nextrusions as shown in Figure 16. The basic tooling \nin hollow extrusions composes of mandrel, die cap, \nbolster, and sub-bolster. The porthole dies are \nsuitable for multi-holes dies and can also be used \nwith the maximum section circumscribing circle \ndiameter relative to the container diameter. A \nfavourable extrusion ratio can be selected by using the optimum material flow; heating to relatively low \ntemperatures should then be sufficient. Moreover, \nthis type of die is used for sections with very critical \ntongues and cross sections. However, the die is \nmanufactured from a single piece of steel, die \ncorrection and die cleaning by polishing are difficult. Mandrel is the projection, fixed or floating that is \npositioned in front of the die cap in order to make the \nempty part of the sections. In porthole die the aluminum flow is split by legs, which support the \ncore of the mandrel. The material flows around the \nlegs, through the feeder holes and is welded together in the welding chamber as shown in Figure 16. The \nprofile shape is formed by  the clearance between the core on the mandrel and th e die opening in the die \ncap. The wall thickness of the extrusion is determined by the difference  in the diameters of the \ndie apertures in die cap and the mandrel. Die cap \ncontains weld chamber, die orifice, bearing length. \nThe hollow shapes are fo rmed by the core and \nbearing surface on die cap. In practice, the distance between core and die orifice is given by determining \nthe contraction cooling of the sections. Bolster and sub-bolster for hollow dies have the functionality \nthan for solid dies, in order to protect the die \ncollapse. \nBesides, the fundamental of die design, a die \ndesigner needs the explicit and implicit knowledge \nduring die design stages in order to obtain the efficiency of the die design. This knowledge \nencompasses the selection the suitable press and \ntooling, the determination number of die opening and the laying out of the die orifices, the calculation \nshrinkage allowances on section dimensions, the \nproviding of additional dimensional allowances for die dishing and tongue deflection, the determination \nof the bearing length to control the profile speed in \neach point on the sections, and the checking of the \nadequate support for critical sections. The knowledge \nabout die design is described the next part.\n \n36  \nCaseXPert A Process Planning System with Feature Based Neural Network Search Strategy for Aluminum \nExtrusion Die Manufacturing  \n \n \n \n Metal flow direction \n Weld \nchambe r\nFeeder \nhole\nCore \nLeg \nBearing \nsurface  \nFigure 16: Hollow die geometry and features \n \n4.3 Die geometry \nDie can be distinguished into three categories as \nsolid, hollow and semi-hollow die as described \nabove. The difference die geometries provide the \nvarious shapes of the profiles. Each die part has different machining processes in order to perform the \nfinal die shape. We proposed that the features of die \nparts are constructed based on OO (Objected-Oriented). Classes describe the feature objects and it \ncan be inherited by deriving a feature subclass from feature head class. Each class has methods and \nattributes to describe their class entity. These feature \nclasses are managed in database system. By example we have a holes feature class and then it can be \nderived to blind hole, through hole, counter bore, \ncountersink, etc. A user defines these features data and translates geometry data from CAD into feature \nclasses for using in process planning. For instance, \nclass hole has methods to calculate holes surface and volume, and attributes of holes feature includes \ndiameter, depth, centre point, axis, tolerance, and \nsurface finishing. Each attribute is used to define holes entity and to calculate holes surface and \nvolume respectively. Moreover, it is used to select \nmachining tools, such drill, centre drill, tap, and so on to make the holes. Figure 17 illustrates the typical \nfeatures of die geometry for principle solid die, for \nthe It has three parts: feeder, die, and back plate. \nA feature is a basic entity or information that has \nproperties to involve in engineering design and \nmanufacturing. It also represents engineering significance, not only geometric and topologic information. Non-geometric property such as, roughness, hardness of material, and so on are \npresented [6]. Die geometry  features are used to \nsupport die design in order to generate die geometry. \nIt can decrease die design lead-time and to facilitate \nfor creating a new die geometry by reusing or \nrevising the die features from library. The die features can be easily modified in order to use the \ninformation from features for supporting the process \nplanning of die manufacturing. This research die \nfeatures can be divided into two categories; die part \ngeometry and die feature lib rary. Die part features \ncompose and describe the f eeder plate, die plate, \nbacker plate, mandrel, die ca p, back plate, bolster and \nsub-bolster. These part feat ures can be reused for \nnew die design by adding the required feature from \ndie feature library. Die pa rts feature library are \nillustrated in Figure 18.  \nThese systems geometry are created by computer-\naided design such as SolidWorks ®. It has feature \nlibrary to facilitate and reuse the standard features in \norder to modify geometry. Thus feature library in \nCAD software can be applied to support die design \nbased on feature base design. Feature library of die \ngeometry can be used to modify feed feature, die orifice, backer opening, porthole, welding chamber, \nand so on. Moreover, each feature has entities to be \nused in machining process or computer aided \nmanufacturing.  \n \n37 \nS. Butdee  et al. \n \n \n Feature classes Die parts Die assembly \nOPEN POCKET CIRCULAR \nEDGE CHAMFER TAP HOLE \nCLOSED POCKET PLANE \nOPEN POCKET \nCIRCULAR EDGE CHAMFER \nTHROUGH HOLE \nPROFILE HOLE \nCLOSED POCKET PLANE \nOPEN POCKET \nCIRCULAR \nDEGE CHAMFER THROUGH HOLE \nCOUNTERBORE \nCLOSED POCKET \nPLANE Part no. \nFPAC245-01 \nPart no. \nBKAC245-01 Part no. DPAC245-01 \n \nFigure 17: Feature class library \n \n Die parts feature\nDie Solid die parts Hollow die parts \nFeede r Backe r Bolste r Mandrel Dieca p Bac k \n \nFigure 18: Die part feature library \n \n \n \n \n38  \nCaseXPert A Process Planning System with Feature Based Neural Network Search Strategy for Aluminum \nExtrusion Die Manufacturing  \n \n4.3.1 Feeder feature \nFeeder has function to control the metal flow through \ndie orifice. For solid die, feed  is in the feeder plate or \ndie plate whereas for hollow die, feed is the internal \npart of the mandrel and is called porthole.  Feeder \nshape affects the flow pattern, the extrusion pressure, the metal flow velocity, and the consequence the \nbearing length. Bearing length is adapted, based on \nthe local wall thickness and the velocity of the metal flow just at the entry thro ugh the die orifice. So the \nbearing length relates directly with feeder shape. For instance in Figure 18 sh ows typical feed feature \nlibrary and simple holes feat ure. Die part feature as \nfeeder plate can be subtracted with feeder feature as dog bone shape as shown in Figure 19. A user can modify feature dimensions to attain a new feed \ngeometry for each design case based on profile \ngeometry.  \n \n \n \nFigure 19: Typical feed and simple holes feature \n \n4.3.2 Welding chamber feature \nMetal flows from each porthole of mandrel in hollow die and joins again before to be pushed through die orifice in a welding chamber. Welding chamber \ngeometry is designed depending on porthole design \nand welding line allocation on the section. In practice, welding line should be avoided allocation \non the exposed surface. A die designer can select the welding chamber feature in die feature library. Welding chamber feature can be modified in order to \nobtain the required shape and dimensions. Moreover, the welding chamber depth is given according to the \nwall thickness of the section and is determined from \nthe total die depth. Figure 20 illustrates typical welding chamber features for using it to create the \nwelding chamber in die cap of hollow die geometry.  \n \n \n \nFigure 20: Typical welding chamber features \n \n39 \nS. Butdee  et al. \n \n5 PROCESS PLANNING OF ALUMINUM \nEXTRUSION DIES \n5.1 Process planning reviews \nProcess planning is defined as the planning and \ndevelopment of detailed instructions for the \nconversion of a raw materi al into a finished part \nbased on some feasible engineering design [7]. \nProcess planning encompasses the activities and \nfunctions to prepare a detailed set of plans. It includes the sequence of operations, selection raw \nmaterial, machines, tools, cutting tools, cutting \nparameters, etc. In general, process planning is based \non the skill and experience of a process planner who \ncan provide the knowledg e of process planning. \nHowever, the human memory may be lost and it is \nnot easy to collect enormous data. Knowledge based \nsystem has been determined to manage the implicit and explicit knowledge fr om any sources in a \ncomputer system. In recent years, artificial intelligence has been discussed and used in order to develop in a computer system for supporting process \nplanning tasks. In general process planning can be \ndistinguished into three methods: \n• The VPP (Variant Process Planning) method                                                                                                                  The hybrid method approach attempts to exploit knowledge in existing plans while generating a \nprocess plan for a new design [8]. Computer aided \nprocess planning can eliminate many of the decisions \nrequired during planning. It has the following \nadvantages: reduces the demand on the skilled \nplanner, reduces the planning time, reduces process planning and manufacturing costs, creates consistent \nplans, produces accurate plans, and increases \nproductivity [9]. The knowledge-based expert systems for process planni ng are reviewed in [10]. \nBy example, it is used to be application for in research [11] and [12] . In addition, Stryczek \nreviewed the application of computational \nintelligence in computer aided process planning [13].  \n• The generative method \n• The hybrid method \nThe variant process planning involves to retrieve an \nexisting plan for similar part and to modify the previous plan for the new part. Variant method is \nbased on a Group Technology (GT), coding and \nclassification approach to id entify a larger number of \npart attributes or parameters. The structure of VPP \nbegins by coding the part by a user and then searches \nthe similar part from part database. Each part of die set has a standard machine routing and retrieve the \nstandard operations from operations sequence file for \nreusing and modifying in order to apply with a new \npart. Finally, the system gives process plan. \nHowever, the machining routing of standard parts \ncan be generated from the knowledge of a die process planner.  \nThe generative method is based on useful knowledge base, production rules, or artificial intelligence to generate process plans. This method approaches \ndeals with generation of new process plans by means \nof decision logics and process knowledge. Data from \nCAD is extracted to recogni ze the feature shapes and \nretrieve the dimensions for generating the process \nplans with an inference engine. The inference engine \ndecides the selection process and machine by \nderiving from the knowledge base. In fact, \nknowledge base for process planning comes from the acquisition of the knowledge from experts, from the \nmachine specifications and capabilities, and so on. In \naddition, knowledge base provides the operations sequence, machines, cutting tools, cutting \nparameters, machining times, etc via the inference \nengine. To realize a generative process-planning module, it is necessary to have a knowledge base \nwhich includes three main components: the part \ndescription, the knowledge base and database, and the decisional logics and calculus algorithm.  \n5.2 The knowledge of process planning \nThe knowledge base is the central component of an \nexpert system. It contains generic knowledge for solving specific domain-related problems. In general, we can classify the knowledge base into three main \ncategories. There are procedural knowledge, \ndeclarative know ledge, and control knowledge. \nProcedural and declarative knowledge are stored in \nthe knowledge base. The control knowledge is used to construct the inference mechanism. Procedural \nknowledge can be sometimes also called production \nrules which represent the relation structure and problem oriented hierarchies of the knowledge stored \nor the production rules. The declarative knowledge \nmay be called the knowledge details or problem facts, represents the factua l part of the knowledge or \nthe specific features or th e problems. The knowledge \nmay be a group of data or a symbolic structure. Additionally, the declarative knowledge can be \nstored in a database. The control knowledge is the \nknowledge about a variety of processes, strategies, and structures used to coordinate the entire problem–\nsolving process, is not stored in the knowledge base. \nThe knowledge base for process planning is used to store the production rules, an inference engine, a \ndatabase, and so on. It can be used in order to \nperform the process planning for aluminum extrusion \n \n40  \nCaseXPert A Process Planning System with Feature Based Neural Network Search Strategy for Aluminum \nExtrusion Die Manufacturing  \n \ndie. The knowledge base of process planning and cost evaluation contains  rules and techniques for \nknowledge representation. It includes cutting tools, \ncomponent features and machining process [14]. The \nproposed knowledge-based system utilizes (rules, \ntables, equations, etc.) for: component specification, tool material selection, machining process, cutting \ntools selection, and cutting conditions. Martin and \nD’Acunto [15] presented a procedure for the design of a production system, based on part modeling and \nformalization of technological knowledge by using \nproduction features, and finally they can calculate \nproduction costs by determining from the multi-\ncriteria analysis. Grabowik and Knosala [16] presented a method of representation of the \nknowledge about the body construction and \ntechnology in an expert sy stem that aids the process \nof designing the machining technology of bodies. \nExpert system is focused on the improvement of the \nproposed method of object representation on the technological knowledge and body construction in \norder to increase the level of the generated \ntechnological documentation by increasing the number of the problems. For this research we \nemployed frame-based system and rule-based in \norder to manage the knowledge base of die process planning and cost estimating.  \n5.3 Die machining process \nMachining processes for making a part of die consist of various processes depe nding on geometry of each \npart as following: turning, drilling, milling, EDM (wire cutting, drilling, sparking), grinding, and \nassembling. Moreover, heat-treatment is the \nimportance method to impr ove mechanical properties \nof die material for supporting hot working and high \npressing.  \n5.3.1 Turning process \nTurning process is the early stage to form raw material cylindrical shape to obtain die disc before \nmachining with another processes. Facing, turning, \nand chamfering are the operations for machining \neach die part. Turning operati on of a die is shown in \nFigure 21. A measurement of  how fast material is \nremoved from a work piece can be calculated by \nmultiplying the cross section area of the chip by the \nlinear travel speed of the tool along the length of the \nwork piece. Material remova l rate in turning can be \ncalculated in the form:   \n MRR = π  × D × d × f × N            (1)   \nWhere: \n  D  is outer (or average) work piece diameter    d  is depth of cut \n f   is feed rate (ipr) \n N   is spindle speed (rpm)  \nThe spindle speed is given by: \n \n   DvN××=π12 (2) \n \nMaterial removal rate becomes: \n \n MRR = 12 × d × f × V            (3)  \n \n \n \nFigure 21: Turning operation \nWhere: \n  V   is cutting speed (fpm) \nIn straight turning, chip width is  ⎟⎟\n⎠⎞\n⎜⎜\n⎝⎛−\n20 fD D\n \nWhere:  \n D o and Df are outer and inner work piece \ndiameters, respectively \nCross section area is  ()\n42 2\n0 fD D−π\nhence the \nmaterial removal rate is  \nN fD DMRRf× ×−=4) (2 2\n0π\n (4) \n5.3.2 Milling process \nMilling is widely applied in machining process due \nto it includes a number of highly versatile machining \noperations capable of producing a variety of configurations. The basic types of milling such as \nslab milling, face milling, end milling, and so on. \nMilling process can be machined feed, porthole, weld chamber, etc. for die part  machining as shown in \nFigure 22.  \n \n41 \nS. Butdee  et al. \n \nMaterial removal rate in milling process can be \ncalculated by determination from milling parameters \nas the cutting speed, V (m/min) , is given by: \n \n V= π ×D V   (5) ×\n \nWhere: \n D is the cutter diameter (mm) \n N is the rotational speed of the cutter (rpm) \nFeed per tooth, mm/tooth can be calculated by: \n    n Nvf×=        (6)  Where: \n v   is linear speed of the work piece or feed \nrate, mm/min \n n  is number teeth on cutter \nThen material removal rate is given by the expression \n MRR = w × d × v (7)  \nor            \n MRR = w × d ×(f × N × n) (8) \nwhere:  \n w  is width of cut (mm) \n d  is depth of cut (mm)  \n \n \n \nFigure 22: Milling operation \n \n5.3.2 Grinding process \nGrinding process is a chip-removal process that uses an individual abrasive grain as cutting tool. The \ngrinding process is used to flatten the die surface in \norder to assemble and to correct die distortion. Die face must be flatness to avoid unbalance of the metal \nflow. Material removal rate in grinding machining is \ngiven by: \n   MRR = d × w × v   (9) \nWhere:  \n d  is depth of cut (mm) \n w is width of cut or grinding wheel \nthickness (mm) \n v   is linear speed of the work piece or feed \nrate, mm/min  \n5.3.3 Wire EDM machining process \nElectrical discharge wire cutting is used to cut contour as thickness plates, punches, dies, and small or complex holes. It will be cut die orifice or deep \nhole in die manufacturing process. The wire is a \ncutting tool that is usually made of brass, copper, or tungsten; zinc or brass coated and multi-coated wires are also used. The wire diameter is typically about \n0.30 mm for roughing cuts and 0.2 mm for finishing \ncuts. The cutting speed is generally given in terms of \nthe cross-sectional area cut per unit time. Typical \nexamples are: 18,000 mm\n2/hr for 50 mm thick D2 \ntool steel, and 45,000 mm2/hr for 150 mm thick \naluminum. These removal rates indicate a linear \ncutting speed of 18,000/50 is 360 mm/hr or 6 mm/min, and 45,000/150 is 300 mm/hr or 5 mm/min, \nrespectively. Typical EDM wire cutting process is \nillustrated in Figure 23. \n5.3.3 Electro discharge machining process \nElectro-discharge or spar k-erosion machining, is \nbased on the erosion of metals by spark discharges. The basic EDM system consists of a shaped tool \n(electrode) and work piece, the metal surface is \nremoved from a transient spark discharges through the dielectric fluid. Elect rodes for EDM are usually \nmade of graphite, brass, copper, and copper-tungsten is also used. EDM sparking is generally used to make bearing length of an extr usion die. Metal removal \nrates usually range from 2 to 400 (mm\n3/min). \n \n42  \nCaseXPert A Process Planning System with Feature Based Neural Network Search Strategy for Aluminum \nExtrusion Die Manufacturing  \n \nMoreover, EDM can be used to make the core shape of mandrel as shown in Figure 24.   \n \n \nFigure 23: EDM wire cutting process \n \n \n \nFigure 24: EDM sparking process \n \n5.3.4 Heat treatment \nIn addition, the important process in die \nmanufacturing is heat treatment. This process has played a role to increase die strength property.  \nFrom machining process as described above for fabrication an extrusion die, we proposed machining process classes as turning process class, drilling \nprocess class, milling pro cess class, and so on are \nderived from machining process class. These classes \nhave methods to calculate machining parameters and \nmetal removal rate in order to calculate machining \ntime of each process.  6 THE KNOWKEDGE BASE OF DIE \nDESIGN AND PROCESS PLANNING \n  \n6.1 The structure of knowledge based system for \ndie manufacturing  \nFor our application, we employed frame-based system and rule-based in order to manage the \nknowledge base of die process planning and cost \nestimating. Figure 25 presen ts the flow chart of the \nknowledge base process planning and cost estimating \nof a die. The knowledge is managed in frame-based \nsystem. \n \n \n \n43 \nS. Butdee  et al. \n \nTo approach process planning of a die, we need form \nfeatures and material type of die part in order to \nselect the feasible machinin g processes. Die features are retrieved by two ways: from die part geometry or \ndie features database.  \n \n F2 Die part Geometr\ny \nFrom features \n& material \nDie features \ndatabase \nCost estimation \ndatabasePossible machining process &  \nparameters\nCalculate machining \nTimes & costs\nSelect process \nOperation processes Constraints \n- cost \n- time - … \n- … \n Machining \nProcesses & \nCutting \nParameters \ndatabase Machines & \nTools database \nFeature -type \n-dimensions -tolerances \n-Finishing \nSurface -Volume Frame-based and rule-based system \nOp: Operation process F: Feature \nFOP\n \nF1 \nF3 OP 1 OP 2 OP 3 OP n \nFn \nFigure 25: Process planning and cost estimating flow chart of an aluminum extrusion die \n \nOn one hand, a user has to define all features of their parts into a database of a new die part case. On the other \nhand, we can reuse die features by reusing previous di e part features from data base. Die feature includes \nfeature class type, dimensions, tolerances, finishing surface , volume, etc. These features are then used to select \nmachining process by frame-based, rule -based system and decision table. Each feature has different operation \nprocesses to be machining. The syst em gives the possible machining processes by frame-based and select the \nsuitable those processes based on rule-based system a nd decision table. The sel ected process composes of \noperation method, machine type, machine tools, and cutting parameters. Machines and tools data can also be \nretrieved from machine and tools database. Machining process and cutting parameters database provide die operation method in the process and cutting conditions in order to calculate cutting time. Each machining \nprocess will be evaluated cutting time by formulas and machining time is then used to calculate machining \ncost. Finally, the system selects the process accord ing to constraints as machining time and cost. \nThis knowledge base system for process planning of the extrusion dies is used to train the data sets of input \nand output in neural network architecture. The next section describes the structure of die process planning with \nartificial neural network technique. \n6.2 The structure of die design and process pla nning system with arti ficial neural network \nThe structure of die design and process planning is illustrated in Figure 26. This structure consists of two main parts, including die design and process planning. The fi rst part is design die features based on the neural \nnetworks as defined in 7.1. The process starts by reading product data from database or from customer’s \nproduct. The product data of aluminum extrusion process is the characteristics of profile. It contains profile shape, section area, dimensions, tongue ratio. This data is  input data layer of the neural network in order to \n \n44  \nCaseXPert A Process Planning System with Feature Based Neural Network Search Strategy for Aluminum \nExtrusion Die Manufacturing  \n \nsearch the previous die design cases from die design case library. The output layer is the similar case which is \nretrieved from library to be reused the features of die design geometry. The machining processes for making \nthe die features can be given by the knowledge base of die machining process planning. \nThe frame based and ruled based systems are managed and derived the machining processes by determining \nthe machined features of die geometry. Tools database can support the system to select the machine tools for \neach machining operation. Process parameters are give n from process database. These machining parameters \nare cutting speed, cut of depth, feed rate, and so on. They can be defined with the knowledge base of the \nsuggestion from tool’s manufacturer and shop floor data  of die maker. Moreover, die machining process \nsequencing is ordered according to di e making routes standard. Each machining process route depends on the \ntype of die part. Eventually, machining process plan is generated by the proposed structure of die manufacturing based on neural network technique. \n \nDIE DESIGN\nInput\nlayer\nHidden\nlayer\nOutput\nlayer\nPROCESS PLANNINGProfile carateristics\nMachining processProduct\ndata\nDie design\ncase library\nTool & process\nparameters\nProcess sequencingTools &\nprocess\ndatabase\nProcess planDie features\nKnowledge\nbase process\nplanningDie design case\n \nFigure 26: The structure of die manufacturing with artificial neural network \n \n \n45 \nS. Butdee  et al. \n \n7 FEATURE BASED NEURAL NETWORK FOR ALUMINUM EXTRUSION DIE DESIGN AND \nMANUFACTUTING \n7.1 Artificial neural network in CAPP \nIn last decade, artificial neural network has been widely used in engineering application domain, especially for \ndesign and manufacturing. Artific ial neural network is a mathematical model for parallel computing \nmechanisms as same as biological brain. They consist of nodes, linked by weighted connections. Neural \nnetworks are constructed by hierarchical layers, which are input, hidden, and output layer respectively.  Neural \nnetworks learn relationships between input and output by iteratively changing interconnecting weight values \nuntil the outputs over the problem domain represent the desired relationship. Furthermore, neural networks \nperform a variety of functions such as pattern matchi ng, trend analysis, image recognition, and so on. CAPP \n(Computer Aided Process Planning) is the important task to couple CAD and CAM by interpreting feature model to machining process. Knapp et al [17] presented the ability of neural network in the process selection \nand within feature process sequencing. In this work, tw o co-operating neural networks were utilized: the first \none, a three layer back propagation neural network, takes in as input the attributes of a feature and proposes a \nset of machining alternatives; another fixed weight ne ural network selects exactly  one of the alternatives. \nParameters of the features are modified by the results of the operation until the final state of the feature has \nbeen reached. Yahia & al [18] proposed a feed forward neural network based intelligent system for computer aided process planning methodology. This methodology suggests the sequence of manufacturing operations to \nbe used, based on the attributes of a feature of the component. By integrating this methodology with computer \naided design (CAD), process planning can be generated, and tested, which helps in realizing concurrent engineering. Yue and al [19] presented a stat e of the art review of research in computer  integrated \nmanufacturing using neural network techniques. Neural network-based  methods can eliminate some \ndrawbacks of the conventional approaches, and  therefore have attracted resear ch attention particularly in \nrecent years. The  four main issues related to the neural network-based techniques, namely the  topology of the \nneural network, input representation, the training method and  the output format are di scussed with the current \nsystems. The outcomes of  this research using neural network techniques are studied, and the limitations and  \nfuture work are outlined.  Praszkiewicz  [20] purposed of this article is to  present the application of neural \nnetwork for time per unit determination in small lot production in machining. A set of features considered as input vector and time consumption in manufacturing process was presented and treated as output of the neural net. A neural network was used as a machining model. Sensitivity analysis was made and proper topology of \nneural network was determined.   \n7.2 Structure of artificial neural networ ks for die design and process planning \nThe proposed structure of neural network for die design and process planning is shown in Figure 27. The input \nparameters consist of the characteristics of aluminum profile including type of profile, profile shape, \ndimensions, cross-section area, extrusion ratio, CCD (Circumscribing Circle Diameter), tongue ratio etc. The \noutput layer contains the type of die, the number die orifice, extrusion ratio, die stack set, and die machining \nprocess routes.  \nShape\nSection area\nDimensions\nTongue ratioDie type\nNo die orifice\nInput\nlayer\nHidden layerOutput\nlayerProfile caracteristic\nDie machining\nprocess routeExtrusion ratio\nDie stack set\nFigure 27: The structure of neural networks for die \ndesign and process planning \n \nThe mathematical model of the biological neuron, there are three basic components as presented in Figure 28.  \n \n46  \nCaseXPert A Process Planning System with Feature Based Neural Network Search Strategy for Aluminum \nExtrusion Die Manufacturing  \n \n∑ ()ϕ\u00000x\n1x\n2x\npx0kw\n1kw\n2kw\nkpwkθ\nThresholdOutput\nSumming\njunctionkνActivation\nfunction\nky\nInput signals\n \nFigure 28: A perceptron neuron model \n \nFirst, the synapses of the neuron are modeled as weight s. The value of weight can present the strength of the \nconnection between an input and a neuron. Negative weight values reflect inhibitory connections, while positive values designate excitatory connections. Second component is th e actual activity within the neuron \ncell. This activity is referred to as linear combination. Finally, an activation function controls the amplitude of \nthe output of the neuron. An acceptable range of output is usually between 0 and 1, or -1 and 1.  \nEach neuron calculates two functions. The first is propagation function as shown in equation 10,  \n \n     (10)  kk wxν=∑ j j\n \nWhere wkj is the weight of the connection between neuron k and j, yk is the output from neuron k. The second is \nan activation function. The output of a neuron in a neural network is between certain values (usually 0 and 1, \nor -1 and 1). In general, there are three types of activation functions, denoted by ϕ(•) as illustrated in equation \n10. Firstly, there is the treshold function which takes on a value of 0 if the summed input is less than a certain \nthreshold value ( v), and the value 1 if the summed input is greater than or equal to the threshold value. \n \n {1  if  ν\n0  if  ν < (ν)=k\nkθ\nθ ϕ≥     (11) \n \nTraining of the networks will be discussed in the next section.  \n \n8 DATASET FOR NETWORK TRAINING \nThe key issues of the developed neural network based methodology for aluminum extrusion die design and \nprocess planning of die manufacturing will be discussed in the following sections: \n8.1 Formulate the knowledge based system of die design and process planning \nA set of rules has been generated to define the feature of die features and machining processes for each feature. These rules have been used to be the set of input and output layer of the neural network structure. These rules \nare captured from the successful die design and machinin g cases which are stored in die design case library. \nThe rules of thumbs may be gathered with the knowledge from die design and die making experts and other \nsource. The rules are formed in IF-THEN.  \n8.2 Design topology of the neural network model \nThe neural network has been trained by using the standard propagation algorithm. This work uses supervised learning, which is one of three categories of the training method. Supervised learning may be called \nassociative learning, is trained by providing with inpu t and matching output patterns. These input-output pairs \ncan be contributed by an external teacher, or by the system which co ntains the neural networks (self-\nsupervised). The learning process or  knowledge acquisition takes place by representing the network with a set \nof training examples and the neural network via the learning algorithm implicitly rules. The topology of the \nproposed neural network model applies feed forward archit ecture. Each variable is th e input value at a node of \n \n47 \nS. Butdee  et al. \n \nthe input layer. The input layer of neuronal node is designed in such a way that one node is allocated for the \nfeature type, and one node is allocated to  each of the above sets of feature a ttributes. Also the values of all the \ninput layer neurons are normalized to lie between 0 and 1.  The number of nodes in the input layer is equal to \none plus the number of all the possible different ranges of feature attributes, encountered in the antecedent part of the rules. The types of profile are represented by 3 nodes to denote solid, semi-hollow and hollow type \nrespectively. Profile shape is also given 20 principle sh apes which are addressed with 20 nodes. In addition, \ngeneral profile thicknesses are designed with 20 nodes, and each node has the difference value ranges in 0.5 \nmm. Moreover, dimensions (maximum width and height), CCD, cross section area, press machine capacity, \nextrusion ratio, perimeter, external perimeter and tongue  ratio define the numbers of the nodes in input layer \nwhich are 15, 15, 15, 15, 3, 30, 15, 17 and 12 respectively in our case. Consequently, the total number of nodes \nin input layer is 170. For example, the typical heat si nk profile is shown in Figure 29. The characteristics of \nprofile consist of: solid profile with rectangular shap e, general wall thickness 2.3 mm, width 24 mm, height \n15.3 mm, CCD 28.5 mm, section area 1.7 cm\n2, perimeter 20.32 mm, without external perimeter, and tongue \nratio 4.0. \n \nFigure 29: Typical aluminum profile \n \nWe can transform the characteristics of the typical profile to input layer of the network in vector format as \nillustrated in Table 2. \n \n \n \nTable 2: The typical vector of input layer \nColumn 1 2 3 4 5 6 7 8 …170 \nValue 1 0 0 0 0 0 0 0  0 \n \nIn the above vector, the column numbe rs [1-3] addresses the type of profile, [4-23] stand the profile shapes, \nand [24-33] stand for the sets corresponding to the diffe rent ranges of general wall profile thickness. Column \nnumbers [34-48], [49-63], and [64-78] are addressed according to the different ranges of maximum width, \nheight dimensions, and CCD respectively. Column numb ers [79-93] are the ranges  of cross section area. \nColumn numbers [94-96] address the press machine capacity as 660, 880, and 1800 tones respectively. Column numbers [97-126] stand for the sets corresponding to extrusion ratio. Column numbers [127-141] and [142-158] are presented the ranges of perimeter and external perimeter. The last column numbers [159-170] \nstand the ranges of tongue ratio.  \nThe output decision variables for the die design and process planning comprise of the various feature of die \ngeometry and die machining process plan. In addition, process sequencing and machining operations are given \nin the output layer. Die features and machining process are derived from the knowledge based system of die design and process planning. The first part of output network is a kind of dies, including 3 nodes as solid, \nsemi-hollow, and hollow die as same as the third 3 nodes input. The next two groups are the nodes of the \nnumber die orifice 15 nodes and the nodes of extrusion ratio  15 nodes respectively. Die set thicknesses are also \ndesigned corresponding to die stack dimensions of each pr ess machine. Die thickness co nsists of feeder plate, \n \n48  \nCaseXPert A Process Planning System with Feature Based Neural Network Search Strategy for Aluminum \nExtrusion Die Manufacturing  \n \ndie plate, backer plate thickness for solid or semi-hollow die, and mandrel, die cap thickness for hollow die. The number of die thickness group is 50 nodes.  Die features are defined following the main types of die \ncomponents, including feeder, die, back, mandrel and die cap part features. These features consist of four main feature categories, are hole, edge, gr oove, and pocket. Hole features have blind, through, tap hole, counter \nsink, counter bore, and deep hole. Edge features includ e edge chamfer and edge fillet. Groove features are v \ngroove, round groove, and rectangular groove. The last group is pocket which comprises open pocket plane, open pocket circular, open pocket scul ptured, closed pocket pl ane, closed pocket circular, and closed pocket \nsculptured. The number node of die features can be classified into 5 nodes following as die part components. The first node is feeder features which have open pocket  circular, open pocket plane,  edge chamfer, tap, and \nclose pocket plane. The second node addresses die featur es, including open pocket ci rcular, open pocket plane, \nedge chamfer, through, deep hole, and close pocket plane. The third node stands back features which are open \npocket circular, open pocket plane, counter bore, and close pocket plane. The fourth node presents mandrel \nfeatures, comprising open pocket circular, open pocket plane, edge chamfer, tap, open pocket sculptured, and \nclose pocket sculptured. The last node of die feature group nodes is die cap featur e, including open pocket \nplane, open pocket circular, edge chamfer, co unter bore, close pocket plane and deep hole.  \nThe last part of output layer is die parts machining processes (turning, drilling, milling, heat treatment, grinding, EDM (sparking, wire cutting, drilling) process) . Turning process includes rough turning, semi-finish, \nfinish turning, round chamfering, and round grooving operation. In addition, facing process is discussed to \nremove the material of open pocket circular in order to control die part thickness. Rough facing, semi-finish \nfacing, and finish facing are operations of facing proce ss. Drilling is the simple process to make a hole in die \ngeometry. There are seven operations: centering, drilli ng, boring, reaming, tapping, counter boring, and \ncountersinking. The important machining process to be cut the open pocket plane, and open pocket sculptured at mandrel, feeder plate, and back orifice of back plat e is milling process. This process comprises rough, semi-\nfinish, and finish milling operation. These machining processes and operations are also grouped in five nodes \naccording to die machining process pl anning routes which have feeder , die, back, mandrel, and die cap \nmachining process routes respectively.  \nIn addition, for aluminum extrusion manufacturing, heat treatment process is applied to enhance die strength property. After heat treatment, die may be distorted then  it must be machined with grinding process to flatten \ndie face. EDM sparking process is used to make beari ng lengths only die plate and di e cap. The last process is \nEDM wire cutting which includes rough and finish wire cutting. Die part of solid die and die cap of hollow die are used wire cutting process to cut die orifice. So the number of nodes in the output layer is 93. The output \nlayer vector is shown in Table 3. \n \nTable 3: The typical vector of output layer \nColumn 1 2 3 4 5 6 7 8 …93 \nValue 1 0 0 1 1 1 1 1  0 \n \nTolerance and surface finish are not se riously determined in aluminum extr usion die manufacturing due to the \nmachining process selection are not ba sed on the tolerance and surface finish value. The process routes of die \nmachining operation are consistency, it can be set the standard process.    \nIn the above vector of output layer,  the column numbers [1-3] stand the type of die. The column numbers [4-\n18] address the number of die orifice and extrusion ratio is presented in column numbers [19-33]. The column \nnumbers [34-43], [44-53], [54-63], [64-73], and [74-83] stand the part thickness of feeder, die, back, mandrel and die cap respectively. The next group column numbers [84-88] are the feature group of die part features. \nFinally, the last 5 nodes as shown in column number [89-93] stand the machining process routes in order to \nmake die parts. They are feeder, die, backer , mandrel, and die cap process planning routes respectively.                 \n8.3 Training the neural network \n \n \n49 \nS. Butdee  et al. \n \n \nFigure 30: Learning curve  \n \nThe standard back-propagation algorithm is used as th e learning mechanism for the neural network. The \ntraining data set has been prepared by translate die design cases data fro m cases library to input and output \nlayer format. Neural network tool box of MATLAB 2008 is employed to simulate the neural network \noperation. There are two alternative training modes possible, depending on the particular way of presenting the \ntraining patterns to the neural network and depending on when the network weights are updated, either after presentation of each training pattern or after presentation of the entire se t of examples. For this paper, we \nfound that the number of hidden layers is 1. The mode of training is pattern. The number of hidden layer is 5. Learning rate is o.1 and momentum rate is 0.7 respectively. The numbers of nodes in input and output layer are 170 and 93. The learning curve is shown in Figure 30.  \n \n9 CASE STUDY \nThis case study is tested with the aluminum extrusion die design and process planning to make a die from the \nexample case in an aluminum extrusion industry. The samp le product is selected to test the developed system. \nThe sample product is created by CAD system in order to  extract the characteristics of  this profile for use in \ndie design and process planning a die. The product is shown in Figure 31.  \nThe characteristics of this profile are including hollow profile with rectangular  shape, cross section area is 3.4 \ncm2, profile width is 50 mm and height is 14.7 mm, perimeter is 30.37 cm, and external perimeter is 19.24 cm, \nand tongue ratio is 1.4 respectively. This product data is transformed to input layer format in order to find out the die features and process planning. The result of this case is illustrated in Table 4 that has been translated \nfrom the codes of the output layer to features of a die and process planning.  \n \n \nFigure 31: The sample product in CAD system \n \n \n50  \nCaseXPert A Process Planning System with Feature Based Neural Network Search Strategy for Aluminum \nExtrusion Die Manufacturing  \n \nThe type of die is hollow die with one cavity. Extrusion ratio is 40. Die components are mandrel and die cap. There fore, the machining process plan s are mandrel, and die cap process plan ning route as shown in Table 4.  \nTable 4: The features and machining pro cess plan of the die design case study \nDie part Features Machining process and operations \nOpen pocket \ncircular \n Rough turning  \nSemi-finish turning \nFinish turning \nRough facing \nSemi-finish facing \nFinish facing \nEdge chamfer Round chamfering \nBlind hole \n Centering  \nDrilling \nTap Tapping Mandrel \nClosed pocket \nsculptured Rough milling \nSemi-finish milling \nFinish milling \nOpen pocket \ncircular \n Rough turning  \nSemi-finish turning \nFinish turning \nRough facing \nSemi-finish facing \nFinish facing \nBlind hole \n Centering  \nDrilling \nCounter bore Centering \nDrilling \nCounter boring  \nClosed \npocket \nsculptured EDM Sparking \nClose pocket plane Rough milling \nSemi-finish milling \nFinish milling Die cap \nDeep hole Rough wire cutting \nFinish wire cutting \n10 SUMMARY \nThis paper presents the result of application feature based method with artificial neural network in order to \nsearch the die design and process planning case from aluminum extrusion die manufacturing library. The \noutputs are die design features and possible die machining process plan. The die machining process plan consists of machining operations and sequences of each process. The detailed description of the neural \nnetwork based methodology includes formulating the knowledge base system for die design and process \nplanning, designing topology of the neural network model, and training the neural network have been given. The knowledge based system of die design and manufacturing are acquired from the die design and \nmanufacturer experts with other sources such as textbooks, researches, and so on. This knowledge is used to \ntrain in the neural network structure. The potential for application of the developed feature based neural network model has been presented with the help to design and plan the machining process of an aluminum \nextrusion die. Die design case stud y is successfully tested with the de veloped system. The number of die \n \n51 \nS. Butdee  et al. \n \nmanufacturing cases from the aluminum extrusion industry is more than 150 cases. The lead time of die design \nand process planning are decreased, and increase the effi ciency of die design and manufacturing to make a die \nto support extrusion process. It can enhance the competitive in order to produce a good aluminum extruded \nprofile.  \nIn addition, the fundamental of aluminum extrusion process, die design and process planning are described to understand the principal knowledge of aluminum extrusion process.   \n11 ACKNOWLEDGMENTS \nWe would like to thank MTAlumet Co., Ltd. Thailand for supporting the information and the resources to be used for this research. In addition, the authors thank Thai research fund and Fren ch government for funding \nour project.  \n12 REFERENCES \n[1] Laue K., Stenger H. (1981). Extrusion Processes,  Machinery, Tooling, American society for metals, \nThird edition, USA. \n[2] Negnevitsky M. (2002). “Artificial Intelligence: A Guide to Intelligent Systems”. Person Addison \nWesley, England.  \n[3] Sheppard T. (1999). “Extrusion of Aluminum Alloys”, Kluwer Academic Publishers, USA. \n[4] Noomtong C. (2006). “Aluminum Extrusion Die Design in A context of Integrated Design”. Thesis, \nInstitut National Polytechnique de Grenoble. \n[5] Noomtong C., Butdee S., Tichkiewitch S. (2008). “The case-based system for aluminum extrusion die \ndesign in a contex t of integrated”. Ninth International Aluminum Extrusion Technology & Exposition , \nVol. 1, pp. 371-380. \n[6] Lee S. F., Huifen W., Youliang Z., Jian C., Kwong W. C. (2003). “Feature-based collaborative design”. \nJournal of Materials Processing Technology , Vol. 139, pp. 613-618. \n[7] Kamrani A.K., Sferro P., Hanelman. (1995). “Critical Issues in Design and Evaluation of Computer \nAided Process Planning Systems”. Computers Industrial Engineering , Vol. 29, No. 1-4 \n[8] Elinson A., Herrmann J.W., Minis I.E., Nau D., Singh G. (1997). “Toward hybrid variant/generative \nprocess planning”.  Proceedings of DETC’97: 1997 ASME design Engineering Technical Conferences \nSeptember 14-17 , Sacramento, California. \n[9] Chang T. C. (1990). “Expert Process Planning for Manufacturing”, Addison-Wesley Publishing \nCompany, Inc, USA \n[10] Kiritsis D. (1995). “A Review of Knowledge-Based Expert Systems for Process Planning Methods and \nproblems”. International Journal of Manufacturing Technology , Vol. 10, pp. 240-262.  \n[11] Shi X., Chen J., Peng Y., Ruan X. (2002). “Development of a Knowledge -Based Process Planning \nSystem for an Auto Panel”. The International Journal of Advanced Manufacturing Technology , Vol. 19, \npp. 898-904. \n[12] Dequan Y., Rui Z., Jun C., Zhen Z. (2006). “Research of knowledge-based system for stamping process \nplanning”. The International Journal of Advanced Manufacturing Technology , Vol. 29, pp. 663-669. \n[13] Stryczek, R. (2007). “Computa tional intelligence in computer aided process planning-A review”, \nAdvance in manufacturing science and technology , Vol. 31, pp. 77-92. \n[14] Abdalla H.S., Edalew K.O., Nash  R.J. (2001). “A computer-based intelligent system for automatic tool \nselection”. Materials and Design , Vol. 22, pp. 337-351. \n[15] Martin P., D’Acunto A. (2003). “Design of a production system: an application of integration product- \nprocess”. Int. J. Computer Integrated Manufacturing , Vol. 16, pp. 509-516. \n[16] Grabowik C., Knosala R. (2003). “The method of knowledge representation for a CAPP system”. Journal \nof Materials Processing Technology , Vol. 133, pp. 90-98. \n[17] Knapp G.M., Wang H.P. (1992). “Neural networks in acquisition of manufacturing knowledge”. \nIntelligent Design & Manufacturing, Edited by Andrew Kusiak, John Wily & Sons Inc. \n \n52  \nCaseXPert A Process Planning System with Feature Based Neural Network Search Strategy for Aluminum \nExtrusion Die Manufacturing  \n \n[18] Yahia N.B., Fnaiech F., Abid S., Sassi B.H. (20 02). “Manufacturing process pl anning application using \nartificial neural networks, System”. Man and Cybernetics, IEEE International Conference , Oct 6-9, Vol. \n5. \n[19] Yue Y., Ding L., Ahmet K., Painter J., Walters M. (2002). “Study of neural network techniques for \ncomputer integrated manufacturing”. Engineering Computations , Vol. 19, pp. 136-157.  \n[20] Praszkiewicz I.K. (2008). “Appli cation of artificial neural network for determination of standard time in \nmachining”. Journal of Intelligent Manufacturing, Vol. 19, pp. 233-240. \n \n \n \n \n \n53",
      "metadata": {
        "filename": "A process planning system with feature based neural network search strategy for.pdf",
        "hotspot_name": "Radiator_Production",
        "title": "A process planning system with feature based neural network search\n  strategy for aluminum extrusion die manufacturing",
        "published_date": "2009-07-03T12:08:13Z",
        "pdf_link": "http://arxiv.org/pdf/0907.0611v1",
        "query": "die casting aluminum sustainability energy reduction manufacturing"
      }
    },
    "Improved iron-tolerance in recycled aluminum alloys via direct strip casting pro": {
      "full_text": "1 \n Improv ed iron-tolerance in recycled alumin um alloys via direct strip casting process  \nL. Jiang1*, R.K.W.  Marceau1, T. Dorin1 \n1Institute for Frontier Materials, Deakin University, 75 Pigdons Road, Waurn Ponds, Victoria, \nAustralia 3216 \nl.jiang@deakin.edu.au  \nGraphical Abstract  \n \n \nAbstract  \nRecycled aluminum alloys are pivotal for sustainable manufacturing, offering strength, durability, and \nenvironmental advantages. However, the presence of iron (Fe) impurities poses a major challenge, \nundermining their properties and recyclability. Conventional manufacturing processes result in coarse Fe-rich intermetallic compounds  that limit the tolerance of Fe content and negatively influence \nperformance of  advanced  aluminum alloys. To address this, rapid solidification techniques like direct \nstrip casting have been  explored. In this work, a detailed study of the strip cast microstructure  was \nconducted by scanning electron microscopy , electron backscattered diffraction  and atom probe \ntomography . Our results reveal  that alloys produced by DSC exhibit  significantly refined \nmicrostructures and are free from coarse Fe -rich intermetallics, thereby  retain ing the majority  of Fe in \nsolid solution. These  findings indicate that strip casting significantly enhances Fe -tolerance  in \naluminum  alloys , making it an attractive process for future aluminum recyclin g, with implications for \nsustainable high -performance applications.  \n \nKeywords: Aluminum recycling, Fe tolerance, intermetallics, direct strip casting  \n2 \n 1. Introduction  \nRecycled aluminum  alloys are indispensable materials in today's sustainable manufacturing practices, \noffering a compelling combination of strength, durability, and environmental benefits. These alloys, \nderived from various sources of aluminum  scrap , hold significant potential for reducing energy \nconsumption and minimizing  environmental impact when compared to primary aluminum  production  \n[1, 2] . However, their widespread adoption faces a critical challenge: the presence of iron (Fe) impurities, \nwhich tend to accumulate during recycling processes and severely compromise the alloys' properties \nand recyclability.  \nFe, as the  most common impurity in aluminum alloys, plays a notori ous role in diminishing their \ncorrosion resistance and mechanical properties. Unlike other metallic alloys, aluminum  has a very \nlimit ed solubility for Fe, maximum 0.05 wt.% at 650° C [3]. In conventional manufacturing, Fe mostly  \nforms coarse intermetallic compounds, such as AlFe and AlFeMnSi, which not only exacerbate pitting \ncorrosion but also limit the tolerance of Fe  content in high- performance aluminum  alloys. This \nconstraint is particularly detrimental in  aerospace and marine industries demanding low Fe levels, where \nthe content is required to b e less than 0.1 wt.%  [2, 4] . Consequently, finding innovative solutions to \nenhance the Fe tolerance of recycled aluminum  alloys is of paramount importance to promote their \nsustainability and utilization  in critical applications.  \nOne promising avenue for overcoming the challenges posed by Fe  impurities in recycled aluminum  \nalloys is through rap id solidification techniques  that offer the advantage of refinement of  the \nintermetallics. Direct strip casting  (DSC)  is an advanced near -net-shape casting technique that processes \nliquid aluminum  directly into sheet , which offers significant cost reduction and energy savings [5] . The \nsolidification rate of DSC is high, 102 to 104 °C/s [6], leading to the formation of fine -grain \nmicrostructure [5, 7] . In our previous work [8], direct strip casting has been proven to effectively refine \nFe-rich intermetallic compounds in aluminum  alloys and consequently enhance the alloy’s corrosion \nresistance. This observation demonstrates the im mense potential of direct strip casting in improving the \nrecyclability of aluminum  alloys. Nevertheless, a comprehensive examination of the microstructure \ngenerated through direct strip casting in aluminum  alloys remains unexplored, which is essential for the \nfuture utilization  of DSC within the aluminum recycling sector.  \nThis contribution explor es the effects of DSC on the microstructure, intermetallics, and solid solution \ncomposition of various Al -Fe alloys  by using scanning electron microscopy (SEM), electron \nbackscattered diffraction (EBSD) , and atom probe tomograph y (APT).  This work  aims to pave the way \nfor sustainable practices that not only extend the recyclability of aluminum  but also enable the \nutiliz ation  of recycled alloys in high- performance applications where Fe  content is traditionally a \nlimiting factor.  \n \n2. Materials and experiment s \nIn this study, we examined aluminum  alloys with different Fe concentrations. The specific chemical \ncompositions of these alloys are detailed in Table 1. Fe content of 0.1 wt.% was chosen because many \nhigh- performance aluminum  alloys maintain a tolerance threshold below 0.1 wt.%. To further our \nresearch objectives, we intentionally elevated the Fe contents to 1.0 and 2.5 wt.%. This was done to \nassess the potential of direct strip casting in increasing Fe tolerance within aluminum  alloys and to \nhighlight the impact of Fe on both the microstruc ture and the overall properties of the material.  \nThe experiments on direct strip casting were conducted using a lab -scale simulator, known as  a dip \ntester, designed at Deakin University. This dip tester simulates the initial interaction between the molten \nmaterial and the twin -roll caster's rolls during the twin-roll direct strip casting process [9]. The observed \nsolidification rate reached approximately 500 °C/s. Each composition was produced through direct strip casting. Additionally, an Al -2.5Fe alloy was crafted using sand casting in a 3 kg rectangular mold, 3 \n achieving a solidification rate of about 0.1 °C/s, similar to traditional ind ustrial casting methods.  The \ncasting methods and cooling rate measurements are described in more detail in our previous work [8] .  \n \nTable 1 . Chemical compositions (wt.%) of the studied alloys. \nAlloy   Fe Al \nAl-0.1Fe  0.1 bal. \nAl-1.0Fe  1.0 bal. \nAl-2.5Fe  2.5 bal. \n \nSamples SEM and EBSD observations were prepared by taking section s perpendicular to the casting \ndirection. The sectioned specimens were manually ground and polished with silicon carbide papers, \nfollowed by 6, 3 and 1  µm polycrystalline diamond suspensions. The initial polishing was conducted \nwith active oxide polishing suspension (OPS) for 2 min. To improve the sample quality for the EBSD \ncharacterization , the samples were then polished by using vibration polishing with OPS for at least 12 \nhrs. SEM images were taken using the backscattered electron (BSE) detector in a JEOL JSM 7800F \nSEM instrument, equipped with energy dispersive spectroscopy (EDS) and EBSD detectors, with an \noperating voltage of 20 kV.  For the EBSD expe riments, the step sizes ranged from 0.5 µm. The EBSD \nscan data was analyzed  using HKL Channel 5 software (Oxford Instruments HKL, Denmark). \nAPT experiments were conducted to determine the chemical  composition of the solid solution as well \nas the local dist ribution of elements. These experiments  were performed on a Local Electrode Atom \nProbe (LEAP 5000  XR, CAMECA ) instrument with a pulse fraction of 20%, a pulse repetition rate of \n250 kHz, a detection rate of 0.5%, and a specimen temperature of 25 K. APT sam ples were electro -\npolished with a standard two- step process [10] . APT data reconstruction and analysis was performed \nusing CAMECA AP Suite 6 containing the Integrated Visualization Analysis Software (IV AS). \nBackground subtraction  was performed using the background correction tool in IV AS. \n \n3. Results and discussion  \nFigure 1 shows the BSE images of the studied alloys, comparing those produced via strip casting (Fig. \n1a-c) with the Al- 2.5Fe alloy produced via sand casting  (Fig. 1d) . Notably, the alloys produced through \nstrip casting exhibit an elongated grain structure aligned with the solidification direction. This is \nconsistent with the previous literature [8, 11] . Interestingly, the strip ca st microstructure is increasingly  \nrefined with increasing  Fe content , as shown in Fig. 1 a -c. This  phenomenon is attributed to  the solute \neffect  that is traditionally explained through growth restriction theory [12- 14]. As solidification  \nprogresses , the Fe in the melt preferentially segregates in the liquid adjacent to the solid -liquid interface, \nwhich impedes the movement of the grain boundaries and thereby slows down the growth of the solid \nphase.  Additionally, t here are no discernible coarse particles in the Al -0.1Fe and Al -1.0Fe strip  cast \nalloys , evident in Fig. 1 a & b, which suggests that  most  of the Fe in these alloys remains in sol id \nsolution , preventing it from diffus ion and the formation of  precipitate s during the rapid solidification.  \nUnlike the Al -0.1Fe and Al -1.0Fe strip cast alloys, the Al -2.5Fe alloy produced by direct strip casting \nexhibits a combination of eutectic and elongated structure.  Furthermore, when examining the Al -2.5Fe \nalloy produced via sand casting (Fig. 1d), the forma tion of coarse intermetallics with a needle -like \nmorphology in the inter -dendritic regions are  observed. These intermetallics range in size from 20 to \n300 µm with an area fraction of ~ 20%. In contrast, these coarse intermetallics are absent in the strip -\ncast Al -2.5Fe alloy , as shown in Fig. 1c. 4 \n  \nFigure 1.  BSE imaging of the studied alloys : (a) Al -0.1Fe  alloy  produced by DSC, (b) Al -1.0Fe \nproduced by DSC, (c) Al -2.5Fe produced by DSC, and (d) Al -2.5Fe alloy produced by sand casting. SD \nrepresents solidification direction.  \n \nTo further investigate the difference in the microstructure of the Al -2.5Fe alloys produced by sand \ncasting and strip casting, in -situ EBSD and EDS were  carried out, as shown in Fig. 2. It can be seen \nfrom  Fig. 2 a  & b that the grain s of the Al -2.5Fe alloy produced by sand casting are significantly coarser \nwith an average diameter of 350 ± 121 µm, which is about 3 times larger than that observed in the Al -\n2.5 alloy produced by strip casting (111.7 ± 54.5 µ m). Grain refinement through  rapid solidification \nassociated with strip casting has been reported previous ly in the  literature [5, 15] , where high \nundercooling is achieved.  \nThe EDS mapping result in Fig. 2c suggests that the coarse needle -shape intermetallic phases in the \nsand cast Al -2.5Fe alloy  that are enrich ed in Fe  are likely the Al 3Fe (sometimes described as Al 13Fe4) \nphase  that is commonly seen in Al -Fe binary alloys [8, 16, 17] . Due to the slow solidification of sand \ncasting, the following high- temperature equilibrium eutectic reaction  occurs : 𝐿𝐿𝐿𝐿𝐿𝐿𝐿𝐿𝐿𝐿𝐿𝐿→ 𝛼𝛼𝐴𝐴𝐴𝐴+𝐴𝐴𝐴𝐴3𝐹𝐹𝐹𝐹, \nwhich is known to occur over the temperature range from  652 – 655°C in aluminum -rich alloys  [16]. \nHowever, these coarse Fe -rich particles are not observed in the allo y produced by direct strip casting \n(Fig. 2d). Instead, the EDS result in Fig. 2d shows that there are refined eutectic Fe -rich phases in the \nAl-2.5Fe alloy produced by strip casting. Upon closer  examination of Fig. 2 b & d, it can be found that \nthe Fe -rich phase primarily forms along the low angle boundaries, possibly dendri tic boundaries. \nAccording to literature, the refined Fe -rich phase could be  identified as the metastable compound Al 9Fe2, \npreviously observed  in rapidly solidified Al-Fe alloys , especially at cooling rates exceeding  20°C/s [16]. \nThe emergence  of the metastable Fe phase  during rapid solidification can be attributed to  the \nprogressively increasing supercooling and to the change in conditions for nucleation and growth [16].  \n5 \n  \nFigure 2.  (a) & (b) are the EBSD -derived grain orientation maps for the Al -2.5Fe alloys produced by \nsand casting and strip casting, respectively. The inverse pole figure (IPF) color  legend shown on the \nright applies to both EBSD images . (c) and (d) are the corresponding EDS maps  for the  Fe distribution \nwithin the same region s shown in (a) and (b) , although note  the scale bar changes.  \n \nAtom probe tomography (APT) is the most accurate tool for near atomic resolution  concentration \nmeasurement  [18]. To determine the Fe  content in solid solution, APT analys es were carried out to \nevaluate the matrix Fe con centration of the Al -2.5Fe alloy  material  produced by sand casting and strip \ncasting. Fig. 3 shows three- dimensional reconstruct ions that  map the  atomic -scale  distributions of Al \nand Fe atoms ( in cyan and pink color s, respectively ) for the se materials . Notably, both Al and Fe are \nuniformly dispersed within the se data volumes for both material conditions. It is not surprising that n o \ndiscernible presence of Fe- rich compounds  was captured  given the analysis volume  dimensions  are on \nthe order of a few tens of nm  as shown in Fig. 2. This suggests that both APT data represent the matrix \nof the two alloys, proving invaluable for determining the chemical composition of the solid solution. In \nAPT analysis , time-of-flight -based mass -to-charge state spectrum peak overlap occurs for AlH+ and \nFe2+ isotopes at  28 Da, and  for AlH+, AlH2+ and Fe2+, at 29 Da . Here , this issue was carefully addressed \nusing the peak decomposition tool with in IVAS, by comparing the relative  natural abundance of the \nelemental isotopes. Table 2 gives the decomposed and background -corrected composition of the matrix \nof the Al -2.5Fe alloys produced via sand casting and strip casting. It can be seen that the amount of Fe \nin solid solution of the strip cast Al -2.5Fe alloy is ~ 0.90 ± 0.06 at.% (1.90 ± 0.13 wt.%), which is \nsignificantly higher than that of the alloy produced by sand casting at 0.21 ± 0.01 at.% (0.44 ± 0.02 \nwt.%). This indicates that more  Fe is retained in the solid solution during strip casting  compared to \ntraditional sand casting . Conversely , the loss of Fe (~ 0.60 wt.%) in the solid solution of the Al -2.5Fe  \nalloy  produced by strip casting still results from the formation of metastable Al 9Fe2 compounds , as \nshown in Fig. 2d. Nevertheless, comparison of the SEM and APT results for the two alloy manufacturing \nprocesses highlights  the capability of direct  strip casting  to avert the formation of  coarse Fe \nintermetallics by forcing Fe into the solid solution  by almost 10 times as much. The measured Fe content \nin solid solution in the sand -cast alloy  exceed ed the equilibrium solubility (0.0 3 at.%) [19, 20]  and this \ncould be due to a coupl e of factors.  Firstly, non -equilibrium solidification could occur in some regions \n6 \n where Fe atoms might  get trapped within the solid solution  [16]. Secondly, there could be Fe micro-\nsegregation commonly seen in traditional casting processes [21, 22], particularly inter -dendritic areas \nwhere the Fe concentration is higher.  Notwithstanding, the marked difference observed between the two \nsamples clear ly indicates that strip casting retains a higher amount of Fe in solid solution.  \n \nFigure 3. Reconstructed APT maps of Al (cyan) and Fe (pink) for the Al -2.5Fe alloys produced by (a) \nsand casting and (b) strip casting.  \n \nTable 2 . Summary of the background- corrected chemical composition ( at.%) of the matrix of the Al -\n2.5Fe alloy  produced by sand casting and strip casting, derived from the APT results. \nCasting  Al Fe Other elements  \nSand  casting  97.42 ± 0.15  0.21 ± 0.01  Bal. \nStrip casting  95.86 ± 0.18  0.90 ± 0.06  Bal. \n \nIn summary, the detailed microstructural analyses presented in this work demonstrates that direct strip \ncasting has the capabilit y to refin e the grain microstructure, free from coarse intermetallics , and retain \na considerable amount of Fe in solid solution in Al -Fe alloys. This contrasts with  the material from \ntraditional sand casting, where coarse and needle- shaped Fe intermetallics dominate the microstructure \nwith much less Fe in the matrix. The absence of these coarse and needle- shaped Fe intermetallics in the \nstrip-cast aluminum alloys has been reported to improve the alloy’s corrosion resistance significantly \n[8]. In addition, as the Fe content in solid solution increases, the strip -cast microstructure is increasingly  \n7 \n refine d, which further improve s the corrosion performance of the alloy  [23, 24] . Furthermore, \nincreasing the Fe content in solid solution coupled with grain refinement can enhance the alloy strength \nthrough a combination of solid solution hardening and Hall -Petch strengthening,  and the absence of \ncoarse and needle -like Fe particles  can serve to avoid  ductility  loss. These properties are especially \nadvantageous for alloys with high Fe  content  that demanding high ductility, such as 1xxx aluminum  \nalloys  [16]. As such, this work  demonstrates the transformative potential of direct strip casting in \naluminum  alloy recycling, particularly to  enhanc e the tolerance of Fe  typically considered a harmful  \nimpurity , yet frequently accumulated during aluminum  recycling processes .  \n \n4. Conclusions  \n1) Alloys produced by strip casting  exhibited an elongated grain structure aligned with the \nsolidification direction, w ith the microstructure increasingly refined with higher Fe content.  \n2) In contrast, the sand -cast Al -2.5Fe alloy exhibited coarser grains with an average diameter \napproximately three times larger than strip -cast Al -2.5Fe. Additionally, coarse needle -shape Fe -\nrich intermetallic phases were found in the sand cast alloy, while these were notably absent in \nthe strip -cast counterpart.  \n3) APT analysis of t he strip-cast Al -2.5Fe alloy measured the Fe content in solid solution to be \n1.90 ± 0.13 wt.%, which is significantly higher than that  measured  in the sand-cast alloy  (0.44 \n± 0.02 wt.%  Fe). \n4) Rapid solidification during direct  strip casting can  substantially enhance the tolerance of \naluminum alloys to Fe, rendering it an appealing process for future aluminum  recycling \nendeavors . \n \n5. Acknowledgements  \nThe authors acknowledge the use of instruments at the Deakin Ad vanced Characterisation Facility.  \nFunding from the Australian Research Council Discovery Project g rant scheme (DP130101887)  is also \nacknowledged.  \n \nConflict  of Interest  Statement: On behalf of all authors, the corresponding author states that there is no \nconflict of interest.  \n ",
      "metadata": {
        "filename": "Improved iron-tolerance in recycled aluminum alloys via direct strip casting pro.pdf",
        "hotspot_name": "Radiator_Production",
        "title": "Improved iron-tolerance in recycled aluminum alloys via direct strip\n  casting process",
        "published_date": "2023-10-10T05:53:40Z",
        "pdf_link": "http://arxiv.org/pdf/2310.06327v1",
        "query": "die casting aluminum sustainability energy reduction manufacturing"
      }
    },
    "Powering the Future_ Innovations in Electric Vehicle Battery Recycling": {
      "full_text": "Nanotechnology Perceptions  \nISSN 1660 -6795  \nwww.nano -ntp.com   \n \nNanotechnology Perceptions 20 No. S13 (2024) 2338 -2351                                             \n Powering the Future: Innovations in  \nElectric Vehicle Battery Recycling  \n \nVenkata  Sai Chandra  Prasanth  Narisetty1, \nTejaswi  Maddineni2 \n \n1Quality Engineer, Independent Researcher, Southern Illinois  \nUniversity Carbondale, United States  \n2Data Engineer, Independent Researcher, Southern Illinois  \nUniversity Carbondale, United States  \nE-mail: 1venkatn0388@gmail.com , 2tejas.maddineni@gmail.com . \n \nAbstract  \nThe global shift towards electric vehicles (EVs) as a sustainable alternative to traditional \ngasoline -powered cars has triggered a significant rise in the demand for lithium -ion \nbatteries. However, as the adoption of EVs grows, the issue of battery disposal and \nrecycling has emerg ed as a critical challenge. The recycling of EV batteries is essential \nnot only for reducing the environmental impact of battery waste but also for ensuring \nthe sustainable supply of critical raw materials such as lithium, cobalt, and nickel. This \npaper ex plores recent innovations in the field of electric vehicle battery recycling, \nexamining advanced techniques such as direct recycling, hydrometallurgical processes, \nand sustainable battery design. It also highlights the role of policy and industry \ncollabora tion in improving recycling infrastructure and addressing the economic and \nenvironmental challenges associated with battery waste. By focusing on both the \ntechnical and regulatory aspects of EV battery recycling, this paper aims to provide a \ncomprehensive overview of the state of the industry and the future outlook for recycling \ntechnologies, ultimately paving the way for a cleaner, more sustainable future in \ntransportation.  \nKeywords: Electric Vehicle (EV) Battery Recycling, Lithium -ion Batteries, \nSustainable Transportation, Battery Lifecycle Management, Recycling Technologies, \nRaw Material Recovery  \nIntroduction  \nThe transition from fossil fuel -powered vehicles to electric vehicles (EVs) represents one of \nthe most significant shifts in the global transportation sector, driven by the urgent need to \nreduce carbon emissions and combat climate change. Electric vehicles , with their promise of \nlower emissions and reduced dependency on non -renewable energy sources, are at the forefront \nof the drive towards a sustainable future. As of 2024, global sales of electric vehicles are \naccelerating rapidly, with estimates suggestin g that EVs will account for over 50% of global \ncar sales by 2030. The widespread adoption of EVs is fueled by advancements in battery \ntechnologies, particularly lithium -ion (Li -ion) batteries, which power the vast majority of EVs \non the market today. Howev er, the environmental and economic challenges associated with EV \nbattery life cycles are becoming increasingly evident. While electric vehicles themselves are \nheralded as a solution to reduce greenhouse gas emissions during operation, their batteries are 2339  Venkata Sai Chandra  Prasanth Narisetty    Powering the Future: Innovations …. \n \nNanotechnology Perceptions 20 No. S13 (2024) 2288–2301                                             \n made up of rare and valuable materials such as lithium, cobalt, nickel, and manganese. As the \nnumber of electric vehicles on the road continues to grow, the issue of what happens to these \nbatteries when they reach the end of their useful life has emerged a s a pressing concern. \nWithout proper recycling and disposal methods, spent batteries could contribute to \nenvironmental degradation, and the demand for raw materials could outstrip sustainable mining \npractices. Electric vehicle battery recycling is thus an essential part of the sustainability \nequation. It holds the promise not only of reducing the environmental impact of EV batteries \nbut also of recovering critical raw materials that can be reused in new batteries, reducing the \nreliance on mining. Moreover, as the number of EVs on the road grows, the volume of spent \nbatteries that will need to be processed increases exponentially. According to a report by the \nInternational Energy Agency (IEA), the number of electric vehicles worldwide is projected to \nexceed 1 45 million by 2030, leading to an urgent need for efficient and scalable recycling \nsolutions.  \nThis paper seeks to explore the innovative developments in the field of electric vehicle battery \nrecycling, providing a comprehensive overview of the technologies, challenges, and \nopportunities in this critical area. It delves into the current state of bat tery recycling, focusing \non emerging technologies such as hydrometallurgical, pyrometallurgical, and direct recycling \nprocesses, which offer the potential to enhance the efficiency and economic viability of \nrecycling operations. The paper also examines the  role of policy frameworks, industry \ninitiatives, and economic incentives in driving forward battery recycling practices, ensuring \nthat the recycling infrastructure can scale to meet the needs of the rapidly growing EV market. \nIn addition to exploring the technological and regulatory aspects of battery recycling, this paper \nwill highlight the environmental impact of improper battery disposal and the critical need for \na circular economy in the electric vehicle industry. The circular economy model, which \nemph asizes the reuse, refurbishment, and recycling of materials, is particularly relevant to \nbattery technologies, where the recovery of valuable metals can reduce both the environmental \nfootprint of mining and the costs of raw materials. Moreover, the transit ion to a circular \neconomy for batteries could help mitigate the e -waste crisis, which is projected to grow \nexponentially in the coming decades. As the global push for electrification intensifies, battery \nrecycling will play a crucial role in ensuring that the transition to a sustainable, low -carbon \nfuture is both feasible and environmentally responsible. The development of efficient and \nscalable battery recycling technologies, alongside the establishment of robust regulatory \nframeworks and industry standard s, will be critical to enabling this transition. This paper aims \nto provide insights into the latest innovations in EV battery recycling, assess the challenges \nthat need to be addressed, and explore the future scope of this rapidly evolving field. Through \na deeper understanding of these issues, we can better prepare for the challenges and \nopportunities that lie ahead in powering the future of transportation.  \n \nFig.1: waste Management Hierarchy  \nPowering the Future: Innovations …. Venkata Sai Chandra  Prasanth Narisetty 2340  \n \nNanotechnology Perceptions 20 No. S13 (2024) 2288–2301                                             \n    Literature Review  \nThe rise of electric vehicles (EVs) has sparked significant changes in the transportation \nindustry, driven by the need to reduce carbon emissions and combat climate change. As of \n2024, global sales of electric vehicles continue to accelerate, with projecti ons indicating that \nEVs could represent over 50% of new car sales by 2030. This shift is largely powered by \nadvancements in lithium -ion (Li -ion) battery technology, which has become the dominant \npower source for EVs. However, as the adoption of EVs grows, the environmental impact of \nbattery disposal and the recycling of these batteries has emerged as a significant challenge. As \nsuch, battery recycling is integral to the lifecycle management of electric vehicles, ensuring the \nsustainability of critical mater ials like lithium, cobalt, nickel, and manganese, which are used \nin the production of EV batteries. In this literature review, we examine the advancements in \nelectric vehicle (EV) battery recycling, exploring key technologies, environmental \nimplications, e conomic considerations, and future trends in the recycling process. This review \nalso highlights the role of regulatory frameworks and industry initiatives in promoting \nsustainable battery recycling practices.  \n1. Technologies in EV Battery Recycling  \nThe recycling of EV batteries involves several technologies that aim to recover valuable \nmaterials while minimizing environmental impact. These technologies can be broadly \nclassified into mechanical , pyrometallurgical , hydrometallurgical , and direct recycling  \ntechniques.  \n1.1 Mechanical Recycling  \nMechanical recycling is one of the most common methods used in battery recycling, involving \nthe physical dismantling of batteries to separate the different components, including metals and \nplastics. This method typically includes crushing, grinding, and si eving processes. While this \nprocess is relatively simple and low -cost, it does not recover high -value materials like lithium \nor cobalt efficiently. Therefore, it is often used in conjunction with other methods such as \nhydrometallurgical processes.  \n1.2 Pyrometallurgical Recycling  \nPyrometallurgical methods involve high -temperature smelting to recover valuable metals from \nbattery waste. In this process, batteries are incinerated at high temperatures, and the resulting \nslag is processed to extract materials such as nickel, cobalt, and  copper. While \npyrometallurgical recycling is highly effective for certain materials, it is energy -intensive and \nresults in the loss of critical materials such as lithium and aluminum. Therefore, it has become \nless favorable due to its environmental impact  and the need for more sustainable methods.  \n1.3 Hydrometallurgical Recycling  \nHydrometallurgy uses aqueous solutions to dissolve and extract metals from spent batteries. \nThis process involves using acids, solvents, or other chemicals to leach valuable materials such \nas lithium, cobalt, and nickel from the battery components. Recent advancements have \nimproved the efficiency of hydrometallurgical processes, making them more selective and \nreducing the amount of hazardous waste generated. For instance, research by Wang and Xiao \n(2023)  suggests that advancements in hydrometallurgical meth ods have led to better separation \nand higher recovery rates of critical materials from spent Li -ion batteries. However, these \nmethods still face challenges related to chemical disposal and scalability.  2341  Venkata Sai Chandra  Prasanth Narisetty    Powering the Future: Innovations …. \n \nNanotechnology Perceptions 20 No. S13 (2024) 2288–2301                                             \n 1.4 Direct Recycling  \nDirect recycling represents a significant innovation in battery recycling, aiming to preserve the \nintegrity of battery materials and reduce energy consumption. Unlike traditional methods, \nwhich break down the battery into its constituent materials, direct recycling seeks to reuse the \nsame active materials in new battery cells. Liu and Zhang (2022)  reviewed various direct \nrecycling methods, noting that these techniques can potentially lower costs and reduce \nenvironmental impact by avoiding the degradation of  critical materials. Direct recycling has \nthe potential to recover materials with high efficiency and maintain the performance of \nbatteries, thus contributing to a more sustainable circular economy.  \n1.5 Solid -State Recycling Technologies  \nEmerging technologies, such as solid -state batteries, may also revolutionize the recycling \nprocess. Solid -state batteries use a solid electrolyte rather than a liquid one, and their design \nmay facilitate easier recycling compared to traditional liquid elec trolyte -based batteries. \nAlthough research in this area is still in its infancy, early studies show promise in terms of \nhigher recyclability and reduced environmental impact.  \n2. Environmental Impact and Resource Recovery  \nThe environmental benefits of EV battery recycling are substantial, particularly in reducing the \ndemand for virgin raw materials, such as lithium and cobalt, which are often sourced through \nenvironmentally destructive mining practices. Dunn and Wang (2023)  conducted a life cycle \nanalysis (LCA) comparing the environmental impact of mining raw materials versus recycling \nbattery materials. The study found that recycling could reduce carbon emissions, energy \nconsumption, and water usage associated with the extr action of these critical minerals. \nFurthermore, battery recycling can help mitigate the growing problem of e -waste. As millions \nof EV batteries reach the end of their lifespan, proper recycling will prevent hazardous \nchemicals, such as cadmium and lead, fr om leaching into the environment. Zhang and Xie \n(2024)  argue that without a reliable recycling infrastructure, the rapid accumulation of EV \nbatteries could result in significant environmental damage. The economic implications of EV \nbattery recycling are al so considerable. By recovering valuable metals from used batteries, \ncompanies can offset the cost of raw material procurement. This is particularly important given \nthe volatile pricing of raw materials, such as lithium, which has seen dramatic increases du e to \nthe growing demand for EVs. Nash and James (2024)  highlight that, by 2030, the market for \nrecycled materials from EV batteries could be worth over $10 billion, offering significant \neconomic opportunities in addition to the environmental benefits.  \n3. Challenges in EV Battery Recycling  \nDespite the technological advancements and environmental benefits, several challenges remain \nin the EV battery recycling industry. These challenges are related to both the technical aspects \nof the recycling process and the regulatory frameworks needed to e nsure that recycling \npractices are efficient and widespread.  \n3.1 Complexity of Battery Designs  \nOne of the primary challenges in EV battery recycling is the complexity of modern battery \ndesigns. As manufacturers continue to innovate with new chemistries, cell configurations, and \nbattery management systems, the standardization of battery components ha s become \nincreasingly difficult. This lack of standardization makes it challenging to develop universal \nrecycling technologies that can efficiently handle a variety of battery designs. Binnemans and Powering the Future: Innovations …. Venkata Sai Chandra  Prasanth Narisetty 2342  \n \nNanotechnology Perceptions 20 No. S13 (2024) 2288–2301                                             \n    Jones (2024)  discuss the challenges posed by these new chemistries, noting that the increased \nvariety in battery types complicates the sorting and recycling process.  \n3.2 Economic Viability  \nThe economic feasibility of battery recycling is another significant challenge. Although the \ntechnology exists, the high cost of recycling —driven by labor, transportation, and \ninfrastructure expenses —has made it less attractive to many companies. In partic ular, current \nrecycling methods such as hydrometallurgy are expensive due to the chemical processes \ninvolved and the energy required for operation. Zeng and Li (2023)  argue that developing \nmore cost -effective recycling techniques will be crucial to scaling  up recycling operations and \nmaking the industry financially sustainable.  \n3.3 Lack of Infrastructure  \nThe lack of infrastructure for battery collection, processing, and recycling is another critical \nhurdle. The current recycling systems are limited, with many countries lacking the necessary \nfacilities for large -scale battery processing. Sun and He (2023)  emphasize the need for \ninvestment in recycling plants, collection systems, and logistics to ensure that used batteries \nare properly processed.  \n3.4 Regulatory Framework  \nThe regulatory environment around battery recycling is still evolving. While some regions, \nsuch as the European Union, have introduced stringent regulations to encourage recycling, \nother regions have lagged behind. There is a need for clear and consistent regulations that set \nminimum recycling rates, establish end -of-life management practices, and incentivize the \ndevelopment of more sustainable recycling technologies. Sullivan and Keith (2024)  argue that \nglobal cooperation and policy harmonization are neces sary to establish a cohesive framework \nthat can drive the growth of the EV battery recycling industry.  \n4. Future Directions and Emerging Trends  \nThe future of EV battery recycling looks promising, with numerous innovations on the horizon. \nResearch into new recycling methods, such as bio -based and electrochemical techniques, is \nunderway. These methods aim to improve efficiency and reduce environment al impact by using \ngreener solvents and materials. He and Zhang (2022)  highlight ongoing research into the use \nof bioleaching, where microorganisms are used to extract metals from spent batteries, offering \na potentially low -impact and cost -effective altern ative. Furthermore, the development of a \n\"closed -loop\" system for battery recycling, where materials are continually recycled and reused \nin new batteries, is gaining traction. Schneider and Tschiesner (2022)  note that a circular \neconomy for EV batteries could significantly reduce the need for mining raw materials, \ndecrease e -waste, and minimize environmental harm. These innovations, combined with \ngovernment incentives and corporate responsibility, are likely to accelerate the adoption of \nmore sustainable re cycling practices.  \nThe recycling of electric vehicle batteries is a critical component in ensuring the sustainability \nof the EV industry and the broader transition to clean energy. While significant progress has \nbeen made in recycling technologies, challenges remain related to economic viability, \ninfrastructure, and regulatory frameworks. The future of EV battery recycling will depend on \ncontinued innovation in recycling technologies, investment in infrastructure, and the \ndevelopment of supportive regulatory policies. As the global demand for electric vehicles \ngrows, effective recycling strategies will play a pivotal role in reducing the environmental 2343  Venkata Sai Chandra  Prasanth Narisetty    Powering the Future: Innovations …. \n \nNanotechnology Perceptions 20 No. S13 (2024) 2288–2301                                             \n impact of batteries, securing the supply of critical materials, and contributing to a circular \neconomy.  \n \nFig.2: Recycling Algorithm  \nScope & Opportunities  \nThe growing adoption of electric vehicles (EVs) presents a paradigm shift in the transportation \nsector. As the global automotive industry moves toward electrification, the demand for EVs \nand their corresponding lithium -ion (Li -ion) batteries is set to surg e in the coming decades. \nHowever, this growth also brings forth significant challenges, particularly with regard to the \nend-of-life management and recycling of EV batteries. Battery recycling is emerging as an \nessential part of the sustainability equation for electric vehicles. It not only reduces the \nenvironmental impact of battery disposal but also ensures the responsible extraction and reuse \nof critical raw materials. This section explores the scope and opportunities of EV battery \nrecycling, focusing on the potential for innovation, the environmental benefits, the economic \nopportunities, and the growing demand for recycling technologies and infrastructure.  \n1. Scope of EV Battery Recycling  \nThe scope of EV battery recycling extends far beyond the recovery of raw materials and \nenvironmental conservation. It encompasses a variety of aspects, including:  \nPowering the Future: Innovations …. Venkata Sai Chandra  Prasanth Narisetty 2344  \n \nNanotechnology Perceptions 20 No. S13 (2024) 2288–2301                                             \n    • Raw Material Recovery:  The global demand for critical materials like lithium, cobalt, \nnickel, and manganese used in EV batteries is skyrocketing. These materials are finite \nand often sourced from mining operations that have significant environmental and \nsocial impacts. Battery recycling offers an opportunity to recover these materials from \nspent batteries and reintroduce them into the supply chain, reducing the need for new \nmining operations.  \n• Environmental Impact Reduction:  Lithium -ion batteries contain harmful substances \nsuch as cadmium, mercury, and lead, which can leach into soil and groundwater if not \nproperly disposed of. Recycling helps mitigate this environmental risk by ensuring \nproper disposal of hazardous materials  and preventing contamination.  \n• Circular Economy Integration:  The circular economy model, which emphasizes the \nreuse, refurbishment, and recycling of materials, aligns perfectly with EV battery \nrecycling. By developing closed -loop systems for battery production and recycling, \nmanufacturers can reduce their dependenc e on virgin resources, lower production costs, \nand minimize the environmental footprint of battery manufacturing.  \n• Sustainability in Battery Manufacturing:  As battery recycling technologies advance, \nmanufacturers can source a significant portion of the raw materials needed for new \nbatteries from recycled components. This reduces the environmental cost of extracting \nvirgin materials and helps build more susta inable and resilient supply chains.  \n2. Opportunities in EV Battery Recycling  \nAs the EV market grows, several opportunities in battery recycling are becoming increasingly \nevident. These opportunities span across technological advancements, economic incentives, \nregulatory frameworks, and business models.  \n2.1 Technological Innovation and Advancement  \nRecent innovations in recycling technologies present significant opportunities for improving \nthe efficiency and cost -effectiveness of battery recycling processes.  \n• Hydrometallurgical Processes:  The development of more advanced \nhydrometallurgical techniques is enabling more efficient extraction of critical materials \nlike lithium, cobalt, and nickel from used batteries. Research has led to the development \nof new chemical agents and processes that offer higher selectivity, lower energy \nrequirements, and reduced chemical waste. Wang and Xiao (2023)  suggest that these \nimprovements can help make hydrometallurgical processes more viable on an industrial \nscale.  \n• Direct Recycling Technologies:  Direct recycling is a relatively new concept in the \nbattery recycling industry, aiming to preserve the active materials of the battery and \nreuse them in new battery cells. This method is still under development but holds great \npromise in reducing the ener gy consumption associated with recycling and maintaining \nbattery performance. Liu and Zhang (2022)  note that direct recycling methods could \npotentially offer a more sustainable and cost -effective alternative to traditional \nrecycling processes.  \n• Solid -State Batteries and Recycling:  Emerging battery technologies, such as solid -\nstate batteries, could offer new opportunities for recycling due to their simpler design \nand safer materials. Solid -state batteries replace the liquid electrolyte with a solid one, \npotentially making them easie r to disassemble and recycle. Researchers are exploring \nways to optimize the recycling processes for these new battery chemistries, which could \nrevolutionize the recycling industry in the future.  2345  Venkata Sai Chandra  Prasanth Narisetty    Powering the Future: Innovations …. \n \nNanotechnology Perceptions 20 No. S13 (2024) 2288–2301                                             \n • Bio-based and Green Technologies:  The growing interest in green chemistry and bio -\nbased technologies is also shaping the future of EV battery recycling. Bioleaching, for \nexample, uses bacteria or fungi to extract metals from battery waste. This approach, \nalthough still in the experimental  phase, offers a low -impact, sustainable alternative to \nconventional recycling techniques. He and Zhang (2022)  highlight that bioleaching \ncould be an environmentally friendly solution for recycling EV batteries without the \nuse of harmful chemicals.  \n2.2 Economic Opportunities  \nThe economic potential of EV battery recycling is immense. As the number of EVs on the road \nincreases, so does the volume of spent batteries, creating a rapidly growing market for recycling \nservices and infrastructure.  \n• Material Recovery and Cost Reduction:  The recovery of valuable raw materials such \nas lithium, cobalt, and nickel from spent batteries can offset the costs associated with \nmining and importing these critical resources. Dunn and Wang (2023)  estimate that \nrecycling could reduce the overall cost of battery production by as much as 20%, \nmaking EVs more affordable in the long run.  \n• Job Creation:  The expansion of battery recycling facilities and the development of \nnew recycling technologies will create numerous job opportunities in the green \neconomy. From research and development (R&D) positions to plant workers and \nlogistics experts, the battery recycling industry has the potential to create a broad range \nof employment opportunities globally.  \n• Recycling as a Profitable Business:  As the value of recycled materials rises, the \nrecycling of EV batteries is becoming an increasingly profitable business model. The \nmarket for recycled materials from EV batteries is expected to reach several billion \ndollars by 2030, driven by the growing demand for critical minerals and the increasing \nneed for sustainable supply chains.  \n• Economic Incentives and Subsidies:  Governments around the world are recognizing \nthe economic benefits of EV battery recycling. Policy incentives, such as subsidies, tax \ncredits, and grants for recycling infrastructure, are encouraging companies to invest in \nrecycling facilities and technol ogies. In addition, regulations requiring higher recycling \nrates and the use of recycled materials in new batteries will further boost the economic \npotential of the industry.  \n2.3 Regulatory and Policy Opportunities  \nAs EV adoption accelerates, governments and regulatory bodies are taking steps to ensure that \nbattery recycling becomes a viable and sustainable part of the EV lifecycle.  \n• Regulations and Standards:  In regions like the European Union, regulatory \nframeworks are already in place that mandate the recycling of batteries and set \nminimum recycling rates for certain materials. For instance, the EU Battery Directive \nsets binding targets for recycling efficie ncy and material recovery for batteries, \nincluding lithium -ion types used in electric vehicles. Sullivan and Keith (2024)  argue \nthat such regulations will become increasingly common in other regions, providing a \nstrong incentive f or businesses to adopt more sustainable recycling practices.  \n• Extended Producer Responsibility (EPR):  Many countries are introducing or \nexpanding Extended Producer Responsibility (EPR) schemes, which place the \nresponsibility of battery disposal and recycling on the manufacturers. This policy shift Powering the Future: Innovations …. Venkata Sai Chandra  Prasanth Narisetty 2346  \n \nNanotechnology Perceptions 20 No. S13 (2024) 2288–2301                                             \n    not only promotes better recycling practices but also helps establish a financial \nframework that supports the collection and recycling of spent EV batteries.  \n• International Collaboration:  The global nature of the EV market requires \ninternational cooperation to standardize recycling practices and promote knowledge \nsharing. Collaborative efforts between governments, industry stakeholders, and \nresearch institutions can help create a global fr amework for EV battery recycling, \naddressing cross -border issues such as transportation of used batteries and uniform \nstandards for recycling technologies.  \n2.4 Sustainability and Circular Economy Integration  \nThe move toward a circular economy for EV batteries represents one of the most exciting \nopportunities in the field of battery recycling. A circular economy emphasizes the reuse, \nrefurbishment, and recycling of materials in a closed -loop system, reducing wa ste and \nminimizing the extraction of virgin resources.  \n• Battery Life Extension:  By integrating more efficient recycling processes, \nmanufacturers can extend the useful life of batteries, reuse the materials in new \nbatteries, and reduce the need for mining new materials. This reduces the carbon \nfootprint of EVs and contributes to a mor e sustainable lifecycle for electric vehicles.  \n• Incentivizing Recycling through Product Design:  There is also an opportunity to \ndesign batteries with recycling in mind. For example, modular battery designs that \nallow for easier disassembly and recycling can make the process more efficient and \ncost-effective. Binnemans and Jones (2024)  suggest that designing batteries with \nstandardized chemistries and components would simplify the recycling process and \nmake it more economically viable.  \n3. Challenges to Overcome  \nWhile the scope and opportunities for EV battery recycling are significant, several challenges \nmust be addressed to fully realize its potential. These challenges include technological barriers, \neconomic considerations, infrastructure limitations, and regul atory complexities.  \n• Complexity of Battery Design:  The increasing diversity of battery chemistries and \ndesigns poses a challenge for recycling systems. Different battery types require \nspecialized recycling processes, and the lack of standardization in battery design makes \nit harder to develop efficient re cycling technologies.  \n• Scalability and Cost -Effectiveness:  Although recycling technologies have made \nsignificant advancements, scaling them up to handle the enormous volume of spent EV \nbatteries will require substantial investment in infrastructure and technology. \nAdditionally, the cost of recycling, especially f or more complex methods like \nhydrometallurgy and direct recycling, remains high, making it less economically viable \nat present.  \n• Logistical and Supply Chain Issues:  The collection and transportation of used EV \nbatteries to recycling facilities can be complex, especially when dealing with \ninternational or cross -border waste streams. Efficient collection and sorting systems \nmust be put in place to ensure that batteries  are processed in a timely and cost -effective \nmanner.  \nThe scope and opportunities for electric vehicle battery recycling are vast, with significant \npotential to impact the environment, economy, and the future of the transportation industry. \nInnovations in recycling technologies, along with the rise of a circu lar economy, offer 2347  Venkata Sai Chandra  Prasanth Narisetty    Powering the Future: Innovations …. \n \nNanotechnology Perceptions 20 No. S13 (2024) 2288–2301                                             \n promising solutions to address the challenges associated with the end -of-life management of \nEV batteries. With the support of regulatory frameworks, economic incentives, and industry \ncollaboration, EV battery recycling can play a pivotal role in creating a  sustainable and efficient \nenergy ecosystem. As the global transition to electric vehicles continues, the importance of \nadvancing recycling technologies and establishing robust infrastructure cannot be overstated. \nThe opportunities for innovation and growt h in this field are immense, and their successful \nimplementation will be crucial for a cleaner, greener, and more sustainable future.  \nReal -Life Case Study  \nThe following table presents a detailed case study of various real -life initiatives and \norganizations involved in electric vehicle (EV) battery recycling. The case studies showcase \nthe strategies, technologies, and outcomes from different players in the EV bat tery recycling \nspace, focusing on the technological, environmental, and economic aspects of each initiative.  \nCase Study  Organization/Company  Technology/Process  Key \nAchievements  Challenges  \n1. Li -Cycle \nBattery \nRecycling  Li-Cycle Corp.  Hydrometallurgical \nRecycling  - Closed -loop \nrecycling.  \n- High recovery of \nlithium, cobalt, \nnickel.  - High upfront \ncosts.  \n- Limited battery \nfeedstock.  \n2. Redwood \nMaterials  Redwood Materials  Direct Recycling & \nHydrometallurgy  - Recovery of \nhigh-value \nmaterials.  \n- Partnerships \nwith major \nautomakers.  - Scaling direct \nrecycling.  \n- Sourcing \nenough batteries.  \n3. Umicore \nBattery \nRecycling  Umicore  Pyrometallurgical \nRecycling  - High -purity \nmaterial recovery.  \n- Significant \nannual capacity.  - High energy \nconsumption.  \n- Logistical \nchallenges.  \n4. BASF & \nAurora \nSustainability  BASF (with Aurora)  Hydrometallurgical & \nClosed -Loop  - Circular supply \nchain for EV \nbatteries.  \n- Recycled \nmaterials used in \nnew battery \nproduction.  - High initial \ncapital \ninvestment.  \n- Complex \nintegration with \nexisting \nprocesses.  \n5. TES Battery \nRecycling  TES Mechanical & \nHydrometallurgical  - End-to-end \nbattery recycling \nsolution.  \n- Focus on \nsustainability.  - Managing \nhazardous \nmaterials.  \n- Complex \nlogistics.  \n6. Duesenfeld \nRecycling  Duesenfeld GmbH  Direct Recycling \n(Cathode Recovery)  - Over 90% \nrecovery of \nmaterials.  \n- Reduced energy \nconsumption.  - Limited supply \nof suitable \nbatteries.  \n- High cost of \nscaling.  \nAnalysis of the Case Studies:  \n1. Technological Innovation : Different companies are leveraging various recycling \ntechnologies, from hydrometallurgy  and pyrometallurgy  to direct recycling . The \nmove towards direct recycling  is particularly noteworthy, as it offers a more Powering the Future: Innovations …. Venkata Sai Chandra  Prasanth Narisetty 2348  \n \nNanotechnology Perceptions 20 No. S13 (2024) 2288–2301                                             \n    sustainable and energy -efficient method for recovering valuable materials like lithium \nand cobalt without the need for extensive high -temperature processes.  \n2. Sustainability : The key objective of all these initiatives is to reduce the environmental \nimpact of EV battery disposal and ensure the recovery of critical raw materials. The \nclosed -loop recycling systems  that many companies are adopting are aligned with the \ncircular economy  model, which helps to lower dependency on mined resources, \nreduces e -waste, and ensures that valuable materials can be reused in new batteries.  \n3. Challenges : The primary challenges faced by these companies include high upfront \ncosts, technological complexity, and logistical issues related to the collection and \ntransportation of used batteries. The lack of a standardized approach to battery design, \ncombined wi th the diverse battery chemistries in use across different vehicle \nmanufacturers, presents significant obstacles for recycling systems.  \n4. Economic Viability : While recycling can be economically beneficial in the long term, \nthe high cost of developing and scaling recycling facilities remains a barrier. \nGovernment incentives and regulatory frameworks that encourage recycling are crucial \nto overcoming these chal lenges and enabling the industry to grow at scale.  \n5. Future Outlook : The EV battery recycling market is poised for significant growth, \ndriven by the increasing volume of end -of-life batteries, technological advancements, \nand the ongoing shift towards more sustainable, circular systems. As more players enter \nthe market and  improve recycling technologies, economies of scale will reduce costs, \nand the industry will become more economically viable.  \nThe case studies provide a clear picture of the progress being made in the field of EV battery \nrecycling. Each company or initiative is addressing different aspects of the challenge, from the \ntechnical to the logistical, but all share the common goal of cr eating a more sustainable, circular \neconomy for electric vehicle batteries. The future of EV battery recycling looks promising, \nwith substantial opportunities for innovation, economic growth, and environmental impact \nreduction. However, continued investmen t in research, infrastructure, and international \ncollaboration will be essential to scale these solutions and meet the growing demand for \nrecycling as EV adoption accelerates worldwide.  \n \nFig.3: Global Battery Recycling Market  \n2349  Venkata Sai Chandra  Prasanth Narisetty    Powering the Future: Innovations …. \n \nNanotechnology Perceptions 20 No. S13 (2024) 2288–2301                                             \n Future Scope  \nThe future scope of electric vehicle (EV) battery recycling is vast and multifaceted, with several \nemerging trends and opportunities that warrant further exploration. As the global demand for \nelectric vehicles continues to rise, the volume of end -of-life batteries will increase, further \nemphasizing the need for efficient, scalable recycling solutions. This paper has examined \ncurrent advancements, bu t several areas require further research and development to fully \nharness the potential of battery recycling for a sustainable future.  \n1. Advancements in Recycling Technologies : Research into more efficient and cost -\neffective recycling methods, including direct recycling  and bio-based  approaches, \nholds great promise. Future studies should focus on optimizing these technologies to \nreduce energy consumption, improve material recovery rates, and make them \ncommercially viable at large scales.  \n2. Integration with Circular Economy : Further exploration into the integration of EV \nbattery recycling within a circular economy  model is essential. Developing better ways \nto close the loop between raw material extraction, battery manufacturing, and recycling \ncould make the entire lifecycle of batteries more sustainable.  \n3. Policy and Regulatory Frameworks : The development of global standards and \nregulations around battery recycling is crucial. Future research should evaluate the \neffectiveness of existing policies and propose new regulatory frameworks to \nstandardize recycling practices, promote Extended Producer Responsibility (EPR) , \nand encourage recycling initiatives at the global level.  \n4. Battery Design for Recycling : One area that remains underexplored is the role of \nbattery design for recycling . Future studies could explore how battery manufacturers \ncan design batteries that are easier to disassemble, recycle, and reuse, thereby \nfacilitating the recycling process and reducing the overall environmental impact.  \n5. Global Collaboration and Infrastructure : Expanding recycling infrastructure, \nparticularly in emerging markets, will be crucial as global EV adoption accelerates. \nFuture work should look at international collaborations to create a global network of \nrecycling facilities that can process batteries  efficiently, especially as the flow of used \nbatteries becomes more complex.  \nSpecific Outcome  \nThe specific outcome of this paper is to provide a comprehensive understanding of the current \nstate and future opportunities in electric vehicle (EV) battery recycling , with a particular \nfocus on the following:  \n1. Technological Advancements : The paper highlights the range of innovative \ntechnologies currently being utilized in EV battery recycling, such as \nhydrometallurgical , pyrometallurgical , and direct recycling  techniques, and the \npotential for future advancements in these areas.  \n2. Economic and Environmental Benefits : It outlines the significant economic \nopportunities  and environmental benefits  of scaling up recycling operations, \nparticularly in the context of reducing reliance on raw material extraction and \nminimizing the carbon footprint of battery production.  \n3. Challenges and Barriers : The paper identifies key challenges  facing the EV battery \nrecycling industry, including high initial costs, logistical issues, technological Powering the Future: Innovations …. Venkata Sai Chandra  Prasanth Narisetty 2350  \n \nNanotechnology Perceptions 20 No. S13 (2024) 2288–2301                                             \n    limitations, and regulatory complexities, and provides insights into overcoming these \nbarriers through innovation and collaboration.  \n4. Case Studies and Real -World Examples : By presenting a selection of real -life case \nstudies, the paper showcases successful recycling initiatives and the strategies used by \ncompanies to address current challenges. This provides a practical framework for \nstakeholders involved in battery recycli ng. \n5. Future Directions : Finally, the paper lays the groundwork for future research, \nincluding the need for better integration of recycling technologies with the broader \ncircular economy , improvements in battery design, and the expansion of recycling \ninfrastructure to meet growing demand.  \nConclusion  \nThis paper demonstrates that electric vehicle (EV) battery recycling is a critical component of \nachieving a sustainable and circular economy in the automotive industry. With the expected \ngrowth of the electric vehicle market, the demand for recycling solut ions to manage end -of-life \nbatteries will increase exponentially. The analysis of current recycling technologies and real -\nworld case studies highlights the significant strides made in improving material recovery rates \nand reducing the environmental impact of battery waste. While there are still challenges to \novercome, such as the high costs associated with recycling technologies, logistical issues, and \nthe need for regulatory frameworks, the opportunities presented by EV battery recycling are \nimmense. As th e industry advances, collaboration between automakers, recycling companies, \nand regulatory bodies will be essential to creating a sustainable, closed -loop system for battery \nproduction, use, and recycling. The future of EV battery recycling lies in the con tinued \ninnovation  of recycling technologies, the integration of circular economy  principles, and the \nestablishment of global recycling infrastructure . By focusing on these areas, stakeholders \ncan ensure that the benefits of electric vehicles extend beyond their operational lifetime, \ncontributing to a greener, more sustainable transportation ecosystem. The findings of this paper \ncontribute to a broader understanding of the challenges and opportunities in the field and lay \nthe foundation for future research t hat will further propel the development of sustainable \nbattery recycling practices.  ",
      "metadata": {
        "filename": "Powering the Future_ Innovations in Electric Vehicle Battery Recycling.pdf",
        "hotspot_name": "Specialized_e-Waste_Processing",
        "title": "Powering the Future: Innovations in Electric Vehicle Battery Recycling",
        "published_date": "2024-12-30T03:47:05Z",
        "pdf_link": "http://arxiv.org/pdf/2412.20687v1",
        "query": "electronic waste recycling sustainable processing methods environmental impact reduction"
      }
    },
    "Technological Progress and Obsolescence_ Analyzing the Environmental Economic Im": {
      "full_text": "Technological Progress and Obsolescence:\nAnalyzing the Environmental & Economic Impacts\nof MacBook Pro I/O Devices\nYun-Chieh Cheng\nCollege of Engineering\nOhio State University\nColumbus, Ohio\ncheng.1995@osu.eduYu-Tong Shen\nCollege of Engineering\nOhio State University\nColumbus, Ohio\nshen.1886@osu.eduGuanqun Song\nCollege of Engineering\nOhio State University\nColumbus, Ohio\nsong.2107@osu.eduTing Zhu\nCollege of Engineering\nOhio State University\nColumbus, Ohio\nzhu.3445@osu.edu\nAbstract —This study investigates how the new release of\nMacBook Pro I/O devices affects the obsolescence of related\naccessories. We also explore how these accessories will impact\nthe environment and the economic consequences. As technology\nprogresses, each new MacBook Pro releases outdated prior acces-\nsories, making more electronic waste. This phenomenon makes\nmodern people need to change their traditional consumption\npatterns. We analyze changes in I/O ports and compatibility be-\ntween MacBook Pro versions to determine which accessories are\nobsolete and estimate their environmental impact. Our research\nfocuses on the sustainability of current accessories. We explore\nalternate methods of reusing, recycling, and disposing of these\naccessories in order to reduce waste and environmental impact.\nIn addition, we will explore the economic consequences of rapid\ntechnological advances that make accessories obsolete too quickly.\nThereby assessing the impact of such changes on consumers,\nmanufacturers, and the technology industry. This study aims to\nrespond to the rapid advancement of technology while promoting\nmore sustainable approaches to waste management and product\ndesign. As the MacBook Pro I/O unit evolves, certain accessories\nbecome obsolete with each subsequent version. The purpose of\nthis study is to identify and quantify the environmental and\neconomic impacts of parts end-of-life. We can detect which\naccessories have become obsolete and assess the environmental\nimpact by comparing I/O port changes and compatibility across\nMacBook Pro generations. In response to these environmental\nimages, methods are developed to reuse, recycle, and dispose\nof obsolete accessories to reduce waste and promote sustainable\ndevelopment. Additionally, we evaluate the economic impact of\nobsolete equipment on consumers and producers.\nI. I NTRODUCTION\nMany people consider the MacBook Pro to be the highest\npoint of technical innovation. It has experienced remarkable\nevolution throughout time. Its evolution goes beyond feature\nand design enhancements. It also reaches the input/output (I/O)\ndevice domain, causing a shift in the accessory compatibility\nenvironment. New features and connectivity possibilities ac-\ncompany every new Apple product, eventually making some\naccessories outdated.\nThe technology of technological products continues to ad-\nvance and change, and its accessories also need to be elimi-\nnated and evolved over time. The evolution of the MacBook\nPro not only improves hardware performance but also changesthe accessories market. To ensure their accessories work with\nthe newest MacBook Pro models, consumers should always be\naware of how new goods work with previous accessories. Even\nthough there will be significant resource waste and occasional\ndiscomfort as a result of this transition, modern technology is\nprogressing, and future convenience is increased.\nRapid advancements in science and technology have led\nto unparalleled levels of creativity, but they have also in-\nfluenced the environment. With every new release, the pos-\nsibilities grow, rendering previously manufactured products\nand outdated accessories useless and generating garbage that\nnegatively affects the environment and the economy.\nIn the modern world, environmental awareness is a matter\nthat people take very seriously. Getting rid of old accessories\nwill result in more electronic trash, which will have an impact\non the environment and the economy. However, developing\ninventive products could boost the economy. As a result, while\ntechnical advancement and the phase-out of outdated items en-\nhance economic growth, the environment suffers concurrently.\nAs such, the appropriate way to handle these antiquated items\nhas emerged as a topic requiring careful consideration.\nThis research will examine the I/O port changes in the\nupdated version of the MacBook Pro. Our goal is not only\nto understand how related I/O devices will evolve but also\nto take a closer look at the impact of this transition. Which\naccessories will become obsolete as we progress? How many\naccessories will be eliminated? How will these impact the en-\nvironment? How can these old accessories be reused, recycled,\nand disposed of? What’s the economic impact?\nII. R ELATED WORK\nA. Obsoleted product\nNowadays, people are beginning to pay attention to the\nenvironmental sustainability of products, making Obsolete\nconcepts in product design and management important. Con-\nsumers are paying attention to the service life of fishery\nproducts and whether they can be recycled and reused. We’ve\ncollected four Obsolete-related research papers to explore thearXiv:2501.14758v1  [cs.CY]  23 Dec 2024impact of obsolescence and what will happen to product life,\nwaste generation, and resource recycling.\nIn [1], Sierra-Fontalvo et al. explore the nature of vari-\nous aspects of obsolescence, including technical, functional,\npsychological, economic, planned obsolescence, and reduction\nManufacturing sources and other types. They identified two\nmain design approaches to reducing obsolescence: long-life\nproduct design and extended product life design. However,\nthese two methods lack the formal identification of design at-\ntributes related to different scrap types, making the researched\nmethods need further research to find ways to solve them.\nThus, they hope that future researchers can develop methods\nto define design attributes. Moreover, it may help change the\ndefinition of obsolescence and enhance methods for measuring\nand predicting different scenarios.\nSimilarly, Alzaydi [2] delves into the topic of obsolescence,\nproduct design, and product sustainability. She initiates her\nexploration of the evolution of obsolescence across time before\nprobing into its manifestation as unintended consequences\nstemming from intentional design choices as well as adjust-\nments to external variables. The research underscores the need\nto strike a balance between a product’s aesthetic appeal and\npracticality in an effort to prolong its useful life while also\ncalling attention to the ethical, environmental, and financial\nramifications of obsolescence.\nIn addition, because obsolete accessories are closely related\nto computers, we also focus on the life cycle of computers\nand their obsolescence trends. In the research paper [3], Yang\net al. provide insights into future trends in the sales and\nproduction of obsolete computers, focusing on the US market.\nThe study used logistic models to predict obsolete computer\npenetration, sales, and production. It highlights the importance\nof smart policy responses to address growing computer waste\nand highlights the need for sustainable management practices.\nThe obsolete accessories we studied can also be reused by\nreferring to the computer waste policy.\nFurthermore, Kastanaki and Giannis provide dynamic es-\ntimates of future discarded notebook computer flows and\nembedded critical raw materials (CRM) in [4] Case study\nperspective. The study assessed the impact of the COVID-19\npandemic on laptop sales in recent years and the resulting\nincrease in obsolete devices. In addition, it highlights the\nimportance of managing end-of-life laptops due to their high\ncontent of CRMs and precious metals, as well as the potential\nfor resource recovery and contribution to the circular economy.\nThese research papers, taken as a whole, focus on how\nproduct design and management might solve obsolescence\nissues in order to reduce wasteful spending, boost resource\nefficiency, and support sustainability. We can learn about the\nrelationship between obsolescence and observed accessories,\nobtain techniques to use when examining the trend of ob-\nserved accessories, and then generate some relevant solution\nstrategies by investigating different facets of obsolescence and\nsuggesting mitigation and management solutions.B. Environment\nThe management of e-waste is important because it has\na significant environmental impact. Moreover, it is closely\nrelated to human health and ecosystems. We synthesize per-\nspectives from three relevant studies that examine the en-\nvironmental impacts of e-waste recycling and management\npractices.\n[5] underline the potential influence of electronic waste\nrecycling on environmental degradation. E-waste contains\nharmful compounds like dioxin, heavy metals, and persistent\norganic pollutants. When these pollutants enter the air, soil, or\nwater, they pose a direct hazard to human life and disrupt the\necosystem. Moreover, they emphasize the importance of creat-\ning green products. Green products can reduce e-waste output\nand promote advanced recycling technologies, hence reducing\nenvironmental contamination. They advocate for anti-pollution\nlegislation and producer take-back schemes; governments must\nwork with producers and customers to prevent environmental\ndamage from e-waste recollection.\nSimilarly, in [6], Ghulam et al. look into the environmental\nconsequences of badly handled e-waste. They highlight the an-\nnual increase in improperly disposed of e-waste, as well as the\npresence of hazardous substances in the e-waste stream. They\ncalled for an ever-evolving, technically sound, scientifically\nbased e-waste management policy that curbs environmental\npollution, fosters a circular economy approach, and is laced\nwith innovation at every step. Their proposal suggested tight\nregulations over e-waste streams through expanded producer\nresponsibility coupled with 3R strategies overseen by interna-\ntional monitoring organizations — drawing parallels with Liu\net al.’s innovative concept.\nMoreover, in [7], Priya and Hait studied the elemental\ncomposition of waste printed circuit boards (PCBs) and their\nbeneficiation process. Examining the physicochemical prop-\nerties of PCBs in different types of end-of-life electrical and\nelectronic equipment (EEE). This study revealed that waste\nPCBs contain precious metals and rare earth elements (REEs),\nand their contents were known. The result also validates Liu\net al. that electronic waste will produce organic pollutants.\nFurthermore, they also highlight the economic issues caused\nby recycling these materials and the potential for secondary\nsourcing of critical rare earth elements. This will help the elec-\ntronic waste recycling industry save resources and contribute\nto the sustainable development of the environment.\nOverall, this study examines environmentally acceptable e-\nwaste disposal alternatives. This new device has the poten-\ntial to help minimize environmental pollution. Furthermore,\nit prioritizes resource conservation and public health. Thus,\ngreen product design is an important method for environmental\nperformance protection since it reduces the production of\ndangerous compounds. Furthermore, appropriate management\ntechniques and cutting-edge recycling technology can help\nmitigate the environmental issues associated with e-waste\nrecycling. It teaches us how to safeguard the environment,\nwhich we can subsequently apply to concerns produced byobsolete accessories.\nC. E-waste\nElectronic waste (e-waste) is an urgent and important global\nissue requiring comprehensive recycling and resource recovery\nstrategies. How can we effectively recycle and utilize elec-\ntronic waste so that the environment is not damaged and these\nwastes can be used and converted into new reusable items?\nIn the study [8], Yken et al. pointed out that setting up\nefficient recycling procedures was important. They showed\nthat there was only a small percentage of e-waste materials\nare recycled internationally despite their potential for financial\nbenefit. They emphasize the need for better infrastructure and\nlaws and regulations. However, they also highlight the daunt-\ning challenges in e-waste processing and recycling, which\nshould underscore the urgency and importance of the issue\nfor our audience. They advocate for consistent collection pro-\ngrams, methodical removal, and material separation processes\nto increase recycling efficiency.\nSimilarly, in [9], Williams et al. discuss how electronic\nwaste should be managed and processed from the environ-\nmental, economic, and social aspects. And they particularly\nfocused on personal computers. They highlighted the global-\nization of reverse supply chains, the significant environmental\nproblems associated with informal recycling operations, and\nthe potential for harm to occupational health. So they propose\npolicy interventions, such as legislating recycling/recycling\nsystems and regulating toxic ingredients, to find ways to\nreduce the production of these toxic substances, use laws to\nbind these suppliers and prevent the impact of some corner-\ncutting recycling processes.\nHeacock et al.’s [10] emphasize the importance of global\ncooperation in managing e-waste to mitigate its adverse ef-\nfects on human health and the environment. They specifically\nproposed international treaties such as the Basel Convention,\nthe Bangkok Statement, the StEP Initiative, and the Bali\nDeclaration to emphasize the need for the world to understand\nand prevent the impact of these electronic wastes, as well as\nmethods and the need for intervention and strategies to prevent\nthe environment from getting worse, with special mention of\nproblems that may cause children’s health.\nThese studies explore various aspects of e-waste manage-\nment and highlight that addressing the impacts of e-waste\nrequires policy development, improved technology, and the\npromotion of international cooperation, which must be im-\nplemented together to address the problem effectively. These\nstudies show many solutions to environmental issues, drivers,\nand influences for developing sustainable e-waste management\nstrategies.\nD. Economy\nSeveral academic articles have been written about the prob-\nlem of handling e-waste and its effects on the environment\nand economics. The author of [11] described the problems that\noutdated electronic devices bring to the world. It highlights the\nimportance of having strong legal structures and innovativetechnological solutions to promote sustainable and circular\neconomic models. [12] quantifies the growing amount of e-\nwaste and highlights our urgent need to strengthen recycling\nmeasures and practice a circular economy to reduce adverse\nenvironmental impacts. [13] discusses more economic factors.\nIt discusses the feasibility and potential benefits of European\nrecycling initiatives and suggests that significant financial\nbenefits can be achieved through improved e-waste recycling\nprocedures. This economic perspective is related to consumer\nbehavior [14], which focuses on the importance of consumers\nbeing aware of and participating in e-waste management.\nFurthermore, [15] points out the correlation between economic\ngrowth and e-waste generation. It also provides some argu-\nments for integrating economic strategies with environmental\nsustainability efforts. Additionally, [15] highlights the link\nbetween economic growth and the production of e-waste. It\nalso offers several arguments for combining economic strate-\ngies with environmental sustainability initiatives. These studies\nnot only emphasize the multifaceted challenges associated\nwith e-waste but also propose approaches to address e-waste\nmanagement from many aspects.\nIII. D ESIGN\nWe followed the objective below to achieve our goal:\n•Identify I/O devices and accessories affected by MacBook\nPro releases.\n•Quantify obsolete accessories.\n•Assess the environmental impact of obsolete accessories.\n•Explore strategies for reusing, recycling, and disposing\nof old accessories.\n•Evaluate the economic implications of I/O device obso-\nlescence.\nA. Identify obsoleted I/O devices\nFirst, we went to Apple’s website to collect the charging and\nexpansion of MacBook Pro in various periods, then organized\nall the information into tables, and finally counted the three\nmost important time points: (1) 2015, (2) 2016-2020 and (3)\n2021-2023\nCharging and Expansion of 2015 MacBook Pro as shown\nin Fig 1 [16]. The charging port at that time used MagSafe 2,\nand it had a complete I/O device port, such as a USB-A port,\nHeadphone port, SDXC card slot, and HDMI port. Based on\nthis comprehensive setup, it appears that no extra adapters are\nrequired unless there are particular requirements.\nSubsequently, between 2016 and 2020 (Fig 2) [17], the\nMacBook Pro’s charging port was replaced with Thunderbolt 3\n(USB-C) connectors. Most of the I/O ports have been removed\nin preference for USB-C slots, which means that we’ll need\nto buy an additional adaptor to use the remaining ports. For\ninstance, you would need to buy an additional ”USB-C to SD\nCard Reader” to read the data if you needed to read the SD\ncard.\nThe latest MacBook Pro (from 2021 to now) has replaced\nthe charging port of the MagSafe series and upgraded to\nMagSafe 3. Additionally, the HDMI connector and SDXCFig. 1. 2015 MacBook Pro Charging and Expansion\nFig. 2. 2016 MacBook Pro Charging and Expansion\ncard slot have been put back, which can lessen the need for\nadapters. The most recent I/O port is visible in Fig 3 [18].\nFig. 3. 2021 MacBook Pro Charging and Expansion\nAfter we gathered all the data, we created the comparison\nchart shown in Fig 4. It is visible that the MacBook Pro\nswitches its power port from MagSafe 2 to USB-C and back\nagain. However, MagSafe 2’s charger will become outdated\nbecause it is incompatible with MagSafe 3. Furthermore,\nthere hasn’t been a USB-A port since 2016, so it cannot use\nUSB-A-related accessories directly. Since every version has\na headphone jack, wired headphones won’t become outdated.\nFig. 4. MacBook Pro Port Comparison\nUltimately, there is no need for a separate adapter because the\nHDMI port and SD card slots were introduced back in 2021.\nWe visited Apple’s official website to look for related\naccessories that were still available but would eventually\nbecome obsolete after comparing every I/O port. These four\naccessories that we discovered would soon become outdated\nare:\n•Apple USB SuperDrive (Fig 5) [19]\n•Apple USB Ethernet Adapter (Fig 6) [20]\n•Apple USB-C to SD Card Reader (Fig 7) [21]\n•MagSafe 2 Power Adapter (Fig 8) [22]\nFig. 5. Apple USB SuperDrive\nFig. 6. Apple USB Ethernet AdapterFig. 7. Apple USB-C to SD Card Reader\nFig. 8. Apple MagSafe 2 Power Adapter\nB. Quantify obsoleted I/O devices\nWe require additional information to determine the number\nof the four obsolete accessories that we have discovered. To\ndetermine how many obsolete accessories will be made, we\nhave to determine each product’s sales volume. We might\nconsult Fig 9 as we move forward.\nWe started by looking for these four I/O devices on Amazon.\nSince the official Apple website could not provide us with\nrelevant sales data, we opted to gather the data via Amazon.\nAfter reaching the product’s sales page, we installed a third-\nparty Chrome extension called AMZScout PRO (Fig 10).\nThis extension allows us to access the product’s sales history\ndirectly from Amazon’s website. However, the free version of\nthe Extension can only obtain the sales history chart, and we\nmust pay to access the product sales history data CVS file.\nTherefore, We built an additional script that simulates mouse\nmovements in order to collect comprehensive sales data from\nthe chart.\nWe used the collected data for analysis once it was finished.\nTo determine when sales were increasing and when they\nwere starting to decline, we first performed some analysis\nand created comparison charts. Simultaneously, we verified\nwhether the MacBook Pro had been updated and whether any\nFig. 9. execution process\nI/O port modifications were made. We may examine the full\ntrend of change from this. Additionally, we will calculate the\napproximate total sales of these I/O devices and project the\nnumber of accessories that will become obsolete when the\nold MacBook Pro is no longer in use. These accessories will\nsubsequently end up as trash and have a harmful impact on\nthe environment.\nC. Research for environmental and economic implications\nRemoving these outdated accessories will have specific\nnegative effects on the environment, such as creating a lot\nof waste, and disposing of this rubbish may cost extra. Aside\nfrom the negative effects on the environment, introducing newFig. 10. AMZScout PRO\nFig. 11. research process\nproducts will boost the economy because no one will purchase\nthese out-of-date accessories, and recycling or reusing them\nwill be expensive. Thus, we first address the environmental\neffects of waste, the repurposing and processing of waste\nmaterials, and the e-waste problem in our research on the\nmanagement of outdated accessories. We will talk about the\neffects of waste and new products on the economy as well\nas potential future trends. As shown in Fig 11, first study the\nrelevant research papers. This will facilitate our analysis by\nrevealing some of the environmental damage society knows\nwill cause and some potential impacts. Then, the impacts of\nthese papers will be analyzed. Through these analyses, we will\nintegrate methods and select appropriate strategies to address\nthe impact of premature production of these obsolete parts.\nIV. E VALUATION\nA. Data Analysis\nIn this study, we analyzed sales data for multiple Apple\naccessories, specifically the Apple MagSafe 2 Power Adapter,\nApple USB SuperDrive, and Apple USB-C to SD Card Reader,\nto evaluate the impact of MacBook Pro hardware updates on\nthe obsolescence of accessories. We focused on sales trends\nfor these accessories. Since some outdated accessories, such as\nthe Apple Ethernet Adapter, are no longer sold in the Apple\nStore on Amazon and the past data records are incomplete,\nour analysis primarily focused on the Apple MagSafe 2 Power\nAdapter, as it has the most comprehensive sales information.\nWe used Python to generate a line chart to visualize the\nsales trends of these accessories over time. (The sales history\nFig. 12. Apple USB SuperDrive Sales\nFig. 13. Apple USB-C to SD Card Reader Sales\nof Apple USB SuperDrive is shown in Fig 12 and Apple USB-\nC to SD Card Reader is shown in Fig 13.) Data for the Apple\nMagSafe 2 Power Adapter supply provides a clear timeline\nof sales fluctuations that can be compared to Apple’s product\nrelease dates.\nWe had data on two wattages of the Apple MagSafe 2 Power\nAdapter, which we used to generate the line chart. (Fig 14\nand Fig 15) Furthermore, the different wattages of MagSafe\n2 Power Adapter are the same type as those of ours, so we\ncombined them into one chart. (Fig 16 and Fig 17)\nThe line chart for the Apple MagSafe 2 power supply shows\nclear sales peaks and troughs. These fluctuations are closely\nrelated to the release of new MacBook Pro models, especially\nmodels with significant changes to I/O ports and charging\noptions. For example, immediately after it was announced\nthat an upcoming model would not include a MagSafe 2\nport, there was a statistically significant increase in sales. It\nsuggests that people were rushing to buy the adapter before\nit became obsolete, with consumers buying the adapter either\nas a replacement or For continued compatibility with older\ndevices.\nThe analysis shows that Apple’s strategic decisions on port\nconfiguration can have a direct and measurable impact on\naccessory sales. This trend highlights the economic impactFig. 14. Apple 60W MagSafe 2 PowerAdapter Sales\nFig. 15. Apple 85W MagSafe 2 PowerAdapter Sales\nFig. 16. Apple MagSafe 2 Power Adapter Sales\nFig. 17. Apple MagSafe 2 Power Adapter Combination Sales\nTABLE I\nSALES DATA FOR APPLE ACCESSORIES ON AMAZON\nProduct Sales\nApple 60W MagSafe 2 Power Adapt 116,591\nApple 85W MagSafe 2 Power Adapt 281,241\nApple USB SuperDrive 29,621\nApple USB-C to SD Card Reader 40,875\nSUM 468,328\nof outdated accessories. Consumers demand for compatibility\nor replacement parts may temporarily boost sales of the retire-\nment of older technology. However, as the product eventually\nbecomes irrelevant in newer hardware environments, these\npeaks are often followed by sharp declines.\nFor TABLE I, we calculated the sales of all obsolete parts so\nthat we can know how many I/O devices will become trash and\nno longer needed. The numbers in the table are not exact. The\ncorrect number will be larger than what we get. However, we\ncan still find that more than 468,328 accessories will become\nobsolete, which means that after this time, these accessories\nwill become obsolete and become environmental pollution.\nAfter generating the chart, we put the chart and the product\ninformation on the website, which is more convenient for\npeople to read and analysis with the information. First, we\nwill look at the table showing the sales of these outdated\naccessories.(Fig 18) Second, we need to select which I/O\ndevice we want to research. Then, the website will display the\ninformation on the Apple official website about the product\non the left and its sales history on the right.(Fig 19) There\nwill be more than one chat for the Apple MagSafe 2 Power\nAdapter because we put the chart of two wattages of the Apple\nMagSafe 2 Power Adapter together and combined the chat.\nB. Environmental Impact\n1) Increase in Electronic Waste Generation: Obsolete Mac-\nBook Pro I/O devices have led to an increase in electronic\nwaste, posing major environmental risks. Previous I/O devices\nbecome obsolete as new MacBook Pro versions or modelsFig. 18. Result on Website -1\nFig. 19. Result on Website -2\nare released. They must be discarded and replaced [23]. As\na result, this continuous cycle generates a large amount of\nelectronic trash. So, we must be carefully controlled and\ndisposed of.\n2) Resource Depletion and Energy Consumption: The man-\nufacturing process of new I/O devices to replace obsolete ones\nrequires significant resources and energy [7]. As Priya and Hait\nemphasize, this process can lead to resource depletion and\nenvironmental damage. Extracting raw materials and energy-\nintensive manufacturing processes further strain our natural\nresources and exacerbate environmental problems.\n3) Landfill Contamination: As discussed in the research\n[9], abandoned outmoded I/O devices frequently end up in\nlandfills. These landfills have the potential to pollute the\necosystem by allowing dangerous compounds to leak into\nthe groundwater and soil. This contamination threatens the\nenvironment and human health, necessitating proper e-waste\nmanagement techniques.\n4) Air and Water Pollution: If Improper handling and\nrecycling of e-waste, including obsoleted I/O devices, can lead\nto air and water pollution [23]. The incineration of e-waste\nreleases harmful pollutants into the air, and it will cause air\npollution. In addition, leachate from landfills contains e-waste\ncomponents. They can contaminate nearby water sources and\nharm ecosystems and human health.\n5) Loss of Biodiversity: As discussed in the research paper\nby Williams et al., environmental pollution caused by elec-\ntronic waste disposal may harm biodiversity [9]. Releasingtoxic substances into the ecosystem will destroy the fragile\necological balance. Moreover, it will threaten the survival\nof various species. Protecting biodiversity is important to\nmaintaining healthy ecosystems and ensuring the survival of\nall life on Earth.\n6) Climate Change Impact: According to Priya and Hait’s\nstudy, the manufacture and disposal of electronic equipment,\nincluding I/O devices, emits greenhouse gases [7]. These\nemissions contribute to climate change and cause various\nenvironmental and socioeconomic consequences. We must ad-\ndress these emissions through sustainable activities. To prevent\nclimate change and protect the world for future generations.\nAll in all, the obsolescence of MacBook Pro I/O devices\nwill profoundly impact the environment, and we need to pay\nattention to sustainable practices and implement effective e-\nwaste management strategies.\nC. Strategies\n1) Promotion of Extended Producer Responsibility (EPR)\nPrograms: Implementing an EPR program can encourage\nmanufacturers to take responsibility for their products’ whole\nlife cycle, including disposal and recycling [6]. The EPR\nprogram reduces electronic waste by requiring manufacturers\nto account for the environmental impact of their products and\nencouraging the development of more durable and recyclable\nequipment.\n2) Development of Circular Economy Practices: Adopting\ncircular economy principles helps to reduce resource con-\nsumption and waste generation related to electronic device\nend-of-life [23]. A circular economy approach can reduce\nenvironmental effects by encouraging product reuse, repair,\nand recycling. This extends the service life of I/O devices and\nfacilitates the recycling of valuable components.\n3) Advancement of Green Product Design: By encouraging\nthe design of green products, greener I/O devices can be\ndeveloped [5]. Products with modular components, recyclable\nmaterials, and energy-saving features can reduce the envi-\nronmental damage caused by electronic devices and promote\nsustainability.\n4) Expansion of E-waste Recycling Infrastructure: Invest-\ning in expanding e-waste recycling infrastructure can improve\nthe proper management and disposal of discarded I/O devices\n[8]. By increasing the availability of recycling facilities and\npublic awareness of e-waste recycling options, more devices\ncan be diverted from landfills and recycled. And then it can\nreduce environmental pollution.\n5) Enforcement of Strict Environmental Regulations:\nStrengthening environmental regulations related to electronic\nwaste management can help mitigate the adverse environ-\nmental impacts of obsolescence [9]. Governments can ensure\ncompliance with environmentally sustainable practices by en-\nforcing regulations on e-waste disposal, hazardous substance\nmanagement, and recycling practices.\n6) Encouragement of Consumer Education and Behavior\nChange: Educating consumers about the environmental im-\npacts of electronic waste and promoting sustainable consump-tion habits can foster a culture of responsible consumption [7].\nBy encouraging consumers to prolong the lifespan of their de-\nvices through repair and reuse, the volume of electronic waste\ngenerated can be reduced, leading to positive environmental\noutcomes.\nD. Economic Implications\nRecycling and reusing obsolete accessories has multiple\neconomic benefits, including cost reduction, job creation, and\nexpanded market opportunities. The study says the reuse and\nrecycling industry not only provides jobs but also fosters a\nmarket for second-hand equipment, which is particularly ben-\neficial to developing countries as they use cheap technology\nto boost their economies [9]. In addition, adopting flexible\npolicies means that reuse and recycling rates can be increased\nduring economic expansions with increasing consumption,\nproduction, or exports [24]. The study also highlights that\nreuse operations tend to be more profitable than recycling,\nespecially in industries such as mobile phones, suggesting that\nbusinesses that focus on reuse are likely to be more financially\nsuccessful [25]. In addition, increased reuse and recycling of\nelectronic devices can lead to significant cost savings and\nbetter resource recovery while also avoiding environmental\nand health hazards associated with unsuitable disposal [26].\nBy promoting products designed to last longer and be easier\nto repair, this kind of policy can support economic growth.\nThis way, it doesn’t use up resources too fast or produce too\nmuch waste. These benefits highlight the importance of reuse\nand recycling in promoting a circular economy and economic\nresilience.\nV. I SSUES WE ENCOUNTERED\nA. Data Collection\nData collection was the most difficult part of this project.\nAlthough it is easy to think about how to get the number of\nproduct sales, a lot of methods we have thought do not work.\nFirst, we wanted to get the number of sales through the Apple\nofficial website. However, there was no data we could collect\nfrom the product detail page. We had research for a long time.\nWe couldn’t get the data from API or other information on the\nwebsite. Thus, we gave up and changed another way to achieve\nour goal.\nSubsequently, we thought that Apple’s official financial\nstatements were helpful information. Maybe we could get the\ndata of the accessories sales on the statement. However, they\nonly displayed the sales of MacBooks or iPhones, these main\nproducts; we could not get the sales of especially accessories.\nWe could only get the sales for all accessories. It’s not the\ndata that we want.\nTherefore, we decided to collect the data from Amazon.\nAt first, we wanted to use crawler technology to obtain data,\nso we first entered the product page we wanted and then\nused a script to obtain the information, but we found that\nAmazon would only display the approximate number of sales\nlast month (Fig 20). Not only was it inaccurate, but there was\nalso only one small amount of data. Thus, we started to find\nFig. 20. Amazon sales history\nFig. 21. Jungle Scout\nhelpful information from the API or other information on the\nwebsite. And there were only product reviews that were the\ndata that we could use. We needed to find or train a model\nto analyze the product reviews and sales. It’s difficult and\ninaccurate, so it’s not a good solution for us.\nFurthermore, we started looking for some third-party tools\nto help us collect data. We found some tools, such as Jungle\nScout (Fig 21) [27] and Perpetua (Fig 22) [28], but some\nof these tools are for Amazon merchants. We didn’t have an\naccount with Amazon merchants, so we could not use them.\nIn addition, these tools require payment to use, and they are\nexpensive that it’s not affordable for us.Fig. 22. Perpetua\nFig. 23. AMZScout\nFinally, with our efforts, we found a relatively suitable tool\ncalled AMZ Scout (Fig 23) [29]. When we went to its website,\nwe found that we could search for and see some information\nabout some products, but we could not find the product we\nwanted on that search page. When we researched how to use\nthis tool, we found that there was a Chrome extension for it. It\nwas free for some features and had a limited free trial. Thus,\nwe used this third-party extension to find the sales history of\nthe obsolete accessories. However, we found another problem\nwhen we collected the data via AMZ Scout: If we wanted to\nget the CSV file of the entire product’s historical sales data, we\nneeded to subscribe to the monthly or yearly plan, which was\nexpensive. We could only see the sales history chart with the\nfree trial plan. So, we decided to write a script to get the data\nfrom the chart. We obtain the data for each point in the line\nchart by simulating mouse movement and saving it as a CSV\nfile. Then, we could use these data to calculate the quantity\nand analyze the trend of these obsolete accessories.\nAlthough the method we used was neither the best nor\nthe most accurate, that was the most data we could collect\nwithout paying. With this data, we could do a lot of analysis\nand observe overall product sales trends to achieve the goal\nwe wanted to research. Looking forward to the future, we\nwill have more free tools that can help to get more data, help\nresearchers make more accurate calculations, and analyze past\nand future trends.B. Data Analysis\nThere are several challenges in the data analysis phase of\nthis project. The main issue is the complexity and variability\nof sales data across different platforms and time. We face a\nlack of standardized sales reporting and incomplete data sets\nsince our primary data sources are online retail platforms,\nparticularly Amazon. Although Chrome extensions are used to\naccess Amazon sales history, the granularity and completeness\nof the data are limited.\nAnother important issue is the lack of correlation data\nbetween sales figures and product release cycles, as well\nas external factors that influence buying behavior (such as\nmarketing campaigns or economic events). Because of this\nlimitation, it is challenging to directly link sales trends to I/O\ndevice obsolescence after the release of the new MacBook Pro.\nFurthermore, Apple’s official website does not provide sales\ndata, and the company’s financial statements do not break\nout accessory sales for our purposes. Since we do not have\nenough official sales data, we analyze third-party data instead.\nHowever, this may not accurately represent the situation of the\nreal market.\nFurthermore, we do not consider the second-hand market.\nOlder devices and accessories may find a home on the sec-\nondary market, which could lessen their negative environmen-\ntal effects. Based on these challenges, we recommend that\nfuture research seek more direct sources of data, possibly\nthrough partnerships or data-sharing agreements with manu-\nfacturers and retailers, in this way to improve the accuracy\nof sales trend analysis and its correlation with technological\nobsolescence.\nC. Future Work\nAs technology develops, people are making faster and faster\nadvances in various areas such as wireless networks, secure\ncommunications, smart life [30]–[37], machine learning, and\nso on. In the future, we can combine various current wire-\nless communication technologies [38]–[55] to improve the\naccuracy and usability of data by streamlining the process of\ncollecting data from various sources such as product reviews\nand sales platforms. Advanced secure communication methods\n[56]–[59] can ensure the integrity and privacy of sensitive\ndata, especially when working with third-party tools or large\norganizations such as Amazon or Apple. Additionally, machine\nlearning-based methods [60]–[69] can be used to create pre-\ndictive models to estimate trends in obsolete devices, optimize\nenvironmental strategies, and more accurately assess economic\nimpact.\nSince the information is incomplete, it may be helpful for\nus to subscribe to a third-party tool to get all the data. This\nwould make the analysis more accurate and credible. Due to\nthe expensive payment, we recommend subscribing if you need\nlong-term research. Otherwise, if we have enough to pay with\nthe tools, we can also try other third-party tools that we have\nmentioned before. Then, we can compare whether different\ntools will have different results. Then, we can choose the\nrelatively correct one to use for analysis, or we can integratethe results of these different tools. This method may help make\nthe analysis clearer and more credible. Of course, the best way\nis to cooperate with Amazon or Apple to obtain official data\nto calculate and analyze.\nOn the other hand, if we have enough datasets, it may be\npossible to train a model to calculate sales via product reviews,\nratings, prices, etc., in the future. Then, we can use crawler\ntechnology to get all the data we need from Amazon and put\nthe data into the model. We will get an estimated product\nsales. This method will be more convenient for people to get\nthe data. However, it’s an estimated number. Thus, it’s more\nsuitable for calculating and assessing trends or total product\nsales.\nAdditionally, it’s possible to train a model to calculate and\nestimate future trends and total product sales. It will be helpful\nto evaluate the total number of obsolete I/O devices that will\noccur because we can only calculate the number from the past\nto now. We can’t know how many obsolete I/O devices will\nbe produced in the future. If there is such a prediction model,\nit will help us delve deeper into this research. Moreover,\nwe can estimate the quantity of obsolete accessories and the\nsubsequent impact that will occur. It will be a significant study\nfor environmental research. We believe it can help improve the\nproblems of the environment.\nNext, regarding environmental protection strategies, we just\nstudied the research papers and used the theories and methods\nto analyze our research. We think there are other better ways\nto develop strategies. For example, create a model that can\ndirectly analyze the best strategies for our research problem.\nIt may be very useful because we only read a few research\npapers, which could only give us a little idea. However, it’s\nnot enough for us to develop the best strategy. If we can use a\nmodel with the enriched dataset, it may have a great analysis\nof the problems, provide more ideas, and also select the best\nmethod for us. Thus, if this model is developed, we believe\nit can help many people to complete environmental protection\ntasks.\nFinally, in the economic impact section, we wish that these\nobsoleted I/O devices not only can be reused but also promote\neconomic growth. Thus, it’s helpful to develop a model that\ncan estimate how the obsoleted I/O devices will impact the\neconomy. For instance, with the historical data of these product\nsales, we can estimate the economic growth caused by these\nproducts. Then, estimate when the product becomes obsolete\nand how it will decrease economic growth. Furthermore, what\nwould be more beneficial to the economy if these discarded\nproducts were continued to be sold or recycled directly?\nTherefore, we need this model to help us analyze the data\nfor the economy. It will be a significant tool for delving into\nan economic problem for this research question in the future.\nVI. C ONCLUSION\nThis study investigates the evolution of MacBook Pro\nI/O devices and their environmental and economic impacts.\nThrough analysis, we can see that while technological progresspromotes the advancement of the industry, it also accelerates\nthe elimination of equipment and also brings challenges.\nOur research results show that frequent updates of MacBook\nPro I/O ports directly lead to shortened life cycles of many\naccessories. This not only leads to an increase in e-waste but\nalso causes considerable environmental harm due to improper\nhandling and recycling methods. Improved waste management\nstrategies are urgent, as are policies that encourage the devel-\nopment of more durable, universally compatible accessories.\nFrom an economic perspective, the rapid elimination of out-\ndated accessories stimulates consumer spending and industry\ngrowth, but it also comes at a cost. It places a financial burden\non consumers, who must regularly update their devices. This\nalso leads to the depletion of natural resources. This study\nhighlights the balance that must be achieved between pro-\nmoting innovation, sustaining the economy, and environmental\nsustainability.\nWe can face these challenges by reuse and recycling strate-\ngies. It not only reduce environmental impact, but also provide\nsignificant economic benefits. For example, it creates many\njobs through recycling and reducing the costs associated with\nproduct manufacturing. In addition to improving sustainable\neconomic growth, policies that support product longevity and\nrepairability can reduce the environmental impact of techno-\nlogical advancement.\nAll in all, the tech industry advances due to the quick\ntechnological innovation found in products such as the Mac-\nBook Pro, but it also needs a commitment to sustainable\npractices. Future research should focus on promoting recy-\ncling technologies, advocating sustainable product design, and\ndeveloping effective policies that combine economic growth\nwith environmental protection. Technology development can\nbe made more sustainable and contribute to a more stable\neconomic future for the technology sector by tackling these\nproblems.",
      "metadata": {
        "filename": "Technological Progress and Obsolescence_ Analyzing the Environmental Economic Im.pdf",
        "hotspot_name": "Specialized_e-Waste_Processing",
        "title": "Technological Progress and Obsolescence: Analyzing the Environmental\n  Economic Impacts of MacBook Pro I/O Devices",
        "published_date": "2024-12-23T16:26:01Z",
        "pdf_link": "http://arxiv.org/pdf/2501.14758v1",
        "query": "electronic waste recycling sustainable processing methods environmental impact reduction"
      }
    },
    "Dynamic Power Reduction in a Novel CMOS 5T-SRAM for Low-Power SoC": {
      "full_text": "Dynamic Power Reduction in a Novel CMOS 5T-SRAM \nfor Low-Power SoC  \nHooman Jarollahi, EIT, Student Member, IEEE, and Ri chard F. Hobson, P. Eng. \nSchool of Engineering Science, Simon Fraser Univers ity, Burnaby, B.C., Canada, V5A 1S6 \nhjarolla@sfu.ca \n \nAbstract  — This paper addresses a novel five-transistor \n(5T) CMOS SRAM design with high performance and \nreliability in 65nm CMOS, and illustrates how it re duces the \ndynamic power consumption in comparison with the \nconventional and low-power 6T SRAM counterparts. Th is \ndesign can be used as cache memory in processors an d low-\npower portable devices. The proposed SRAM cell feat ures \n~13% area reduction compared to a conventional 6T c ell, \nand features a unique bit-line and negative supply voltage \nbiasing methodology and ground control architecture  to \nenhance performance, and suppress standby leakage p ower.  \nKeywords:  Five-transistor SRAM; low-power low-area \nSRAM; cache memory; standby and dynamic power \nreduction.  \n1 INTRODUCTION  \nToday’s microprocessor chips consist of cache memor ies \nand computing cores. It is predicted that cache mem ories may \nreach 90% of the chip area in some applications by 2013 [4]. \nIn addition, cache memories consume a significant p ortion of \nthe power budget in SoC applications [3]. This is p articularly \nimportant in portable and battery-powered electroni cs such as \ncellular phones, PDAs, wireless, and low-power biom edical \ndevices since dynamic and standby leakage power det ermine \nthe battery life. With recent aggressive growth of technology \nscaling, standby leakage power is increased nearly five times \neach technology generation while active power remai ns \nconstant [3]. Also, process variations and hence pe rformance \nfluctuations are widely noticed in 65nm and beyond in \nCMOS technologies [5]. Five-transistor Static Random \nAccess Memories  (5T SRAMs) are attractive due to their \nadvantage in area and power efficiency compared to 6T \nSRAMs [1][2][8][9][16]. Research in the past on thi s type of \nmemory has been mostly focused on improving perform ance \nand stability while maintaining the promised area s aving in a \nparticular technology node. On the other hand, with  \ncontinuous scaling down of CMOS transistors, new \ntechniques have been developed in 6T SRAMs such as \nDynamic Standby Mode [4][12], DRV method [3], and w ell \nbiasing, some of which are summarized in [3] and [4 ]. \nTherefore, in order to suppress leakage power consu mption \nand combat performance fluctuations due to process \nvariations, the previous research in 5T SRAMs such as [8] \nand [9], can no longer compete with current 6T SRAM s and \nthat is why 6T SRAMs are still predominantly used i n current systems.  \nIn [1], standby power reduction has been described for the \nnew 5T SRAM design. In this paper, an improved low- power \ndesign with the focus on its dynamic power reductio n \nadvantage is addressed. The new 5T SRAM cell with d ual \ngrounds (5TSDG) features a novel bit-line biasing t echnique, \nand guarantees operation under all process variatio ns and \ntemperatures while taking benefit of area reduction . In \naddition, 5TSDG has an improved performance compare d to \nprevious research in [2][8][9].  \n  \n(a)  (b)  \n \n(c) \nFig. 1 (a) Conventional 6T SRAM cell, (b) Proposed 5T \nSRAM cell (5TSDG), (c) 5T SRAM architecture (M \ncells/sub-column) with sub-column circuitry and V SSM  \ncontrol. This research is supported by CMC Microsystems and funded in part by \nNSERC of Canada.  TABLE I. Different types of SRAM cells used in this  paper,                          \nVDD  = VDDM =1.3V, β = (WN2 /L N2 )/(W N3 /L N3 ). \nType Inverter \nTr.’s Access \nTr.(‘s) β VSSM  BL pre-\ncharge \n5TSDG HVT SVT 1.0 600mV 600mV \nLow-Power 6T HVT SVT 1.4 600mV V DD  \nConv. 6T HVT SVT 1.4 0mV V DD  \n2 5TSDG  DESIGN  \nA conventional 6T cell in comparison with the 5TSDG  is \ndemonstrated in Fig. 1 (a) and Fig. 1 (b) respectiv ely. A block \ndiagram of 5TSDG cell including the sub-column circ uitry is \ndepicted in Fig. 1 (c). Standby and Ground control circuits are \nrequired one per every sub-column while V SSM  control is \nshared in the entire memory array. TABLE I specifie s some \nof the design parameters of 5TSDG, low-power 6T, as  well as \nconventional 6T cells used in this paper for compar ison. An \narea reduction of ~13% is predicted compared to a \nconventional 6T cell using standard 65nm design rul es [1]. \nThe “portless” 5T SRAM in [16] does not use a dedic ated \nread-write port transistor, but has an “access tran sistor” that \nshorts Q and Qz nodes during read and write. V DDM  nodes are \nreplaced by dual bit lines for I/O and power reduct ion. A \ndetailed comparison between 5TSDG and the portless 5T \nSRAM of [16] would be useful future work. The portl ess \ndesign appears to need larger PMOS and access trans istors \nthan 5TSDG. \n2.1   Standby Mode \nOne of the effective and proven methods to suppress  \nleakage power during standby in 6T SRAMs is to use \ndynamic sleep design while maintaining a sufficient  Static \nNoise Margin (SNM),  which ultimately determines the \nintegrity of the stored data [4][6]. The most effec tive way to \nuse this method is by raising the negative supply v oltage of \nthe memory cells, V SSM , as opposed to lowering the positive \none, V DDM , to minimize bit line and cell leakage power \n[1][4][12].  \nConsidering this method in 5T SRAM, a prominent \nfeature of 5TSDG is that instead of using an extern al on-chip \npower supply to raise V SSM  voltage above ground in standby, \nwith existence of enough leakage sources especially  sub-\nthreshold and gate leakage currents in advanced tec hnologies, \nthe leaking memory array can be used as a power sou rce to \ncollect these charges from Vg 1 and Vg 2 via M g1  and M g2 \ncausing a natural rise of V SSM  to a desired biasing level using \nVSSM  control circuit for fine tuning [1]. After evaluat ing \nperformance, stability and power consumptions by \nsimulations, with various combinations of threshold  voltages \n(V th ) for each single transistor in 5TSDG, it is found that V th \nof the inverter pairs have the most significant imp act on the \nleakage power while the access transistor, N 3 has the most \nsignificant impact on performance and stability. Th erefore, \nthe two inverter pairs N 1-P1 and N 2-P2 are selected to have \nhigh threshold voltages (HVT) while the access tran sistor N 3 \nhas a smaller V th , (in this case Standard V th , SVT). All cell \ntransistors are selected to have equal sizes (W i= 0.15 µm, \nLi=0.06 µm).  Using two carefully sized diode-connected transisto rs, M 1 \nand M 2, the voltage across the cell in standby can be bia sed to \nremain static for various temperatures and process corners     \n(See also [14]). In this design, a minimum voltage across the \ncell, V min = V DDM -VSSM , of 0.7V is selected to yield sufficient \nstability [7], resulting in a simulated SNM between  181-\n222mV in all corners and temperatures at V DDM =1.3V [10]. A \n64Kbit memory array arranged in 64x16 blocks was \nsimulated in standby mode using BSIM v.4 and HSPICE  at \nVDD =V DDM =1.3V. The large capacitance of V SSM  consisting \nof mostly junction and wire capacitors and sufficie nt \navailable leakage current are the key factors in st ability of \nVSSM  during standby/write/read modes. In case of lack o f \nleakage especially due to HVT transistors, in some corners or \ntemperatures, M 1 is turned on more strongly to provide the \ncharges to V SSM . During read and write operations, V SSM \nremains within about 20 mV of the standby steady st ate \nvalue. \nAnother unique feature of 5TSDG that makes it diffe rent \nfrom previous research work is that V SSM  can also be used to \npre-charge the bit line, BL, in standby via M stby  as shown in \nFig. 1 (c) so that 1) channel and gate leakage thro ugh N 3 is \nreduced and minimized by up to 90% especially when a ‘0’ is \nstored, and 2) the cell maintains a reasonable Read Noise \nMargin  (RNM) when accessed close to the optimum \nachievable point and 3) To accelerate read/write op eration \nexplained in the next sections.  \nTABLE II shows standby leakage current and worst ca se \nRNM for various types of SRAM cells introduced in T ABLE \nI. Fig. 2 compares the power consumption of 5TSDG \nincluding peripheral circuits with a low-power 6T d esign in \nvarious process corners. Traditional 5T designs as in [8][9], \nwhere V SSM  is held at V SS  level, require lower V th  for internal \ncell transistors in 65nm technology, such as N 1, and P 2 (Fig. \n1), to enable write ‘1’ operation discussed in sect ion 2.4. \nThus, even though some leakage power is saved by cu tting a \nbit line and biasing the other to a lower voltage, the overall \nleakage is quite high, being about half of the conv entional 6T \ncell value in TABLE II. \nTABLE II. Leakage current and RNM comparison in \ndifferent SRAM types (not including peripheral circ uits) \n64 cells 5TSDG Low-Power 6T Conventional 6T \nLeakage (nA) 80.6 88.7 2020.0 \nRNM (mV) 172.3 123.2 123.2 \n \nFig. 2 5TSDG vs. low-power 6T SRAM designs: (64Kb \nSRAM array and peripheral circuits, FF corner on th e left, \n120 agSoud, 50% ‘0’s 50% ‘1’s stored in the array, 1 ≈1.3mW) 00.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\nProposed 5T Low-Power 6T Normalized Standby Power FF \nFS \nSF \nTT \nSS 2.2  Read Operation \nThe read operation is similar to a 6T SRAM except that \nonly one bit line is used. In 5TSDG, the bit line i s pre \ncharged in standby by V SSM  which is near the optimum point \nto maximize RNM in the worse case (FS). Another adv antage \nof this pre-charge method compared to [9]  \nrequire an additional power supply on chip such as a DC \nconverter or a level shifter which will add to the chip area a \npower consumption. A simple sense amplifier circuit  used in \n5TSDG is shown in Fig. 3 (b). A lthough not the fastest type, \nit is attractive due to its simplicity and that it does not need a \nclock signal [13]. During read, rd signal in Fig. \ncausing Vg 1 and Vg 2 to be pulled down to V \nMg2rd , which will maximize RNM and read performance. The  \nglobal bit line, Gbit, is the output of the sense a mplifier and is \npre-charged to V SSM  through M 8 in standby and is pulled \ndown to V SS  by M 7 during a read ‘0’. Therefore, a read \nalways implied unless Gbit is pulled down. Inverter  M \nshould have a sufficient noise margin to prevent a false \ntrigger. This sense amplifier can be shared by two bit lines \nfrom two adjacent sub- columns. For instance, in a 128 \ncolumn composed of two 64 cell sub- columns, the sense \namplifier is placed in between bit lines BitL and B itR. SelL \nand SelR signals should be selected by a row decode r to \nselect the appropriate bit line to read from. M \nused to pre-charge the input of the inv erter M \nFor similar bit line capacitances, read speed in 5T  and 6T \nSRAMs is comparable. \nFig. 4 (a) shows simulation results of the rea \na conventional 6T cell using a sense amplifier show n in \n(a). In this simulation, WL pulse is artificially g enerated such \nthat BL reaches about V DD /2 in read for power saving \nreasons. Gbit and Gbitz are the outputs of the sens e amplifier, \nand are pre- charged high using prez pulses before the read \noperation. \nFig. 4 (b) demonstrates the read operation of 5TSDG \nusing a sense amplifier shown in Fig. 3  \nmemory array arranged in 64x16 bit blocks for two \nneighboring cells sharing the same word line, WL, s toring a \n‘0’ and a ‘1’ on Q0 and Q1 nodes, and having two bi t lines \nBL0 and BL1 respectively. Gbit load in 5TSDG is the  same \nas that in Gbit and Gbitz in the 6T counterpart. \nGbit0 and Q1-Q1z-BL1- Gbit1 are related to a cell in 5TSDG \nstoring a ‘0’ and ‘1’ respectively and sharing the same word \nline (WL). \nA dynamic increase in Q0 node occurs while reading a \n‘0’ due to the current flow from the bit line to N \nthe word line is raised and as shown in   \nDuring read ‘1’ a drop of voltage in Q1 node is obs erve \na similar reason (Q min ). Q max  and Q min  should not cause a \nread upset i.e. they should be less and more than t he tripping \nvoltage of the inverter pairs, respectively, to avo id turning a \nread into a write, especially in FS corner. To furt her reduce \nthe probability of read- upset in 5T cell, it is possible to \nincrease word- line rise time and make the bit lines shorter to \nreduce their capacitances [2] . The latter may also improve \nread speed by reducing bit line swing delay. operation is similar to a 6T SRAM except that \nonly one bit line is used. In 5TSDG, the bit line i s pre -\nwhich is near the optimum point \nto maximize RNM in the worse case (FS). Another adv antage \n is that it does not \nrequire an additional power supply on chip such as a DC -DC \nconverter or a level shifter which will add to the chip area a nd \npower consumption. A simple sense amplifier circuit  used in \nlthough not the fastest type, \nsimplicity and that it does not need a \nFig. 1 (c) is raised \nto V SS  by M g1rd and \n, which will maximize RNM and read performance. The  \nglobal bit line, Gbit, is the output of the sense a mplifier and is \nin standby and is pulled \nduring a read ‘0’. Therefore, a read ‘1’ is \nalways implied unless Gbit is pulled down. Inverter  M 5-M6 \nshould have a sufficient noise margin to prevent a false \ntrigger. This sense amplifier can be shared by two bit lines \ncolumns. For instance, in a 128 -cell \ncolumns, the sense \namplifier is placed in between bit lines BitL and B itR. SelL \nand SelR signals should be selected by a row decode r to \nselect the appropriate bit line to read from. M 3 and M 4 are \nerter M 5-M6 in standby. \nFor similar bit line capacitances, read speed in 5T  and 6T \nshows simulation results of the rea d operation in \na conventional 6T cell using a sense amplifier show n in Fig. 3 \n(a). In this simulation, WL pulse is artificially g enerated such \n/2 in read for power saving \nreasons. Gbit and Gbitz are the outputs of the sens e amplifier, \ncharged high using prez pulses before the read \ndemonstrates the read operation of 5TSDG \n (b) in a 64Kbit \nmemory array arranged in 64x16 bit blocks for two \nneighboring cells sharing the same word line, WL, s toring a \n‘0’ and a ‘1’ on Q0 and Q1 nodes, and having two bi t lines \nBL0 and BL1 respectively. Gbit load in 5TSDG is the  same \nand Gbitz in the 6T counterpart. Q0-Q0z-BL0-\nGbit1 are related to a cell in 5TSDG \nand sharing the same word -\nA dynamic increase in Q0 node occurs while reading a \nfrom the bit line to N 3 and N 2 as \nthe word line is raised and as shown in   Fig. 4 (b) (Q max ). \nDuring read ‘1’ a drop of voltage in Q1 node is obs erve d for \nshould not cause a \nread upset i.e. they should be less and more than t he tripping \nvoltage of the inverter pairs, respectively, to avo id turning a \nread into a write, especially in FS corner. To furt her reduce \nupset in 5T cell, it is possible to \nline rise time and make the bit lines shorter to \n. The latter may also improve \nread speed by reducing bit line swing delay.  BL \nM1 \nM3\nM4M2VDD \nSQ \nVSS M5selz \nse \nGbit M6 \nM8 VDD prez \n(a) \n(b) \nFig. 3 Sense amplifier used in the read operation of the \nconventional 6T (a) and 5TSDG \n \n(a) \n \n(b) \nFig. 4 (a) R ead operation in a conventional 6T SRAM (b) \nRead operation in 5TSDG, ( typical corner (TT) selL \nselR M3M4BitL \nBitR M1\nM2\nSQz BLZ \nse \nGbitz M7 \nM9 prez VDD \n \n \nthe read operation of the \n5TSDG  cells (b). \n \n \nead operation in a conventional 6T SRAM (b) \ntypical corner (TT) , 120 agSoud). Gbit M5\nM6M7M8stby VSSM  \nTABLE III shows the trip voltage vs. Qmax and Qmin while \nreading in different corners in 5TSDG . These results when \ncompared with RNM measurements for various mismatch  \ncases in worst case corner FS, show the stability of \nRNM is measured with V SSM  on the bit line similar to \nThe best biasing value for V SSM  maximizes RNM for the FS \ncorner. \nTABLE III. 5TSDG : Trip point voltage vs. Q \n120 agSoud) \n5T Cell Trip Voltage (mV) /Q max \nFF FS SF TT  \n883/193 866/185 934/202 899/195 \n5T Cell Q min  (V) \n1.19 1.18 1.24 1.22  \n2.3  VSSM  Stability in Dynamic M\nDuring standby mode of the 5TSDG cell, V \na power supply to raise V g1  and V g2 above V \ncharge the bit- line. During read operation, V \ndriven to V SS to maximize RNM and the read speed.  On the \nother hand, after a read oper ation is completed, V \nthe bit-line are driven back to V SSM since the memory cell \nwill be in standby again. This voltage swing of V \nthe bit-line affects voltage level of V SSM  since each re \nof these voltages takes charges away fr om V \ndrop by an amount of ∆Vi, where i \nconsecutive read operations. In a case of reading a  ‘1’, the bit \nline will actually add charges to V SSM  but that amount is \nmuch less than the effect of the ground lines takin g away \ncharges after being driven low for a read . Fortunately, V \nis highly capacitive with much higher capacitance than \nand V g2 , and many memory cells in standby provide electric\ncharges to it. Therefore, V SSM  changes very little \noperation especially when it has large capacitance (attached \nto large memory arrays), and even if it does, it wi ll actually \nhelp the read operation in terms of performance and  read \nnoise margin (see Fig. 5 ). In addition, V \ndecrease beyond a steady- state value, and when reading is \ncomplete, it is pulled back towards its standby le \nan increase in memory cell leakage (see Fig. \nFig. 8 shows how V SSM reaches a steady \nmany read operations for different SRAM array sizes (64 \n1Mb, and 2Mb) in FF corner. This figure demonstrates that \nwhen larger number of me mory cells are attached to V \nthe initial values of ∆Vagu9S9 which are instantaneous voltage \ndecays after each read, and the total decay to reac h the \nsteady-state value,  ∆V agu9odagu9u5agu9od , will be smaller than that of smaller \narrays. After each read cycle, ∆Vagu9S9 is reduc ed until it reaches \n0V. At this point (steady state), the memory leakag e is \nsufficiently increased such that it can fully reple nish the lost \ncharges between read cycles.  V SSM  voltage after each read \ncycle ( i) can be described by equation 1. \n              Vagu9doagu9doagu897 aglNNNi agoo97 1 aglNN7agS5lu  φaglNNNiaglNN7. V agu9doagu9doagu897 aglNNNiaglNN7agoo97agu9S9agoS7SagoS59agoS8dagoSN5 \nagu887agoS5lagoS5SagoS5SagoSl5 \nwhere φaglNNNiaglNN7 agoldlagu887agoS5lagoS5SagoS5SagoSl5  aglNNNagu9u8agu9S5agu9SSagu9Sl aglNN7agu878 aglNNNagoS5lagoSolagoSll agoSSN\nagoS5lagoS5SagoS5SagoSl5 aglNNNagoSN7aglNN7 agu898agoSolagoSSNagu878 agoS5l\n agu887agoS5lagoS5SagoS5SagoSl5 aglNNNagu9u9agu9odagu9Suagu9o5aglNN7\nCagu9dNagu9doagu9doagu897 aglNNNstbyaglNN7 agoldl C agu9dNagoS5SagoS5SagoSl5 aglNNNreadaglNN7 agoo97 C aglNNNSubCol shows the trip voltage vs. Qmax and Qmin while \n. These results when \ncompared with RNM measurements for various mismatch  \nthe stability of 5TSDG. \non the bit line similar to [9]. \nmaximizes RNM for the FS \n: Trip point voltage vs. Q max  and Q min, \nmax (mV) \n SS \n899/195  905/196 \n 1.24 \nMode \ncell, V SSM is used as \nabove V SS and pre-\nline. During read operation, V g1 and V g2  are \nto maximize RNM and the read speed.  On the \nation is completed, V g1 , V g2  and \nsince the memory cell \nwill be in standby again. This voltage swing of V g1 , V g2 and \nsince each re -charge \nom V SSM causing it to \n is the index of \nconsecutive read operations. In a case of reading a  ‘1’, the bit \nbut that amount is \nmuch less than the effect of the ground lines takin g away \n. Fortunately, V SSM \nwith much higher capacitance than  Vg1  \n, and many memory cells in standby provide electric  \nchanges very little  during read \noperation especially when it has large capacitance (attached \nto large memory arrays), and even if it does, it wi ll actually \nhelp the read operation in terms of performance and  read \n). In addition, V SSM  does not \nstate value, and when reading is \ncomplete, it is pulled back towards its standby le vel due to \nFig. 6 and Fig. 7). \nreaches a steady -state value after \nread operations for different SRAM array sizes (64 Kb, \nThis figure demonstrates that \nmory cells are attached to V SSM , \nwhich are instantaneous voltage \ndecays after each read, and the total decay to reac h the \n, will be smaller than that of smaller \ned until it reaches \n0V. At this point (steady state), the memory leakag e is \nsufficiently increased such that it can fully reple nish the lost \nvoltage after each read \nagoS59agoS8dagoSN5 aglNNNagu9S9aglNN7.∆agu9od\nagoS5lagoS5SagoS5SagoSl5 aglNNNagu9u9agu9odagu9Suagu9o5aglNN7           (1)                                    \nagoS5lagoSolagoSll agoSS7\nagoS5lagoS5SagoS5SagoSl5 aglNNNagoSN7aglNN7 agu898agoSolagoSS7aglNN7agu887agoSolagoSll , \naglNNNSubCol aglNN7, CaglNNNSubCol aglNN7agoldl   aglNNNN agu88NagoSSNagoo97 N agu88NagoSS7aglNN7\nCagu9dNagu9S7agu8N9, Cagu9dNagu9S7agu87d, Cagu88Nagu89N , and are the capacitances of V \nthe bit-line respectively. I t is assumed that V \nbeen driven to 0V initially. Vagu88Nagu89N agoSSNand \nvoltages after a read ‘0’ and a read ‘1’ respectively. \nNagu88NagoSS7are the number of ‘0’ and ‘1’ bits \n(16 bits/word in the simulation results of this \nstandby, Cagu9dNagu9doagu9doagu897 aglNNNstbyaglNN7 includes the capacitance of \ncolumn connections, and V SSM  interconnections. During \nread, a single sub- column with capacitance of \nremoved. iagu9uoagoS59agoS8dagoSN5 aglNNNiaglNN7 is the average memory leakage \nover the i-th read cycle period, ∆t. It \nreduced. Similarly, Vagu9doagu9doagu897 aglNNNiaglNN7 is the Vagu9doagu9doagu897 \nthe i-th read cycle. A s the memory array size is increased, \nφaglNNNiaglNN7 approaches to one since C\nCagu9dNagu9doagu9doagu897 (stby). Part of ∆Vagu9S9 is caused by a small amount of \noverlap between rd and stby signals in \nThis effect on V SSM occurs also in write operation when \nthe bit-lines are charged and discharged. However, for \nexplanatory purposes, read operation, which is the most \nsevere, is selected to be demonstrated. The number of cells \nper bit-line and number of bits per word \nthis effect. \nFig. 5 RNM variations vs. bit- line biasing \nvertical line) for various corners in the \nFig. 6 The response of VSSM when forced to ‘0’ volts and \nleft floating in standby for 64Kb, 128Kb, 256Kb, an d 512Kb \nfrom left to right for 5T SRAM array (FF corner, 0.15 0.17 0.19 0.21 0.23 0.25 0.27 \n0.42 0.46 0.5 0.54 RNM (V) \nBit-line pre- charge voltage (V) \naglNN7aglNNNCagu88Nagu89N agoo97 C agu9dNagoSN5agoSS7agoo97 C agu9dNagoSN5agoSS8aglNN7, \nthe capacitances of V g1 , V g2 , and \nt is assumed that V g1  and V g2 have \nand Vagu88Nagu89N agoSS7are the bit-line \na read ‘1’ respectively.  Nagu88NagoSSN and \nare the number of ‘0’ and ‘1’ bits in a word respectively \n(16 bits/word in the simulation results of this paper). In \nthe capacitance of all sub-\ninterconnections. During \ncolumn with capacitance of CaglNNNSubCol aglNN7 is \nmemory leakage current \n. It  is increased as V SSM  is \nagu9doagu9doagu897  voltage at the end of \ns the memory array size is increased, \nCagu9dNagoS5SagoS5SagoSl5 aglNNNread aglNN7 approaches \nis caused by a small amount of \noverlap between rd and stby signals in Fig. 4 (b). \noccurs also in write operation when \nlines are charged and discharged. However, for \nexplanatory purposes, read operation, which is the most \nto be demonstrated. The number of cells \nline and number of bits per word also contribute to \n \nline biasing  (=V SSM , e.g. \nfor various corners in the 5TSDG cell \n \nThe response of VSSM when forced to ‘0’ volts and \nleft floating in standby for 64Kb, 128Kb, 256Kb, an d 512Kb \nfrom left to right for 5T SRAM array (FF corner, 120 agSoud) 0.58 0.62 \ncharge voltage (V) SS \nFS \nTT \nSF \nFF \nPoints of interest \nVSSM  = 0.6V Fig. 7 Effect of read operation on V SSM (64Kb array 16 \nbits/word, FF, 120 agSoud, ∆t agoldN 1. 4\n \nFig. 8 V SSM  Saturation during repeated read operations for \n64Kb, 1Mb and 2Mb 5T SRAM arrays (64 bits/word, FF,  \n120 agSoud)     \n2.4  Write Operation \nSince a 5T SRAM cell only has a single bit \neither a ‘0’ (W0) or a ‘1’ (W1) into the cell \nusing the same bit- line. This is different from the 6T structure \nwhere there is technically no difference between a W0 or a \nW1, i.e. by selectively pulling down one of the bit  lines \ndepending on the data status, a W0 operation is app lied on \none side of the cell and the feed back will recover the opposite \nstorage node to the complement value. In \nperformed in a similar way. On the other hand, in W 1, the bit \nline is pulled high by global write signal, Gwr, so that wh en \nthe word-line is selected , state toggle is initiated \ndriven high by the write circuit in W1 and is drive n to V \notherwise. Using conventional 6T transistor ratios and sizing, \nit is almost impossible to write a ‘1’ in a 5T \n6T cell: 1) N 2 needs to be stronger than N 3 by \nfactor β, typically between (1.2~1.5) to maintain read stab ility \n[4]. 2) P 1 and P 2 need to be weak enough, usually minimum \nsize for write-ability  purposes. 3) The access transistor is an \nNMOS, which does not pull up strongly due to its physical \nnature. These constraints will oppose raising Q if applied in a \n5T memory cell for a W1 using a single bit line. To combat \nthis problem, [9] suggests using different (W/L) sizes for the \ntransistors such as, using a CR of ~0.45, weakening  P \n \n(64Kb array 16 \n4ns ) \n \nread operations for \n64Kb, 1Mb and 2Mb 5T SRAM arrays (64 bits/word, FF,  \nSince a 5T SRAM cell only has a single bit -line, writing \neither a ‘0’ (W0) or a ‘1’ (W1) into the cell is performed \nline. This is different from the 6T structure \nwhere there is technically no difference between a W0 or a \nW1, i.e. by selectively pulling down one of the bit  lines \ndepending on the data status, a W0 operation is app lied on \nback will recover the opposite \nstorage node to the complement value. In 5TSDG, W0 is \nperformed in a similar way. On the other hand, in W 1, the bit -\npulled high by global write signal, Gwr, so that wh en \n, state toggle is initiated . Gwr is \ndriven high by the write circuit in W1 and is drive n to V SS  \n6T transistor ratios and sizing, \n5T  cell because in a \nby cell ratio (CR) \n, typically between (1.2~1.5) to maintain read stab ility \nneed to be weak enough, usually minimum \npurposes. 3) The access transistor is an \ndoes not pull up strongly due to its physical \nnature. These constraints will oppose raising Q if applied in a \na single bit line. To combat \nsuggests using different (W/L) sizes for the \ntransistors such as, using a CR of ~0.45, weakening  P 1, strengthening P 2 and N 1 with the cost of noise margin. \nopposed to 5TSDG in this paper, design in \n50% reduction of RNM when co mpared to \ncell and therefore is more susceptible to performance \nfluctuations in more advanced technologies \nprocess variations.  \nOn the other hand, to make W1 possible, \ndisconnecting Vg 2 from V SS  and letting it \nvoltage by using a capacitor while keeping Vg \nwrite. This method will weaken N 2 by lowering its V \nwill facilitate W1. However, this method does not take \nadvantage of leakage power reduction opportunities \nAs illustrated in Fig. 1 and \n5TSDG, V SSM  is connected to Vg 1, Vg \nstandby mode. In W0, Vg 2 stays connected to V \nwhile Vg 1 floats near V SSM . In W1, Vg1 is pulled down to V \nthrough M g1w1 . M equ  is turned on by Gwr signal which is \nwhen W1 and is at V SS  otherwise . The role of this transistor is \nto limit ∆Vg=Vg 2-Vg 1 as shown in Fig. \nthe disturbed cells in the same sub -\nMequ is chosen through simulation to limit write disturb \nprocess corners especially for fast NMOS corner cases \nThis disturbance can also be minimized by reducing the write \npulse period to its limit. In summary, in W1, N \nstronger current drive than N 2 since its V \nincreased by V SSM .  \nThe threshold voltage of access transistor, N \nrole in W1 performance. Simulation results \nstandby power varies less than 2% using high, stand ard or \nlow V th  (HVT, SVT, LVT) for N 3. In order to improve W1 \nperformance, the V th  of N 3 can be reduced \nRNM. In 5TSDG, V th  of N 3 can be between the HVT and \nLVT to maintain a reasonable RNM/W1 performance \nshown in TABLE IV (f or W1 delay measurement see \nRNM can be further increased by reducing bit \ncapacitance and/or increasing word- line rise time \nTABLE IV . RNM and W1 comparison for different V \nN3, at worst case RNM (FS corner, \n5T Cell RNM (mV) / W1 Delay (ps) for Various N3 V \nLVT (~230 mV) SVT(~440 mV) \n144.1/96.8 172.3/116.4 \n \nFig. 9 W1 operation of 5TSDG cell \n(120 agSoud) \nBL \nWL Qz  Q \nW1 Delay  with the cost of noise margin. As \nin this paper, design in  [9] will cause a \nmpared to conventional 6T \nmore susceptible to performance \nin more advanced technologies , especially due to \nOn the other hand, to make W1 possible, [8] suggests \nand letting it float near a biasing \nvoltage by using a capacitor while keeping Vg 1 at V SS  during \nby lowering its V DS  which \nthis method does not take \nadvantage of leakage power reduction opportunities .  \nand as discussed earlier, in \n, Vg 2 and the bit lines in \nconnected to V SSM via Mg 2 \nn W1, Vg1 is pulled down to V SS  \nis turned on by Gwr signal which is  high \n. The role of this transistor is \nFig. 9 to improve SNM of \n-column. The strength of \nchosen through simulation to limit write disturb  for all \nespecially for fast NMOS corner cases [2]. \nThis disturbance can also be minimized by reducing the write \npulse period to its limit. In summary, in W1, N 1 will have a \nsince its V DS  is maximized i.e. \naccess transistor, N 3, plays a key \nSimulation results reveal that \nstandby power varies less than 2% using high, stand ard or \n. In order to improve W1 \ncan be reduced with some loss of \ncan be between the HVT and \nLVT to maintain a reasonable RNM/W1 performance  as \nor W1 delay measurement see Fig. 9). \nRNM can be further increased by reducing bit -line \nline rise time [2]. \n. RNM and W1 comparison for different V th  for \n, at worst case RNM (FS corner, 120 agSoud) \n5T Cell RNM (mV) / W1 Delay (ps) for Various N3 V th  \nmV)  HVT(~600 mV) \n225.1/170.6 \n \ncell in slow corner (SS) \n \nVg 1 Vg 2 5T SS \n∆Vg  There fore, to improve read stability an \n(particularly W1), the solution is to find a reasonable mid \npoint considering the fact that N 3 does not play a key role in \nstandby power consumption. Limited to three choices  for V \nselection, SVT for N 3 is reasonable as shown in \nHowever, in chip foundries, even a lower threshold \nsomewhere between LVT and SVT can be achieved by \nchanging gate oxide thickness. Fig. 11 compares W0 and W1 \nperformance of 5TSDG with a low- power 6T SRAM \ndescribed in TABLE I. For both cases, W1 delay is measured \nfrom when WL = 50%V DDM  to when Q=80%V \ndelay is measured similarly but when Q=20%V \nVSS . This measurement is different from what was re \nTABLE IV (word-line to Q- Qz cross point) \n31% slower than a conventional 6T design \nimproved by reducing V th  of N 3. W0 performance is similar \nto conventional 6T cell. \nFig. 10 (a) shows how the voltage of V g1 \nSNM on disturbed cells while driving V g2  a\n(at V SSM ) mimicking that there is no M \ndemonstrates the reverse scenario where V \nand V g2  varies from 0V to V SSM . Similarly, this figure shows \nthat with no weak equalization between V \ndisturbed cells are susceptible to data corruption due \nenvironmental disturbances. The strength of M \ndetermine the limitation on this disturbance by bot h lowering \nVg2 from V SSM and not allowing V g1 to be pulled down so \nmuch. In 5TSDG, M equ was ratioed suc \ndisturbed SNM was greater than ~50mV. \nFig. 10 SNM in W1 disturb cells vs. V g1  and V \n(120 agSoud) \n \n \nFig. 11 W1 (left) and W0 (right ) delay comparison in \ndifferent corners between 5TSDG and low \n(120 agSoud) \n050 100 150 200 250 300 350 \nPropoed 5T Low-Power 6T Write '1' Delay (ps) FF \nFS \nSF \nTT \nSS 010 20 30 40 50 60 \nProposed 5T Write '0' Delay (ps) fore, to improve read stability an d write-ability \nsolution is to find a reasonable mid -\ndoes not play a key role in \nstandby power consumption. Limited to three choices  for V th  \nas shown in TABLE IV. \nin chip foundries, even a lower threshold \nsomewhere between LVT and SVT can be achieved by \ncompares W0 and W1 \npower 6T SRAM \nW1 delay is measured \nto when Q=80%V DDM , and W0 \ndelay is measured similarly but when Q=20%V DDM  above \nThis measurement is different from what was re ported in \nQz cross point) . W1 can be ~11-\n6T design  and can be \nW0 performance is similar  \ng1  in W1can affect \nat a fixed voltage \n) mimicking that there is no M equ. Fig. 10 (b) \ndemonstrates the reverse scenario where V g1  is fixed at 0V, \n. Similarly, this figure shows \nthat with no weak equalization between V g1  and V g2 , the \ncells are susceptible to data corruption due to \nThe strength of M equ will \ndetermine the limitation on this disturbance by bot h lowering \nto be pulled down so \nsuc h that the W1 \n \nand V g2  voltages \n \n) delay comparison in \nand low -power 6T cell The write margin of the proposed 5T SRAM design can  \nbe divided into W0 margin (W0M), and W1 margin \nsince as opposed to the 6T cell counterpart, W0 and  W1 have \ndifferent WMs. One of the common methods to measure  WM \nin con ventional 6T SRAMs is by measuring the maximum \nBL voltage able to flip the cell state \nis defined to be the difference between the positiv e supply \nvoltage, V DDM , and the minimum BL voltage \n‘1’ into the cell while W0M is defined to be the ma ximum \nBL voltage able to write a ‘0’ into the cell. In the \n(V DDM =1.3V), for a typical- typical corner (TT), W \n~0.5V, and W0M is ~0.4V. \n3 DYNAMIC POWER CONSUMPTION \nDynamic power consumption of 5TSDG can be divided \ninto read and write power. Power consump \na function of Vmin, which determines V \nDuring several consecutive reads, V \ndriven to V SS  and V SSM  frequently. Active power \nconsumption is changed as supply voltage is changed  due to \nthe square law dependency. This power is also depen dent on \nthe frequency of V SSM swing during read. Equation 2 shows \nthe dynamic power consumed due to the voltage \nground lines of 5TSDG, where agS8u9agodSo, is the summation of V \nand V g2 capacitances, ∆agS8l8 is V SSM -VSS \nof voltage swing. \n                  agS8luagodoSagod5uagodlS agoldl agS8u9 agodSo∆agS8l8agu87dagS858\nReading a ‘0’ (R0) consumes more power than reading  a \n‘1’ (R1) since in R0, the bit- line is pulled sufficiently low to \ntrigger the sense amplifier, and the global bit \namplifier is also pulled down. In R1, bit \nto be p ulled high enough to avoid \namplifier, and the global bit- line stays at V \nread power and standby power for variou \nkeeping Vmin=V DDM -VSSM  constant at 0.7V for a 64x16 bit \nblock of 5TSDG. As V DDM is increased, V \naccordingly causing ∆agS8l8 in equation 2 to increase during read \noperation. Therefore, read power is increased quadr atically \nwith higher V DDM . \nFig. 12 Comparison of normalized read and standby power \nvs. V DDM  for 5TSDG cell, 64x16 bit block, reading 16  ‘0’s \ncontinuously from 16- bit words (FF corner, \nProposed 5T Low-Power 6T FF \nFS \nSF \nTT \nSS 0.00 0.20 0.40 0.60 0.80 1.00 \n1 1.1 1.2 Normalized Power \nVDDM (V) The write margin of the proposed 5T SRAM design can  \nbe divided into W0 margin (W0M), and W1 margin (W1M) \nsince as opposed to the 6T cell counterpart, W0 and  W1 have \ndifferent WMs. One of the common methods to measure  WM \nventional 6T SRAMs is by measuring the maximum \nBL voltage able to flip the cell state [15].  For 5TSDG, W1M \nis defined to be the difference between the positiv e supply \n, and the minimum BL voltage able to write a \n‘1’ into the cell while W0M is defined to be the ma ximum  \nto write a ‘0’ into the cell. In the 5TSDG \ntypical corner (TT), W 1M is \nONSUMPTION  \nDynamic power consumption of 5TSDG can be divided \ninto read and write power. Power consump tion during read is \na function of Vmin, which determines V SSM biasing level. \nDuring several consecutive reads, V g1 and V g2  in Fig. 1 are \nfrequently. Active power \nconsumption is changed as supply voltage is changed  due to \nthe square law dependency. This power is also depen dent on \nswing during read. Equation 2 shows \nthe dynamic power consumed due to the voltage  swing of the \n, is the summation of V g1  \nSS  and agS858 is the frequency \nagS858                                   (2) \nReading a ‘0’ (R0) consumes more power than reading  a \nline is pulled sufficiently low to \ntrigger the sense amplifier, and the global bit -line of the sense \namplifier is also pulled down. In R1, bit -line is only required \nulled high enough to avoid activating the sense \nline stays at V SSM . Fig. 12 shows \nread power and standby power for variou s V DDM  values while \nconstant at 0.7V for a 64x16 bit \nis increased, V SSM also increases \nin equation 2 to increase during read \noperation. Therefore, read power is increased quadr atically  \n \nComparison of normalized read and standby power \ncell, 64x16 bit block, reading 16  ‘0’s \nbit words (FF corner, 120 agSoud) 1.3 Read \nStandby Fig. 13 demonstrates case study results of worst-ca se (FF, \n120 agSoud) normalized power consumption in standby mode, \nread, and write operations of 5TSDG in comparison w ith \nlow-power 6T design. Other corners have similar res ults \ncomparable to Fig. 2. Read power consists of standb y power \nof the idle memory cells, and the dynamic power des cribed \nby equation 2. In this case study where a 64Kbit ar ray \nconsisting of 64x16 bit blocks was studied (reading  \ncontinuously from a 16-bit word), 5TSDG could achie ve up \nto ~30% power reduction in read mode compared to th at of \nthe low-power 6T structure. In this example, R1 con sumes \n~7% less power in 5TSDG compared to a R0 as explain ed \nearlier. Obviously, larger number of read operation s will \nresult in a linearly higher power consumption diffe rence in \ncomparison with standby power due to larger values of agS858 in \nequation 2. Read operation of the low-power 6T and 5TSDG \ndesigns in this experiment were similar to Fig. 4(a ) and Fig. \n4(b) respectively. In a pipelined “smart” memory, b ack-to-\nback reads from the same sub-block would consume le ss \ndynamic power if V g1 and V g2 are held at V SS between \nconsecutive reads. \nThe 5TSDG write power can be divided into W0 and W1  \npower, each consisting of idle cell standby power, plus the \ndynamic power. In Fig. 13, a 64Kbit array consistin g of \n64x16 bit blocks was studied while writing into a 1 6-bit \nword. In this example, W0 consumes ~80% less power,  and \nW1 consumes ~9% less power compared with a low-powe r \n6T structure in worst case scenario (FF corner, 120 agSoud). Since \nR0 and R1 use similar power, storing bits to favor W0 (i.e. \ncell inverted) may reduce total power. \n \nFig. 13 Case study results of the worst-case write power \nconsumption in comparison with read and standby pow er for \n5TSDG vs. low power 6T design (FF, 120 agSoud), 1 ≈ 33.8mW \n4 CONCLUSION  \nIn this paper, the operation of a new low-power and  high \nperformance design for a 5T SRAM cell was addressed  which \nhas improvements in static and dynamic power consum ption, \nstability margins and performance when compared to \nprevious designs in this area. The stability of the  novel \nbiasing scheme in dynamic mode was analyzed. The \nreduction in dynamic power consumption in compariso n with a low-power 6T counterpart was demonstrated. A sign ificant \narea saving is predicted compared to a conventional  6T cell.  \n5 REFERENCES  \n[1]  Jarollahi, Hooman, Hobson, R.F., “Power  and  Area  \nEfficient  5T-SRAM  with Improved Performance in 65 nm \nCMOS for Low-Power SoC,“ IEEE International Midwest  \nSymposiom on Circuits and Systems, 2010, MWSCAS \n2010., Aug. 2010, In Press. \n[2]  Hobson, R.F., \"A 5T SRAM Cell with 4 Power \nTerminals for Read/Write/Standby Assist,\" Proceedin gs of \nthe 2009 Intl Conference on Computer Design, CDES 2 009, \npp.10-16, 13-16 Jul. 2009. \n[3]  H.Qin, et al., \"Standby supply voltage minimization  for \ndeep sub-micron SRAM\", Microelectronics Journal, vo l 36, \npp. 789-800, Mar. 2005.  \n[4]  Sui Huang, et al., \"A novel SRAM structure for leak age \npower suppression in 45nm technology,\" Communicatio ns, \nCircuits and Systems, 2008. ICCCAS 2008. Internatio nal \nConference on,  pp.1070-1074, 25-27 May 2008.  \n[5]  Fu-Liang Yang, et al., \"Electrical Characteristic \nFluctuations in Sub-45nm CMOS Devices,\" Custom \nIntegrated Circuits Conference, 2006. CICC '06. IEE E, \npp.691-694, 10-13 Sep. 2006. \n[6]  Hamzaoglu, F., et al.,  \"A 153Mb-SRAM Design with \nDynamic Stability Enhancement and Leakage Reduction  in \n45nm High-Κ Metal-Gate CMOS Technology,\" Solid-State \nCircuits Conference, 2008. ISSCC 2008. Digest of Te chnical \nPapers. IEEE International, pp.376-621, 3-7 Feb. 20 08. \n[7]  Khellah, M., et al., \"A 256-Kb Dual-VCC  SRAM \nBuilding Block in 65-nm CMOS Process With Actively \nClamped Sleep Transistor, \" Solid-State Circuits, I EEE \nJournal of, vol.42, no.1, pp.233-242, Jan. 2007. \n[8]  Tran, H., \"Demonstration of 5T SRAM and 6T dual-por t \nRAM cell arrays, \" VLSI Circuits, 1996. Digest of T echnical \nPapers., 1996 Symposium on, pp.68-69, 13-15 Jun. 19 96. \n[9]  Carlson, I., et al., \"A high density, low leakage, 5T \nSRAM for embedded caches,\" Solid-State Circuits \nConference, 2004. ESSCIRC 2004. Proceeding of the 3 0th \nEuropean, pp. 215- 218, 21-23 Sep. 2004. \n[10]  Seevinck, et al., \"Static-noise margin analysis of MOS \nSRAM cells,\" Solid-State Circuits, IEEE Journal of , vol.22, \nno.5, pp. 748- 754, Oct. 1987. \n[11]  Hook, T.B., et al., \"Noise margin and leakage in ul tra-\nlow leakage SRAM cell design,\" Electron Devices, IE EE \nTransactions on, vol.49, no.8, pp. 1499- 1501, Aug.  2002. \n[12]  Romanovsky, S., et al., \"Leakage reduction techniqu es \nin a 0.13 um SRAM cell,\" VLSI Design, 2004. Proceed ings. \n17th Intl Conference on, pp. 215- 221, 2004. \n[13]  Hobson, R.F., \"A New Single-Ended SRAM Cell With \nWrite-Assist,\" Very Large Scale Integration (VLSI) Systems, \nIEEE Transactions on, vol.15, no.2, pp.173-181, Feb . 2007. \n[14]  Lysinger, M., et al., \"A Radiation Hardened Nano-\nPower 8Mb SRAM in 130nm CMOS,\" Quality Electronic \nDesign, 2008. ISQED 2008. 9th International Symposi um \non, pp.23-29, 17-19 Mar. 2008. \n[15]  Zheng Guo, et al., \"Large-scale read/write margin \nmeasurement in 45nm CMOS SRAM arrays,\" VLSI Circuit s, \n2008 IEEE Symposium on, pp.42-43, 18-20 June 2008. \n[16]  Wieckowski, M., Patil, S., Margala, M.; , \"Portless  \nSRAM—A High-Performance Alternative to the 6T \nMethodology,\" Solid-State Circuits, IEEE Journal of , vol.42, \nno.11, pp.2600-2610, Nov. 2007. \n \n \n 0.00 0.20 0.40 0.60 0.80 1.00 1.20 \nRead '0' Read '1' Write '0' Write '1' Standby Normalized Power Proposed 5T \nLow-power 6T ",
      "metadata": {
        "filename": "Dynamic Power Reduction in a Novel CMOS 5T-SRAM for Low-Power SoC.pdf",
        "hotspot_name": "Standby_Power_Consumption",
        "title": "Dynamic Power Reduction in a Novel CMOS 5T-SRAM for Low-Power SoC",
        "published_date": "2013-02-18T21:24:22Z",
        "pdf_link": "http://arxiv.org/pdf/1302.4464v1",
        "query": "electronic device standby power reduction energy efficient design methods"
      }
    },
    "Enabling Lower-Power Charge-Domain Nonvolatile In-Memory Computing with Ferroele": {
      "full_text": " \n  Abstract—Compute-in-memory (CiM) is a promising approach to alleviating the memory wall problem for domain-specific applications. Compared to current-domain CiM solutions, charge-domain CiM shows the opportunity for higher energy efficiency and resistance to device variations. However, the area occupation and standby leakage power of existing SRAM-based charge-domain CiM (CD-CiM) are high. This paper proposes the first concept and analysis of CD-CiM using nonvolatile memory (NVM) devices. The design implementation and performance evaluation are based on a proposed 2-transistor-1-capacitor (2T1C) CiM macro using ferroelectric field-effect-transistors (FeFETs), which is free from leakage power and much denser than the SRAM solution. With the supply voltage between 0.45V and 0.90V, operating frequency between 100MHz to 1.0GHz, binary neural network application simulations show over 47%, 60%, and 64% energy consumption reduction from existing SRAM-based CD-CiM, SRAM-based current-domain CiM, and RRAM-based current-domain CiM, respectively. For classifications in MNIST and CIFAR-10 data sets, the proposed FeFET-based CD-CiM achieves an accuracy over 95% and 80%, respectively.  Index Terms—CiM, process in memory, ferroelectric, charge-domain computing-in-memory, ferroelectric transistors, FeFET. I. INTRODUCTION HE computing capability and energy efficiency of modern computers based on the von Neumann architecture are hindered by the data movement between the memory component and the processing units, known as the ³memory wall´ problem [1]. This problem has deteriorated with the advent of the big-data era. To tackle this challenge, recent attempts of computing in the memory (CiM) have become intriguing by reducing the data transfer activities [3][4]. As the conventional memories were not designed for the CiM purpose, a key CiM enabler is to facilitate the memory component with a computable circuit structure and/or a flexible interface, under the constraints of cost, power consumption, scalability, and reliability. From the application perspective, the data-intensive convolutional neural network (CNN) acceleration is of particular interest because of the data formality in computing parallelism and simplicity [2]. Recent exploration works ranging from devices and circuits to architectures and algorithms have indicated the benefits of such a co-design [3]-[5]. These existing CiM methodologies could be roughly classified in two dimensions in Fig. 1: (i) memory devices being volatile or nonvolatile, and (ii) computing and sensing methods being in a current mode or a voltage/charge mode [3]-[6], [28]. Compared with the conventional volatile CMOS solution, NVM has potentially higher density and the intrinsic zero standby power. Compared with the current-mode computing and interfacing, a voltage-mode charge-domain CiM (CD-CiM) consumes only dynamic power, which is appealing for low-power applications. More importantly, the capacitor-based CD-CiM may provide higher immunity to PVT variations, including the on-state current mismatch, which is challenging to handle for both MOSFET and RRAM [5].  Therefore, it is highly motivated and timely to start the adventure of NVM-based CD-CiM in the bottom-right quadrant of Fig. 1 to enable the combined advantages of NVM and charge-domain computing for denser, more reliable, and lower-power solutions. Previously, this was challenging due to the low on/off ratio of MRAM (typically ~2) and RRAM (typically 102-103 [22]), as to be discussed subsequently. Today, the emerging of the nonvolatile ferroelectric field-effect- transistor (FeFET) with a highly-scalable CMOS-compatible process and an ultra-high on/off ratio (>106) has articulated a promising design space for a new CD-CiM paradigm. Besides, the other FeFET features, such as DC-power-free write, separate read and write ports, and the compact integration of NVM and transistor also contribute to more design flexibilities in the device-circuit co-design. This work proposes the concept, analysis, and design of NVM-based CD-CiM, and exploits FeFETs for the circuit implementation. Evaluation against SRAM and RRAM solutions suggests significantly improved trade-offs between density, performance, and power consumption. Itemized contributions include: x A FeFET-based 2-transistor-1-capacitor (2T1C) compact CiM cell that supports charge-domain DC-power-free XNOR operations, along with the analysis on the cell-level comparison with other NVM technologies; x A CD-CiM macro array based on the proposed 2T1C cell for low-power, parallel, and reliable multiply-and-accumulate (MAC) operations in binary neural network (BNN) applications; Enabling Lower-Power Charge -Domain Nonvolatile  \nIn-Memory Computing with Ferroelectric FETs  Guodong Yin,  Student Member, IEEE,  Yi Cai, Juejian Wu, Student Member, IEEE,  Zhengyang Duan, \nZhenhua Zhu,  Student Member, IEEE,  Yongpan Liu, Senior Member, IEEE,  Yu Wang, Senior \nMember, IEEE,  Huazhong Yang, Fellow, IEEE , and Xueqing Li, Senior Member, IEEE  \nT \nManuscript received Apr. 1, 2020. Revised July 18, 2020 , Nov.  19, 2020 . \nAccepted Jan. 2, 2021 . This work is supported in part by NSFC (#61874066, \n#61720106013), in part by The National Key Research and Development \nProgram of China (2019YFA0706100, 2018YF A0701500 ), in part by Key \nLaboratory of Artificial Intelligence, Ministry of Education , and in part by The \nBeijing Innovation Center for Future Chips . Corresponding author: X. Li.  \nAll authors are with BNRist, The Department of Electronic Engineering, \nTsinghua University, Beijing 100084, China. Email: {ygd 20, caiy17, wujj19, \nduanzy18, zhuzhenh18}@mails.tsinghua.edu.cn , {ypliu , yu-wang , yanghz , \nxueqingli }@tsinghua.edu.cn .  \n    Digital Object Identifier XXXX/XXXX.XXXX.   \n \nFig.1. Extending CiM to the NVM -based charge -domain quadrant.  \n Volatile Memory\nNonvolatile MemoryCharge DomainCurrent DomainProcess reliabilityDensityIdle leakageComputing powerVariation limitation\nProcess reliabilityDensityIdle leakageComputing powerVariation limitation\nProcess reliabilityDensityIdle leakageComputing powerVariation limitation\nProcess reliabilityDensityIdle leakageComputing powerVariation limitation\nExploredExploredTo be ExploredExplored x Evaluations of the proposed charge-domain CiM, including (i) 2T1C cell-level CiM circuit performance, (ii) array-level analysis on energy savings of the computing operation and the impact of major array-level variations, and (iii) variation-aware classification accuracy in MNIST and CIFAR-10 datasets. II. PROPOSED 2T1C CHARGE-DOMAIN CIM  This section introduces BNN and FeFET background briefly and presents the proposed FeFET-based 2T1C XNOR cell for the MAC CiM computing methodology of BNN applications. A. Binary CNN Convolutional neural networks (CNNs) may achieve high accuracy in computer vision applications, such as image classification and face detection. In CNNs, the multiply-and-accumulate (MAC) is a critical operation during inference and consumes major power [5]. The trained neural networks with binary weights and activations, i.e., +1 and -1, drastically simplify multiplications to be atomic XNOR operations [8]. While BNN has shown its success in smaller data sets like MNIST, recent works have extended the use of low-resolution quantization in larger data sets like ImageNet, showing improved accuracy and much lower costs with algorithm optimizations [8][24]. Binary MAC operations can be performed in three steps: (i) perform XNOR logic in atomic cells; (ii) accumulate the results; (iii) restore the binary value. These operations deliver the corresponding matrix multiplication functions in the convolutional/fully-connected layer and nonlinear functions in the neuron layer. Below, (1) shows the details of the batch normalization and the activation function ȡ [9], and (2) shows the particular case for binary batch normalization and the activation function by a sign comparison between the MAC result and a reference voltage:                          𝑂𝑈𝑇௜ൌ𝜌ሺ𝛾௜௉஺೔−ఓ೔ఙ೔మ൅𝛽௜ሻ                                 (1)                                𝑂𝑈𝑇௜ൌ𝑠𝑖𝑔𝑛ሺ𝐼𝑁௜െ𝛼௜ሻ                                    (2) where PAi  is the pre-activation tensor,  OUTi is the output tensor, ȝi is the mean of PAi, ıi is the standard deviation of PAi, and Ȗi, ȕi, Įi are batch normalization parameters. Compared to 32-bit CNNs, binary CNNs need only 1/32 memory and also much fewer data accesses, leading to drastic power savings.  XNOR circuits could be implemented in the current mode (current domain) or the voltage mode (charge domain) [3][5]. In the current-mode design, the XNOR results are calculated in custom designed XNOR memory cells based on the Kirchhoff's current law. By identifying the amount of the output current, the sense amplifiers (SA) could tell from different XNOR input scenarios. Due to the DC-power consumed by computing and sensing, the current-mode XNOR operation may not fit well in power-sensitive applications [3][5]. This is more challenging with device variations in the output currents.  In contrast, in the charge domain, XNOR and MAC operations could be implemented more efficiently with the charge conservation law. Fig. 2 illustrates an example using the SRAM array. Each SRAM cell is accompanied by a local capacitor. It performs XNOR operations between the SRAM state and the input pair IA/IAb. Each single XNOR operation result is reflected as the charge stored at the local capacitor near each SRAM in Fig. 2. The charge of the three capacitors is then collected with the source line ScL for a summation of the XNOR results. As the SRAM states are directly linked to the supply voltage VDD or the ground voltage GND (thanks to the high on/off IDS ratio of MOSFETs), the major source that determines the MAC computation accuracy is the mismatch between the capacitors rather than the MOSFET VTH variations. With a proper VDD, this significantly improves the immunity to MOSFET on-state drain current variations (as compared with the current-mode sensing of summed currents). In addition, the operation does not occur with static currents, leading to significant power savings.  The main drawbacks of the SRAM-based charge-domain XNOR and MAC operations are (i) low density due to large SRAM cells, and (ii) idle-state static leakage current. As to be shown subsequently, the proposed new design with FeFETs solves these problems elegantly. B. The FeFET Device Basics An FeFET is essentially a MOSFET with a ferroelectric layer embedded at the gate [13]. The polarization of this extra layer brings a knob to tune and keep a nonvolatile VTH, which further leads to a tunable state of the drain-source current IDS. Fig. 3. shows the FeFET IDS-VGS curve with two states and adopted parameters of the model used in the simulation. Detailed device operating mechanisms have been reported in prior works [12][13][26]. Generally, to reduce (or increase) VTH of an n-type FeFET, a positive Vwrite (or negative –Vwrite) voltage pulse could be applied to the FeFET gate. Vwrite should be sufficiently high to trigger partial or full polarization switching. A negative gate voltage may be practiced with a negative VGS to avoid the use of a negative supply. While sensing the FeFET VTH state, the gate should be biased at a moderate voltage below Vwrite to prevent disturbance. Recent use of hafnium-based materials makes FeFETs well scalable [14][15]. With the amplification of the embedded MOSFET, IDS could exhibit an ultra-high range beyond 106, which is particularly preferred for computing in large memory arrays [17][29]. FeFETs also own a moderate endurance (up to 1012 [18]), a moderate operating voltage range (low to 1.5V [13]), and speed (up to ns [19]).   \nFig. 2.   Existing  SRAM charge -domain XNOR and MAC  (rotated  view ) [5]. \n \n \nFig. 3. (a) FeFET I DS-VGS curve; (b) write methods.  \n \n \nFig. 4. Proposed FeFET -based CD -CiM: (a) cell ; (b) array structure.  BLBLBScLWL1IA1IAb1WL2WL3IAb2IA2IAb3IA3\nVGS(V)0.00.51.0-0.5-1.0-0.50.00.51.01.52.02.5\nVwrite-Vwrite-VDDVDDFerroelectric layer thickness=9nmKinetic coefficient=0.1\n(a)GNDWrite µ0¶Vwrite(b)VwriteWrite µ1¶GNDGNDVwriteIDS(ȝA)\nOn-off ratio=105\n2T1C\nWLNWLB1WL1BLNBLBNBL2BLB2BL1BLB1\nVRefNOUTNScLNSABitlineDriver\n2T1C\n2T1C2T1C2T1C\n2T1C2T1C2T1C\n2T1CWL2WLB2WLBNScLCMM1M2WLBLxWLB(a)(b)Input BufferVref2OUT2ScL2SAVref1OUT1ScL1SABLB\n Notably, recent reports show that highly-scaled FeFETs maintains the high on/off ratio, but may also suffer from large IDS variations. This limits the accuracy of current-sensing-based CiM [20]. Therefore, innovations that exploit the ultra-high on/off ratio rather than the absolute IDS is more preferred for computing purposes. This is the contribution of this work when compared with existing FeFET-based current-mode solutions. C. Proposed 2T1C CD-CiM macro Cell and array structures. Fig. 4 (a) shows the proposed 2T1C cell. It consists of two n-type FeFETs (M1 and M2), and one capacitor CM. WL and WLB are wordlines. BL, BLB, and ScL are bitlines. The cell can store bits as FeFET states: µ1¶ for positive polarization state (negative VTH) and µ0¶ for negative polarization state (positive VTH). In the cell, the two FeFETs store one µ0¶ and one µ1¶, similar to SRAM. Fig. 4 (b) shows the proposed FeFET-based CD-CiM array macro implementation based on the 2T1C cells. Multiple rows and columns can be activated simultaneously to compute in parallel.  Cell and array write operations. TABLE I shows the write setup, with an example of writing µ1¶ to M1 and µ0¶ to M2 in one cell. It has two phases (Phase 1 and Phase 2), similar to the methods in [12][20]. In TABLE I, VBL/VBLB is set to Vwrite/GND to write µ1¶ to M1 and µ0¶ to M2. In Phase 1, VWL and VWLB are connected to GND. In Phase 2, VWL and VWLB are driven concurrently to Vwrite for a period of time. An effective write of µ0¶ and µ1¶ occurs with VGS = –Vwrite and VGS = Vwrite, respectively. The write operations of one cell could be easily extended to an array. WL/WLB of the selected row are connected to GND in Phase 1 and Vwrite in Phase 2. WL/WLB of unselected rows are set to Vwrite/2, and BL/BLB is set to Vwrite to write µ1¶ for the selected FeFET, or GND to write µ0¶ for the selected FeFET. For unselected rows, |VGS| = Vwrite/2 is maintained to avoid state disturb.  Cell XNOR operation. Fig. 5 shows the XNOR logic in one cell. Initially, all bitlines and wordlines are set to GND. Then, ScL is left floating at GND. As mentioned above, either M1 or M2 has low resistance, so the internal node voltage VX is GND. Next, VWL/VWLB is set to VDD/GND and GND/VDD for an input pair of µ1/0¶ and µ0/1¶, respectively. Note that VDD is set lower than Vwrite to avoid FeFET state disturbance. With the complementary µ0¶ and µ1¶ storage within each cell, when WL or WLB biased at VDD is connected to an on-state FeFET, VX will be pulled up to VDD. Otherwise, VX remains GND. As ScL is floating, the change of VX is linearly delivered to the top plate of CM, i.e., ScL. Fig. 5 has illustrated VScL in two input scenarios (without considering the impact of parasitics capacitance): equal to VWL=VDD in Fig.5(a) for output µ1¶, and equal to VWLB=GND in Fig. 5(b) for output µ0¶. Fig. 6 shows a snapshot of transient simulation results, where both the voltage of bitlines and wordlines and FeFET polarization are included for XNOR outputs of µ1¶ and µ0¶. Operating non-idealities will be analyzed in Section III. Array MAC operation. Fig. 7 shows the array MAC operation with the shared ScL, BL/BLB, and WL/WLB between unary XNOR CiM cells. At the array-level MAC operation, the output VScL is driven by multiple XNOR cell outputs in each column. Note that cells in the same column have the same inputs with shared BL and BLB. Therefore, VScL will be lifted linearly as a function of the number of pulled-up cells. If M cells out of a total of N XNOR cells deliver µ1¶, VScL is shifted from GND to VDD*M/N. Given a mapping scheme between -1/+1 and GND/VDD, the pre-trained weights are stored as FeFET states in the array, and the input signals are set through the WL/WLB lines. For the convolutional layer, the weights of the same filter are stored in the same column. For the fully connected layers, the weights are loaded similarly. Every column performs MAC operations, then the nonlinear activation and the binary batch normalization operations are performed with the outputs presented at ScL. Wordlines (WL and WLB) and bitlines (BL and BLB) could be set to GND to make corresponding rows and columns inactive so as to support a smaller network. Large networks may also be supported by matrix splitting with several smaller CiM macros, as discussed in [23].  III. EVALUATION AND DISCUSSION This section evaluates the energy, area, and accuracy performance of the proposed 2T1C MAC CD-CiM macro against other existing techniques, in the presence of non-idealities. A. Benchmark Settings  The 2T1C cells are simulated using the calibrated FeFET SPICE model in [21], as shown in Fig. 3. This model has been adopted for prior circuit works. All MOSFETs are the 10nm PTM models [27]. In the benchmarking, CM is 1.2fF for all charge-domain solutions, adopted from [5]. We use RON = 10KΩ and ROFF = 1MΩ for RRAMs. The array size is set to 128x128 as a typical case. RC parasitic parameters are from [23]. As shown in recent reports, the SA may consume a significant portion of energy. For fair comparisons, the adopted SAs are based on those in [7].  TABLE  I: WRITE OPERATION CONFIGURATION  Target  example  Phases VScL VBL VBLB VWL VWLB M1: µ1¶;  \nM2: µ0¶ Phase1  GND  Vwrite  GND  GND  GND  Phase2  Vwrite  Vwrite  \n \nFig. 5.  Proposed XNOR logic of one cell: (a) µ1¶ output; (b) µ0¶ output.  \n \nFig. 6.  Transient waveforms of the 2T1C cell (settings see III.A).  \n \n \nFig. 7.  Proposed 2 -phase MAC operation (3 cells in a column example): (a) \ndischarge capacitors and keep ScL floating; (b) XNOR and accumulate.  ScL:GND ->VDDCMM1M2WL:VDDBL:GNDBLB:GNDxWLB:GND‘1’‘0’(a)ScL:GND -> GNDCMM1M2WL:VDDBL:GNDBLB:GNDxWLB:GND‘0’‘1’(b)\n0255075100Time (ns)-2020Write ‘1’Write ‘0’Input ‘1’Input ‘1’Input ‘0’Input ‘0’\n-20200.00.5M2 polarizationScLM1 polarizationBLBBLWLBWL0.01.00.01.00.01.01.00.0Voltage (V)P (μC/cm2)\nScLM1M2GNDGNDx\nM1M2xCMM1M2xGNDGND\nGNDGNDGNDGNDGND(a)M1M2WL1GNDx\nM1M2x\nM1M2xGNDWLB1\nGNDGND(b)WL2WLB2WL3WLB3µ1¶\nµ0¶µ0¶VDD/3GNDVDDCMCMCMCMCM B. Energy And Latency Evaluation 1) Theoretical Analysis In CD-CiM, most energy is consumed by capacitance charging. Fig. 8 shows the capacitor network model in the MAC operation of a column. During the charging process, the right-side plate of CM in Fig. 8 (a) is kept floating and the left-side plate, i.e., node X, is clamped to VDD or GND according to the XNOR result. The equivalent charging capacitor CEQ of a column could be calculated: 𝐶ாொൌ𝑀ൈሺ128െ𝑀ሻൈ𝐶ெ/128,                          (3) where M is the number of cells whose XNOR result is µ1¶. As observed, there is no charging load when all cells are delivering XNOR results of µ0¶ or µ1¶, as ScL is floating. The maximum charging load occurs when half cells are delivering µ1¶ (and the other half delivering µ0¶). Differently, in the SRAM-based CD-CiM design [5], CM is charged when the XNOR result is µ1¶, and is not charged when it is µ0¶. Fig. 8(c) compares the equivalent capacitor load, in which the proposed design at p1=0.5 is only half of the SRAM-based design. Here, p1 denotes the percentage of XNOR cells in a column that produce µ1¶. On average, CEQ of 2T1C design is only 33% of the SRAM-based design. Also, the 2T1C design consumes no idle power with the FeFET non-volatility, which also outperforms the SRAM-based design. For current-domain CiM solutions, the energy is consumed while settling down and sensing the bitline currents, along with maintaining the reference currents. Although a latch-style dynamic current-SA could be used, there still a trade-off between the current amplitude, supply and the operating frequency to minimize the energy.  2) Experimental Simulation To comply with the FeFET model, the supply voltage is set to 0.45V to avoid FeFET state disturbance. For other designs, an extra 0.90V supply is provided to investigate more options. In the evaluation, we set a clock cycle as the time window for each bitline and wordline controls, including the precharging and clamping, and a clock cycle for sensing and latching the outputs. Evaluations are done at 100MHz and 1.0GHz, each with custom optimizations, e.g. low or high VTH options.  The comparison of energy consumption with related works is shown in Fig. 9. This work achieves the highest energy efficiency. Compared with the current-domain solutions, the minimum improvement is 2.5x at 1.0GHz, and up to 24x at 100MHz in which more time is spent on bitline settling-down. Practically, the operating frequency could be limited by the influence of the PVT variations in current-sensing CiM. Compared with the SRAM-based CD-CiM, the energy efficiency improvement is 1.9x at 0.45V and 7.8x at 0.90V, which confirms the theoretical analysis above. CD-CiM evaluation results are not sensitive to the frequency unless one fails to reach the operation speed. For example, the SRAM-based CD-CiM fails to reach 1GHz at 0.45V.  C.  Precision Analysis 1) Theoretical Analysis and Array-Level Simulation The energy evaluation above has assumed no variation impact. However, in the current-mode sensing, variations could be playing a key role as the variations of currents directly affect the summed result. For the proposed CD-CiM, it is also important to investigate how the variations of FeFET and CM affect the overall computing accuracy. Intuitively, as FeFETs have a very large on-off ratio, the drain-source leakage current IOFF is negligible when compared with the on-state current ION. Therefore, the internal node X in each cell is well set at GND or VDD. Further, the non-ideality of the computation based on the charge re-distribution is determined by the CM capacitor mismatch, which affects the amount of charge re-distribution at the output ScL. Theoretically, the MAC result from (3) is reshaped as                𝑉ெ஺𝐶ൌ𝑉ௌ௖௅ൌ1∑𝐶೔ಿ೔సభ∑𝑉௑௜ൈ𝐶௜ே௜=1  ൌ௏஽஽∑𝐶೔ಿ೔సభൈቀ∑𝐶೔ൈோೀಷಷ೔ோೀಿ೔+ோೀಷಷ೔ெ௜=1൅∑𝐶೔ൈோೀಿ೔ோೀಿ೔+ோೀಷಷ೔ே௜=ெ+1ቁ.   (4) where 𝐶௜ is the capacitor of the ith cell,  𝑉௑௜ is VX of the ith cell, 𝑅ைே௜ is the on-state FeFET drain-source resistance of the ith cell, 𝑅ைிி௜ is the off-state FeFET drain-source resistance of the ith cell, N is the total number of cells in the MAC operation, and M is the number of cells whose XNOR result is µ1¶.   In the analysis, N is set to 128; CM is modeled as a Gaussian distribution with a mean value of 1.2fF. With the normalized standard CM deviation ıc between 1% - 5% and the on-off ratio set infinite, Fig. 10 shows the normalized standard deviation of VMAC. Considering all corners, the worst ıMAC occurs at {p1=0.5, ıc=5%} and is below 0.25%. This indicates a much smaller impact than the direct current summing errors caused by typical RRAM or MOSFET ION variations. Fig. 11 shows normalized MAC errors with a different on-off ratio, in which RON and ROFF are logarithmic Gaussian random variables with  \nFig. 8. ScL charging capacitance: (a) Model of proposed 2T1C design; (b) \nModel of prior SRAM -based design; (c) Comparison as a function of p1. \n \n \nFig. 9. MAC operations comparisons between diff erent  works in an array.  \n \n \nFig. 10.  Normalized standard deviation ıMAC vs p1. \n \nFig. 11.  Normalized MAC error vs p1 with ıC=5% and different on -off ratios.  \n VCM1CM2CM128VXNORresultµ1¶CM1CM2CM128(a) ProposedScLp1XNORresultµ0¶XNORresultµ0¶XNORresultµ1¶Prior SRAM-based [5]Proposed 2T1CEquivalent load capacitance (fF)\nScL(b) SRAM-based [2](c) Load capacitance comparisonCM = 1.2fF, N = 128\nx139x24x148x24x7.8x1.9x1x16x2.8x16x2.5x7.8x1110100100M Hz1G HzEnergy (pJ)\nSRAM current domain [25]RRAM (1T1R)SRAM charge domain [5]This work@ 0.9V@ 0.45V@ 0.45V@ 0.9V@ 0.45V@ 0.9V@ 0.45V\np1\np1p1\np1p1(a) Mean on-off ratio=1e2Normalized MAC Error\nNormalized MAC ErrorNormalized MAC Error\nNormalized MAC Error(b) Mean on-off ratio=1e3\n(c) Mean on-off ratio=1e4(d) Mean on-off ratio=1e5 a normalized standard deviation of 15% and ıC=5%. When the on-off ratio is over 105, the normalized MAC error has a chance of ~99.2% to be below that caused by flipping an XNOR cell. In contrast, with an on-off ratio around 102, which is a typical value for RRAM, the accumulated error could be so significant that the average normalized MAC error could be as high as 5%. This finding actually answers the fundamental question: why is it challenging to explore CD-CiM using RRAM and MTJ in the fourth quadrat in Fig. 1?  2) Application Simulation  The proposed FeFET-based CD-CiM macro is evaluated for classification applications while considering the impact of CM variations. The Pytorch framework is used to build the binary LeNet on the MNIST test set [11] and the binary NIN [10] on the CIFAR-10 test set. To evaluate the effect of the non-idealities of the core array, it is assumed that peripherals, such as the reference voltage generator, the SAs, and the quantization blocks, do not lower the overall accuracy.  Fig. 12 scatters the classification accuracy as a function of ıC. Ideally, XNOR-Net in [8] achieves ~ 99.0% classification accuracy on the MNIST test set and 85.6% classification accuracy on the CIFAR-10 test set. As shown in Fig. 12, as long as the capacitor mismatch is within a reasonable range of 20% for CIFAR-10 and 30% for MNIST, the classification accuracy is almost uncontaminated. In practical designs, this matching requirement could be used to guide the CM design given a specific technology for the optimized trade-off between the target accuracy, power consumption, and the layout area.  D. Area The proposed FeFET-based cell consists of two transistors and one capacitor. Because the capacitor can be placed on top of the transistors, the area overhead of the capacitor is significantly reduced. In contrast, the SRAM-based CD-CiM cell needs one capacitor and a total of 9 transistors, including 8 for XNOR cell and one extra transistor to connect the cell to ScL for accumulation in a MAC [5]. In addition, the 2 transistors in the proposed design are both n-type and could be placed in a more compact layout than SRAM transistors.   IV. CONCLUSIONS This paper has presented the concept and design of an NVM-based charge-domain computing-in-memory approach. A 2T1C XNOR CiM cell is proposed based on FeFET, a nonvolatile CMOS-compatible NVM device with an ultra-high on/off ratio. The array implementation for MAC CiM macro based on the proposed cell is presented and evaluated. Comparisons show higher density and lower power than prior current-domain and charge-domain CiM designs. Circuit and application evaluations have shown the potential of improving the performance and energy efficiency of BNN accelerators while achieving high accuracy. Acknowledgment The authors would like to thank Prof. Nan Sun for helpful discussions, and Prof. Sumeet Gupta and Kai Ni for model support. REFERENCES [1] W. A. Wulf et al., \"Hitting the memory wall: implications of the obvious,\" ACM SIGARCH computer architecture news, vol. 23, no. 1, pp. 20-24, 1995. [2] M. Peemen et al., \"Memory-centric accelerator design for Convolutional Neural Networks,\" 2013 IEEE 31st ICCD, pp. 13-19. [3] D. Bankman et al., \"An Always-On 3.8 ȝJ/86% CIFAR-10 Mixed-Signal Binary CNN Processor With All Memory on Chip in 28-nm CMOS,\" in IEEE JSSC, vol. 54, no. 1, pp. 158-172, Jan. 2019. [4] J. Yue et al., \"7.5 A 65nm 0.39-to-140.3 TOPS/W 1-to-12b unified neural network processor using block-circulant-enabled transpose-domain acceleration with 8.1× higher TOPS/mm 2 and 6T HBST-TRAM-Based 2D Data-Reuse Architecture,\" 2019 IEEE ISSCC, pp. 138-140.  [5] H. Valavi et al., \"A 64-Tile 2.4-Mb In-Memory-Computing CNN Accelerator Employing Charge-Domain Compute,\" IEEE Journal of Solid-State Circuits, vol. 54, no. 6, pp. 1789-1799, 2019. [6] X. Sun et al., \"XNOR-RRAM: A scalable and parallel resistive synaptic architecture for binary neural networks,\" 2018 DATE pp. 1423-1428 [7] S. Yu, et al., \"Compute-in-Memory with Emerging Nonvolatile-Memories: Challenges and Prospects,\" IEEE CICC 2020, pp. 1-4. [8] M. Rastegari et al., \"Xnor-net: Imagenet classification using binary convolutional neural networks,\" in ECCV, 2016: Springer, pp. 525-542. [9] S. Ioffe, et al., \"Batch normalization: Accelerating deep network training by reducing internal covariate shift,\" arXiv:1502.03167, 2015. [10] M. Lin, et al. \"Network in network.\" arXiv preprint arXiv:1312.4400.  [11] Y. LeCun et al., \"Gradient-based learning applied to document recognition,\" in Proceedings of IEEE, vol. 86, no. 11, Nov. 1998. [12] J. Wu et al., \"A 3T/Cell Practical Embedded Nonvolatile Memory Supporting Symmetric Read and Write Access Based on Ferroelectric FETs,\" in 56th Design Automation Conference (DAC'2019) [13] K. Ni et al., \"SoC Logic Compatible Multi-bit FeMFET Weight Cell for Neuromorphic Applications,\" in 2018 IEDM, pp. 13-2. [14] S.-Y. Wu, \"A new ferroelectric memory device, metal-ferroelectric-semiconductor transistor,\" in TED, vol. 21, no. 8, pp. 499-504, Aug 1974. [15] J. Jo and C. Shin, \"Negative Capacitance Field Effect Transistor with Hysteresis-Free Sub-60-mV/Decade Switching,\" in IEEE Electron Device Letters, vol. 37, no. 3, pp. 245-248, March 2016.  [16] J. Mller et al., \"Ferroelectricity in HFO2 Enables Nonvolatile Data Storage in 28 nm HKMG,\" in 2012 VLSIT, pp. 25–26. [17]  X. Li et al., \"Enabling Energy-Efficient Nonvolatile Computing with Negative Capacitance FET,\" TED, vol. 64, no. 8, pp. 3452- 3458, 2017.  [18] C. H. Cheng et al., \"Low-leakage-current DRAM-like memory using a one-transistor ferroelectric MOSFET with a hf-based gate dielectric,\" IEEE Electron Device Lett., 35(1):138–140, Jan 2014.  [19] J. Muller et al., \"Nanosecond Polarization Switching and Long Retention in a Novel MFIS-FET Based on Ferroelectric HfO2,\" in IEEE Electron Device Letters, vol. 33, no. 2, pp. 185-187, Feb. 2012. [20] D. Reis et al., \"Computing in Memory with FeFETs,\" in I6LPED ¶18. New York, NY, USA: ACM, 2018, pp. 24:1–24:6. [21] A. Aziz et al., \"Physics-Based Circuit-Compatible SPICE Model for Ferroelectric Transistors,\" in EDL, vol. 37, no. 6, pp. 805-808, June 2016 [22] S. Salahuddin, et al., \"The Era of Hyper-Scaling in Electronics, \" Nature Electronics, vol. 1, no. 8, pp. 442–450, 2018. [23] X. Chen et al., \"Design and optimization of FeFET-based crossbars for binary convolution neural networks,\" 2018 DATE, pp. 1205-1210. [24] J. Choi, et al., \"Accurate and efficient 2-bit quantized neural networks,\" in Proc. Conf. Syst. Mach. Learn. (SysML), 2019. [25] A. Agrawal et al., \"Xcel-RAM: Accelerating Binary Neural Networks in High-Throughput SRAM Compute Arrays,\" in IEEE TCAS-I, vol. 66, no. 8, pp. 3064-3076, Aug. 2019. [26] S. K. Thirumala et al., \"Nonvolatile Memory utilizing Reconfigurable Ferroelectric Transistors to enable Differential Read and Energy-Efficient In-Memory Computation,\" 2019 IEEE/ACM ISLPED, pp. 1-6. [27] \"Predictive technology model,\" [Online] Available: ptm.asu.edu. [28] Z. Jiang, et al., \"C3SRAM: An In-Memory-Computing SRAM Macro Based on Robust Capacitive Coupling Computing Mechanism,\" in JSSC,  vol. 55, no. 7, pp. 1888-1897, July 2020. [29] M. Lee et al, \"FeFET-based low-power bitwise logic-in-memory with direct write-back and data-adaptive dynamic sensing interface,\" in ACM/IEEE International Symposium on Low Power Electronics and Design (ISLPED'20), August 10–12, 2020, Boston, MA, USA.  \nFig. 12.  Impact of CM mismatch on classification.  \n ",
      "metadata": {
        "filename": "Enabling Lower-Power Charge-Domain Nonvolatile In-Memory Computing with Ferroele.pdf",
        "hotspot_name": "Standby_Power_Consumption",
        "title": "Enabling Lower-Power Charge-Domain Nonvolatile In-Memory Computing with\n  Ferroelectric FETs",
        "published_date": "2021-02-02T11:32:21Z",
        "pdf_link": "http://arxiv.org/pdf/2102.01442v1",
        "query": "electronic device standby power reduction energy efficient design methods"
      }
    },
    "Cyber-physical Control of Road Freight Transport": {
      "full_text": "1\nCyber-physical Control of Road Freight Transport\nB. Besselink, V . Turri, S.H. van de Hoef, K.-Y . Liang, A. Alam, J. M ˚artensson, K.H. Johansson\nAbstract —Freight transportation is of outmost importance for\nour society and is continuously increasing. At the same time,\ntransporting goods on roads accounts for about 26% of all\nenergy consumption and 18% of all greenhouse gas emissions\nin the European Union. Despite the inﬂuence the transportation\nsystem has on our energy consumption and the environment,\nroad transportation is mainly done by individual long-haulage\ntrucks with no real-time coordination or global optimization. In\nthis paper, we review how modern information and communica-\ntion technology supports a cyber-physical transportation system\narchitecture with an integrated logistic system coordinating ﬂeets\nof trucks traveling together in vehicle platoons. From the reduced\nair drag, platooning trucks traveling close together can save about\n10% of their fuel consumption. Utilizing road grade information\nand vehicle-to-vehicle communication, a safe and fuel-optimized\ncooperative look-ahead control strategy is implemented on top of\nthe existing cruise controller. By optimizing the interaction be-\ntween vehicles and platoons of vehicles, it is shown that signiﬁcant\nimprovements can be achieved. An integrated transport planning\nand vehicle routing in the ﬂeet management system allows both\nsmall and large ﬂeet owners to beneﬁt from the collaboration.\nA realistic case study with 200 heavy-duty vehicles performing\ntransportation tasks in Sweden is described. Simulations show\noverall fuel savings at more than 5% thanks to coordinated\nplatoon planning. It is also illustrated how well the proposed\ncooperative look-ahead controller for heavy-duty vehicle platoons\nmanages to optimize the velocity proﬁles of the vehicles over a\nhilly segment of the considered road network.\nI. I NTRODUCTION\nThe freight transportation sector is of great importance to\nour society and the demand for transportation is strongly\nlinked to economic development. As a result of the growing\nworld economy, the road freight transportation sector in the\nOECD in 2050 is projected to have grown by roughly 90%\nwith respect to 2010 levels, according to a prediction of the\nInternational Transport Forum [40]. For developing countries,\na signiﬁcantly larger growth is expected. At the same time,\nthe transportation sector is responsible for a large part of\nthe world’s energy consumption and (greenhouse gas) emis-\nsions. As an example, in 2012, the road transportation sector\namounted for 26% of the total energy consumption and 18% of\nall greenhouse gas emissions in the European Union [15]. This\nimpact on the environment provides a strong motivation for\ndeveloping a more fuel-efﬁcient freight transportation sector,\nwhich is further encouraged by the fact that about one third\nof the cost of operating a heavy-duty vehicle is associated to\nits fuel consumption [45].\nB. Besselink, V . Turri, S.H. van de Hoef, K.-Y . Liang, J. M ˚artensson,\nand K.H. Johansson are with the ACCESS Linnaeus Centre and the Depart-\nment of Automatic Control, School of Electrical Engineering, KTH Royal\nInstitute of Technology, Stockholm, Sweden. Email: bart.besselink@ee.kth.se,\nturri@kth.se, shvdh@kth.se, kyliang@kth.se, jonas.martensson@ee.kth.se,\nkallej@kth.se.\nK.-Y . Liang and A. Alam are with Scania CV AB, S ¨odert ¨alje, Sweden.\nEmail: assad.alam@scania.com.\nFigure 1. Four heavy-duty vehicles in a platoon.\nModern information and communication technologies en-\nable such development, as the use of vehicle-to-vehicle and\nvehicle-to-infrastructure communication and the availability\nof ubiquitous computing power allow for the real-time co-\nordination and automatic control of large groups of vehi-\ncles. In particular, the formation of groups of closely-spaced\nheavy-duty vehicles allows them to cooperatively reduce fuel\nconsumption through a reduction in aerodynamic drag, see\nFigure 1. Experiments have shown that these platoons can\nlead to fuel savings of about 10% [11], [4]. Consequently,\na cooperative approach offers great potential for developing\na more efﬁcient road freight transportation sector, especially\nsince road transportation is currently mainly done by individ-\nual long-haulage vehicles that do not exploit the beneﬁts of\nplatoon formation.\nIn this paper, we present a cyber-physical approach to the\ncontrol and coordination of a large ﬂeet of heavy-duty vehicles\nthat exploits the beneﬁts of platooning, see Figure 2. In\nparticular, a three-layer architecture is proposed that supports\na hierarchical approach towards the minimization of the total\nfuel consumption. The bottom layer in this architecture deals\nwith the automatic control of individual heavy-duty vehicles\nand is referred to as the vehicle layer. This vehicle control\nexploits vehicle-to-vehicle communications and advanced sen-\nsor technology to achieve a stable and safe platoon formation,\nleading to a reduction in fuel consumption through reduced\naerodynamic drag. The middle layer, referred to as the co-\noperation layer, achieves additional fuel savings through the\ncomputation of fuel-optimal vehicle trajectories for the entire\nplatoon. In addition, the formation of platoons is addressed\nin this layer through local decision-making and the execution\nof merging maneuvers. Finally, the ﬂeet layer (i.e., the top\nlayer) is aimed at the coordination of a potentially largearXiv:1507.03466v1  [cs.SY]  13 Jul 20152\nFigure 2. An illustration of a cyber-physical approach to road freight transport\nwith large-scale optimization of vehicle ﬂeets and platoons.\nﬂeet of vehicles belonging to multiple ﬂeet owners. Here, the\nminimization of the fuel consumption is pursued by updating\nthe plans of individual vehicles in order to achieve the most\nsuitable platoon conﬁgurations.\nThrough the careful layering of the architecture, it is pos-\nsible to signiﬁcantly optimize the overall system performance\nwhile keeping the complexity at a manageable level. The\ntight integration of system components through vehicle-to-\nvehicle and vehicle-to-infrastructure communications as well\nas advanced on-board computations linked to cloud computa-\ntions makes the road freight transportation system an excellent\nexample of how major progress for such infrastructure appli-\ncations is possible largely thanks to recent developments in\ncyber-physical systems.\nThe remainder of this paper is outlined as follows. Section II\ndiscusses the opportunities cyber-physical systems bring to\nfreight transport and automated driving. Enabling information\nand communication technologies are reviewed and the pro-\nposed freight transport architecture is introduced. From the\nextensive amount of related literature on intelligent transporta-\ntion systems and vehicle platooning, a small subset of the most\nrelevant work is being treated in this section as well. The three\nlayers of the freight transport architecture are presented in the\nnext three sections. Section III introduces the vehicle layer\nincluding the heavy-duty vehicle model, the vehicle control\narchitecture, and the control strategy for vehicle platooning.\nThe topics of look-ahead control for platooning and the coor-\ndination and control of merging maneuvers aim at optimizing\nthe cooperative behavior of vehicle platoons and are treated\nin the cooperation layer discussed in Section IV. The ﬂeet\nmanagement layer is presented in Section V and coordinates\nthe platoon planning and execution. Section VI presents an\nevaluation of the freight transportation system through of a\ncase study and is followed by the conclusions in Section VII.\nII. C YBER -PHYSICAL SYSTEMS OPPORTUNITIES\nA. Enabling technologies\nTremendous advances in computing, communication, and\nsensor technologies have enabled the current rapid develop-Fleet layer\nCooperation layer\nVehicle layer\nFigure 3. Layered freight transport system architecture.\nment of intelligent transport systems [1]. Today’s high-end\nroad vehicles are typically equipped with extensive computing\ncapabilities, multiple radio interfaces, and radar, camera and\nother sensor devices. Low-cost wireless local and wide area\nnetwork transceivers facilitate vehicle-to-vehicle and vehicle-\nto-infrastructure communications [20], [28]. By integrating\nvehicular communication with existing sensor technologies,\napplications that enhance safety, efﬁciency, and driver comfort\nare being developed.\nAnother set of technologies that support cooperative trans-\nportation systems is given by cloud computing and service\narchitectures [6]. They offer large computing and storage\ncapabilities together with a seamless integration of a diverse\ngroup of third-party tools and services. For vehicular and\ntransportation applications, new possibilities are emerging to\nbuild systems spanning over large geographic areas with close\nto real-time data gathering and decision making [59]. Vehicular\nposition and velocity data are an important example of such\ndata that are readily available through various sensing devices\nincluding mobile phones [22]. Such data have proven to be\nvery useful in many contexts including the understanding of\nroad usage patterns in urban areas [58]. For freight trans-\nportation it is shown in this paper how trafﬁc, weather and\nother public and private data can be utilized in a transport\nplanning and logistics application implemented through cloud\ntechnologies. The overall functional architecture of such a\nsystem is described next.\nB. Freight transport system architecture\nThe freight transportation system discussed in this paper\nintegrates potentially thousands of heavy-duty vehicles into a\nlarge-scale planning, cooperation, and real-time optimization\nand automation system. It is truly a complex and large-scale\nsystem built upon existing and emerging communication and\ncomputing infrastructures into a tightly coupled cyber-physical\nsystem with many human and social components. In order\nto manage the complexity of this large-scale coordination\nproblem, the layered architecture in Figure 3 is naturally\nadopted. Herein, the control of individual vehicles is addressed\nin the vehicle layer, whereas the cooperation layer targets the3\nFigure 4. Automatic control of a merging maneuver, where a single vehicle\nis about to merge with a three-vehicle platoon after a road intersection. The\noperation is supported by vehicle-to-infrastructure communication.\nbehavior and formation of platoons of vehicles. The large-scale\ncoordination of vehicle ﬂeets is handled in the ﬂeet layer.\nSpeciﬁcally, the vehicle layer builds upon existing vehicle\ncontrol systems to achieve the desired longitudinal behavior as\nneeded to safely and automatically operate vehicles and vehi-\ncle platoons. Hereto, a decentralized controller is synthesized\nthat exploits vehicle-to-vehicle communication and advanced\nsensor information (e.g., radar) to guarantee the tracking of\na speciﬁed inter-vehicular distance as well as the rejection\nof disturbances. We recall that the operation of vehicles in\nclosely-spaced platoons reduces fuel consumption.\nThe aim of the cooperation layer is twofold. First, it\ncomputes fuel-optimal velocity proﬁles for vehicle platoons\ntaking road topography and trafﬁc into account. For example,\nby exploiting look-ahead information about the road topogra-\nphy when driving over hilly terrain, braking can be avoided\nand additional fuel savings can be obtained. Second, the\ncooperation layer locally coordinates the behavior of vehicles\nor platoons with overlapping route segments by deciding\nwhether neighbouring vehicles should form a platoon. In\naddition to this decision-making process, the optimal control\nof merging maneuvers for platoon formation is handled in\nthis layer, as illustrated in Figure 4. Vehicle-to-vehicle and\nvehicle-to-infrastructure communication are exploited for this\ncoordination, which extend only to the relative vicinity of the\nvehicle and platoon.\nThe ﬂeet layer targets the large-scale coordination over a\nsigniﬁcant geographic area for a large group of vehicles from\npotentially different ﬂeet owners, see Figure 2. By updating the\nroutes and transport plans of individual vehicles, the formation\nof platoons can be encouraged and the total fuel consumption\nof the ﬂeet can be minimized. In addition to this coordination,\nthe ﬂeet layer includes the task of transport planning to target\na better utilization of the capacity of the freight transport\nsystem. Optimization criteria in this layer can incorporate not\nonly costs directly associate with individual ﬂeet owners, but\ncan include societal aspects such as trafﬁc congestion and\nenvironment impact.The layers in Figure 3 are presented in some more detail\nin Sections III to V and the overall system is evaluated in a\ncase study in Section VI, but ﬁrst we give a brief overview of\nrelated work.\nC. Background on vehicle platooning\nThe freight transport architecture in Figure 3 is motivated\nby the concept of an automated highway system [57], [24],\nin which cars are organized in platoons to increase trafﬁc\nﬂow. Further examples of such systems are given in [43]\nand [54]. The layers in these architectures typically range\nfrom vehicles in the bottom layer to a road network in the\ntop layer. Our architecture focuses on heavy-duty vehicles\nand aims at optimizing the transportation of goods rather than\ntrafﬁc ﬂow in general. We note that similar architectures are\nalso used in many related engineering systems, such as air\ntrafﬁc management [60] and spacecraft formation [7].\nThe idea of highway automation and platooning has a long\nhistory, with ﬁrst visions dating back at least to the 1930s [18].\nApart from early analysis of the dynamics of vehicle fol-\nlowing [12], the ﬁrst control strategies for vehicle platooning\nappeared in [33] and [37], [13]. Many results have appeared\nsince, focusing on topics ranging from analysis of spacing\npolicies [26], [53] to experimental validation [39]. For heavy-\nduty vehicles, platooning is mainly motivated by a reduced\nfuel consumption and several experimental evaluations have\nfocussed on this aspect [11], [31], [3].\nThe operation of platoons relies on the (partial) automa-\ntion of heavy-duty vehicles. Large research efforts are being\nundertaken in the development of fully autonomous vehicles,\nof which an early prototype is discussed in [14]. Several chal-\nlenges organized by DARPA have further spurred development\nin this area [2], whereas a recent overview is given in [9].\nIII. V EHICLE LAYER\nA. Vehicle model\nThe heavy-duty vehicle control and cooperation algorithms\nare based on a dynamic model of the powertrain. Speciﬁcally,\nthe longitudinal dynamics of a vehicle indexed iis modeled as\n_si=vi;\nm_vi=\u0000Fr(\u000b(si))\u0000Fg(\u000b(si))\u0000Fd(\u001ci;vi)\n+Fe;i\u0000Fb;i:(1)\nHere,siandvidenote its longitudinal position and velocity,\nrespectively, which are collected in the state xi= (si;vi)T.\nFor ease of exposition, we let all vehicles have identical\nparameter values, but the results in the paper extend directly to\nheterogeneous vehicle groups. In (1), mrepresents the vehicle\nmass, whereas FrandFgdenote the rolling resistance and the\nlongitudinal component of gravity, respectively. The latter is\ngiven as\nFg(\u000b(si)) =mgsin\u000b(si); (2)\nwith\u000b(si)the road gradient at position siandgthe gravita-\ntional acceleration. The aerodynamic drag Fdsatisﬁes\nFd(\u001ci;vi) =1\n2cd(\u001ci)\u001aAv2\ni; (3)4\n0 1 2 30:20:40:6\n\u001ci[s]cd[-]\nFigure 5. Air drag coefﬁcient cd(\u001ci)as a function of time gap \u001cifor\nc0\nd= 0:6,\u000b1= 0:53, and\u000b2= 0:81. The function is estimated based\non experimental data (circles) reported in [25].\nwhere\u001ais the air density and Adenotes the frontal area of\nthe vehicle. The air drag is dependent on the time gap \u001ci\nbetween vehicle iand its predecessor, as captured through the\nair drag coefﬁcient cd(\u001ci). Here, the time gap represents the\ntime difference between two successive vehicle passing the\nsame point on the road. The air drag coefﬁcient is modeled as\ncd(\u001ci) =c0\nd\u0012\n1\u0000\u000b1\n1 +\u000b2\u001ci\u0013\n; (4)\nwherec0\ndrepresents the nominal air drag coefﬁcient for a\nheavy-duty vehicle driving alone, and the parameters \u000b1and\n\u000b2characterize the air drag reduction as the time gap between\nvehicles decreases. Figure 5 shows an illustration of cd(\u001ci)\nas estimated from experimental data. This air drag reduc-\ntion obtained through smaller inter-vehicular distances offers\na potential for saving fuel, which is extensively exploited\nthroughout the paper.\nFinally, the forces Fe;iandFb;iin (1) denote the traction\nforce at the wheels and the force exerted by the brakes, re-\nspectively. They are control inputs. The corresponding injected\nfuel ﬂow'idepends on the instantaneous power Fe;ivi, which\nis bounded as Pmin\u0014Fe;ivi\u0014Pmax, and obtained through\n'i=p1Fe;ivi+p0: (5)\nHere, the parameters p0andp1aggregate the effects of engine\nand gear box efﬁciency. Speciﬁcally, p0captures the fuel\nﬂow when the engine is idling. Consequently, the nominal\nfuel consumption (normalized with respect to the travelled\ndistance) of a single vehicle reads\nJnom;i=1\nsi(t0)\u0000si(t1)Zt1\nt0'i(t) dt; (6)\nfor any time interval satisfying t1\u0000t0>0. The remainder\nof this paper will be focused on systematically reducing the\nnominal fuel consumption by exploiting platooning.\nB. Vehicle control architecture\nThe vehicle control architecture for the powertrain is de-\npicted in Figure 6. A controller area network [27] inside the\nvehicle communicates radar and positioning data together with\ndata from other vehicles through the wireless sensor unit toRadar\nGPS\nWSUEMS\nBMS\nGMSCAN bus\nVehicle\ncontrollerData\nprocessing\nFigure 6. A controller area network enables the communication of sensor data\nto the vehicle controller, which computes control commands to be executed\nby the engine, braking, and gear management systems.\nBMS EMS GMSVehiclei\u00001\nplatoon controller\nBMS EMS GMSVehiclei\nplatoon controllerCooperative look-ahead controlVehicle layer\nFigure 7. Control architecture corresponding to the vehicle layer in Figure 3.\na data processing unit. The vehicle controller computes low-\nlevel commands and sends them to the engine management\nsystem, brake management system, and gear management\nsystem. These systems implement the desired longitudinal\nvehicle behavior. Automatic velocity control is often achieved\nby letting the vehicle controller execute cruise controller or\nadaptive cruise controller algorithms. A cruise controller uses\nmeasurements of the vehicle speed to maintain a constant\nreference velocity in order to improve fuel economy and driver\ncomfort. The adaptive cruise controller is an extension that\nincludes radar information to obtain an estimate of the position\nand velocity of a preceding vehicle, improving safety and\nconvenience.\nNext, an alternative vehicle controller is presented that\nexploits additional information about the preceding vehicle\nobtained through wireless communication. By sharing infor-\nmation, automatic control of small inter-vehicular distances\nwith guaranteed safety can be achieved.\nC. Vehicle control for platooning\nThis section presents a strategy for the longitudinal control\nof heavy-duty vehicle platoons. It is positioned in the vehicle\nlayer of the freight transport architecture in Figure 3 and is\ndetailed in Figure 7. The objective of the platoon controller\nis to achieve small inter-vehicular distances while tracking\na varying reference velocity vref(\u0001). The reference velocity,\nwhich is speciﬁed as a function of the position on the road, is5\nthe result of the cooperative look-ahead control strategy that\nis discussed in Section IV-A.\nAs it is well-known that standard policies for specifying the\ninter-vehicular distance in a platoon are not compatible with\ntracking a spatially varying reference velocity proﬁle [5], [10],\nwe adopt the delay-based spacing policy\nsref;i(t) =si\u00001(t\u0000\u001cref); (7)\nwheresref;idenotes the desired longitudinal position of vehicle\ni. It is convenient to express (7) in the spatial domain. To this\nend, let the spatial position sbe the independent variable and\ndenoteti(s)as the time instance at which vehicle ipassess.\nBy introducing the time gap tracking errors\n\u0001i(s) =ti(s)\u0000ti\u00001(s)\u0000\u001cref; (8)\n\u00010\ni(s) =ti(s)\u0000t0(s)\u0000i\u001cref; (9)\nthe policy (7) is equivalent to \u0001i= 0. The condition (9)\nrepresents the time gap tracking error with respect to the ﬁrst\nvehicle in a platoon. Similarly, a velocity tracking error eican\nbe deﬁned for each vehicle, representing the deviation from\nthe desired reference velocity proﬁle vref(s). On the basis of\nthe time gap and velocity tracking errors, a weighted error\nsignal is introduced as\n\u000ei(s) = (1\u0000h0)\u0001i(s) +h0\u00010\ni(s) +hei(s); (10)\nin which the parameters 0\u0014h0<1andh > 0provide\na measure of the inﬂuence of the lead vehicle and velocity\ntracking, respectively.\nA distributed controller design can be achieved on the basis\nof the weighted error signal (10) and powertrain dynamics (1),\nhereby satisfying two objectives. First, the controller for vehi-\ncleishould guarantee the existence of a unique equilibrium\npoint for which \u000ei= 0 and, second, it should asymptotically\nstabilize this equilibrium. Namely, any controller that achieves\nthis ensures asymptotic stability of the desired spacing policy\n(7) throughout the platoon. A controller based on feedback\nlinearization that achieves these objectives is given in [10]. It\nis stressed that, as each vehicle individually addresses the local\ngoal of achieving \u000ei!0, the controller is distributed. Herein,\nvehicles exploit radar measurements as well as information\nfrom the preceding vehicle and (potentially) the lead vehicle\nobtained through wireless communication.\nFor any controller that asymptotically stabilizes the equilib-\nrium corresponding to \u000ei= 0, it can be shown that the velocity\ntracking errors of two successive vehicles satisfy\nZs\n0jei(\u001b)j2d\u001b\u0014Zs\n0jei\u00001(\u001b)j2d\u001b; (11)\nwhich indicates that any perturbations do not grow as they\npropagate through the platoon. The inequality is strict when\ninformation of the lead vehicle is included, i.e., h0>0, which\nalso ensures robustness with respect to external disturbances\nacting on the vehicles, see [10] and related work in [47].\nCondition (11) is an example of string stability, which provides\nstability notions for vehicle platoons. An early notion of string\nstability can be found in [41], whereas a formal deﬁnition is\ngiven in [52]. For an overview and examples of alternative\ndeﬁnitions, see [42] and [17], [49], respectively.IV. C OOPERATION LAYER\nA. Cooperative look-ahead control\nThe aim of the cooperative look-ahead control strategy is\nto compute a velocity proﬁle vref(\u0001)that is feasible for each\nindividual heavy-duty vehicle in the platoon and fuel-optimal\nfor the overall platoon. The speed proﬁle is communicated\nto the vehicle layer, as described in Figure 7, where each\nvehicle controller tracks vref(\u0001)while guaranteeing stability and\nsafety. The computation of the speed proﬁle is accomplished\nby solving a receding horizon control problem that includes the\ndynamics and corresponding constraints of each vehicle and\nminimizes a cost function depending on the fuel consumption\nof the whole platoon. To this end, the vehicle model (1) is\nexpressed in the spatial domain as is the constraint that all\nvehicles in the platoon track the same velocity proﬁle:\nvi(s) =vref(s); i= 1;:::;N; (12)\nwhereNis the number of vehicles in the platoon. Note that\nthe delay-based spacing policy (7) also requires equal velocity\nproﬁles in the spatial domain, which therefore corresponds to\nthe constraint (12). Moreover, as the road altitude is dependent\non the position, this policy is well-suited for platooning over\nroad segments with varying topography [55].\nThe cost function for the cooperative look-ahead controller\nto minimize is deﬁned as the sum of the fuel consumption for\nall vehicles in the platoon:\nJCLAC=1\nNHNX\ni=1Zs0+H\ns0'i(s)1\nvref(s)ds; (13)\nwhere'iis the injected fuel ﬂow (5) (expressed in the spatial\ndomain),s0the current position of the leading vehicle (i.e.,\ns1(t)) andHthe horizon length. The average speed request\nfor a given road segment as imposed by the ﬂeet management\nlayer is denoted by \u0016vand is enforced through the constraint\n1\nHZs0+H\ns01\nvref(s)ds=1\n\u0016v: (14)\nThe cooperative look-ahead controller is implemented with\na receding horizon and can be summarized as follows:\nmin Platoon fuel consumption (13)\nsubj. to Vehicle dynamics (1) (in the spatial domain),\nConstraints on state and input,\nConstraint on the average velocity (14),\nCommon platoon velocity (12) :\nHere, the constraints on state and input refer to the speed limits\nas well as the bounds on engine power and braking force.\nThe receding horizon problem can be solved using dynamic\nprogramming [8], see [55] for details. In the special case that\nthe platoon consists of only N= 1 vehicle, the proposed\nplatoon controller corresponds to the single-vehicle look-ahead\ncontroller [21].\nAltitude variations have a signiﬁcant impact on the behavior\nof heavy-duty vehicles. Due to their inertia and limited engine\npower, they are typically not able to maintain a constant veloc-\nity while driving over steep up-slopes and down-slopes. This6\nss\n1ts\n1\nss\n2ts\n2tm\nsmtf\nsf\nFigure 8. Schematic illustration of a two-vehicle optimal merging problem.\neffect is critical when a group of vehicles, that can signiﬁcantly\ndiffer in mass and powertrain characteristic, needs to maintain\nthe short inter-vehicular distances required by platooning.\nExperimental results have for instance shown how follower\nvehicles in a platoon driving downhill need to brake in order\nto compensate their different inertia and experienced air drag\nforce [3]. Therefore, the particular structure of the cooperative\nlook-ahead controller proposed here with common velocity\nproﬁles (12) seems to have several advantages [55]. Earlier\nwork on look-ahead control for the fuel-efﬁcient traversal of\nhilly road segments has focussed on single vehicles only, with\nearly work considering simple road proﬁles and exploiting\nanalytical solutions [46], [50]. Algorithms based on dynamic\nprogramming suitable for more generic road proﬁles have also\nbeen proposed [23], [38], [21].\nB. Optimal control of merging maneuvers\nLet us now focus on the formation of platoons through the\nmerging of individual vehicles or platoons that approach a\ncommon point after a highway intersection or an on-ramp.\nThis maneuver is essential for platoon formation and it enables\nthe high-level coordination of platoons as will be discussed in\nSection IV-C.\nConsider the simple merging problem for two vehicles\ni= 1;2illustrated in Figure 8. Here, smdenotes the location\nof an intersection and ss\n1,ss\n2the positions on two road\nsegments from which the merging maneuver is initiated. The\ntimests\n1andts\n2at which the vehicles arrive at these positions\nare taken as the starting times for the merging maneuver, for\nwhich the initial states xs\ni= (ss\ni;vs\ni)T,i= 1;2, hold for some\nvelocityvs\ni. A common ﬁnal state xf= (sf;vf)Tand timetf\nis chosen after the intersection to obtain the desired average\nvelocity over the road segment. Suppose the vehicles merge\nto form a platoon at smat timetm, so that approximately\nx1(t) =x2(t);8t2[tm;tf]: (15)\nThe merging time tmis not ﬁxed a priori, but is the result of an\noptimization. Due to a reduced aerodynamic drag, the vehicle\ndynamics and the total fuel cost is obviously different after the\nmerging point compared to before. Therefore, the total fuel\nconsumption for the overall operation can be expressed as\n2X\ni=1Ztm\nts\ni'i(t) dt+Ztf\ntm2X\ni=1'i(t) dt: (16)\nThis cost function can be minimized subject to the dynamics\n(1) using a two-step hybrid optimal control approach [51],CLAC\n CLAC\nControl of merging maneuversOpportunistic platoon formationCooperation layerFleet layer\nFigure 9. Platoon coordination architecture according to the cooperation layer\nin Figure 3. The lower blocks correspond to the cooperative look-ahead control\nof the platoons linked to the vehicle layer in Figure 7.\n[48], as detailed in [30]. In the ﬁrst step, after selecting a\nﬁxed merging time tm, the problem reduces to the fuel-optimal\ntraversal of a given road segment. The partitioning of the\ntotal cost in (16) corresponds exactly to these road segments.\nFor the last road segment traversed as a platoon, the platoon\ndynamics satisfy the constraint (15). In the second step, an\noptimization of the merging time tmis performed. When this\nprocess is repeated iteratively, the optimality of the overall\nproblem can be guaranteed through the hybrid maximum\nprinciple [51], which is an extension of the Pontryagin max-\nimum principle. The two-vehicle merging problem discussed\nhere is easily extended to cases in which the optimization\nincludes more vehicles, constraints on the desired velocity\nat the merging instant, and successive merging maneuvers.\nMoreover, a receding horizon implementation of the optimal\nmerging procedure can be used to guarantee robustness with\nrespect to disturbances such as the inﬂuence of surrounding\ntrafﬁc. These extensions can be found in [30].\nC. Opportunistic platoon formation\nIn the previous discussion on the optimal control of merging\nmaneuvers, the decision on forming a platoon had already been\nmade. Next we discuss how such a decision-making process\ncan take place and how an opportunistic platoon formation\nﬁts into the cooperation layer according to Figure 9. The aim\nof the opportunistic platoon formation is to decide whether it\nis fuel-efﬁcient to form a platoon with a nearby heavy-duty\nvehicle and, if so, determine where the merge should take\nplace to maximize the fuel savings.\nA pairwise platoon formation strategy is proposed. Let s0\n1,\ns0\n2denote the initial positions of a pair of vehicles and sf\ntheir common destination. The decision on whether to form\na platoon will be based on the computation of the optimal\nmerging point sm. Contrary to the detailed merging maneuver\nin the previous section, the current platoon formation scenario\nis performed over a potentially large geographical region and7\nlarge distances. As a result, vehicle dynamics can be neglected\nand constant vehicle velocities \u0016viand platoon velocity \u0016vpare\nassumed. This assumption additionally implies that no detailed\nroad topography information is needed for this decision-\nmaking. Recall that the cooperative look-ahead controller is\nsupposed to guarantee such average velocities even over roads\nwith varying topography, whereas the merging controller will\nexecute the actual merging maneuvers when the vehicles are\nsufﬁciently close.\nThe optimal fuel cost of forming a platoon will be compared\nto the fuel consumption of the two vehicles driving to their\ndestination independently. As a result, only the effect of\naerodynamic drag has to be considered and the average fuel\nﬂows follow from (3) and (5) as\n\u0016'i=1\n2p1c0\nd\u001aA\u0016v3\ni+p0; i2f1;2g; (17)\n\u0016'p=1\n2p1\u0000\nc0\nd+cd(\u001cref)\u0001\n\u001aA(\u0016vp)3+ 2p0: (18)\nHere, \u0016'igives the fuel ﬂow of a vehicle without a predecessor\nas captured through the nominal air drag coefﬁcient c0\nd, while\n\u0016'pis the fuel ﬂow of the two-vehicle platoon. Obviously, \u0016'p<\n\u0016'1+ \u0016'2. The corresponding fuel cost now reads\n\u0016JOPF=2X\ni=1\u0016'ism\u0000s0\ni\n\u0016vi+ \u0016'psf\u0000sm\n\u0016vp; (19)\nin which the merging point smcan be expressed as\nsm=\u0016v2s0\n1\u0000\u0016v1s0\n2\n\u0016v2\u0000\u0016v1: (20)\nThen, the fuel-optimal platoon formation problem can be\nstated as\nmin Total average fuel consumption (19),\nsubj. to Constraint on the merging point (20),\nConstraints on the average velocity,\nin which the constraints on the average velocity are such that\nthe platoon formation does not lead to a delayed arrival at the\nﬁnal destination sfnor that road speed limits are violated. If\n\u0016J\u0003\nOPFdenotes the optimal solution, then a platoon is formed\nbetween the considered two vehicles if this total fuel cost is\nless than that of the two vehicles driving independently, i.e.,\n\u0016J\u0003\nOPF<2X\ni=1\u0016'isf\u0000s0\ni\n\u0016vnom;i; (21)\nwith \u0016vnom;ithe nominal average velocity of vehicle i. Details\non this opportunistic platoon formation can be found in [35],\nwhereas an alternative heuristic approach is given in [32].\nV. F LEET MANAGEMENT LAYER\nA. Fleet management architecture\nThe ﬂeet management layer handles transport planning,\nrouting and coordination, as detailed in Figure 10. Transport\nplanning amounts to distributing the ﬂow of goods over the\navailable vehicles in the ﬂeet. This is a logistics problem in\nwhich the available resources are managed to meet customer\nrequirements. The assignment of goods to vehicles is opti-\nmized by combining similar assignments to the same vehicle.Transport planning\nRouting\nCoordinated platoon planningFleet layer\nCooperation layer\nFigure 10. Fleet management architecture according to the ﬂeet layer in\nFigure 3.\nSize, weight, and type of cargo must be considered. The\navailability of drivers and the drivers’ legal resting times are\nother parameters that should be regarded.\nRouting is the process of ﬁnding the most suitable path\nfrom the origin to the destination. In our setting the aim is to\nﬁnd the most fuel-efﬁcient route. The topography of the road\nhas a large inﬂuence on the fuel consumption, in particular\nfor heavy-duty vehicles. The trafﬁc conditions, estimated from\nhistoric and real-time data, and current and predicted weather\nshould also be taken into account. Equally important is the\nreliability of the plan, as accurate predictions of the time\nof arrival and of the corresponding fuel consumption are\nessential.\nThe platoon coordination jointly adjust the motion along the\nvehicles’ paths. Of particular interest is the ability to adjust\nthe velocity proﬁles in order to form fuel-efﬁcient platoons. A\nprocedure for such coordinated platoon planning is described\nin the following section.\nB. Coordinated platoon planning\nThe modern communication infrastructure allows for the\nfusion of real-time position, velocity, and assignment informa-\ntion of heavy-duty vehicles together with external inﬂuences\nsuch as trafﬁc data and thus enables a centralized coordination\nof a large number of vehicles over great distances. In this\nsection, a method for the coordination of a potentially large\nﬂeet of vehicles is described, aimed at achieving fuel savings\nthrough the formation of platoons. This approach can be\nregarded as an extension of the opportunistic platoon formation\napproach of Section IV-C, where the latter is inherently local\nin nature.\nIn order to efﬁciently obtain platoon conﬁgurations and the\ncorresponding average velocities for each vehicle, a three-step\napproach is taken. The ﬁrst step comprises ﬁnding the most\nsuitable route for each vehicle, taking factors such as road\ntopography and trafﬁc information into account.8\nSecond, for a given vehicle, which will be referred to\nas a coordination leader, its fuel-optimal velocity proﬁle is\ncomputed. The starting time and arrival deadline are taken into\naccount together with constraints such as driver resting times.\nThe proﬁle speciﬁes the desired average velocity, which will\nlater be reﬁned by the use of cooperative look-ahead control.\nThen, for each vehicle with a partially overlapping route with\nthe coordination leader, the pairwise analysis of Section IV-C\nis used to determine whether it is beneﬁcial to adapt its\nvelocity proﬁle to form or join a platoon with the coordination\nleader. If so, this vehicle is referred to as a coordination\nfollower. In this pairwise analysis, the coordination leader does\nnot adapt its velocity proﬁle, such that several coordination\nfollowers can be assigned to a single coordination leader.\nArrival deadlines are taken into account when adapting the\nvelocity proﬁles of the coordination followers.\nThe selection of the most suitable coordination leaders is\ncrucial in obtaining signiﬁcant fuel savings. This selection\nforms the third step. Repeating the pairwise analysis for every\npotential coordination leader leads to a data set that can\nbe conveniently represented as a graph. In this graph, the\nnodes represent the vehicles and their incoming edges denote\nthe fuel savings obtained when this vehicle is selected as a\ncoordination leader. From graph clustering algorithms [29]),\nan algorithm can be derived to compute a suitable set of\ncoordination leaders. Speciﬁcally, a greedy algorithm that\nincrementally adds or removes individual vehicles from the\nset of coordination leaders provides a computationally efﬁcient\nand scalable approach [56]. Instead of coordination of vehicles\nthrough adaptation of their velocity proﬁles, vehicle sorting\nfor platooning has been considered [19], as well as other\nplatooning algorithms [36].\nC. Incentives for cooperation\nThere are many incentives for individual owners of truck\nﬂeets to optimize their long-haulage transportation tasks. By\ncoordinating timing and routing of vehicles, the ﬂeet owner\ncan utilize their available resources (fuel, vehicles, drivers,\netc.) as efﬁciently as possible. Through vehicle platooning,\nthe tasks can be further optimized and fuel consumption de-\ncreased, as discussed in this paper. The long-haulage transport\nand logistics industry consists of a large and diverse set of ﬂeet\nowners, however, and it is for obvious reasons hard for many\nof them to cooperate without ﬁnancial guarantees and trust.\nTo be able to capitalize on vehicle cooperation, we need to\nhave as big pool as possible of heavy-duty vehicles that travel\non the same (or similar) route and at the same time. It is\nrarely the case for small ﬂeet owners to have so many similar\ntasks to satisfy this criterion. One solution to this problem is\ninstead to create a ﬂeet management service for the owners and\ntheir vehicles. In such a service, the ﬂeet owners can privately\nprovide their routes and timetables so that the service provider\ncan pair the vehicles for cooperation. For participating in this\nservice, the ﬂeet owners may need to pay a subscription fee in\naddition to invest in devices to facilitate cooperation. The case\nof cooperative heavy-vehicle platooning is discussed next.\nA ﬂeet management service for heavy-duty vehicle coop-\neration focusing on platooning can be evaluated consideringTable I\nModel parameters used in the experimental evaluation.\nm 40 000 kgp0 5:36\u000110\u00004kg s\u00001\nA 10 m2p1 5:15\u000110\u00008kg s\u00001W\u00001\nc0\nd0:6 \u001a 1:29 kg m\u00003\n\u000b1 0:53 Pmin\u00009 kW\n\u000b2 0:81 s\u00001Pmax 300 kW\nexisting patterns of long-haulage goods delivery. Based on\nposition data from thousands of heavy-duty vehicles, it has\nbeen shown that many vehicles have other vehicles in their\nvicinity, even when only a single vehicle brand is consid-\nered [34]. Hence, by simply slowing down a bit or speeding\nup, it is possible with minimum effort to form a vehicle\nplatoon, as was described in Section IV. It is also clear from\nthese data that quite a few vehicles are actually driving in\nspontaneous platoons already today. To automate a platooning\nservice, it is essential to present transparent information on\nbeneﬁts and costs to individual ﬂeet owners and drivers. By\nutilizing economic theory on technology adoption [44] and\ndata from actual transportation tasks [34], it is possible to\nreason how a market for such a service can be established [16].\nOne example is centralized cooperation, in which ﬂeet owners\npay to subscribe to a third-party service provider and then can\ncooperate with any other ﬂeet owner who is part of the system.\nThe pricing strategy needs to be carefully developed for such\na service, as the marginal beneﬁt for joining such a system\nfor a large ﬂeet owner might be smaller than for a ﬂeet owner\nwith few vehicles.\nVI. C ASE STUDY\nA. Scenario\nThe platoon control and coordination algorithms presented\nin this paper are demonstrated by means of a simulation\nscenario representing a part of the highway network of Swe-\nden, see Figure 11. On this network, 200 heavy-duty vehicles\noriginating from six locations in the Stockholm area in the\neast travel to ﬁve destinations in the west. The starting times\nfor these vehicles are taken from a two-hour interval, whereas\nthe parameter values for each vehicle are given in Table I. The\ncoordinated platoon planning for this scenario is evaluated in\nSection VI-B before focusing on the cooperative look-ahead\ncontrol for one speciﬁc platoon in Section VI-C.\nB. Coordinated platoon planning evaluation\nThe methodology for coordinated platoon planning in Sec-\ntion V-B is used to select suitable coordination leaders and\ntheir respective followers. Herein, pairwise plans are consid-\nered in which the coordination followers catch up with their\nleaders and platoon until either of their routes end or their\nroutes split up.\nFor this scenario, the coordination algorithm selects 54\ncoordination leaders and 139 coordination followers, which\nadjust their velocity proﬁles to catch up with the coordination\nleaders in order to form platoons. The maximum number of\ncoordination followers per coordination leader is 8, whereas9\n152\n8\n6\n1312\n10537\n17\nGteborgTrollhttan\nJnkpingLinkpingKarlstad\nSdertljeUppsala\nStockholmNorrtlje\nNynshamn\nNorrkpingrebroVsters\nMariestadStart Locations\nDestinations\n20 km69, 2.711, 1.1\n50, 2.433, 1.8\n47, 2.915, 1.8\n5, 1.3127, 3.0\n15, 3.042, 3.099, 1.9\n30, 2.015, 1.4\n45, 1.6\n45, 2.649, 1.5\n20, 1.1\n4, 1.3\n14, 1.482, 2.9\n!4, 1.1\n 82, 3.3\nFigure 11. The Swedish road network used in the case study. Starting locations and destinations are indicated by red circles and blue squares, respectively,\nand the boldface numbers represent intersections. The two numbers next to the road segments indicate the number of vehicles that traverse this segment and\nthe average platoon size on this segment, respectively, as a result of the applied coordination algorithm. The road segment between nodes 7 and 8 is traversed\nin both directions and the statistics for vehicles travelling in either direction are indicated separately. Three routes, indicated by dashed lines, are highlighted\nas an example for a group comprising a coordination leader (black) and two coordination followers (blue and red).\nthe median is 2. The remaining 7 vehicles do not platoon but\ntraverse their routes individually.\nThe coordinated platoon planning amounts to a fuel saving\nof5:7%, when compared to all vehicles driving independently.\nConsidering that the maximum fuel saving is 12:0% when\nvehicles platoon continuously, the coordination layer is fairly\nefﬁcient in this scenario. Recall that the velocity adjustments\nnecessary for coordination lead to an increased fuel consump-\ntion. The total fuel savings amount to 1 045 liters of diesel\nfuel and a reduction of CO 2emissions of 2 770 kg.\nThe routes of one particular coordination leader and its\ntwo coordination followers are highlighted in Figure 11. The\ncorresponding trajectories are presented in Figure 12, where\nthe time gaps with respect to the coordination leader as a\nfunction of the position on the road are shown. Note that the\nﬁrst coordination follower (blue) shares the ﬁrst part of its\nroute with the coordination leader (black), but as it starts 1.25\nhours later it catches up at maximum speed, indicated by a\ndecreasing gap to the leader in Figure 12. It then meets the\nplatoon consisting of the coordination leader and the other\ncoordination follower (red) between nodes 5 and 7, in which\nit stays until its destination at node 10 is reached. The route\nof the second coordination follower intersects with the route\nof the coordination leader at node 5 and the coordination fol-\nlower’s start time is such that it catches up to the coordination\nleader at a velocity that is lower than the maximum speed. The\ncoordination follower and coordination leader form a platoon\nat node 5 and platoon until node 10 where their routes split up.\nC. Cooperative look-ahead control evaluation\nThe cooperative look-ahead control and the vehicle layer\ngovern the local behavior of each platoon by explicitly taking\ninto account topography information and trafﬁc. Figure 13\nillustrates the effective behavior of the three-vehicle platoon\ndiscussed in the previous section when driving along a 4km0 200 400 60001002003002 635 78 15 1017 1213\ns[km]t\u0000s=v nom[s]\nFigure 12. Platoon plans for a coordination leader (black) and two coordi-\nnation followers (blue and red) that catch up with the coordination leader\nto form a platoon. The graph shows the time gap to the platoon leader as\na function of the position on the road, where this position is taken along\nthe routes of the individual vehicles. The dashed lines denote the position\nof the nodes representing road intersections in Figure 11, with the top labels\ndenoting the node number. As an example, note that the coordination leader\nstarts from Norrt ¨alje (node 2) and drives to Trollh ¨attan (node 15). When the\ntime gap is zero and the routes of the vehicles overlap (between nodes 5 and\n10), the vehicles operate in a platoon.\nroad stretch in the latter part of the segment between node 5\nand node 7. It can be observed that the three vehicles follow\napproximately the same velocity proﬁle, albeit shifted in time\nas required by their cooperative look-ahead control strategy.\nThis translates into the vehicles following approximately the\nsame velocity proﬁle in the spatial domain and, due to the\ndependence of the slope on position, results in similar power\nproﬁles.\nIn order to respond to the fuel-optimality criterion, the\ncooperative look-ahead control requires the vehicles to follow\na particular speed proﬁle depending on the road topography.\nSpeciﬁcally, it requires the vehicles to keep a constant speed\nof80km/h during the uphill segment and to drop the speed\ndown to 68km/h at the top of the hill. This allows the vehicles10\n0 50 100 150 2002030405060altitude [m]\n0 50 100 150 200708090velocity [m/s]HDV 1\nHDV 2\nHDV 3\n0 50 100 150 2005101520distance [m]\n0 50 100 150 2000100200300\ntime [s]power [kW]\nFigure 13. Cooperative look-ahead control Local behavior of the three-\nvehicle platoon depicted in Figure 12. The plots show the road topography\nexperienced by the leading vehicle, the vehicle speeds, the inter-vehicle\ndistances, and the generated power, respectively. The generated power is the\nsum of the engine power and the power dissipated by the braking system.\nThe dashed lines represent the minimum and maximum engine powers.\nto gain speed during the downhill without reaching the speed\nlimit of 90km/h. In particular, the required downhill speed\nproﬁle is such that the lead vehicle fuels slightly, whereas the\nfollower vehicles coast (i.e., they do not fuel). Hereby, the\ndesired inter-vehicular distances are maintained even though\nthe follower vehicles experience a reduced aerodynamic drag.\nHence, the proposed cooperative controller avoids braking and\nexploits the combined potential of both platooning and look-\nahead control.\nThe cooperative look-ahead control combines the potential\nof platooning and look-ahead control and achieves larger fuel\nsavings than for each of the methods independently. For the\nhilly stretch shown in Figure 13, it allows to save approxi-\nmately 10% of energy compared to the vehicles driving alone\nusing look-ahead control and 7% compared to the vehicles\nplatooning without cooperating and exploiting topography\ninformation.\nVII. C ONCLUSIONS\nA cyber-physical systems approach towards the control\nand coordination of a large-scale transportation system was\npresented in this paper. The approach relies on modern vehicle-\nto-vehicle and vehicle-to-infrastructure communication and is\nsupported by ubiquitous computation power as offered through\ncloud services and onboard computers. The coordination ofheavy-duty vehicles is aimed at the reduction of fuel con-\nsumption and a layered freight transport system architecture\nwas developed that achieved this reduction through exploiting\nthe formation of closely-spaced groups of vehicles, which\nexperienced a reduced aerodynamic drag. The distributed\ncontrol of platooning vehicles was handled in the low-level\nvehicle layer of the system architecture, whereas the middle-\nlevel cooperation layer employed look-ahead control to further\nreduce fuel consumption. The formation of platoons was also\nhandled in this cooperation layer. Finally, the ﬂeet layer on top\nperformed the large-scale coordination of the platoons with\nintegrated routing and transport planning. This allowed both\nsmall and large ﬂeet owners to beneﬁt from the fuel-saving\npotential of cooperation. A case study involving 200 vehicles\nconﬁrmed the feasibility of this cyber-physical approach to\nfreight transport.\nExtensive real-world experimental evaluation of the ap-\nproach developed in the paper is the scope of future work.\nSuch evaluation should include both small- and large-scale\ntests. Experiments with vehicles on public roads are obviously\nneeded to study many practical implications. Such experiments\ncan build on earlier experiences of individual platoon experi-\nments on Swedish roads [4], [3].\nACKNOWLEDGEMENTS\nThe work presented in this paper has greatly beneﬁted from\na long-term collaboration between KTH and Scania. We would\nin particular like to acknowledge important contributions by\nMagnus Adolfsson, Henrik Pettersson, and Tony Sandberg.\nFunding is received from the European Union Seventh Frame-\nwork Programme under the project COMPANION, Sweden’s\ninnovation agency VINNOV A-FFI, the Knut and Alice Wal-\nlenberg Foundation, and the Swedish Research Council.",
      "metadata": {
        "filename": "Cyber-physical Control of Road Freight Transport.pdf",
        "hotspot_name": "Transportation",
        "title": "Cyber-physical Control of Road Freight Transport",
        "published_date": "2015-07-13T14:16:22Z",
        "pdf_link": "http://arxiv.org/pdf/1507.03466v1",
        "query": "road transport logistics optimization fuel efficiency reduction strategies"
      }
    },
    "Urban Logistics in Amsterdam_ A Modal Shift from Roadways to Waterway": {
      "full_text": "Urban Logistics in Amsterdam: A M odal Shift from Roadways to Waterways  \nNadia Pourmohammad -Ziaa , Mark van Koningsvelda,b \naDepartment of Hydraulic Engineering , Delft University of Technology, The Netherlands  \nbVan Oord Dredging and Marine Contractors, 3068 NH Rotterdam, The Netherlands  \nAbstract–   The efficiency of urban logistics is vital for economic prosperity and quality of life in cities. \nHowever, rapid urbanization poses significant challenges , such as conges tion, emissions, and strained \ninfrastructure. This paper addresses these challenges by proposing an optimal urban logistic network that \nintegrates urban waterways and last -mile delivery in Amsterdam. The study highlights the untapped \npotential of inland wa terways  in addressing logistical challenges in the city center. The problem is \nformulated as a two -echelon location routing problem with time windows, and a hybrid solution approach \nis developed to s olve it effectively . The proposed algorithm consistently outperforms existing approaches, \ndemonstrating its effectiveness in solving existing benchmarks and newly developed instances. Through a \ncomprehensive case study, the advantages of implementing a waterway -based distribution chain are \nassessed, revealing su bstantial cost savings  (approximately 28%)  and reductions in vehicle weight  (about \n43%)  and travel distances (roughly 80%) within the city center. The incorporation of electric vehicles \nfurther contributes to environmental sustainability. Sensitivity analy sis underscores the importance of \nmanaging transshipment location establishment costs as a key strategy for cost efficiencies and reducing \nreliance on delivery vehicles  and road traffic congestion. This study provides valuable insights and practical \nguidan ce for managers seeking to enhance operational efficiency, reduce costs, and promote sustainable \ntransportation practices. Further analysis is warranted to fully evaluate the feasibility and potential benefits, \nconsidering infrastructural limitations and c anal characteristics.  \nKeywords : Urban Logistics; Modal Shift ; Two -Echelon Location Routing; Waterways ; Amsterdam    \n1. Introduction  \nEfficient urban logistics can be seen as  a fundamental prerequisite  for the economy and livability \nof the cities. The ever -increasing population in urban areas puts this efficiency under pressure and \nforces serious challenges such as congestion, emissions, noise, and safety issues on the other hand. \nAccordingly, seeking innovative solutions to mitigate the adverse effects of urban logistics and \nimprove its performance is imperative . One such soluti on lies in the exploration of alternative \ntransportation modes. While road transport currently dominates urban freight, inland waterways \npresent untapped potential in many cities.  \nIn Amsterdam, the growing strain on public spaces, along with congestion and  the considerable \ntask of maintaining bridges and quay walls, has prompted the need to reassess the city's current \nlogistics design.  The situation is particularly challenging in the historic center, where freight \ntransport contributes to quay wall deterior ation and congestion on the narrow roads alongside the canals of Amsterdam (Korff et al., 2022) . Based on the topological features of the city center, \nlocated on the bank of canals, inland waterways can play a vital role in addressing its logistical \nneeds . Currently, very few initiatives incorporate these canals for  freight, indicating that there is \nstill capacity within canals for a shift from roadways to waterways.   \nAs highlighted by the municipality (Nepveu & Nepveu, 2020) , the canals of Amsterdam present \npotential opportunities for a modal shift in three key areas: construction material, waste, and food \nsupply flows.  Construction material flows are typically project -based, with temporary barges \nserving as transshipment points at construction sites.  For waste, specific transshipment \ninfrastructure is generally unnecessary, and transfer to vessels occurs through dumping.  However, \nfor food supplies, dedicated transshipment locations are required, where goods are unloaded from \nvessels and loaded onto light electric vehicles for last -mile delivery. The lack of these \ntransshipment locations poses a significant obstacle to achieving a modal shift in Amsterdam.  The \nestablishment of such points is influenced not only by considerations of subsequent routing \ndecisions for vessels and last -mile de livery but also by the availability of space, the condition of \nsurrounding infrastructure such as quay walls and bridges, and the characteristics of the canal \nclasses . \nDriven by the profound impact of efficient urban logistics on the overall well -being and  \ndevelopment of cities , this study designs an  optimal  HoReCa  logistic s network for the historical  \ncenter of Amsterdam . The proposed network comprises a synergistic integration of urban \nwaterways and last -mile delivery via road transportation. To this end, the problem is formulated \nas a two-echelon location routing problem  with time windows , where heterogeneous vessels in t he \nfirst echelon and moving jacks together with light electric vehicles in the second echelon are \napplied . To tackle this intricate problem, a hybrid solution approach is devised, leveraging a \ncustom -designed Adaptive Large Neighborhood Search (ALNS), loca l search techniques, K -\nmeans clustering, and branch and price methods. This comprehensive approach is adept at solving \nmedium to large -sized instances of the problem, facilitating effective decision -making in urban \nlogistics optimization for Amsterdam . \nAms terdam is not the only city facing the  challenges outlined . Many cities worldwide share the \nneed for urgent action to safeguard their historical infrastructure while enhancing their logistical \nnetworks. The delicate balance between conserving heritage and accommodating new demands is \na shared concern. Historically, canals served as vital transportation routes in many city centers \nacross the globe, leaving behind remnants of old quay walls and bridges . By learning from \nAmsterdam's experience and exploring innovative approaches, cities globally can strive to find \nsustainable solutions that enhance both their cultural legacy and efficient transportation systems .   \nThe research on waterway s for  urban logistic s is highly confined , and when it comes to \noperational and tactical decisions, this deficiency is even more highlighted. As such, a significant \ncontribution of this research lies in developing  an optimal logistic s chain specifically tailored for \nurban wate rway distribution.  In this respect, we take canal classes into account, which  impacts  \nderiving the distances between different nodes for different vessel sizes  and, thereby , their routing \nscenarios . The other novel features of this paper are raised by the incorporated modeling \nassumptions, which are hardly heeded even in classic urban distr ibution models (see  Table 1). \nThese include the deci sions on establishing transshipment points (location), allowing for applying \nelectric vehicles, synchronization, a nd applying moving jacks together with light vehicles . To efficiently address this complex problem, an advanced solution algorithm has been devised, \ncombining the branch and price technique for the first tier with an Adaptive Large Neighborhood \nSearch (ALN S) that incorporates custom -designed destroy and repair operators. The ALNS \nemploys local search and K -means clustering techniques to enhance solution intensification in the \nsecond tier.  \nThe subsequent sections of this paper are structured as follows: In Section 2, the relevant \nliterature is reviewed , allowing for the identification of existing research gaps . Section 3 provides \nthe problem description and the mathematical model . The solution methodology employed is \nelaborated upon in Section 4. Section 5 encompasses a detailed analysis of numerical results, the \ncase study, sensitivity analysis, and discussion . Finally, in Section 6, the paper is concluded, \nemphasizing key findings  and offering suggestions for potential  future research avenues . \n2. Literature Review  \nIn this section, we briefly review the existing literature on urban logistics, where operational \nplanning and logistics chain design are particular points of interest. Our  resear ch mainly builds on \ntwo streams : the application of inland waterways in urban logistics and two -echelon routing in \nurban logistics , each of which will be reviewed  as follows . \n2.1. Application of Inland Waterways in Urban Logistics  \nDespite its potential, the literature on waterborne freight transport in urban logistics is fairly \nconfined, and there are limited research works in this area.  \nJanjevic a nd Ndiaye (2014) , Maes et al. (2015) , Miloslavskaya et al. (2019 ), and Wojewódzka -\nKról and Rolbiecki (2019)  have reviewed successful practices of waterborne urban logistics \nworldwide. Several initiatives have been introduced in these papers ; among those are:  \n• Beer Boat in Utrecht for deliveries to caf és, hotels, restaurants  \n• “Vracht door de gracht” (freight through canals) by Mokum Maritiem  in Amsterdam for \ndeliveries to local shops and waste collection  \n• The DHL floating service center in Amsterdam for parcel delivery  \n• Vert Chez Vous in Paris for parcel delivery  \n• Sainsbury’s in London for deliveries to supermarkets  \n• POINT -P in Paris for construc tion material  \n• Franprix in Paris for deliveri es to supermarkets  \n• Domestic waste transport in Lille  \n• Paper recycling by Barge in Paris  \n• Waste transport by barge in Tokyo  \nKortmann et al. (2018)  investigated the potential of waterborne distribution for same -day \ndelivery to shopkeepers in Amst erdam. They developed a simulation model to analyze the \nperformance of this distribution system and determine the appropriate fleet size. Their results show \nthat waterborne distribution with few hubs can be a competent and sustainable delivery mode in \nAmst erdam, provided that further studies on its financial viability are carried out.  Gu and Wallace (2021)  developed one of the few optimization models for waterborne urban \nlogistics where the application of autonomous vessels is investigated. Their mixed -integer \nprogramming model tackles the facility location, fleet allocation, and routing de cisions in the daily \noperations of vessels in Bergen, Norway. While t heir result s demonstrate the potential benefits of \nautonomou s vessels at the operational level, further investigations are required on the impact of \ninitial investment costs .  \nDivieso et al. (2021)  studied the feasibility of using waterways for urban logistics in Br azil. \nThey identified features of  waterway urban logistics practices worldwide and then comparatively \nanalyzed the case for a city (Belém) in Brazil. Their analysis highlights the great potential of Belém \nfor the application of waterways as an aid to urban  logistics.  Nepveu and Nepveu (2020)  assessed \nthe potential of implementing urban waterway transport in Amsterdam by exploring the success \nand failure factors for such a modal shift.  \n \n2.2. Two -Echelon Routing  Problem  in Urban Logistics  \nTwo-Echelon Vehicle Routing Problem (2E -VRP) is an affluent  area of academic research in \nurban logistics. Intermediate facilities, known as transshipment points or satellites applied for \nconsolidation and transshipment of the items between the two echelons, are an essential part of \ntwo-echelon networks. When these  points are not pre -established , and their locations need to be \ndetermined, the problem turns into a Two-Echelon Location Routing Problem  (2E-LRP) . Other \nvariants of the problem , such as two -echelon inventory routing, truck -and-trailer routing, and \nproduct ion routing, are not the point of our interest in this paper.  \nCrainic et al. (2009)  introduced the first 2E -VRP model in a multi -product and multi -depot \nsetting. Zhou et al. (2018)  investigated a multi -depot 2E -VRP with delivery options, allowing the \ncustomers to pick up their parcels at satellites. They developed a hybrid multi -population genetic \nalgorithm to solve the problem. Belgin et al. (2018)  studied a variant of the problem for which \npick-up and deliveries are considered and applied this two -echelon distrib ution system in a \nsupermarket chain in Turkey.  \nLi et al. (2020)  investigated the application of Unman ned Aerial Vehicles (UAVs) in the second \nechelon, where the first echelon  distribution vans are considered mobile satellites for UAVs. \nEnthoven et al. (2020)  introduced covering locations in 2E -VRPs, from where customers can pick \nup their parcels. Similarly, Vincent et al. (2021)  designed a two -echelon distribution system \nconsidering covering locations and occasional drivers. They showed that using crowds as \noccasional drivers in addition to the city freighters increases the efficiency of the distribution \nnetwork.  \nSynchronizing the arrival and departure of the vehicles in the first and second echelons is \nessential in designing a seamless distribution network. Yet, this is mostly overlooked in the \nexisting literature. Anderluh et al. (2021)  provided one of the few works that took satellite \nsynchronization into account . They considered synchroniza tion in a multi -objective setting. In \norder to address the desires of citizens and municipalities, they applied a second objective function \nthat accounts for the negative effects of transport, such as emissions. Li et al. (2021)  and Jia et al. \n(2022)  are the other researchers who took synchronization into account in their classic 2E -VRP.   The decision to establish  transshipment points, leading to 2E -LRPs, increases the complexity \nof the already complex 2E -VRPs. Thereby, few papers  have taken both locating  and routing \ndecisions into account. Among those are Zhao et al. (2018) , Darvish et al. (2019) , and \nMirhedayatian et al. (2021) , who have invest igated 2E -LRP in capacitated, timely -flexible  and \nsynchronized settings, respectively. Two -echelon electric vehicle routing is the other extension of \n2E-VRP, where the vehicles (mostly in the second echelon) have a limited driving range. This \nassumption i s heeded in three ways : limiting the traveled distance or considering refilling  batteries \nat charging or battery swap stations. Breunig et al. (2019) , Jie et a l. (2019) , and Wu and Zhang \n(2021)  are examples of this extension.  \nDeveloping efficient solution approaches to solve the well -established 2E -VRP and its variants  \nis an active research direction in the area. The idea is to obtain better solutions for the existing \nbenchmark instances and improve the gaps. Adaptive Large Neighborhood Search  (ALNS)  \n(Grangier et al., 2016) , Memetic Algorithm  (MA)  (Bevilaqua et al., 2019) , Branch and Price \n(B&P)  (Mhamedi et al., 2022) , Variable Neighborhood Search  (VNS)  (Akbay et al., 2022)  and \nother heuristics such as a novel Construction Heuristic  (CH)  (Yu et al., 2020) , Graph-Guided \nHeuristic  (GGH)  (Huang et al., 2021) , Sample  Average Approximation (SAA) technique  (Pina-\nPardo  et al., 2022), and Cluster -based Artificial Immune Algorithm  (C-AIA)  (Liu et al., 2023 ) are \namong applied methods. Table 1 provides a general overview of the existing literature on the two-\nechelon routing problem in urban logistics.  \nComing up with a general overview, the exploration of waterways as a viable option for urban \nlogistics has garnered limited attention within the research community. Existing studies primarily \nfocus on examining successful use cases rather than delving into comprehensive investigations . \nOn the other hand, 2E -VRP in urban logistics is a relatively affluent area of academic resear ch. \nHowever, amidst this rich tapestry of research, several promising avenues have remained \nundervalued or overlooked. These areas encompass the identification of optimal transshipment \npoints, satellite synchronization for improved efficiency, integration of electric vehicles into the \nlogistics network, consideration of multiple delivery modes, and harnessing the potential of \nwaterways in the primary echelon of transportation. By shedding light on these neglected aspects, \nwe can uncover new insights and opp ortunities to pave the way for a more sustainable and efficient \nurban logistics ecosystem.  \n3. Problem Description and Mathematical Model  \nThis paper considers a multi -modal two -echelon location and routing problem with time windows \nthat rises in urban logis tics. The network embraces a combination of inland waterways and streets. \nThe first echelon involves the flow of inland vessels from a central hub to transshipment locations \nin the city center and then back to the hub. The transshipment locations are the p oints where the \nvessels are unloaded, and the last -mile delivery starts by Light Electric Vehicles (LEVs)  with a \nmaximum weight of 700 kg or moving jacks. Each vessel can serve several transshipment \nlocations, and each transshipment location can be visite d by more than one vessel, implying that \nsplit delivery is admissible in the first echelon. The second echelon includes the flow of LEVs \nfrom vehicle depots to the transshipment locations and then navigat ing a prescribed  route  to serve  \ndesignated  demand po ints (HoReCa businesses) and finally return ing to the depot. The demand  Table 1. General overview of the existing literature  on 2E -VRP in urban logistics  \nReference  Location  Electric \nVehicles  Satellite \nCapacity  Time \nWindows  Synchronization  Multiple Delivery \nModes Waterways  Solution Approach  \nCrainic et al. (2009)    ✓ ✓    HD \nGrangier  et al. (2016)     ✓    ALNS  \nBelgin et al. (2018)    ✓     VND+LS  \nZhao et al. (2018)  ✓  ✓     CAH  \nZhou et al. (2018)    ✓   ✓  HMPG  \nBevilaqua et al. (2019)         MA \nBreunig et al. (2019)   ✓ ✓     LNS  \nDarvish et al. (2019)  ✓  ✓     B&P \nJie et al. (2019)   ✓ ✓     ALNS + B&P  \nEnthoven et al. (2020)       ✓  ALNS  \nLi et al. (2020)    ✓ ✓    ALNS  \nYu et al.  (2020 )  ✓  ✓    CH+LNS  \nAnderluh et al. (2021)      ✓   LNS  \nHuang et al. (2021)    ✓     GGH  \nLi et al. (2021)     ✓ ✓   ALNS  \nMirhedayatian et al. (2021)  ✓  ✓ ✓ ✓   DBH  \nVincent et al. (2021)     ✓  ✓  ALNS  \nWu and Zhang (2021)   ✓      B&P  \nJia et al. (2022)    ✓ ✓ ✓   ALNS  \nMhamedi et al. (2022)     ✓    B&P  \nAkbay et al. (2022)   ✓  ✓    CH \nPina-Pardo  et al. (2022)  ✓  ✓     SSA \nLiu et al. (2023)         C-AIA \nThis Study  ✓ ✓ ✓ ✓ ✓ ✓ ✓ ALNS +LS+ B&P  \n* HD: Hierarchical Decomposition ; ALNS : Adaptive Large Neighborhood Search; B &P: Branch and Price; VND: Variable Neighborhood Descent \n(VND); LS: Local Search;  HMPG: Hybrid Multi -Population Genetic ; CAH: C ooperative Approximation Heuristic ; MA: Memetic Algorithm ; LNS: Large \nNeighborhood Search; B&B: Branch and Bound; GGH: Graph -Guided Heuristic; DBH: Decomposition -Based Heuristic ; CH: Con struction Heuristic ; \nSSA: sample average approximation ; C-AIA: cluster -based artificial immune algorithm  points proximate enough to the transshipment locations are served by moving jacks instead of \nLEVs.  \nThe problem is modeled on a directed graph G( V, E), where V represents the set of vertices \nand E is the set of arcs. 𝑉=𝑉1∪𝑉2 includes the set of vertices in the first echelon ( 𝑉1) and second \nechelon ( 𝑉2). The set 𝑉1={𝐶𝐻}∪𝑇𝑃 involves the central hub ( 𝐶𝐻) and the transshipment \nlocations ( 𝑇𝑃). The set 𝑉2=𝑉𝐷∪𝑇𝑃∪𝐻𝑅𝐶  is comprised of the vehicle depots ( 𝑉𝐷), the \ntransshipment locations ( 𝑇𝑃), and the demand points ( 𝐻𝑅𝐶 ). 𝐸={(𝑖,𝑗)| 𝑖,𝑗∈𝑉,𝑖≠𝑗,(𝑖,𝑗)∈\n𝐴∪𝐵}  where  A and B are the sets of admissible arcs for the first and second echelon, respectively. \n𝐴=𝐴1∪𝐴2 and   𝐵=𝐵1∪𝐵2∪𝐵3 where:   \n𝐴1={(𝑖,𝑗)| 𝑖=𝐶𝐻,𝑗∈𝑇𝑃}, 𝐴2={(𝑖,𝑗)|𝑖∈𝑇𝑃,𝑗∈𝑇𝑃∪{𝐶𝐻}} \n𝐵1={(𝑖,𝑗)| 𝑖∈𝑉𝐷,𝑗∈𝑇𝑃}, 𝐵2={(𝑖,𝑗)|𝑖∈𝑇𝑃,𝑗∈𝐻𝑅𝐶 }, 𝐵3={(𝑖,𝑗)|  𝑖∈𝐻𝑅𝐶 ,𝑗∈𝐻𝑅𝐶 ∪\n𝑉𝐷} \nIt should be noted that the distance between any two nodes in the first echelon, is not driven \nonly based on  the shortest path method but concerning different canal classes . Thereby, the \ndistance between two identical nodes can differ for various vessel types with different sizes. Figure \n(1) illustrates a typical solution on the described graph.  \n \nFigure 1. The graph of the problem  \nIn order to complete the flow in the first echelon, we need to locate the transshipment points. These \nlocations are specified from a set of potential sites for transshipment. The transshipment locations \nare assumed to be heterogeneous, implying that their establishment cost, capacity, and allowed \nlaying time are different. Having the set of designated locations that are used by the vessels, LEVs, \nand moving jacks, the routing decisions of the vessels and LEVs are determined. The remainder \nof the notations w hich are used to formulate the model are as follows:  \nParameters  \n𝐷𝑖  Demand of  𝑖∈𝐻𝑅𝐶  \n𝑆𝑖𝐼𝐷  Service time of vertex i (ID =1  first echelon, ID=2  second echelon ) \n \n𝑇𝑖𝑗𝑘𝐼  Travel time of arc ( i,j) for vessel k \n𝑇𝑖𝑗𝑘𝐼𝐼 Travel time of arc (i,j) for LEV  k \n𝑇𝑖𝑗𝐼𝐼𝐼 Travel time of arc ( i,j) for moving jacks  \n𝐴𝐿𝑖  Allowed laying time at transshipment point i \n𝑇𝐴 𝑖  Lower bound for admissible service time at vertex i \n𝑇𝐵 𝑖  Upper bound for admissible service time at vertex i \n𝐶𝐴𝑃 𝑖  Capacity of transshipment point 𝑖∈𝑇𝑃 \n𝑄𝑘𝐼  Capacity of vessel k \n𝑄𝑘𝐼𝐼  Capacity of LEV k \n𝐷𝐿  Driving range limit for LEVs  \n𝐶𝑖𝑗𝑘  Cost of  traveling arc ( i,j) by vehicle k \n𝐷𝐼𝑆 𝑖𝑗  Average traveling distance of arc ( i,j), 𝑖,𝑗∈𝑉2 \n𝐷𝑇𝑟   Threshold distance  to use moving jacks  \n𝐹𝐶𝑖  Period equivalent f ixed cost of establishing transshipment point 𝑖∈𝑇𝑃 \n𝑙𝑖𝑗  1: if demand point 𝑗∈𝐻𝑅𝐶  is located at a distance shorter than DTr from point 𝑖∈𝑇𝑃 \n 0: otherwise  \n𝑚1,…,𝑚6  Lower bound  for the left-hand side of the respective constraints  \n𝑀1,…,𝑀4  Upper bound for the left-hand side of the respective constraints  \n  Decision variables  \n𝑥𝑖𝑗𝑘𝐼  1: if vessel 𝑘∈𝐾1 travels from i∈𝑉1 to j∈𝑉1  \n 0: otherwise  \n𝑥𝑖𝑗𝑘𝐼𝐼  1: if LEV 𝑘∈𝐾2 travels from i∈𝑉2 to j∈𝑉2  \n 0: otherwise  \n𝑢𝑖𝑗  1: if the demand point 𝑗∈𝐻𝑅𝐶  is served by a moving jack  from transshipment point  𝑖∈𝑇𝑃 \n 0: otherwise  \n𝑦𝑖  1: if transshipment point 𝑖∈𝑇𝑃 is established  \n 0: otherwise  \n𝑝𝑖𝑗𝑘  1: if the items of the demand point 𝑗∈𝐻𝑅𝐶  are delivered by vessel 𝑘∈𝐾1 to 𝑖∈𝑇𝑃 \n 0: otherwise  \n𝑣𝑘𝑘̂𝑖  1: if LEV 𝑘̂∈𝐾2 meets vessel 𝑘∈𝐾1 at 𝑖∈𝑇𝑃 \n 0: otherwise  \n𝑠𝑡𝑖𝑘𝐼  Time when vessel 𝑘∈𝐾1  starts to service vertex i∈𝑉1 \n𝑠𝑡𝑖𝑘𝐼𝐼  Time when LEV 𝑘∈𝐾2  starts to service vertex i∈𝑉2 \n𝑠𝑡𝑖𝑗𝐼𝐼𝐼  Time when a moving jack assigned to serve 𝑗∈𝐻𝑅𝐶  starts to service vertex 𝑖∈𝑇𝑃∪𝐻𝑅𝐶  \n𝑎𝑡𝑖𝑘𝐼  Time when vessel 𝑘∈𝐾1  arrives at vertex i \n𝑞𝑖𝑘  The amount delivered by vessel 𝑘∈𝐾1 to the transshipment point 𝑖∈𝑇𝑃 \n𝑝𝑢𝑖𝑗𝑘   Auxilary binary variable  \n \n Optimization Model  \n𝑷𝟏:𝑚𝑖𝑛  𝑍=∑ ∑ ∑𝐶𝑖𝑗𝑘𝐼𝑥𝑖𝑗𝑘𝐼\n𝑗∈𝑉1 𝑖∈𝑉1 𝑘∈𝐾1+∑ ∑ ∑𝐶𝑖𝑗𝑘𝐼𝐼 𝑥𝑖𝑗𝑘𝐼𝐼\n𝑗∈𝑉2 𝑖∈𝑉2 \n𝑘∈𝐾2+∑ 𝐹𝐶𝑖 𝑦𝑖\n𝑖∈𝑇𝑃 (1) \ns.t. \n∑ 𝑥𝑖𝑗𝑘𝐼\n𝑗∈𝑇𝑃≤1 ∀𝑖∈{𝐶𝐻},𝑘∈𝐾1 (2) \n∑ 𝑥𝑖𝜐𝑘𝐼\n𝑖∈𝑉1−∑ 𝑥𝜐𝑗𝑘𝐼\n𝑗∈𝑉1=0 ∀𝜐∈𝑇𝑃,𝑘∈𝐾1 (3) \n∑𝑥𝑖𝑗𝑘𝐼\n𝑖∈𝑉1≤𝑦𝑗 ∀𝑗∈𝑇𝑃,𝑘∈𝐾1 (4) \n𝑎𝑡𝑗𝑘𝐼=∑(𝑠𝑡𝑖𝑘𝐼+𝑆𝑖𝐼+𝑇𝑖𝑗𝑘𝐼)𝑥𝑖𝑗𝑘𝐼\n𝑖∈𝑉1 ∀𝑗∈𝑉1,𝑘∈𝐾1 (5) \n𝑠𝑡𝑖𝑘𝐼≥𝑎𝑡𝑖𝑘𝐼 ∀𝑖∈𝑉1,𝑘∈𝐾1 (6) \n𝑠𝑡𝑖𝑘𝐼+𝑆𝑖𝐼−𝑎𝑡𝑖𝑘𝐼≤𝐴𝐿𝑖 ∀𝑖∈𝑇𝑃,𝑘∈𝐾1 (7) \n∑ 𝑞𝑖𝑘\n𝑖∈𝑇𝑃≤𝑄𝑘𝐼 ∀𝑘∈𝐾1 (8) \n𝑞𝑗𝑘≤𝑀1∑𝑥𝑖𝑗𝑘𝐼\n𝑖∈𝑉1 ∀𝑗∈𝑇𝑃,𝑘∈𝐾1 (9) \n𝑞𝑖𝑘=∑ 𝐷𝑗 𝑝𝑖𝑗𝑘\n𝑗∈𝐻𝑅𝐶 ∀𝑖∈𝑇𝑃,𝑘∈𝐾1 (10) \n∑ ∑ 𝑝𝑖𝑗𝑘\n𝑖∈𝑇𝑃 𝑘∈𝐾1=1 ∀𝑗∈𝐻𝑅𝐶  (11) \n𝑝𝑖𝑗𝑘≤∑𝑥𝜐𝑖𝑘𝐼\n𝜐∈𝑉1 ∀𝑖∈𝑇𝑃,𝑘∈𝐾1 (12) \n∑ ∑ 𝐷𝑗  𝑝𝑖𝑗𝑘\n𝑗∈𝐻𝑅𝐶 𝑘∈𝐾1≤𝐶𝐴𝑃 𝑖 ∀𝑖∈𝑇𝑃 (13) \n𝐷𝑇𝑟 −𝐷𝐼𝑆 𝑖𝑗≤𝑀2𝑙𝑖𝑗 ∀𝑖∈𝑇𝑃,𝑗∈𝐻𝑅𝐶  (14) \n𝐷𝑇𝑟 −𝐷𝐼𝑆 𝑖𝑗≥𝑚1(1−𝑙𝑖𝑗) ∀𝑖∈𝑇𝑃,𝑗∈𝐻𝑅𝐶  (15) \n𝑦𝑖+𝑙𝑖𝑗≤1+𝑢𝑖𝑗 ∀𝑖∈𝑇𝑃,𝑗∈𝐻𝑅𝐶  (16) \n𝑦𝑖+𝑙𝑖𝑗≥2𝑢𝑖𝑗 ∀𝑖∈𝑇𝑃,𝑗∈𝐻𝑅𝐶  (17) \n∑ ∑𝑥𝑖𝑗𝑘𝐼𝐼≤1\n𝑗∈𝑉2 𝑖∈𝑉𝐷  𝑘∈𝐾2 (18) \n∑𝑥𝑖𝑗𝑘𝐼𝐼\n𝑖∈𝑉2≤𝑦𝑗 ∀𝑗∈𝑇𝑃,𝑘∈𝐾2 (19) \n∑ 𝑥𝑖𝜐𝑘𝐼𝐼\n𝑖∈𝑉2−∑ 𝑥𝜐𝑗𝑘𝐼𝐼\n𝑖∈𝑉2=0 ∀𝜐∈𝑇𝑃∪𝐻𝑅𝐶 ,𝑘∈𝐾2 (20) \n∑ ∑𝑥𝑖𝑗𝑘𝐼𝐼+∑𝑢𝑖𝑗\n𝑖∈𝑉2=1\n𝑖∈𝑉2 𝑘∈𝐾2 ∀𝑗∈𝐻𝑅𝐶  (21) ∑ ∑𝐷𝐼𝑆 𝑖𝑗 𝑥𝑖𝑗𝑘𝐼𝐼\n𝑗∈𝑉2 𝑖∈𝑉2≤𝐷𝐿 ∀𝑘∈𝐾2 (22) \n∑ ∑𝐷𝑗  𝑥𝑖𝑗𝑘𝐼𝐼≤𝑄𝑘𝐼𝐼\n𝑗∈𝑉2 𝑖∈𝑉2 ∀𝑘∈𝐾2 (23) \n𝑠𝑡𝑖𝑘𝐼𝐼≥(𝑠𝑡𝑖𝑘𝐼𝐼+𝑆𝑖𝐼𝐼+𝑇𝑖𝑗𝑘𝐼𝐼)𝑥𝑖𝑗𝑘𝐼𝐼 ∀𝑖,𝑗∈𝑉2,𝑘∈𝐾2 (24) \n𝑠𝑡𝑗𝑗𝐼𝐼𝐼=(𝑠𝑡𝑖𝑗𝐼𝐼𝐼+𝑆𝑖𝐼𝐼+𝑇𝑖𝑗𝐼𝐼𝐼)𝑢𝑖𝑗 ∀𝑖∈𝑇𝑃,𝑗∈𝐻𝑅𝐶  (25) \n𝑠𝑡𝑖𝑘̂𝐼𝐼−𝑠𝑡𝑖𝑘𝐼−𝑆𝑖𝐼≥𝑚2(1−𝑣𝑘𝑘̂𝑖) ∀𝑖∈𝑇𝑃,𝑘∈𝐾1,𝑘̂∈𝐾2 (26) \n𝑠𝑡𝑖𝑗𝐼𝐼𝐼−𝑠𝑡𝑖𝑘𝐼−𝑆𝑖𝐼≥𝑚3(1−𝑝𝑢𝑖𝑗𝑘) ∀𝑖∈𝑇𝑃,𝑗∈𝐻𝑅𝐶 ,𝑘∈𝐾1 (27) \n𝑝𝑖𝑗𝑘+𝑢𝑖𝑗≤1+𝑝𝑢𝑖𝑗𝑘 ∀𝑖∈𝑇𝑃,𝑗∈𝐻𝑅𝐶 ,𝑘∈𝐾1 (28) \n𝑝𝑖𝑗𝑘+𝑢𝑖𝑗≥2𝑝𝑢𝑖𝑗𝑘 ∀𝑖∈𝑇𝑃,𝑗∈𝐻𝑅𝐶 ,𝑘∈𝐾1 (29) \n𝑣𝑘𝑘̂𝑗≤∑𝑥𝑖𝑗𝑘𝐼\n𝑖∈𝑉1 ∀𝑗∈𝑇𝑃,𝑘∈𝐾1,𝑘̂∈𝐾2 (30) \n∑ 𝑣𝑘𝑘̂𝑖\n𝑘∈𝐾1=∑𝑥𝑖𝑗𝑘̂𝐼𝐼\n𝑗∈𝑉2 ∀𝑖∈𝑇𝑃,𝑘̂∈𝐾2 (31) \n∑ ∑ 𝑣𝑘𝑘̂𝑖\n𝑖∈𝑇𝑃 𝑘∈𝐾1≤1 ∀𝑘̂∈𝐾2 (32) \n∑ 𝑣𝑘𝑘̂𝑖\n 𝑘̂∈𝐾2+𝑢𝑖𝑗≥𝑝𝑖𝑗𝑘 ∀𝑖∈𝑇𝑃,𝑗∈𝐻𝑅𝐶 ,𝑘∈𝐾1 (33) \n𝑇𝐴𝑗∑ 𝑥𝑖𝑗𝑘𝐼𝐼≤𝑠𝑡𝑗𝑘𝐼𝐼\n𝑖∈𝑉2≤𝑇𝐵𝑗∑ 𝑥𝑖𝑗𝑘𝐼𝐼\n𝑖∈𝑉2 ∀𝑗∈𝐻𝑅𝐶 ,𝑘∈𝐾2 (34) \n𝑇𝐴𝑗∑ 𝑢𝑖𝑗≤𝑠𝑡𝑗𝑗𝐼𝐼𝐼\n𝑖∈𝑇𝑃≤𝑇𝐵𝑗∑ 𝑢𝑖𝑗\n𝑖∈𝑇𝑃 ∀𝑗∈𝐻𝑅𝐶  (35) \n𝑥𝑖𝑗𝑘𝐼,𝑥𝑖𝑗𝑘𝐼𝐼,𝑦𝑖,𝑝𝑖𝑗𝑘,𝑣𝑘𝑘̂𝑖,𝑢𝑖𝑗,𝑝𝑢𝑖𝑗𝑘 ∈{0,1}  ∀𝑖,𝑗∈𝑉,𝑘∈𝐾  (36) \n𝑠𝑡𝑖𝑘𝐼,𝑠𝑡𝑖𝑘𝐼𝐼,𝑠𝑡𝑖𝐼𝐼𝐼,𝑞𝑖𝑘≥0  ∀𝑖,𝑗∈𝑉,𝑘∈𝐾 (37) \nThe objective function (1) minimizes the total cost, including the travel cost of both echelons \nand period equivalent establishment cost of transshipment points.   \nConstraints (2) ensure that each vessel leaves the central hub at most once. Constraints (3) are \nflow constraints in the first echelon. Constraints (4) guarantee that a transshipment location can \nonly be visited if that point is established. Consistency of the arrival time and service time of vessels \nis guaranteed by constraints (5) and (6). Admis sible laying times are respected by constraints (7). \nConstraints (8) ensure that the capacity limits of the vessels are considered.  \nConstraints (9) express that the delivery volume of a specific vessel to a particular TP can be \nnon-zero only if that vesse l visits that TP. Quantities delivered to a TP should satisfy the demand \nof the HoReCa businesses allocated to this point. This is addressed by constraints (10). Each \ndemand point is served by one TP that is guaranteed by constraints (11). Constraints (12)  ensure \nthat a TP can serve a demand point only if it is already established. Capacity limits of transshipment \nlocations are met by constraints (13).  \nIf a demand point is allocated to a TP located in its proximity (the distance between two is less \nthan a p re-specified threshold), that point is served by a moving jack instead of an LEV. This is illustrated through constraints (14) -(17). Constraints (18) guarantee that each LEV can leave one of \nthe vehicle depots and at most once. An LEV can enter a TP if tha t point is already established. \nThis is addressed by constraints (19). Constraints (20) are flow constraints in the second echelon. \nConstraints (21) guarantee that each demand point is served either by an LEV or a moving jack. \nConstraints (22) respect the limited driving range of LEVs. As these vehicles are of small capacity, \nen-route charging is not logical for them, and battery refilling  takes place at depots. Constraints (23) \nensure that the capacity limits of LEVs are respected.  \nConsistency of t servic e time of the LEVs is guaranteed by constraints (24). Constraints (25) \nensure the same logic for moving jacks. Constraints (26) -(33) are synchronization constraints. \nExpressly, constraints (26) indicate that if a vessel and an LEV are synchronized, then th e service \ntime consistency should be met. Constraints (27) -(29) express the same logic for moving jacks. \nConstraints (30) -(33) illustrate how the synchronization of a vessel and an LEV takes place. T ime \nwindows are represented by constraints (34) and (35) . Finally, constraints ( 36) and ( 37) imply the \ntype of variables.    \nEquations (5), (2 4), and (2 5) are non -linear and are linearized as follows:  \n𝑎𝑡𝑗𝑘𝐼−𝑠𝑡𝑖𝑘𝐼−𝑆𝑖−𝑇𝑖𝑗𝑘𝐼≥𝑚4(1−𝑥𝑖𝑗𝑘𝐼) ∀𝑖,𝑗∈𝑉1,𝑘∈𝐾1 (38) \n𝑎𝑡𝑗𝑘𝐼−𝑠𝑡𝑖𝑘𝐼−𝑆𝑖−𝑇𝑖𝑗𝑘𝐼≤𝑀3(1−𝑥𝑖𝑗𝑘𝐼) ∀𝑖,𝑗∈𝑉1,𝑘∈𝐾1 (39) \n𝑠𝑡𝑗𝑘𝐼𝐼−𝑠𝑡𝑖𝑘𝐼𝐼−𝑆𝑖−𝑇𝑖𝑗𝑘𝐼𝐼≥𝑚5(1−𝑥𝑖𝑗𝑘𝐼𝐼) ∀𝑖,𝑗∈𝑉2,𝑘∈𝐾2 (40) \n𝑠𝑡𝑗𝑗𝐼𝐼𝐼−𝑠𝑡𝑖𝑗𝐼𝐼𝐼+𝑆𝑖−𝑇𝑖𝑗𝐼𝐼𝐼≥𝑚4(1−𝑢𝑖𝑗) ∀𝑖∈𝑇𝑃,𝑗∈𝐻𝑅𝐶  (41) \n𝑠𝑡𝑗𝑗𝐼𝐼𝐼−𝑠𝑡𝑖𝑗𝐼𝐼𝐼−𝑆𝑖−𝑇𝑖𝑗𝐼𝐼𝐼≤𝑀4(1−𝑢𝑖𝑗) ∀𝑖∈𝑇𝑃,𝑗∈𝐻𝑅𝐶  (42) \nWhere constraints ( 38) and ( 39) are linearized versions of constraint (5) and constraints ( 40) \nare of constraint (2 4). Constraints (2 5) are linearized by constraints ( 41) and ( 42). \n4. Solution Methodology  \nOur Two -Echelon Location Routing problem is solved by a hybrid solution algorithm that \ndecomposes the problem into two nested sub -problems, including the first echelon and second \nechelon problems. We first develop an Adaptive Large Neighborhood Search (ALNS) \nmetaheuristic  to determine the location and routing decisions in the second echelon. Then, based \non the provided results, we apply a Branch and Price (B&P) algorithm using the Dantzig -Wolfe \ndecom position principle to transf orm the first echelon routing model into a master problem and a \nsubproblem.  Algorithm 1. provides the pseudocode of our proposed solution methodology . \nBy applying the initial solution ( 𝑆𝑖𝑛), our developed ALNS optimizes the decisions to be made \nfor the second echelon ( 𝑆𝑓2). Then, based on the provided solution, the time windows and \naggregated demand for the established points are derived. The procedure re -iterates until the \ntermination crite ria are met. The termination criteria are to reach the maximum number of \niterations (𝕋𝐺) or the maximum number of iterations with no improvement.  \nTo enhance the performance of our solution approach, we first apply preprocessing, where we \nremove  non-admissible arcs in the second echelon concerning time  windows and vehicle capacity \nconstraints.  Algorithm 1.  The Two-Echelon Location Routing  Algorithm  \nInput:  Input Parameters Data  \nOutput:  Best-found feasible solution ( 𝑆𝑓∗) \n0 𝑂𝑏𝑗(𝑆𝑓∗)=∞ \n1 Generate the initial solution for the second echelon ( 𝑆𝑖𝑛2) \n2 Based on 𝑆𝑖𝑛2 generate the initial solution for the first echelon ( 𝑆𝑖𝑛1) \n3 𝑆𝑖𝑛={𝑆𝑖𝑛1,𝑆𝑖𝑛2} \n4 while  termination criteria are not met   \n5  𝑆𝑓2∗←𝐴𝐿𝑁𝑆 (𝑆𝑖𝑛2)   Apply ALNS to generate the best feasible solution \nfor the second echelon  \n6  𝑆𝑓1∗←𝐵&𝑃(𝑆𝑓2∗)   Based on  𝑆𝑓2∗ generate the best feasible solution for \nthe first echelon applying B&P  \n7  𝑆𝑓={𝑆𝑓1∗,𝑆𝑓2∗} \n8  if  𝑂𝑏𝑗(𝑆𝑓)≤𝑂𝑏𝑗(𝑆𝑓∗) \n9       𝑆𝑓∗←𝑆𝑓 \n10  end if  \n11  𝑆𝑖𝑛←𝑆𝑓 \n12 end while  \n13 return  𝑆𝑓∗ \n4.1. Feasibility and Penalty Calculation  \nOur developed AL NS allows infeasible solutions to be a part of the search procedure and applies \npenalties for vehicle capacity, transshipment location capacity, and time  windows violations . The \ngeneralized cost function of a solution S is formulated as:  \n 𝑓𝑔𝑒𝑛(𝑆)=obj+𝜚1𝑉𝐶𝑎𝑝𝑣(𝑆)+𝜚2𝑉𝐶𝑎𝑝𝑡(𝑆)+𝜚3𝑉𝑇𝑊(𝑆)+𝜚4𝑉𝐷𝑖𝑠(𝑆) (43) \nWhere obj is the objective function (Eq. ( 43)) and the violations of vehicle capacity ( 𝑉𝐶𝑎𝑝𝑣(𝑆)),  \ntransshipment point  capacity ( 𝑉𝐶𝑎𝑝𝑡(𝑆)), time windows ( 𝑉𝑇𝑊(𝑆)), and driving range  are scaled by \nthe penalty weights 𝜚1,𝜚2,𝜚3 and 𝜚4, respectively. Every 𝜓𝑃 iterations , referred to as the penalty \nupdate period, the penalty weights  are dynamically adjusted . If a constraint has been violated in at \nleast ℒ 𝜓𝑃 out of 𝜓𝑃 iterations, its respective penalty weight is multiplied by 𝜔𝑖 and it is divided \nby the same value if the limitation is met  in at least ℒ 𝜓𝑃 iterations. To control the magnitude of \n𝑓𝑔𝑒𝑛, the penalty factors are restricted to fall within the minimum and maximum values.  \nEfficient calculation of the changes in cost the function plays a crucial  role in the performance \nof our solution algorithm. The changes in capacity and driving range violations are trivially \nobtained in 𝒪(1). In order to calculate the time  windows violations , we incorporate the approach \nproposed by Nagata et al. (2010)  and Schneider et al. (2013) . Based on  this approach, the violations \nin a node  do not propagate to subsequent nodes  of a sequence . As service time at different vertices  \nis independent of the route sequence, the changes in time  windows violations can be calculated in \n𝒪(1).  \n \n 4.2.  Initial Solution  \nA route in the second echelon starts from a vehicle depot, heads to a transshipment location, then \nnavigates through several demand points, and finally returns to the initial vehicle depot. A solution \nfor the second echelon is comprised of several routes, an d each route is represented by a series of \npoints <𝑉𝐷 𝑛,𝑇𝑃𝑚,𝐻𝑅𝐶 1,…,𝐻𝑅𝐶 𝑖,…,𝐻𝑅𝐶 𝑘,𝑉𝐷 𝑛>, where 𝑉𝐷 𝑛 is the starting and ending depot \nfor the route, 𝑇𝑃𝑚 is the selected and established transshipment point, and \n𝐻𝑅𝐶 1,…,𝐻𝑅𝐶 𝑖,…,𝐻𝑅𝐶 𝑘 are the covered demand points within this route.  \nBarreto et al. (2007), Wang et al. (2013), and Akpunar and Akpinar (2021) are among the \nresearch works showing that grouping approaches have great potential in providing high -quality \nsolutions for capaci tated location routing problems. There exists several clustering algorithms in \nthe literature such as DBSCAN, Gaussian Mixture, and K -means. Despite its straightforward \napproach, K -means has shown a good performance in clustering spatial data ( Akpunar and \nAkpinar , 2021).  In this respect, we apply the K -means clustering algorithm to specify the number \nand location of established transshipment points in our initial solution. K -means partitions our set \nof demand points into K initial clusters, within each of which a transshipment point is established \nto serve the demand points. Interested readers are referred to Likes et al. (2003) for comprehensive \ninformation on the structure and performance of the K -means algorithm.   \nIn our algorithm, selecting the number o f clusters is a crucial step. We first take K as the rough \nestimation of the lower bound on the number of required transshipment points by dividing the total \ndemand of all demand points by the average capacity of transshipment locations. Cluster centers \nare chosen and updated from the set of potential candidates for locating transshipment points. Once \ndemand points are assigned to these clusters, the required items’ volume at each TP (cluster center) \nis obtained. If the capacity of at least one point is v iolated by over 25%, the number of clusters is \nincreased by one, and K -means is re -iterated. This procedure continues until no more than 25% \nviolation is observed. Once the established transshipment points are finalized, the closest depot to \neach point is selected as the starting and ending depot of the vehicles serving that transshipment \npoint. Accordingly, the first two points for each route representation can be specified.  \nBy having the established transshipment points, the demand points located within a  distance of \n𝐷𝑇𝑟 , which will be served by moving jacks, are specified and removed from the set of uncovered \ndemand points. In order to complete the remainder of each route, we apply the Semi -Parallel \nConstruction (SPC) heuristic proposed by Paraskevopoulos et al. (2008) . Our clustering  scheme \ncan enhance the performance of the applied s emi-parallel construction heuristic . SPC is an iterative \napproach , and  at each iteration,  we first select a cluster randomly and thereby specify the first two \npoints in the route. The set of uncovered demand points considered for insertion in this cluster is \ndecreased from the global set to the uncovered points within this cluster and outside the cluster but \nlocated within a maximum distance from the transshipmen t point. Accordingly, a unique point can \npotentially be a part of constructed routes of more than one cluster, implying that our choice of the \ninitially selected cluster may affect the constructed routes. In this respect, we specify the \nneighboring cluster s starting with the initial cluster. These are the clusters whose assigned demand \npoints (by K -means) are among the set of uncovered points of the initial randomly chosen cluster. \nThese clusters are added to the list of our candidate clusters, and for each  one, the set of within -cluster uncovered customers is specified. The procedure continues for all clusters in the candidate \nlist until no further neighboring cluster exists.  \nAt each iteration, an LEV for each cluster in the candidate list is  taken to serve a part of  \nuncovered  demand points . Then, for each LEV , a route covering a part of un covered  demand points  \nis constru cted based on a greedy approach while taking capacity constraints into account. For \ndiversification, we take a Restricted Ca ndidate List (RCL) of 𝓃 demand points  with the highest \nevaluation score and apply roulette wheel selection to select and insert a demand point . After \nconstructing the routes , the best cluster -route  is selected, and its covered customers are removed \nfrom t he set of global uncovered demand points. The procedure re -iterates until no further \nuncovered  demand points  are left. The best vehicle -route in each iteration has the lowest Average \nCost per Unit Transferred (ACUT), which divides the associated costs by t he sum of the demand \nof visited demand points . \n4.3. Adaptive Large Neighborhood Search  \nAdaptive Large Neighborhood Search (ALNS) is introduced by Ropke and Pisinger (2006a)  and \nincorporates  destroy and repair operators based on their success rates in previous iterations. We \nhave taken ALNS  as the core of our solution algorithm  for the second echelon  and adapt ed it to fit \ninto our problem by allowing infeasible solutions, incorporating problem -specific destroy  and \nrepair operators, and applying local search for intensification. In order to e nhance the speed of our \nalgorithm, the potential demand points for insertion in repair and local search operators are selected \nfrom the neighboring clusters. That is, a demand point that is considerably far from a transshipment \nlocation cannot be served by  that TP. The overview of our proposed ALNS is illustrated through \nAlgorithm 2.  \nThe termination criteria are to reach the maximum number of iterations ( 𝕋𝑚) or maximum \nnumber of iterations with no improvement ( 𝕋𝑛−𝑚). W e further apply a restarting mechanism \n(Hiremann et al.,  2016) to escape possible local optima. More specifically, a fter certain iterations \n(𝜓𝑅) with no improvement, the current solution is replaced by the best -obtained solution ( 𝑆2∗). \nWe will take a more in -depth look a t different components of the proposed ALNS in the following \nsubsections.  \nDestroy and Repair Operators  \nWe have two types of destroy operators. The first , with a large impact , changes a part of the \nproblem’s configuration by removing transshipment points, and the second , with a small impact , \naffects only a part of the constructed routes. A destroy operator removes at least 𝒢 nodes from the \ncurrent solution , where 𝒢 is a percentage of all nodes. This percentage is randomly drawn from a \ngiven interval [Λ𝑚𝑖𝑛,Λ𝑚𝑎𝑥]. The following destroy operators are used in our ALNS algorithm , \nwhere the first four are operators with small impact and the rest with large .  \nRandom removal  removes 𝒢 arbitrary nodes from the current solution, where all nodes have \nequal removal chances.  \n \n \n Algorithm 2.  ALNS  \nInput: Initial Solution ( 𝑆𝑖𝑛2) \nOutput: Best obtained feasible solution for the second echelon ( 𝑆𝑓2∗) \n1 𝑆2←𝑆𝑖𝑛2 and 𝑆2∗←𝑆𝑖𝑛2 \n2 if 𝑆𝑖𝑛2 is feasible  \n3  𝑆𝑓2∗←𝑆𝑖𝑛2 \n4 end if  \n5 𝑖,𝑗←0  \n6 while  termination criteria are not met  \n7  𝑆2′← Destroy&Repair ( 𝑆2) \n8  𝑆2′← LocalSearch ( 𝑆2′) \n9  if 𝑆2′ is accepted  \n10   𝑆2← 𝑆2′ \n11   if 𝑓𝑔𝑒𝑛(𝑆2′)≤𝑓𝑔𝑒𝑛(𝑆2∗)   \n12    𝑆2∗← 𝑆2′ \n13   end if  \n14    if  𝑆2′is feasible and 𝑓𝑔𝑒𝑛(𝑆2′)≤𝑓𝑔𝑒𝑛(𝑆𝑓2∗)   \n15    𝑆𝑓2∗←𝑆2′  \n16    𝑗←𝑖  \n17   end if  \n18  end if  \n19  UpdateScore( 𝑆2′)  \n20  if 0≡(𝑖+1)  𝑚𝑜𝑑 (𝜓𝑃)  \n21        UpdatePenalty( 𝑆2′) \n22  end if  \n23  if 0≡(𝑖−𝑗+1)  𝑚𝑜𝑑 (𝜓𝑅)  \n24   𝑆2←𝑆2∗  \n25  end if  \n26  if 0≡(𝑖+1)  𝑚𝑜𝑑 (𝜓𝐿)  \n27   AdaptSelectionScores()  \n28  end if  \n29  𝑖←𝑖+1  \n30 end while  \n31 return  𝑆𝑓2∗ \nWorst removal  was introduced by Ropke and Pisinger (2006b) , where the idea is to remove \nthe most expensive  parts of the solution. The associated cost of each node is determined as \nthe difference between the costs of the solution, with and without that node. Then, we take a \nRestricted Candidate List (RCL) of 𝓃 nodes with the highest costs and apply roulette wh eel \nselection to select and remove a node. The procedure continues till 𝒢 nodes are removed.  \nShaw removal  was introduced by Shaw (1997) and removes the demand points  concerning \ntheir relatedness. The relatedness between any two pairs of customers is defin ed based on their \ndistance, demand, and earliest service start time and is formulated as:  \n  \n   𝑅𝑒(𝑖,𝑗)=𝜛1𝐷𝑖𝑠𝑡 𝑖𝑗\n𝑚𝑎𝑥\n𝑖,𝑗∈𝜗𝐷(𝐷𝑖𝑠𝑡 𝑖𝑗) +𝜛2|𝐷𝑖−𝐷𝑗|\n𝑚𝑎𝑥\n𝑖∈𝜗𝐷(𝐷𝑖)−𝑚𝑖𝑛\n𝑖∈𝜗𝐷(𝐷𝑖)  +𝜛3|𝑇𝐴𝑖−𝑇𝐴𝑗|\n𝑚𝑎𝑥\n𝑖∈𝜗𝐷(𝑇𝐴𝑖)−𝑚𝑖𝑛\n𝑖∈𝜗𝐷(𝑇𝐴𝑖)   (44) \nWhere 𝜛1, 𝜛2, and 𝜛3 are normalizing weights . The operator removes the first demand point  \nrandomly. Then, a point  is randomly selected from the list of removed demand points , and the Shaw relatedness measure is derived for any pairs containing that customer and non -removed \ncustomers. We take a Restricted Candidate List (RCL) of 𝓃 demand points  with the largest \nrelatedness values and apply roulette wheel selection to select a nd remove a demand point . \nThe procedure continues till 𝒢 nodes are removed.  \nRoute removal  has two variants : Random Route removal and Inefficient Route removal. The \nfirst selects random routes to be removed until at least 𝒢 demand points  are removed from the \ncurrent solution. The second one takes a Restricted Candidate List (RCL) of 𝓃 routes with the \nlargest ACUT and applies a roulette wheel selection to select and remove a route. The \nprocedure continues until at least 𝒢 nodes are removed.  \nTransshipment point removal  is introduced by Hemmelmayer et al. (2012), where a \ntransshipment point is chosen randomly f rom the list of open ones and gets closed. Therefore, \nall routes originating from this TP are removed, adding their covered demand points together \nwith points served by moving jacks from that TP to the customer pool. Furthermore, a TP is \nrandomly chosen, a nd if it is not already established is opened. This prevents situations in \nwhich all transshipment points  would be closed . By opening the new TP, demand points to be \nserved by moving jacks from that transshipment location are specified and removed from the ir \ncurrent routes or customer pool.   \nTransshipment point  opening  is introduced by Hemmelmayer et al. (2012), where we \nchoose a transshipment point randomly among unestablished ones and open it. Then, all \ndemand points that can be served by moving jacks fr om this TP are removed from their current \nroutes. If the number of these removed points is smaller than 𝒢, we continue by removing the \nremainder of demand points from the set of closest ones to this TP.   \nTransshipment point  swap  is introduced by Hemmelma yer et al. (2012), where the TP \nremoval operator is applied first. Then, we use a Restricted Candidate List (RCL) of 𝓃 \nunestablished clusters with the shortest distance from the removed TP and apply a roulette \nwheel selection to select and establish a new  transshipment location. Similarly, By opening \nthe new TP, demand points to be served by moving jacks from that transshipment location are \nspecified and removed from their current routes or customer pool.   \nOur ALNS applies the following repair operators.  \nGreedy insertion investigates the increase in cost associated with adding each unassigned \ndemand point  to each position of a route. Then, the route and position with the lowest cost \nincrease are selected.  \nRegret insertion was introduced by Ropke and Pising er (2006b), where the idea is to first insert \na demand point  in the best route and position for which a later insertion causes the highest \nadditional costs. The operator specifies the best position and insertion cost of all current partial \nroutes for each demand point . A k -regret value of a customer, which projects the cost difference \nbetween the cheapest route and k -1 next cheapest routes, is calculated. Then, the demand point  \nwith the largest k -regret value is selected, and the procedure re -iterates until  all demand points  \nare inserted.  \nSPC -based insertion modifies the SPC heuristic of the construction phase by inserting demand \npoints  to existing and new routes , provided th at capacity constraints are met.  \n Local Search  \nTo further improve the results of the destroy and repair operators, a Local Search (LS) is applied \nthat uses a composite neighborhood , including the well -known 2 -opt (Potvin & Rousseau, 1995) , \n2*-opt (Potvin & Rousseau, 1995) , Reinsertion (Savelsbergh, 1992) , and Swap( 𝑛−1), where 𝑛=\n1,…,4 (Savelsbergh, 1992)  moves.  \nThe list of the applied neighborhoods is randomly ordered, and the algorithm starts by searching \nthe first one until no further cost reductions c an be achieved. Then, the next neighborhoods are \ntaken and searched one after another. Once the last neighborhood of the list is searched, the \nprocedure starts again and iteratively continues until a local minimum is obtained. To speed up the \nprocedure, in  each iteration of our LS, the first 𝒥 generated moves are taken, and the best -\nimproving move is selected. If no improving move exists  among these moves, the procedure \ncontinues until an improving solution is found or the neighborhood is searched complete ly.  \nAcceptance Criteria  \nWe apply  Simulated Annealing  (SA)-based acceptance criteria in our developed ALNS: If a \nsolution is improving, always accept it. Decide about non -improving (deteriorating) solutions \nbased on a probability that depends on the amount of solution deterioration and the temperature \nfollowing a cooling scheme.  \nWe store two best solutions during iterations of ALNS: the best feasible solution found so far \n(𝑆𝑓2∗) and the best non -necessarily feasible solution ( 𝑆2∗) with its current penalty values. So, we \nneed to adapt the value of the best solution after each change in penalty weights. This can lead to \na 𝑆2∗ that is higher than the best feasible solution, where it should be replaced by 𝑆𝑓2∗. \nAdaptive Mechanism  \nThe destroy and rep air operators are selected using a roulette wheel mechanism. The selection \nprobability of each operator relies on its historical performance, which is projected by a weight. \nThese weights are equal at the beginning of the algorithm and are updated after ea ch 𝜓𝐿 iterations , \nwhich is referred to as the adaption period. The weight of an operator i in adaption period t is \nupdated as:  \n𝒲𝑖𝑡=𝜃SN𝑖𝑡  \nNN 𝑖𝑡  +(1−𝜃)𝒲𝑖  𝑡−1  (45) \nWhere 𝜃 is the smoothing factor, SN 𝑖  shows the success score of the operator in the current update \nperiod and NN 𝑖𝑡  reflects the number of times the operator has been applied in update period t. In \neach update period, SN 𝑖𝑡  is initially set to zero and is updated by a scoring scheme using    \n𝑆𝑁 𝑖𝑡=𝑆𝑁 𝑖𝑡+𝜎𝑖,𝑖=1,2,3.  \n• 𝜎1 is applied when the generated solution is the best solution found so far.  \n• 𝜎2 is applied when the generated solution has not been accepted before and improves the \ncurrent solution.  \n• 𝜎3 is applied when the generated  solution has not been accepted before, is non -improving \nbut accepted.  \n 4.4. Branch and Price  \nOnce the solution of the second echelon is obtained, the problem at the first echelon is seen as a \nsplit delivery vehicle routing problem with time windows, where the established transshipment \npoints are seen as the demand points.  Next, we need to specify the demand and the time windows \nat TPs. Since split deliveries are admissible, we cannot treat the set of all allocated demand points \nof a TP as a unit point. This is because all points visited by a single LEV should be served by a \nunique vessel due to syn chronization constraints. Accordingly, we will have the accumulation of \ndemands served by each LEV as one unique demand, located at its initial TP and with time \nwindows respecting the time windows of all those demand points. Then, we need to create copies \nof TPs with demands equal to the demand of points served by a moving jack or accumulated \ndemand of each LEV.  \nThis potentially can lead to many serving points with the same location and thereby high \ndegeneracy of the problem. In order to mitigate this, dem and points can be merged under certain \nconditions. Since split delivery was to resolve the problem of limited vessel capacity and \nconsiderably different time windows, the following model can be applied to merge the points in \nan efficient way for each TP:  \nParameters  \n𝐶𝐴𝑃  A percentage of the smallest vessel’s capacity (e.g., 25 %)  \n𝑇𝑅  The threshold for the difference in time windows   \nDecision variables  \n𝜇𝑖  1: if group 𝑖∈𝐼 is formed   \n 0: otherwise  \n𝜆𝑖𝑗  1: if point  𝑗∈𝐽 is merged into group  𝑖∈𝐼 \n 0: otherwise  \n \n𝑷𝟐:𝑚𝑖𝑛  𝑍=∑𝜇𝑖\n𝑖∈𝐼 (46) \ns.t. \n∑𝐷𝑗\n𝑗∈𝐽𝜆𝑖𝑗≤𝐶𝐴𝑃 .𝜇𝑖 ∀𝑖∈𝐼 (47) \n∑𝜆𝑖𝑗\n𝑖∈𝐼=1 ∀𝑗∈𝐽 (48) \n|𝑇𝐴𝑗−𝑇𝐴𝑗′|𝜆𝑖𝑗𝜆𝑖𝑗′≤𝑇𝑅 1 ∀𝑖∈𝐼,𝑗,𝑗′∈𝐽 (49) \n|𝑇𝐵𝑗−𝑇𝐵𝑗′|𝜆𝑖𝑗𝜆𝑖𝑗′≤𝑇𝑅  ∀𝑖∈𝐼,𝑗,𝑗′∈𝐽 (50) \n \n1 Constraints  (49) and ( 50) are non -linear and can be linearized as:  \n(𝑇𝐴𝑗−𝑇𝐴𝑗′)𝜂𝑖𝑗𝑗′≤𝑇𝑅  ∀𝑖∈𝐼,𝑗,𝑗′∈𝐽  \n(𝑇𝐴𝑗′−𝑇𝐴𝑗)𝜂𝑖𝑗𝑗′≤𝑇𝑅 ∀𝑖∈𝐼,𝑗,𝑗′∈𝐽  \n(𝑇𝐵𝑗−𝑇𝐵𝑗′)𝜂𝑖𝑗𝑗′≤𝑇𝑅  ∀𝑖∈𝐼,𝑗,𝑗′∈𝐽  \n(𝑇𝐵𝑗′−𝑇𝐵𝑗)𝜂𝑖𝑗𝑗′≤𝑇𝑅 ∀𝑖∈𝐼,𝑗,𝑗′∈𝐽  \n𝜆𝑖𝑗+𝜆𝑖𝑗′≤1+𝜂𝑖𝑗𝑗′  ∀𝑖∈𝐼,𝑗,𝑗′∈𝐽  \n𝜆𝑖𝑗+𝜆𝑖𝑗′≥2 𝜂𝑖𝑗𝑗′ ∀𝑖∈𝐼,𝑗,𝑗′∈𝐽  \n 𝜇𝑖,𝜆𝑖𝑗 ∈{0,1} ∀𝑖∈𝐼,𝑗∈𝐽 (51) \nThe problem is a variant of the bin packing problem for which a strong valid inequalit y exist s \nas follow s (Correia et al., 2008) . \n𝜆𝑖𝑗≤𝜇𝑖  ∀𝑖∈𝐼,𝑗∈𝐽 (52) \n∑ 𝜇𝑖 𝑖∈𝐼≥|𝐼|−𝑟+1  ∀𝑖∈𝐼,𝑗,𝑗′∈𝐽 (53) \nWhere |𝐼| is the size of potential groups, and r is a value satisfying the following inequality:  \n(|𝐼|−𝑟−1 )𝐶𝐴𝑃 <∑ 𝐷𝑗 𝑗∈𝐽≤(|𝐼|−𝑟)𝐶𝐴𝑃   ∀𝑖∈𝐼,𝑗,𝑗′∈𝐽 (54) \nSince the capacity of transshipment points is limited, the size of 𝐽 for each TP is rather confined. \nTherefore, applying this valid inequality , the problem can be solved to optimality in a reasonable \ntime.  \nIn this way, the problem at the first echelon is transformed into a classic VRP with time \nwindows, to solve which there exist s extensive research applying branch and price. The approach \nworks based on Dantzig Wolfe Decomposition (DWD), where the main optimization problem is \ndecomposed into a m aster and several sub -problems (see Desaulniers  et al. (2006)  for details ). It \nexploits the fact that in a classic VRP with time windows , the constraint associated with serving \nthe demand point with one of the existing vehicles is the only constraint linking the vehicles \ntogether. By n eglecting this  constraint, the problem can be decomposed into sub -problems (each \nfor one vehicle) that take the form of the shortest path problem with resource constraint (time \nwindows and vehicle capacities).  \n \n5. Numerical Results  \nIn this section, we provide the resul ts of the conducted numerical experiments on the developed  \nsolution approach for solving our proposed variant of the two-echelon location routing problem \nwith time windows and synchronization. As illustrated in Table 1, the problem is new , for which  \nno ben chmark instances exis t. Accordingly, the experiments are carried out on our newly -\ngenerated benchmark instances. Furthermore, we assess the performance of our developed solution \napproach on the available benchmark instances for 2E -VRP. Finally, a case stud y is presented to \nillustrate the results of the problem in a practical setting for the city of Amsterdam, followed by  \nsensitivity analysis on the input parameters and discussion . \nThe experiments are conducted on a computer with Intel® Core i7 -8650U CPU 1.9 GHz, 2.11 \nGHz, and 32 GB memory available.  Our developed solution approach  was implemented and run \non Python 3.6  and applied IBM ILOG CPLEX Optimization St udio 12.7 .  \n5.1. Parame ter Tuning and Generation of Benchmark Instances  \nFor tuning parameters, we first searched the literature for the existing values and applied values in \ndifferent reasonable ranges for the newly introduced (non -existing) parameters.  To achieve a \nsetting with a good performance, we then investigated the impact of modifying the values of these \nparameters on a number of problem instances, each time altering one and remaining others unchanged. The selected setting, that provided the best results found and is applied in our \nnumerical experiments, is represented in Table 2.  \nTable 2. Applied parameter setting of the developed solution algorithm  \nGeneral  ALNS  \n𝕋𝐺 50 𝕋𝑚 2000  \n(𝜚1, 𝜚2, 𝜚3,𝜚4) (10,10,10 ,10) 𝕋𝑛−𝑚 250 \n(𝜚1𝑚𝑖𝑛, 𝜚2𝑚𝑖𝑛, 𝜚3𝑚𝑖𝑛,𝜚4𝑚𝑖𝑛) (0.1,0.1,0.1 ,0.1) 𝜓𝑅 200 \n(𝜚1𝑚𝑎𝑥, 𝜚2𝑚𝑎𝑥, 𝜚3𝑚𝑎𝑥,𝜚4𝑚𝑎𝑥) (5000,5000,5000 ,5000 ) 𝜓𝐿 50 \n𝜓𝑃 10 [Λ𝑚𝑖𝑛,Λ𝑚𝑎𝑥] [0.05,0.15]  \nℒ 𝜓𝑃 0.25 ×𝜓𝑃 (𝜛1, 𝜛2, 𝜛3) (6,4,5)  \n(𝜔1, 𝜔2, 𝜔3) (1.2,1.2,1.2)  𝒥 50 \n𝓃 5 𝜃 0.6 \nThe benchmark instances involve ten instance sets classified based on their number of \ncustomers ranging from 5 to 200 and labeled as small (5 -25 customers), medium (50 and 75 \ncustomers) , and large (100 -200 customers) size sets. The customer locations are randomly selected \nwithin a target zone, and the zone is considered to occupy an area of 0.5 km2 for instances of 5, \n10, 15, 20, and 25 customers, 1 km2 for 50 customers, 5 km2 for 75 customers, 10 km2 for 100 \ncustomers, 15 km2 for 150  customers,  and 25 km2 for 200 customers. The number of LEV depots \nin small instance s is either 1 or 2, while for medium and large size instances , this value is 3 or 4. \nThe number of transshipment points ranges from 2 to 4 in small size and 5 to 10 in medium and \nlarge size instances. Then, an instance with ID SI-Dm-Cn-To represents  a small size instance with \nm depots, n customers , and o transshipment points.  \nDistances are transformed i nto travel time by considering speeds of 30 km/h for LEVs  and 5 to \n15 km/h for vessels  trave ling from the central hub to TPs . The unit energy consumption cost is \n0.27 € /km for LEV s and ranges between 1.8 and 2.5 €/km for vessel s. The period equivalent fixed \ncost of establishing transshipment point s ranges from 100  to 175 €, based on the space of that TP.  \n5.2. Experiment on Existing Benchmark Instances  \nIn order to assess the performance of our proposed ALNS+B&P , we conduct experiments on \nexisting benchmark instances for 2E-VRP , including two instance sets (sets 2 and 3) from Perboli \net al. (2011)  and one larger instance set (set 5) from Hemmelmayer et al. (2012) . In this regard, \nassumptions associated with locating TPs, electric vehicles, time windows, synchronization , and \nmultiple delivery modes are relaxed. Therefore, all related procedures are eliminated  from the \nsolution approach , and our ALNS+B&P is decreased to solve a classic 2E-VRP .  \nThese three sets include 21, 18, and 18 instances, where for se ts 2 and 3 , the number of \ncustomers ranges from 21 to 50, with 2 or 4 transshipment locations , and for set 4, the number of \ncustomers is either 100 or 200, with 5 or 10 transshipment locations . In Table 3, the performance \nof our proposed ALNS on these benc hmark instances is compared with those of  Hemmelmayer et \nal. (2012) , Breunig et al. (2016) , Enthoven et al. (2020) , and Vincent et al. (2021) .   \nThe s econd column of the table provides the Best -Known Solution (BKS) reported in Breunig \net al. (2016) . The next column s present the gap of the four papers’  results to this BKS , and finally , \nwe have the results of our proposed ALNS+B&P . The average run  time and average gaps are reported in the last two rows of the table. It should be noted that these algorit hms were run on \ndifferent platforms, and our proposed solution approach is the only one among these four that \nincorporates an exact algorithm (B&P) in its structure. Accordingly , a precise comparison of time \nis not possible.  \nTable 3. The results of  experi ments on existing 2E-VRP  benchmark instances  \nInstance  BKS  𝜟𝑩𝑲𝑺  \nHemmelmayr et al. \n(2012)  Breunig et al. \n(2016)  Enthoven et al. \n(2020)  Vincent et al. \n(2021)  Our proposed \napproach  \nSet 2        \nE-n22-k4-s6-17 417.07  0.00 % 0.00 % 0.00 % 0.00 % 0.00 % \nE-n22-k4-s8-14 384.96  0.00 % 0.00 % 0.00 % 0.00 % 0.00 % \nE-n22-k4-s9-19 470.6  0.00 % 0.00 % 0.00 % 0.00 % 0.00 % \nE-n22-k4-s10-14  371.5  0.00 % 0.00 % 0.00 % 0.00 % 0.00 % \nE-n22-k4-s11-12 427.22  0.00 % 0.00 % 0.00 % 0.00 % 0.00 % \nE-n22-k4-s12-16 392.78  0.00 % 0.00 % 0.00 % 0.00 % 0.00 % \nE-n33-k4-s1-9 730.16  0.00 % 0.00 % 0.00 % 0.00 % 0.00 % \nE-n33-k4-s2-13 714.63  0.00 % 0.00 % 0.00 % 0.00 % 0.00 % \nE-n33-k4-s3-17 707.48  0.00 % 0.00 % 0.00 % 0.00 % 0.00 % \nE-n33-k4-s4-5 778.74  0.00 % 0.00 % 0.00 % 0.05 % 0.05 % \nE-n33-k4-s7-25 756.85  0.00 % 0.00 % 0.00 % 0.00 % 0.00 % \nE-n33-k4-s14-22 779.05  0.00 % 0.00 % 0.00 % 0.00 % 0.00 % \nE-n51-k5-s2-17 597.49  0.00 % 0.00 % 0.00 % 0.00 % 0.00 % \nE-n51-k5-s4-46 530.76  0.00 % 0.00 % 0.00 % 0.00 % 0.00 % \nE-n51-k5-s6-12 554.81  0.00 % 0.00 % 0.00 % 0.04 % 0.00 % \nE-n51-k5-s11-19 581.64  0.00 % 0.00 % 0.00 % 0.00 % 0.00 % \nE-n51-k5-s27-47 538.22  0.00 % 0.00 % 0.00 % 0.00 % 0.00 % \nE-n51-k5-s32-37 552.28  0.00 % 0.00 % 0.00 % 0.00 % 0.00 % \nE-n51-k5-s2-4-17-46 530.76  0.00 % 0.00 % 0.00 % 0.00 % 0.00 % \nE-n51-k5-s6-12-32-37 531.92  0.00 % 0.00 % 0.00 % 0.00 % 0.00 % \nE-n51-k5-s11-19-27-47 527.63  0.00 % 0.00 % 0.00 % 0.00 % 0.00 % \nSet 3        \nE-n22-k4-s13-14 526.15  0.00 % 0.00 % 0.00 % 0.00 %  0.00 % \nE-n22-k4-s13-16 521.09  0.00 % 0.00 % 0.00 % 0.00 %  0.00 % \nE-n22-k4-s13-17 496.38  0.00 % 0.00 % 0.00 % 0.00 %  0.00 % \nE-n22-k4-s14-19 498.8  0.00 % 0.00 % 0.00 % 0.00 %  0.00 % \nE-n22-k4-s17-19 512.8  0.00 % 0.00 % 0.00 % 0.00 %  0.00 % \nE-n22-k4-s19-21 520.42  0.00 % 0.00 % 0.00 % 0.00 %  0.00 % \nE-n33-k4-s16-22 672.17  0.00 % 0.00 % 0.00 % 0.00 %  0.00 % \nE-n33-k4-s16-24 666.02  0.00 % 0.00 % 0.00 % 0.00 %  0.00 % \nE-n33-k4-s19-26 680.36  0.00 % 0.00 % 0.00 % 0.00 %  0.00 % \nE-n33-k4-s22-26 680.36  0.00 % 0.00 % 0.00 % 0.00 %  0.00 % \nE-n33-k4-s24-28 670.43  0.00 % 0.00 % 0.00 % 0.00 %  0.00 % \nE-n33-k4-s25-28 650.58  0.00 % 0.00 % 0.00 % 0.00 %  0.00 % \nE-n51-k5-s12-18 690.59  0.00 % 0.00 % 0.00 % 0.66 % 0.00 % \nE-n51-k5-s12-41 683.05  0.00 % 0.00 % 0.00 % 1.70 % 0.00 % \nE-n51-k5-s12-43 710.41  0.00 % 0.00 % 0.00 % 0.00 %  0.00 % \nE-n51-k5-s39-41 728.54  0.00 % 0.00 % 0.00 % 0.00 %  0.00 % \nE-n51-k5-s40-41 723.75  0.00 % 0.00 % 0.00 % 0.38 % 0.00 % \nE-n51-k5-s40-43 752.15  0.00 % 0.00 % 0.00 % 0.26 % 0.00 % \nSet 5        \n100-5-1 1564.46  0.06  % 0.00 % 2.63 % 0.31 % 0.00  % \n100-5-1b 1108.62  0.25  % 0.00 % 1.18 % 1.88 % -0.41  % \n100-5-2 1016.32  0.00  % 0.00 % 0.57 % 0.50 % 0.00  % \n100-5-2b 782.25  0.00  % 0.00 % 0.00 % 0.06 % 0.00  % \n100-5-3 1045.29  0.00  % 0.00 % 0.07 % 0.05 % 0.00  % \n100-5-3b 828.54  0.05  % 0.00 % 0.05 % 0.13 % 0.00  % \n100-10-1 1124.93  0.47  % 0.05 % 0.01 % 0.02 % 0.00  % Instance  BKS  𝜟𝑩𝑲𝑺  \nHemmelmayr et al. \n(2012)  Breunig et al. \n(2016)  Enthoven et al. \n(2020)  Vincent et al. \n(2021)  Our proposed \napproach  \n100-10-1b 916.25  0.03  % 0.00 % 0.55 % 0.90 % -0.49  % \n100-10-2 990.58  0.00  % 2.18 % 2.03 % 2.57 % 0.00  % \n100-10-2b 768.61  0.00  % 1.65 % 0.86 % 3.14 % 0.00  % \n100-10-3 1043.25  0.00  % 0.62 % 0.94 % 0.63 % 0.00  % \n100-10-3b 850.92  0.00  % 0.47 % 0.98 % 1.05 % -0.14  % \n200-10-1 1556.79  1.11  % 1.51 % 0.91 % 0.13 % -1.18  % \n200-10-1b 1187.62  1.19  % 0.33 % 0.63 % 0.09 % -0.99  % \n200-10-2 1365.74  0.66  % 0.05 % 1.71 % 1.92 % 0.00  % \n200-10-2b 1002.85  0.09  % 0.56 % -0.02 % 0.42 % 0.00  % \n200-10-3 1787.73  0.00  % 0.56 % 2.79 % 1.47 % -0.45  % \n200-10-3b 1197.9  0.24  % 0.36 % 1.84 % 0.95 % 0.00  % \nAvg. Gap   0.077 % 0.155 % 0.328 % 0.354 % -0.067 % \nAvg. Time (min)   4.31 5.67 5.71 4.76 6.48 \nAs the table represents, the average gap of our proposed solution approach is 0.14% lower than \nHemmelmayr et al. (2012) , 0.22% lower than Breunig et al. (2016) , 0.40% lower than Enthoven \net al. (2020) , and 0.42% lower than Vincent et al. (2021) . It should be noted that for sets 2 and 3, \nour results are almost identical to BKS. This is while for set 5, which embraces larger sizes,  our \nproposed methodology  improves BKS in six cases, with an average gap of -0.20 %. Considering \nthese three sets of instances, our developed method provides better results than Hemmelmayr et \nal. (2012), Breunig et al. (2016), Enthoven et al. (2020), and Vincent et al. (2021)  in 12, 13, 16, \nand 24 cases, respectively.    \n5.3. Experiment on Newly Generated Benchmark Instances  \nThis section provides experimental results on our newly generated benchmark instances . We first \nuse the insta nce sets with 5 -25 customers , 1 or 2 LEV depots,  and 2 to 4 transshipment points  to \nanalyze the performance of our proposed solution algorithm on small -size instances. To this end, \nthe results of the proposed solution algorithm are compared with the optimal (global or local) \nsolutions provided by CPLEX.  \nTable 4 provides the results of  our experiments on small -size instances. For CPLEX, the \nobjective function value ( 𝑍1) and run time ( t) in seconds are reported. The computing time of \nCPLEX is limited by 2 hours (7200 seconds). So, optimality is not guaranteed for instances that \nhave reached this upper boun d. In our proposed solution algorithm,  𝑍1 is associated with the best -\nfound soluti on in ten runs of the algorithm. Δ𝐶𝑃𝐿𝐸𝑋  represents the gap of the obtained objective \nfunction value to the one provided by CPLEX.  \nTable 4. The results of  experiments on newly generated small -size benchmark instances  \nInstance  CPLEX   Proposed Solution Algorithm  \n𝑍1 t (s)  𝑍1        Δ𝐶𝑃𝐿𝐸𝑋  t (s) \nSI-D1-C5-T2 119.5287  1.06  119.5287  0.000  % 5.28 \nSI-D2-C5-T2 119.4544  1.17  119.4544  0.000  % 5.32 \nSI-D1-C5-T3 119.3063  1.16  119.3063  0.000  % 5.28 \nSI-D2-C5-T3 119.4544  1.19  119.4544  0.000  % 5.64 \nSI-D1-C5-T4 119.3063  1.18  119.3063  0.000  % 7.12 \nSI-D2-C5-T4 119.4544  1.26  119.4544  0.000  % 6.44 SI-D1-C10-T2 119.9658  2.42  119.9658  0.000  % 11.43  \nSI-D2-C10-T2 120.1536  1.97  120.1536  0.000  % 14.02  \nSI-D1-C10-T3 119.9658  2.64  119.9658  0.000  % 11.69  \nSI-D2-C10-T3 120.1536  2.73  120.1536  0.000  % 13.23  \nSI-D1-C10-T4 119.9658  2.81  119.9658  0.000  % 12.01  \nSI-D2-C10-T4 120.1536  2.75  120.1536  0.000  % 15.87  \nSI-D1-C15-T2 286.1563  43.16   286.1563  0.000  % 31.34  \nSI-D2-C15-T2 286.1212  52.84   286.2178 0.034 %  28.82  \nSI-D1-C15-T3 231.3617  54.25   231.3617  0.000 %  32.53  \nSI-D2-C15-T3 230.9398  580.52   230.9398  0.000 %  35.72  \nSI-D1-C15-T4 231.3617  675.63   231.3617  0.000 %  34.03  \nSI-D2-C15-T4 230.9398  652.23   230.9398  0.000 %  37.45  \nSI-D1-C20-T2 295.9807  7200   295.98 07 0.000  % 88.74  \nSI-D2-C20-T2 295.7998  7200   295.7854  -0.005  % 84.67  \nSI-D1-C20-T3 296.8966  7200   296.7154  -0.061  % 95.62  \nSI-D2-C20-T3 296.9625  7200   296.8275  -0.045  % 105.35  \nSI-D1-C20-T4 296.7437  7200   296.7154  -0.010  % 82.38  \nSI-D2-C20-T4 296.8345  7200   296.8275  -0.002  % 104.95  \nSI-D1-C25-T2 298.0373  7200   297.5612  -0.160  % 137.42  \nSI-D2-C25-T2 299.1454  7200   297.5629  -0.434  % 121.06  \nSI-D1-C25-T3 299.3157  7200   298.2126  -0.369  % 132.14  \nSI-D2-C25-T3 298.8595  7200   298.0089  -0.285  % 130.01  \nSI-D1-C25-T4 298.3536  7200   298.2126  -0.047  % 141.77  \nSI-D2-C25-T4 298.5121  7200   298.3093  -0.068  % 162.82  \nAvg.  2949.36   -0.048 % 56.67  \nAs the results in Table 4 illustrate, our proposed solution algorithm establishes a good performance \nin solving small -size problems to optimality in a short time.  For instance , sets of 5, 10, and 15 \ncustomers, the optimality of the results provided by CPLE X was guaranteed , and the computation \ntime was shorter than two hours. Since the obtained gap is zero in these instances, the results \nprovided by our proposed algorithm are also globally optimum. In instances with 20 and 25 \ncustomers, CPLEX was unable to r each the global optim al within two hours. In these instances, \nthe gap of CPLEX to the linear relaxation of the objective function found in iterations of branch \nand bound was less than 0.7%. This implies that the obtained solutions by CPLEX were  either \nglobally optim al or very near to the global optim al. In these 12 instances, our solution algorithm \nachieved  results equal to or smaller than those provided  by CPLEX.  \nTable 5 presents the result of investigating the performance of our proposed algorithm that \napplies ALNS, hybridized with K-means  and LS, together with B&P on benchmark instances of \n50-200 customers.  To this end, the objective function value of the Best Known Solution (BKS), \nwhich is found during the overall testing of the algorithm, is reported. Then, the performance of \nthe proposed algorithm with and withou t its hybridization with K-means  and LS is investigated, \nand related results , associated with the best solution of ten runs , including the objective function  \nvalue  (𝑍1), its gap to BKS ( Δ𝐵𝐾𝑆), and run time ( t), are reported .  \n \n  \nTable 5. The results of  experiments on newly generated medium and large -size benchmark instances  \nInstance  BKS  ALNS+B&P(+ K -means + LS)  ALNS+B&P  \n𝑍1 Δ𝐵𝐾𝑆 t (m)  𝑍1 Δ𝐵𝐾𝑆 t (m) \nMI-D3-C50-T5 395.9445  395.9445  0.000  % 5.36  396.8985  0.241  % 4.98 \nMI-D4-C50-T5 394.9225  394.9225  0.000  % 5.88  396.3533  0.362  % 5.35 \nMI-D3-C50-T10 369.9841  369.9841  0.000  % 5.64  371.8903  0.515  % 5.19 \nMI-D4-C50-T10 368.2105  368.4828  0.074 % 6.12  370.1435  0.525  % 5.81 \nMI-D3-C75-T5 437.5509  437.5509  0.000 % 8.18  446.5644  2.056 %  6.47 \nMI-D4-C75-T5 435.0217  435.313 1 0.067 %  9.27  447.6808  2.913 %  7.21 \nMI-D3-C75-T10 398.1412  398.4636  0.081 %  8.71  410.2845  3.048 %  6.95 \nMI-D4-C75-T10 396.8405  397.2135  0.094 %  9.43  409.6187  3.215 %  7.89 \nLI-D3-C100 -T5 498.3201  498.8184  0.102 %  11.32   514.4656  3.235 %  9.62 \nLI-D4-C100 -T5 492.0076  492.598 1 0.117 %  11.69   510.753 1 3.812 %  10.05  \nLI-D3-C100 -T10 471.0568  471.5749  0.105 %  11.61   489.9461  4.007 %  9.75 \nLI-D4-C100 -T10 468.1005  468.6622  0.122 %  12.05   487.3862  4.116 %  10.12 \nLI-D3-C150 -T5 614.2237  615.4275  0.196 %  15.21   641.9374  4.512 %  12.01  \nLI-D4-C150 -T5 601.5589  602.756 1 0.199 %  15.33   628.2801  4.442 %  12.55  \nLI-D3-C150 -T10 583.0153  584.1171  0.189 %  15.12  ` 610.5394   4.721 %  12.84  \nLI-D4-C150 -T10 580.1116  581.2892  0.203 %  15.56   608.5892  4.909 %  13.09  \nLI-D3-C200 -T5 807.401 2 809.589 1  0.271 %  18.42   858.4045  6.317 %  16.35  \nLI-D4-C200 -T5 804.361 3 806.5006  0.266 %  18.87   856.8133  6.521 %  16.58  \nLI-D3-C200 -T10 785.0 109 787.0264  0.257 %  18.59   835.579 2 6.442 %  16.49  \nLI-D4-C200 -T10 783.078 4 785.0591   0.253 %  19.01   833.5943  6.451 %  16.73  \nAvg.   0.131 %  12.07   3.618 %  10.31 \n \n The observed results indicate that the intensified algorithm, which incorporates local search and \nK-means clustering in addition to adaptive large neighborhood search and branch and price, \nconsistently outperforms the alternative algorithm in terms of solu tion quality. The average gap \nwith the best-known  solution (BKS) is approximately 3.5% lower for the intensified algorithm. \nAlthough the intensified algorithm requires an average execution time that is 1.5 minutes longer, \nthe inclusion of K-means clusterin g helps mitigate this increase, resulting in a reasonably close \nexecution time to the alternative algorithm. Furthermore, as the problem instances increase in size, \nthe intensified algorithm exhibits a more significant performance advantage. This suggests that the \nadditional time taken by the local search component is worthwhile, as it contributes to improved \nsolutions for larger problem instances.  \n5.4. Case Study  \nAmsterdam’s city center is a well -known hub, teeming with many hotels, restaurants, and cafés \ntightly clustered together . More than 1500 HoReCa spots exist in this area, requiring a huge \ndistribution chain for their input food supplies. This section provides an optimal design for the \ndistribution chain of restaurants and café s located in Amsterdam’s historical center.  \nThe location of restaurants and cafés is derived from the municipality of Amsterdam’s provided \nmaps1. The municipality has also specified a set of potential locations for transshipment points and \nhas classified them into poor, moderate, and spacious points based on the available space on the \nquay. Figure 2 illustrates the location of these TPs, together with restaurants and cafés.  \n \nFigure 2. Restaurants and Caf és in the City Center of Amsterdam  \n \n1 Available at https://maps.amsterdam.nl/functiekaart/  \nIn order to estimate the daily demand of these businesses  (in 𝑚3), we have incorporated a Deep \nNeural Network (DNN) that works based on the input labels of business type ( restaurant or café ), \nweekday or weekend, season, longitude, and latitude.  In order to o btain  train data for our DNN, \ndemand data were collected through field trip. More precisely, the city center was clustered into \nfive location groups based on the longitude and latitude of the existing restaurants and cafés. \nWithin each cluster, a number of  restaurants and cafés was selected as visiting candidates. The \nnumber of visits was proportional to the number of all existing restaurants and cafés (based on \nintensity varying between 12% -20%). The daily  demand during weekdays and weekend, and \nregarding four seasons , were estimated based on the collected information at visits.  \nResults  \nFigure 3 illustrates the result of solving the problem for the case of Amsterdam.  \nThe results of our case study , as depicted in Figure 3 , illustrat es the establishment of 10 \ntransshipment locations (TPs) strategically positioned within the city center, with a focus on \ndensely populated areas. Among these TPs, the majority are categorized as moderate, while one is \nspacious, and two are considered poor in terms of size and capacity.  A closer examination of the \nmap reveals that the potential spacious TPs are not located in densely populated segments. This \nobservation justifies their absence in the establi shed set. The poor established TPs, on the other \nhand, are found only in segments where there is a lack of spacious or moderate TP candidates.  To \nefficiently cater to the demands of the restaurants and cafés, a fleet of seven  small and two medium \nvessels i s employed, without the inclusion of any large vessels. Despite the potential cost -saving \nbenefits associated with economies of scale, the use of large vessels was not feasible  concerning \nthe canal classes  due to the limited width or depth of the canals within the city center . In the second \nechelon of our proposed system, 74 LEVs and 133 moving jacks are utilized to deliver the items \nto their final destinations. This indicates the appropriate location of established TPs  has effectively \nreduced the number of LEVs  required, thanks to the inclusion of moving jacks for serving points \nthat are in close proximity to waterways.  \nA fundamental question in evaluating the efficiency of this waterway -based chain is if it can \nimprove the distribution of the HoRe Ca demands. In order to answer this question, we compare \nthe designed distribution chain with the one currently implemented in Amsterdam, for which \ntrucks with a weight limit of 3500 kg deliver the items to demanded spots.  This transforms the \nproblem into a routing p roblem with time windows.  Figure 4 compares these two distribution \nchains in terms of the total cost, the total number of applied road vehicles , their associated weight , \nand the average distance driven by each vehicle within the city center .  \n \n \n \n \n \n \n \n \n \n  \nFigure 3. Case study Results  \n \n7  \n2 \n0 \n74 \n133 \n \nFigure 4. Comparison between waterway -based and roadway -based logistics chain  \n \nThe analysis reveals that the waterway -based food distribution chain presents a noteworthy \nadvantage in terms of total cost, potentially leading to cost savings of approximately 28% \ncompared to the truck -based system. Despite the higher number of vehicles employed in the bi -\nmodal setting, it is important to note that these vehicles are light  vehicles , resulting in a 43% \nreduction in the total weight of vehicles driven within the city center. This not only offers the \npotential to preserve  the lifetime of physical infrastructure such as quay walls and bridge s but also \nindicates a more distributed and flexible delivery system.  \nMoreover, the waterway -based food chain demonstrates a significant 80% reduction in the \naverage distance driven within the city center to serve the HoReCa spots compared to the truck -\nbased system. This reduction in distance traveled has the potential to alleviate traffic congestion, \nimprove efficiency in terms of time and fuel consumption, and contribute to decreased emissions. \nFurthermore, since the bi -modal sett ing utilizes electric vehicles, it offers a substantial reduction \nin carbon  emissions as well.  \nIn conclusion, the results strongly suggest that implementing the waterway -based distribution \nchain has the potential to enhance the efficiency of HoReCa demands  in Amsterdam greatly . The \nadvantages encompass lower total cost, a more distributed fleet of lighter vehicles, a significant \nreduction in average distance driven within the city center, and a notable decrease in exhaust  \nemissions.  \n5.5. Sensitivity Analysi s \nIn this sub -section, we will investigate the impact of different input parameters on our designed \ndistribution chain. In order to be able to track this impact on all decision variables, a medium  \ninstance with three  depot s, five transshipment locations,  and 50 customers is selected.  \nThe parameters associated with the period equivalent establishment cost and travel cost are the \ntwo cost factors characterizing the economic  competency of our designed waterway -based \ndistribution chain. In order to investigate their impact  on total cost, number of established TPs, \nnumber of applied vessels , and number of applied LEVs, sensitivity analyses are carried out on \nthese parameters, a nd the results are provided in Table 6. \nTable 6. The impact of changes on different cost parameters  \nParameter  Changes (%)  Total Cost  𝒏𝑻𝑷 𝒏𝑽 𝒏𝑳𝑬𝑽 \n𝐹𝐶𝑖 -75 % 129.85  5 3 21 \n-50 %  230.55  5 3 21 \n-25 %  308.07  4 3 26 \n0 395.94  4 3 26 \n+25 %  486.24  4 3 26 \n+50 %  575.24  3 3 32 \n+75 % 664.24  3 3 32 \n𝐶𝑖𝑗𝑘𝐼 -75 %  362.54  4 4 27 \n-50 %  379.88  4 3 26 \n-25 %  391.51  4 3 26 \n0 395.94  4 3 26 \n+25 %  405.51  4 3 26 \n+50 %  414.54  4 3 26 \n+75 %  442.38  4 3 26 \n -75 %  390.99  4 3 28 \n -50 %  392.64  4 3 27 \n -25 %  394.29  4 3 27 \n𝐶𝑖𝑗𝑘𝐼𝐼 0 395.94  4 3 26 \n +25 %  397.59  4 3 26 \n +50 %  399.23  4 3 26 \n +75 %  400.89  4 3 25 \n \nThe results indicate that changes in transshipment location establishment costs have a \nsignificantly larger influence on total costs compared to variations in first and second echelon \ntransportation costs. Specifically, by reducing the establishment costs,  the number of established \nlocations may increase, leading to a decreased reliance on LEVs for transportation. This reduction \nin LEV usage was attributed to the improved efficiency achieved through the utilization of moving \njacks for item delivery. This indicates that by effectively lowering this cost, one not only benefits \nfrom reductions in cost but also can decrease the road traffic load by fewer applied vehicles. \nConversely, the variations in first and , specifically , second echelon transportation  costs were found \nto have a relatively minor impact on overall costs. These findings underscore the importance of \neffectively managing and optimizing transshipment location establishment costs as a key strategy \nfor achieving cost efficiencies in transporta tion operations , while lower attention can be devoted \nto the second echelon transportation .  \n5.6. Discussion  In our study, we conducted a thorough numerical analysis to explore the performance and potential \nbenefits of a proposed algorithm and the implemen tation of a waterway -based distribution chain \nin Amsterdam. The results provide  an intriguing story of improved efficiency and cost savings.  \nOur findings revealed that our algorithm consistently outperformed the previous approaches, \nindicating its effectiv eness in solving the existing benchmarks and further our developed \nbenchmark instance . This suggests that investing in algorithm development and optimization can \nlead to improved performance and efficiency in business operations.  Motivated by these promising \nresults, we delved deeper into the advantages of implementing a waterway -based distribution \nchain. Through a comprehensive case study, we compared this system to a traditional truck -based \napproach. The analysis considered various factors such as total cost, vehicle weight, distance \ntraveled within the city center and environmental impact.  \nThe outcomes of our case study form a compelling narrative. Implementing the waterway -\nbased distribution chain yielded significant cost savings of approximately 28% compared to the \ntruck -based system. This financial advantage was accompanied by a more distributed fleet of \nlighter vehicles. By utilizing lighter vehicles, we reduced the total weight of vehicles driven within \nthe city center by 43%. This not only ben efits the infrastructure's longevity but also allows for a \nmore flexible and adaptable delivery system  that can be of particular interest to city authorities and \npolicymakers. Moreover, the waterway -based food chain showcased an impressive 80% reduction \nin the average distance traveled within the city center, offering the potential to alleviate traffic \ncongestion, enhance efficiency in terms of time and fuel consumption, and contribute to decreased \nemissions. In addition, the adoption of electric vehicles i n the bi -modal setting further reduced \ncarbon emissions, reinforcing the environmental advantages of the waterway -based distribution \nchain.  \nOur sensitivity analysis show s that e ffectively managing and optimizing transshipment \nlocation establishment costs i s crucial for achieving cost efficiencies in transportation operations. \nReducing these costs not only leads to overall cost savings but also decreases reliance on LEVs  \nand reduces road traffic congestion. While first and second echelon transportation costs  have a \nrelatively minor impact, organizations should prioritize optimizing transshipment location \nestablishment costs as a key strategy for cost efficiency.  \nIn conclusion, our numerical analysis presents a compelling case for both the proposed \nalgorithm a nd the implementation of a waterway -based distribution chain in Amsterdam. The \nalgorithm showcased superior performance, while the waterway -based system offered notable cost \nsavings, reduced traffic congestion, improved efficiency, and decreased emissions.  These insights \nprovide valuable guidance for managers seeking to enhance operational efficiency, reduce costs, \nand contribute to sustainable transportation practices.  \n6. Conclusion  \nThis study addresses the need for efficient urban logistics in Amsterdam by proposing an optimal \nurban logistic network that integrates urban waterways and last -mile delivery. The research \nhighlights the untapped potential of inland waterways and the benefits they can offer in terms of \ncapacity and addressing logistic al challenges in the city center.  In this respect, we  propose an \noptimal urban logistic network for Amsterdam, integrating urban waterways and last -mile delivery via road transportation. The problem is formulated as a two -echelon location routing problem w ith \ntime windows, and a hybrid solution approach is developed to solve it effectively.  \nThe proposed algorithm consistently outperforms existing ones, demonstrating its \neffectiveness in solving existing benchmarks and newly develope d instance s. Through a \ncomprehensive case study, the advantages of implementing a waterway -based distribution chain \nwere assessed. The waterway -based chain showcased significant cost savings of approximately \n28% compared to the traditional truck -based system. Additiona lly, the adoption of lighter vehicles \nled to a 43% reduction in total vehicle weight within the city center, enhancing infrastructure \nlongevity and enabling a more flexible delivery system.  \nFurthermore, the waterway -based chain demonstrated an impressive 8 0% reduction in the \naverage distance traveled within the city center, which has the potential to alleviate traffic \ncongestion, improve efficiency in terms of time and fuel consumption, and contribute to decreased \nemissions. The incorporation of electric ve hicles further reduced carbon emissions, highlighting \nthe environmental advantages of the proposed system.  The sensitivity analysis emphasized the \nimportance of effectively managing and optimizing transshipment location establishment costs as \na key strateg y for achieving cost efficiencies and reducing reliance on LEVs  and road traffic \ncongestion.  By drawing insights from Amsterdam's experience and embracing innovative \napproaches, cities around the world can endeavor to discover sustainable solutions for the ir urban \nlogistics challenges.  \nOverall, this study provides valuable insights and guidance for managers in their pursuit of \nenhancing operational efficiency, reducing costs, and promoting sustainable transportation \npractices. However, further analysis is n ecessary to fully assess the viability and potential benefits \nof implementing the waterway -based chain, including considerations of infrastructural limitations \nand canal features.  It is also important to study the impact of such a modal shift on the traffi c over \nwater, and increase in the propeller wash that in turn may lead to further deterioration of bed level \nand quay walls. A digital twin of Amsterdam’s city center canals can be developed in that respect, \nwhich would illustrate the impact of changes in network design. Establishing a feedback loop \nbetween the optimization and the digital twin is another interesting way forward. The digital twin \ncan get insights into the quality of solutions obtained by optimization and provide it with guidance \non specific  promising or forbidden solution space directions. Via such a setting, one can benefit \nboth from design capability of the optimization and what -if scenario analysis of the digital twin.  ",
      "metadata": {
        "filename": "Urban Logistics in Amsterdam_ A Modal Shift from Roadways to Waterway.pdf",
        "hotspot_name": "Transportation",
        "title": "Urban Logistics in Amsterdam: A Modal Shift from Roadways to Waterway",
        "published_date": "2023-09-01T09:05:40Z",
        "pdf_link": "http://arxiv.org/pdf/2309.00345v1",
        "query": "road transport logistics optimization fuel efficiency reduction strategies"
      }
    }
  },
  "query_mapping": {
    "search_queries": {
      "Housing Production": "injection molding process optimization energy efficiency plastic manufacturing",
      "Radiator Production": "die casting aluminum sustainability energy reduction manufacturing",
      "PCBA Production": "printed circuit board soldering energy efficient electronics manufacturing",
      "EMC Shield Production": "steel punching and bending process optimization sustainable manufacturing",
      "Transportation": "road transport logistics optimization fuel efficiency reduction strategies",
      "ECU Operation": "automotive electronics low power consumption design optimization techniques",
      "Standby Power Consumption": "electronic device standby power reduction energy efficient design methods",
      "Incineration of Plastics": "plastic waste management sustainable disposal methods energy recovery techniques",
      "Specialized e-Waste Processing": "electronic waste recycling sustainable processing methods environmental impact reduction"
    },
    "downloaded_papers": {
      "Housing Production": [
        {
          "title": "DRL-Based Injection Molding Process Parameter Optimization for Adaptive\n  and Profitable Production",
          "hotspot_name": "Housing Production",
          "pdf_link": "http://arxiv.org/pdf/2505.10988v1",
          "published_date": "2025-05-16T08:35:31Z",
          "query": "injection molding process optimization energy efficiency plastic manufacturing",
          "filename": "DRL-Based Injection Molding Process Parameter Optimization for Adaptive and Prof.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=injection%20molding%20process%20optimization%20energy%20efficiency%20plastic%20manufacturing&start=0&max_results=2"
        },
        {
          "title": "Enhancing the Product Quality of the Injection Process Using eXplainable\n  Artificial Intelligence",
          "hotspot_name": "Housing Production",
          "pdf_link": "http://arxiv.org/pdf/2503.02338v1",
          "published_date": "2025-03-04T06:59:01Z",
          "query": "injection molding process optimization energy efficiency plastic manufacturing",
          "filename": "Enhancing the Product Quality of the Injection Process Using eXplainable Artific.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=injection%20molding%20process%20optimization%20energy%20efficiency%20plastic%20manufacturing&start=0&max_results=2"
        }
      ],
      "Radiator Production": [
        {
          "title": "A process planning system with feature based neural network search\n  strategy for aluminum extrusion die manufacturing",
          "hotspot_name": "Radiator Production",
          "pdf_link": "http://arxiv.org/pdf/0907.0611v1",
          "published_date": "2009-07-03T12:08:13Z",
          "query": "die casting aluminum sustainability energy reduction manufacturing",
          "filename": "A process planning system with feature based neural network search strategy for.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=die%20casting%20aluminum%20sustainability%20energy%20reduction%20manufacturing&start=0&max_results=2"
        },
        {
          "title": "Improved iron-tolerance in recycled aluminum alloys via direct strip\n  casting process",
          "hotspot_name": "Radiator Production",
          "pdf_link": "http://arxiv.org/pdf/2310.06327v1",
          "published_date": "2023-10-10T05:53:40Z",
          "query": "die casting aluminum sustainability energy reduction manufacturing",
          "filename": "Improved iron-tolerance in recycled aluminum alloys via direct strip casting pro.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=die%20casting%20aluminum%20sustainability%20energy%20reduction%20manufacturing&start=0&max_results=2"
        }
      ],
      "PCBA Production": [
        {
          "title": "SolderNet: Towards Trustworthy Visual Inspection of Solder Joints in\n  Electronics Manufacturing Using Explainable Artificial Intelligence",
          "hotspot_name": "PCBA Production",
          "pdf_link": "http://arxiv.org/pdf/2211.10274v1",
          "published_date": "2022-11-18T15:02:59Z",
          "query": "printed circuit board soldering energy efficient electronics manufacturing",
          "filename": "SolderNet_ Towards Trustworthy Visual Inspection of Solder Joints in Electronics.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=printed%20circuit%20board%20soldering%20energy%20efficient%20electronics%20manufacturing&start=0&max_results=2"
        },
        {
          "title": "Detecting Manufacturing Defects in PCBs via Data-Centric Machine\n  Learning on Solder Paste Inspection Features",
          "hotspot_name": "PCBA Production",
          "pdf_link": "http://arxiv.org/pdf/2309.03113v1",
          "published_date": "2023-09-06T15:52:55Z",
          "query": "printed circuit board soldering energy efficient electronics manufacturing",
          "filename": "Detecting Manufacturing Defects in PCBs via Data-Centric Machine Learning on Sol.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=printed%20circuit%20board%20soldering%20energy%20efficient%20electronics%20manufacturing&start=0&max_results=2"
        }
      ],
      "EMC Shield Production": [
        {
          "title": "Power consumption prediction for steel industry",
          "hotspot_name": "EMC Shield Production",
          "pdf_link": "http://arxiv.org/pdf/2307.07597v1",
          "published_date": "2023-07-14T19:41:53Z",
          "query": "steel punching and bending process optimization sustainable manufacturing",
          "filename": "Power consumption prediction for steel industry.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=steel%20punching%20bending%20process%20optimization%20sustainable%20manufacturing&start=0&max_results=2"
        },
        {
          "title": "Cold Isostatic Pressing to Improve the Mechanical Performance of\n  Additively Manufactured Metallic Components",
          "hotspot_name": "EMC Shield Production",
          "pdf_link": "http://arxiv.org/pdf/1908.03003v1",
          "published_date": "2019-08-08T10:28:28Z",
          "query": "steel punching and bending process optimization sustainable manufacturing",
          "filename": "Cold Isostatic Pressing to Improve the Mechanical Performance of Additively Manu.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=steel%20punching%20bending%20process%20optimization%20sustainable%20manufacturing&start=0&max_results=2"
        }
      ],
      "Transportation": [
        {
          "title": "Cyber-physical Control of Road Freight Transport",
          "hotspot_name": "Transportation",
          "pdf_link": "http://arxiv.org/pdf/1507.03466v1",
          "published_date": "2015-07-13T14:16:22Z",
          "query": "road transport logistics optimization fuel efficiency reduction strategies",
          "filename": "Cyber-physical Control of Road Freight Transport.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=road%20transport%20logistics%20optimization%20fuel%20efficiency%20reduction%20strategies&start=0&max_results=2"
        },
        {
          "title": "Urban Logistics in Amsterdam: A Modal Shift from Roadways to Waterway",
          "hotspot_name": "Transportation",
          "pdf_link": "http://arxiv.org/pdf/2309.00345v1",
          "published_date": "2023-09-01T09:05:40Z",
          "query": "road transport logistics optimization fuel efficiency reduction strategies",
          "filename": "Urban Logistics in Amsterdam_ A Modal Shift from Roadways to Waterway.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=road%20transport%20logistics%20optimization%20fuel%20efficiency%20reduction%20strategies&start=0&max_results=2"
        }
      ],
      "ECU Operation": [
        {
          "title": "12-bit Delta-Sigma ADC operating at a temperature of up to 250C in\n  Standard 0.18 $μ$m SOI CMOS",
          "hotspot_name": "ECU Operation",
          "pdf_link": "http://arxiv.org/pdf/2501.00482v1",
          "published_date": "2024-12-31T15:11:05Z",
          "query": "automotive electronics low power consumption design optimization techniques",
          "filename": "12-bit Delta-Sigma ADC operating at a temperature of up to 250C in Standard 0.18.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=automotive%20electronics%20low%20power%20consumption%20design%20optimization%20techniques&start=0&max_results=2"
        },
        {
          "title": "Optimizing RAG Techniques for Automotive Industry PDF Chatbots: A Case\n  Study with Locally Deployed Ollama Models",
          "hotspot_name": "ECU Operation",
          "pdf_link": "http://arxiv.org/pdf/2408.05933v1",
          "published_date": "2024-08-12T06:16:37Z",
          "query": "automotive electronics low power consumption design optimization techniques",
          "filename": "Optimizing RAG Techniques for Automotive Industry PDF Chatbots_ A Case Study wit.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=automotive%20electronics%20low%20power%20consumption%20design%20optimization%20techniques&start=0&max_results=2"
        }
      ],
      "Standby Power Consumption": [
        {
          "title": "Enabling Lower-Power Charge-Domain Nonvolatile In-Memory Computing with\n  Ferroelectric FETs",
          "hotspot_name": "Standby Power Consumption",
          "pdf_link": "http://arxiv.org/pdf/2102.01442v1",
          "published_date": "2021-02-02T11:32:21Z",
          "query": "electronic device standby power reduction energy efficient design methods",
          "filename": "Enabling Lower-Power Charge-Domain Nonvolatile In-Memory Computing with Ferroele.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=electronic%20device%20standby%20power%20reduction%20energy%20efficient%20design%20methods&start=0&max_results=2"
        },
        {
          "title": "Dynamic Power Reduction in a Novel CMOS 5T-SRAM for Low-Power SoC",
          "hotspot_name": "Standby Power Consumption",
          "pdf_link": "http://arxiv.org/pdf/1302.4464v1",
          "published_date": "2013-02-18T21:24:22Z",
          "query": "electronic device standby power reduction energy efficient design methods",
          "filename": "Dynamic Power Reduction in a Novel CMOS 5T-SRAM for Low-Power SoC.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=electronic%20device%20standby%20power%20reduction%20energy%20efficient%20design%20methods&start=0&max_results=2"
        }
      ],
      "Incineration of Plastics": [
        {
          "title": "Impact of high-pressure torsion on hydrogen production from\n  photodegradation of polypropylene plastic wastes",
          "hotspot_name": "Incineration of Plastics",
          "pdf_link": "http://arxiv.org/pdf/2408.10579v1",
          "published_date": "2024-08-20T06:37:01Z",
          "query": "plastic waste management sustainable disposal methods energy recovery techniques",
          "filename": "Impact of high-pressure torsion on hydrogen production from photodegradation of.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=plastic%20waste%20management%20sustainable%20disposal%20methods%20energy%20recovery%20techniques&start=0&max_results=2"
        }
      ],
      "Specialized e-Waste Processing": [
        {
          "title": "Technological Progress and Obsolescence: Analyzing the Environmental\n  Economic Impacts of MacBook Pro I/O Devices",
          "hotspot_name": "Specialized e-Waste Processing",
          "pdf_link": "http://arxiv.org/pdf/2501.14758v1",
          "published_date": "2024-12-23T16:26:01Z",
          "query": "electronic waste recycling sustainable processing methods environmental impact reduction",
          "filename": "Technological Progress and Obsolescence_ Analyzing the Environmental Economic Im.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=electronic%20waste%20recycling%20sustainable%20processing%20methods%20environmental%20impact%20reduction&start=0&max_results=2"
        },
        {
          "title": "Powering the Future: Innovations in Electric Vehicle Battery Recycling",
          "hotspot_name": "Specialized e-Waste Processing",
          "pdf_link": "http://arxiv.org/pdf/2412.20687v1",
          "published_date": "2024-12-30T03:47:05Z",
          "query": "electronic waste recycling sustainable processing methods environmental impact reduction",
          "filename": "Powering the Future_ Innovations in Electric Vehicle Battery Recycling.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=electronic%20waste%20recycling%20sustainable%20processing%20methods%20environmental%20impact%20reduction&start=0&max_results=2"
        }
      ]
    },
    "total_papers": 17,
    "download_timestamp": "2025-06-22 23:28:05"
  },
  "processing_summary": {
    "total_processed": 17,
    "processing_timestamp": "2025-06-22 23:28:25.568442"
  }
}