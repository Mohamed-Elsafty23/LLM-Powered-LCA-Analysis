{
  "processed_papers": {
    "A Novel Approach for Establishing Connectivity in Partitioned Mobile Sensor Netw": {
      "full_text": "A Novel Approach for Establishing Connectivity in Partitioned \nMobile Sensor Networks Using Beamforming Techniques  \n \nAbbas Mirzaei*  \nDepartment of Computer Engineering, Ardabil Branch, Is lamic Azad University, Ardabil,  Iran  \na.mirzaei@iauardabil.ac.ir  \nShahram Zandi yan \nDepartment of Computer Engineering, Ardabil Branch, Islamic Azad University, Ardabil, Iran  \nShahram.zandian.8872@gmail.com  \n \nAbstract  \nNetwork connectivity is one of the major design issues in the context of mobile sensor networks. Due to diverse \ncommunication patterns, some nodes lying in high -traffic zones may consume more energy and eventually die out resulting \nin network partitioning.  This phenomenon may deprive a large number of alive nodes of sending their important time critical \ndata to the sink. The application of data caching in mobile sensor networks is exponentially increasing as a high -speed data \nstorage layer. This paper prese nts a deep learning -based beamforming approach to find the optimal transmission strategies \nfor cache -enabled backhaul networks. In the proposed scheme, the sensor nodes in isolated partitions work together to form \na directional beam which significantly inc reases their overall communication range to reach out a distant relay node connected \nto the main part of the network. The proposed methodology of cooperative beamforming -based partition connectivity works \nefficiently if an isolated cluster gets partitioned  with a favorably large number of nodes. We also present a new cross -layer \nmethod for link cost that makes a balance between the energy used by the relay. By directly adding the accessible  auxiliary \nnodes  to the set of routing links, the algorithm chooses  paths which  provide maximum dynamic  beamforming usage for the \nintermediate nodes . The proposed approach is then evaluated through simulation results. The simulation results show that the \nproposed mechanism achieves up to 30% energy consumption reduction t hrough beamforming as partition healing in addition \nto guarantee user throughput.  \n \nKeywords : Mobile sensor networks ( MSNs); Connectivity Restoration; Network Partitioning; Cooperative Beamforming; \nFault Recovery  \n1- Introduction  \nMobile sensor networks  (MSN)  are effective  platforms  \nfor the industrial  and military communications  which  are \napplied  for commercial  affairs  of the manufacturing sector \nand the identification of enemy frontiers in the military. In \nmost applications, sensors have the role of a data source and \nsend information from event triggers to each eNodeB  or \ncentral receiver . The unique role of the base station makes \nit a natural target for the enemies who intend to carry out \nthe deadliest attack with the least possible effort against the \nmobile sensor network. Even if the mobile sensor network \nuses common security mechanisms such as encryption and \nauthentication, the enemy may use traffi c analysis \ntechniques to identify the base station. However, the \nattractiveness  of mobile sensor  networks and their \nadvantages  make them vulnerable to potential attack by the \nevil enemy. A typical mobile sensor network consists of \nseveral relays  that iteratively  transfer  new information to \nthe existing BS. In this model , because the unique role of \nthe base station makes it possible to carry out the most \neffective attack against the target mobile sensor network \nwith the least possible effort, this station be comes the center \nof enemy attacks. That is, the enemy assumes that a Denial \nof Service (DoS) attack against the base station will actually cripple the larger mobile sensor network, because the base \nstation not only acts as a data well .  \nOne of the main effective  ways to protect a base station \nfrom a vicious enemy attack is to keep its role, identity, and \nlocation unknown. However, conventional security \nmechanisms that provide confidentiality, integrity, and \nauthentication are not capable of this type of protection [1] \n[2]. One of the major portions of the studies relevant to \nanonymous communications were  so far related  to \nanalyzing routing algorithms  with the aim of  \nconcealing  actual paths from the transmitter  well [3] , [4]. It \nshould be noted that,  in sp ite of the fact that secure  routing \nalgorithms  can greatly reduce path discovery attack , the \nenemy can gain important data via monitoring  the link layer \nand the relevance  between pairs of nodes, based on which it \ncan identify the location and role of the b ase station [5] [6].  \nAccording to  [7] [8], the authors suggested  an \napproach in the lower  layer which  uses dynamic  \nbeamforming to further identify the base station. Nowadays, \ndistributed beamforming seems a very attract ive way to \nimprove the network performance , throughput  and power \nutility , provide data link  safety , in addition to  increasing  \nSINR  in the multi -layer  cooperative systems  [9] [10]. Based \non dynamic  beam modulation, several mobile sensor \nnetwork nodes work together to share the existing \npropagation capabilities  in order to create a dynamic  \nmulti ple transmission  network . Various relays  are able to  \n \n \n 2 \nconcurrently transmit information, taking into account the \nconditions of the wireless channel and the precise control of \nthe signal phase, in suc h a way that all the signals are \ncombined at the destination. For example, ideally, N \ntransmitters send the same messages with the same power, \nwhile tolerating a path loss during the transmission of the \nsignal to a normal destination increases the power at  the \ndestination by N times. This feature has been shown to \nincrease base station anonymity in mobile sensor networks . \nThis protocol appropriately  disrupts the evidence \nhypothesis (EH) and it doesn’t  consider the real base station \nof the mobile sensor netw ork well.  \nThis protocol is an effective technique for enhancing \nthe probability of l ow-cost, multi -hop paths usage , and the \npower needed to transmit the signal to the destination is \nused as the cost of the L -link. Because, the average energy \nconsumption cost of the protocol increases with increasing \nthe number of auxiliary relays |L|, the use of L link selecting \nthe paths to maximize |L| can increase the base station \nanonymity with energy costs equal to the mobile sensor \nnetworks with anonymous protection  [11].  \nAs far as we know, participatory communication was \nfirst used to reinforce the base station anonymity in Ref. \n[12]. As a result, the former  studies of distributed beam \nformation and base station anonymity  will be discussed \nseparately . Researchers on  the subject of base station \nanonymity initially defined a quantitative way of measuring \nanonymity . Some researche rs developed sub-optimal  \neffective  approaches  for measuring anonymity in the \nconnection entropy  [13], GSAT test [14], and belief [15] \n[16]. En tropy and GSAT methods impose certain \nlimitations o n the enemy. They give the a priori possibility \nthat the location of the base station is known to the enemy \nor that the enemy can estimate the location of the base \nstation . The functionality of the belief  index , according to  \nthe evidence theory, does not have any of these hypotheses \nand so it has attracted a lot of attention as a metric for \nrecognizing anonymity. In Section 4, we discuss the \nevidence theory and the metric of belief to evaluate base \nstation security . Many published techniques for dealing \nwith the traffic insecurity  in mobile sensor networks applied \nvarious approaches to  make  the location of data sources  \nhidden  [17] [18]. Such as [19] in which the authors  propose \nvarious approaches such as uniform packet speed and false \npaths to confuse the enemy. Similarly,  the authors in  [20] \nsuggest that network paths be modified by considering \nvirtual sinks. Two techniques have been proposed in [21]. \nIn the first techn ique, the base station re -transmits a package  \nof received packets  at various degrees  and the base station \nlooks like an ordinary node for the  enemy. The base station \ncan also be  considered and can move to a safer location.  \nThe above techniques are used in the network layer of \ncommunication protocols. The protocol uses distributed \nbeamforming in the physical layer to improve the base \nstation anonymity [22]. This paper compensates for this shortcoming of the previous protocol by providing a multi -\nlayer ed routing algorithm  considering  data link constraints  \nand auxiliary intermediate nodes  in order to decrease  the \ntotal power utilization . \nThe authors in [ 23] proposed an efficient smart control \nplan for the dynamic transmission  in the wireless sensor \nnetworks  using cooperative protocols . This algorithm \nsupports the dynamic operations of block data and third -\nparty public validation to provide high security against data \nforgery and replacement. In [ 24], a QoS model for resource \nallocation  algorithm was proposed for data replicas based \non the servers existing in a network in order to improve the \nconnectivity approach  service and decrease total cost . In \n[25], a distributed  algorithm was proposed to reduce the \naccess delay and expand the network bandwidth. In this \nscheme, a new data analysis  strategy was proposed to \nmitigate the costs of data storage and information transfer \nfor applications. In [ 26], a close -loop content -oriented \nscheme  was reviewed achieving higher performance for \ndata-intensive applications. Also,  some researchers \naddressed main critical challenges of this criterion, such as \nenergy efficiency [ 27] availability [ 28], and security [29] of \ndata access. However, heterogeneous MSN  has security \nchallenges, including vulnerability for sensors  and \nassociat ion acknowledgment, that delay the rapid adoption \nof computing models.  \nUnfortunately, the abovementioned works cannot be \nconsidered as  a proper approach  for large -sized networks \ndue to reliability conditions and high computational \ncomplexity at the centra l unit that significantly increases to \nthe number of sensors  in the network.  \nIn this paper, we present the cost of the L link, which \nhas been optimized for multi -hop paths that minimize the \naverage power consumption of the mobile sensor network. \nUsing simulations, we show that the cost of our link is such \nthat it maintains the anonymity of the base station while \nreducing the communication energy consumption. This \narticle continues as follows: Section 2 examines the system \nmodel and the problem formulati on. Section 3 provides a \nframework for Distributed beamforming and the energy \nefficiency of the protocol. Section 4 describes the proposed \napproach in Power Optimization in mobile sensor networks. \nThe numerical results and discussion of the proposed \napproa ch are presented in Section 5. Finally, Section 6 \ndraws the conclusion and highlights future challenges to \nmotivate the effective integration of beamforming -based \nmobile sensor network s with the diagnosis.  \n \n2- System Model and Problem Formulation  \nA- Networ k model  \nIn this paper, we consider a homogeneous model for a \nmobile sensor network in which all sensor nodes have the \nsame capabilities in terms of battery life, type of radio \ncommunication, and network protocols. In this paper the  \n 3 \nsensors were considered as mobile nodes. The base station \nacts as a well for all data traffic generated by the sensor \nnodes. There is only one base station on the network. Our \nhypothesis is that the sensors  can be  aware about  the \nlocation s of the base station and the neighbor  sensors as \nwell [30]. In addition, the cells are well -informed about the  \nlevel of transmission energy  needed  to get all subsequent \nhops. Multi hop paths are followed to deliver data frames to \nthe base station. Also, the cross -wave propagation model \nhas been c onsidered in this paper.  \nWe assume that precautions are used in the design and \noperation of the base station to prevent enemy infiltration. \nFor example, the base station maintains the transmi ssion  \npower level equal to the other cells (for example, updated  \npath exploration and authentication messages) so that it \ncannot be detected from other sensor nodes by radio \nfrequency analysis. Messages are transmitted with the \nheader and encrypted message body. We assume that the \nTDMA Media Access Control Protocol (MA C) operates by \nsynchronizing sufficient time on all wireless network \nsensors at tolerable shielding intervals [ 31]. All nodes in \nmobile sensor networks are considered as auxiliary relay \noptions . \nB-Problem Formulation  \nAssume that the mobile sensor network transmits the target \nsensitive data, which is a desirable target for the enemy. \nAfter identifying and abusing the base station, the enemy \naims to carry out a DoS attack against the base station at any \ncost, such as physically destroying the base station. A lso, \nthe enemy is actively engaged in eavesdropping by being \npresent in all parts of the mobile sensor network [ 32] [33]. \nThe enemy is able to identify the location of all radio \ncommunications at the location of the network [ 34]. While  \nthe enemy monitors  the traffic , we assume that the \ncryptographic system is robust enough so that  the enemy \ncannot use the cryptographic system analysis to retrieve the \ncontents of the body or header. The enemy uses the \nevidence theory traffic analysis to localize the base sta tion, \nunaware that the mobile sensor network is using the \ndistributed beamforming.  \nThe enemy starts via monitoring the transmit links \ndemonstrated  by 𝐸(𝑈) in which  U is a direct connection  \namong  the nodes  (𝑆𝑖 & 𝐷𝑖). It also obtains the paths by \ncorrelating all the evidence for the node pair. The overall \npath containing two or more nodes is denoted by V, and the \nassociated evidence 𝐸(𝑉) is calculated as follows:  \n𝐸(𝑉)=min\n𝑈⊆𝑉{𝐸(𝑈)},        |𝑉|≥2,                    (1)    \nNormalized evidence 𝑚(𝑉)=𝐸(𝑉)/Σ𝐸(𝑉) shows a \nproportion of all the evidence gathered by the enemy that \nsupports the 𝐵(𝑢) which indicates the enemy's certainty \nthat there is a path of length n in any given node and is \nexpressed as follows:  𝐵(𝑢)=∑ 𝑛 𝑚 (𝑈).\n𝑈|𝑢⊆𝑉                                     (2) \nIn this paper, the Belief index  has been applied in order to \nevaluate  base station anonymity. The small belief metric \nmeans less confidence of the enemy or more anonymity of \nthe base station coordination . To reduce the computational \ncomplexity of the calculations needed, it’s supposed that the  \nenemy splits the mobile sensor network into an 𝑀×𝑀 \nnetwork consisting of 𝑁𝐶 square cells. This means that the \nenemy only needs to identify the target location w ithin the \ncell. As a result, the belief metrics 𝐵(𝑢) generated by cell \nanalysis cause u to indicate that the sensor is not a specific \nsensor, but one of the 𝑁𝐶 cells in the enemy's target \nnetwork. Section 5.b has presented an example of evidence \ntheory  analysis. Figure 1 illustrates the network \nconfiguration and the communication beamforming links \nbetween the sensors and the base station.  \n \n \nFig. 1. The network configuration and the beamforming between \nsensors and the base station.  \n \n3- Distributed Beamf orming Model  \nA. Distributed Beamforming   \nBased  on the proposed methodology, a distributed \nbeamforming approach has been applied to further identify \nthe base station. Distributed beamforming uses the \nbroadcast nature of wireless transmission. Adjacent nodes \nmay also hear all the frames sent to a particular receiver. \nAccording to Figure 2, these adjacent nodes may act as \nauxiliary re lays in cooperation with the transmission source \n𝑆𝑖 so that the transmitted signal travels to 𝐷𝑖 through a \ndiverse set of transmitters. Because each 𝑅𝑗 relay sends the \nsame message 𝑆𝑖 with the exact time and synchronization of \nthe carrier, the sign als at the destination 𝐷𝑖 are combined \n \n \n \n 4 \nunder the conditions of ideal scheduling and carrier \nsynchronization [ 35]. \n \n     Fig. 2. Distributed beamforming model  \nIdeal received signal in 𝐷𝑖 by source 𝑆𝑖 and |𝐿| auxiliary \nrelay 𝑅𝑗 is sent as shown below:  \n𝑟𝐷𝑖(𝑡)≜𝑟𝑆𝑖𝐷𝑖(𝑡)+∑ 𝑟𝑅𝑗𝐷𝑖(𝑡)|𝐿|−1\n𝑗=0 \n= ℜ(𝐴𝑆𝑖(𝑡)𝑤𝑆𝑖𝐷𝑖𝛽ℎ𝑆𝑖𝐷𝑖𝑒𝑗(2𝜋𝑓𝑐𝑡+𝜃(𝑡)+𝜑(𝑡))+ \nℜ(∑ 𝐴𝑅𝑗(𝑡)|𝐿|−1\n𝑗=0𝑤𝑆𝑖𝐷𝑖𝛽ℎ𝑆𝑖𝐷𝑖𝑒𝑗(2𝜋𝑓𝑐𝑡+𝜃(𝑡)+𝜑(𝑡))+𝑛(𝑡)  (3) \nIn this equation , h indicates the channel shock res ponse,  𝛽 \nindicates the sharing efficiency  𝑓𝑐 is the carrier frequency, \n𝜃(𝑡) illustrates the phase modulation expression, 𝜑(𝑡) is \ntotal phase variation term and 𝑛(𝑡) represents the thermal \nnoise contained in the 𝐷𝑖 receiver. 𝑟𝐷𝑖(𝑡) consists of two \nexpressions: Information received from source 𝑆𝑖 (i.e., \n𝑟𝑠𝑖,𝐷𝑖(𝑡)) and total information received from  the relay |𝐿| \nused 𝑅𝑗∈𝐿 is equal to ∑ 𝑟𝑅𝑗,𝐷𝑖(𝑡)|𝐿|−1\n𝑗=0. \nThe meaning of Equation 3 in terms of achieving base \nstation anonymity in the physical layer is that the \ndistribution of the distributed beamforming makes it \npossible for the component of the signal received 𝑟𝑠𝑖,𝐷𝑖(𝑡) \nfrom the transmitted information 𝑆𝑖 decreases by \n∑ 𝑟𝑅𝑗,𝐷𝑖(𝑡)|𝐿|−1\n𝑗=0 at the same time , the power level of the \nsignal received by 𝐷𝑖 remains constant during the phase \noffset 𝜑(𝑡). Therefore, if 𝑆𝑖 and 𝑅𝑗𝜖𝐿 transmit only data at \na specified SINR  with the power required to reach 𝐷𝑖, each \ntransmitter can reduce the resource via  10𝑙𝑜𝑔(|𝐿|+1) dB \nand properly  prevents the enemy from distinguishing \n𝐸(𝑆𝑖,𝐷𝑖). Accordingly, the elimination of 𝐸(𝑆𝑖,𝐷𝑖) from \nthe enemy evidence set increases the confidentiality of the \nbase station, as it reduces its role in detecting 𝐵(𝑢=𝐷𝑖). \nFigure  2 shows the functionality of the distributed \nbeamforming  used in any relay via  the node 𝑆𝑖, which \nintend s to apply dynamic  beamforming to increase the base station anonymity while sending a packet  to the next relay  \nnode  𝐷𝑖. 𝑆𝑖 should  first choose  the appropriate subset of \nauxiliary nodes  by handshaking (according to steps (a) & \n(b)). When a vector  of auxiliary relays is used, 𝑆𝑖 transfers \nthe packet  body in step d to 𝑅𝑗∈𝐿. In step e, the distributed \nbeamforming is submitted  and t hen the authentication \nmessage is transmitted from node  𝐷𝑖 to 𝑆𝑖 in stage  (f) to \nconfirm the correct reception of the cooperative message.  \nB. Distributed beamforming protocol  \nFigure 3 is an example of an enemy theory (ET) analysis \nwith and without using distributed beamforming for seven \nnetwork sensors. In this example, the enemy divides the \ntarget area into 𝑁𝑐=9 cells. In Figure 3, the transmission \nof information in the main method along with the \ncooperative transmission by the distributed beamforming. \nIn continu ation , the paper shows the evidence collected and \nthe belief calculation in unit hops in the main meth od and \nthe distributed beamforming  protocol, in which the relay  in \ncluster  28 sends the packet  to the relay  in cluster  35. \n \nFig. 3. Applying enemy -theory for a mobile sensor network  \n \nC- Distributed beamforming energy analysis  \nIn addition to the higher ano nymity of the base station, the \nenergy cost of communications resulting from the use of the \ndistributed beamforming  protocol should also be \nconsidered. While it may save up to 10𝐿𝑜𝑔 (|𝐿+1| on the \ntransmi ssion  power, additional signaling in the distributed \nbeamforming  protocol causes increase of  the data overhead. \nDedicated  power  for each sent bit 𝜀𝑏 can be computed  by \nratio of  the mean  transmit energy  𝑃𝑇𝑆𝑙,𝐷𝑙 ̅̅̅̅̅̅̅ needed  to reach the \n𝑆𝑖 to 𝐷𝑖 informatio n by the signal to noise ratio (SNR) and \nthe set speed r, where 𝜀𝑏𝑆𝑙,𝐷𝑙 ̅̅̅̅̅̅̅≜𝑃𝑇𝑆𝑙,𝐷𝑙̅̅̅̅̅̅̅̅̅\n𝑟. Therefore, the mean  \ncommunication energy consumed by the non -cooperative \nsystem is equal to 𝜀𝑏𝑎𝑠𝑒̅̅̅̅̅̅≜𝜀𝑏𝑆𝑙,𝐷𝑙𝛽̅ ̅̅̅̅̅̅̅̅, where 𝛽̅ is equal to the \naverage body siz e. Accordingly, the mean  distributed \nbeamforming power  is calculated based on a cooperative \nhop and is obtained based on the following formula:  \n \n 5 \n𝜀𝐷𝐼𝐵𝐴𝑁̅̅̅̅̅̅̅̅≜(𝜀𝑏𝑆𝑙𝑅𝑗̅̅̅̅̅̅(𝛾𝑅𝑅+𝛾𝑑𝑎𝑡𝑎 +𝛽̅+𝐾)+𝜀𝑏𝑆𝑙𝐷𝑙̅̅̅̅̅̅\n(|𝐿|+1)𝛽̅+ \n|𝐿|𝜀𝑏𝑅𝑗𝑆𝑙̅̅̅̅̅̅𝛾𝐴𝑐𝑘+|𝐿|𝜀𝑏𝑅𝑗𝐷𝑙̅̅̅̅̅̅̅\n(|𝐿|+1)𝛽̅+𝜀𝑏𝐷𝑙𝑆𝑙̅̅̅̅̅̅𝛾𝐴𝑐𝑘),            (4) \nWhere 𝑌𝑅𝑅 represents the Relay Request, Υ𝐷𝑎𝑡𝑎 denotes the \nData Multihop and Υ𝐴𝑐𝑘 is equal to the  header size of  \nacknowledgement/negative  acknowledgement .  \nIn this scenario, the channel  was considered to be equal to \nK bits at time t microseconds (i.e., 𝐾=𝑡×𝑟) [7].  \nIn equation (4), we have two key observations of the \naverage the distributed beamforming energy consumption \nper hop ( 𝜀𝐷𝑖𝐵𝐴𝑁̅̅̅̅̅̅̅̅). First, 𝑆𝑖 does not make saving on the \ndistributed beamforming for power transmission ( 𝑃𝑇𝑆𝑙,𝑅𝐽 ̅̅̅̅̅̅̅) \nwhich involves the use of relays. As a result, 𝑆𝑖 uses relays \nthat are as close as possible to 𝑆𝑖, thus minimizing ( 𝜀𝑏𝑆𝑙,𝑅𝐽 ̅̅̅̅̅̅̅). \nSecond, 𝑆𝑖 seeks to maximi ze |𝐿| by applying the \nmaximum number of auxiliary relays possible and at the \nsame time minimizing the use of transmi ssion  power \n(𝑃𝑇𝑆𝑙,𝑅𝐽 ̅̅̅̅̅̅̅). \n \nD- Selection of relay in the distributed beamforming \nprotocol  \nThe distributed beamforming  protocol requires an approach \nto select a relay in order to use a set of 𝑅𝑗∈𝐿 in each hop \nand has three objectives: higher anonymity of the base \nstation, conservation or reduction of communication energy \n𝜀𝐷𝑖𝐵𝐴𝑁̅̅̅̅̅̅̅̅ compared to the main system 𝜀𝑏𝑎𝑠 𝑒̅̅̅̅̅̅ without \ndistributed beamforming and achieving the best CSI \nmeasurement. Increased | L | causes the reduction of the \nability to communicate directly with 𝑆𝑖 and 𝐷𝑖 and thus \nimproves the base station anonymity. But as we said before \nin the previous se ction, the increase of |𝐿| requires higher \ntransmi ssion  power ( 𝑃𝑇𝑆𝑙,𝑅𝐽 ̅̅̅̅̅̅̅) to be used during relay \noperation. By iteration, we obtain the expected number of \npotential relays that 𝑆𝑖 made available by increasing the \npower level ( 𝑃𝑇𝑆𝑙,𝑅𝐽 ̅̅̅̅̅̅̅). Given the constraint 𝑃𝑇𝑆𝑖,𝑅𝑗<𝑃𝑇𝑆𝑙,𝑅𝐽 ̅̅̅̅̅̅̅, \nwe maintain the anonymity of the base station to prevent the \nenemy from collecting the evidence of 𝐸(𝑆𝑖,𝐷𝑖) linking the \ntransmitter and receiver.  \nFirst, the quantity  of the potential intermediate nodes \nwas considered  equal to |𝐿𝐷|=𝜆(𝜋𝑑2𝑆𝑖,𝑅𝑗\n8) where λ is the \ndensity of the node in the region, which is 𝜆=𝑆𝑈\n𝑀×𝑀 in a \nsemicircle with radius 𝑑𝑆𝑖,𝑅𝑗=𝑑𝑆𝑖,𝐷𝑖\n𝛿 is calculated based on \nthe number of expected nodes in the receiving interval when \n𝑆𝑖 is transmitted with power 𝑃𝑇𝑆𝑖,𝐷𝑖. This algorithm adjusts \n𝑃𝑇𝑆𝑖,𝑅𝑗 using δ through iteration where δ> 1, because δ = 1 means that 𝑃𝑇𝑆𝑖,𝑅𝑗=𝑃𝑇𝑆𝑖,𝐷𝑖 and there is an undesirable link \nbetween 𝑆𝑖 and 𝐷𝑖. \nOur relay selection algori thm is briefly performed based on \nthe below stages : \n1- 𝑆𝑖 chooses  the primary  amount  of δ and mathematically  \ncalculate  |𝐿𝐷|. In the beginning, the set of the distributed \nbeamforming  relays is 𝐿=∅. \n2- 𝑆𝑖 sends the relay request message and 𝑃𝑇𝑆𝑖,𝑅𝑗 is \ncalculated based on the δ to reach |𝐿𝐷|. \n3- Nodes that respond to 𝑆𝑖 with a confirmation message \ninvolves the number of available relays |𝐿𝐴| | in a way that \n𝑅𝑗𝜀𝐿𝐴. \n4. One of the following three results occurs for 𝑅𝑗∈𝐿𝐴: \nA- If |𝐿𝐴|<|𝐿𝐷|, then  𝛿=𝛿−𝛿𝑆𝑇𝐸𝑃 is used to increase \n𝑃𝑇𝑆𝑖,𝑅𝑗 and reach more candidate relays in each iteration. \nReturn to step 2. If 𝛿=𝑚𝑖𝑛 (𝛿), the algorithm ends with \n𝐿=∅ and the distributed beamforming is not used in this \nhop. \nB- If |𝐿𝐴|=|𝐿𝐷|, then 𝐿=𝐿𝐴 and the algorithm \nterminates.  \nC- If |𝐿𝐴|≥|𝐿𝐷|, L becomes |𝐿𝐷| of the relay of the \nhighest quality 𝑅𝑗∈𝐿𝐴 which are prioritized based on the \nbest condition  of channel state information . \nThe proposed node  selection approach  is used at  all relays  \nnext to the transmitter  path to the base station. Decreasing δ \nis the only node solution  to use more intermediate nodes  to \nenhance  𝑃𝑇𝑆𝑡,𝑅𝐽 ̅̅̅̅̅̅̅. Nevertheless , the multi -layer routing \napproach  offers another option to increase | L | for the \nmobile sensor network: In this case, the paths are selected \nas a functionality  of the accessible  auxiliary intermediate \nnodes  |𝐿𝐴| in all hops. \nThe cost of a ℒ𝑖 link consists of two parameters, each \nwith a unique objective for the distributed beamforming. \nFirst, energy saving is the most important factor in the initial \ndesign of mobile sensor networks and is one of the \nfundamental technical design constraints for  further \nanonymity of the base station. Instantaneous energy \nrequired to use a set of the auxiliary relay |𝐿𝐴| is equal to:  \n𝜀𝑅𝑅≜∑((𝜀𝑏𝑆𝑖𝑅𝑗×𝛾𝑅𝑅)+𝛾𝐴𝑐𝑘 ∑ 𝜀𝑏𝑅𝑗𝑆𝑖|𝐿𝐴|−1\n𝑗=0),\n𝑛       (5) \nThe reader is reminded that while selecting a relay, the \nenergy to bit 𝜀𝑏𝑆𝑖,𝑅𝑗 required to use |𝐿𝐴| the relay is \ndependent on δ. quantity  of re -transfers needed  for \nsuccessful use of a suitable set of |𝐿𝐴| of the relay, a relay \nin which the relay selection condition is met is represented \nby n. Secondly, the average energy consumption of the \ndistributed beamforming decreases with increasing the \nnumber of |𝐿𝐴| of the auxiliary relays available in each hop. \nAs a result, the cost of the beam links is calculated based on \nthe following formula:   \n \n \n 6 \nℒ𝑖={(𝜀𝑅𝑅\n|𝐿𝐴|)𝑓𝑜𝑟 |𝐿𝐴|>0\n∞       𝑓𝑜𝑟  |𝐿𝐴|=0}                                            (6) \nWhen the auxiliary relays are available, the cost of link ℒ𝑖 \nis decimal and when the relays are not available it becomes \n∞. The cross -layer scheme integrates with the distr ibuted \nbeamforming  relay selection algorithm so that each node \ncan calculate |𝐿𝐴| and make better use of the distributed \nbeamforming by selecting routes with higher relay \ndensities.  \n4. Power Optimization in Mobile Sensor \nNetwork  \nAccording  to the main sources in this field, there is no \nstandard model for energy consumption in beamforming -\nbased mobile sensor backhaul networks. However, the \napplication of nonlinear prediction energy consumption in \nsuch systems has attracted more satisfaction.  Here, this \npaper uses adaptive resource allocation in which the \nbackhaul connection has been modeled, in which C5 and C6 \nare the maximum transmission power constraints for \nsensors and macro base stations, respectively.  \nA. Content -Caching model  \nIn this ne twork, we suppose that content can be modelled as \na distinct set of packet data as ℱ={ℱ1,ℱ2,…,ℱ𝑓,…,ℱ𝐹} \nwhich ℱ𝑓 represents the 𝑓-th data frame. The request \nprobability for data frame 𝑓 is expressed as                                                      \n𝑝𝑓(0≤𝑝𝑓≤1), 𝑤ℎ𝑖𝑐ℎ,∑𝑝𝑓 ≤𝐿𝑖,   ∀𝑓∈ℱ,𝐹\n𝑓=1   (7) \nIt should be noted that the caching model presented for this \npaper is stochastic caching so that we can calculate the \nprobability of caching data packet f via base station  i 0≤\n𝑞𝑓𝑖≤1 where 𝐿𝑖 illustrates the cache size. In addition, \n{𝑞𝑓𝑖} of base station 𝑖 should satisfy below condition:  \n∑𝑞𝑓𝑖≤𝐿𝑖,   ∀𝑖∈ℬ,𝑓∈ℱ,𝐹\n𝑓=1                                           (8) \n𝐿𝑖=𝐿𝑀 means the cell is macro cell, otherwise, 𝐿𝑖=𝐿𝑆. \nB. Resour ce Control Model  \nBased on the approach’s principles, the resources of the \nbase stations can be supplied by conventional smart grid and \nrenewable energy harvesting. During this scenario, the \ntransmi ssion  power relevant to base station 𝑖 can be \ndemonstrated by 𝑃𝑖(𝑖∈𝐵), and the applied energy from the \ngrid network is illustrated by 𝐺𝑖. The harvested renewable \nresource is shown as 𝐸𝑖. Based on the enabled power \nsharing capability, the shared power among cell 𝑖 and cell \n𝑖′ is equal  to 𝜀𝑖𝑖′, where 𝛽∈[0,1] denotes the power -\nsharing index among base stations. So, we can conclude that \n(1−𝛽) is equal to the loss percentage in the power sharing stage. The following condition should be satisfied during \nthe power sharing process.  \n𝑃𝑖<𝐺𝑖+𝐸𝑖+𝛽∑ 𝜀𝑖′𝑖\n𝑖′∈ℬ,i′≠i− ∑ 𝜀𝑖𝑖′.\n𝑖′∈ℬ,i′≠i                  (9) \nAccording to the defined conditions, the overall power \nefficiency can be affected by the transmission strategies, \npower sharing and the level of harvested energy from \nrenewable resources . \nC. Transmission Model  \nTaking fairness into account, we tried to provide data rate \nbalancing throughout the network. In which, 𝑥𝑖𝑗(𝑖∈ℬ,j∈\n𝒰) denotes the association indicator, for example, 𝑥𝑖𝑗=1 \nrepresents that node 𝑗 is associated with BS 𝑖 and otherwise \nthe node has not been associated with the base station. \nSubsequently, 𝑘𝑖=∑ 𝑥𝑖𝑗 j∈𝒰  represents the number of \nsensors associated to cell 𝑖. (∑ 𝑝𝑓𝑞𝑓𝑖𝐹\n𝑓=1 )𝑘𝑖 express the \nprobability of serving 𝑘𝑖 associated nodes by base station i. \nif 𝑥𝑖𝑗=1, we can calculate the efficiency of the 𝑗-th \nsensors as 𝜇𝑖𝑗=log(𝑅𝑖𝑗) which 𝑅𝑖𝑗 is the throughput so \nthat the 𝑅𝑖𝑗 is obtainable as.  \n𝑅𝑖𝑗=(∑𝑝𝑓𝑞𝑓𝑖𝐹\n𝑓=1)𝑘𝑖\nℬ𝛽\n∑ 𝑥𝑖𝑗 𝑗∈𝑢log(1+𝛾𝑖𝑗)                   (8) \nIn this framework, the ratio of signal to interference -noise \ncan be computed via (11)  \n𝛾𝑖𝑗=𝑃𝑖ℎ𝑖𝑗\n∑ 𝑃𝑖′ℎ𝑖′𝑗+𝜎2𝑖′∈ℬ,𝑖′≠𝑖                                           (11) \nIn this formulation, ℎ𝑖𝑗 and ℎ𝑖′𝑗 indicate the main channel \ngain and the interfering channel gain respec tively, B denotes \nthe frequency bandwidth. 𝜎2 is also a noise figure. We can \nconsider the goal function equivalent to minimization of the \napplied grid power. Consequently, we have the goal \nfunction as the following.  \nP1: max\n𝑞,𝑥,𝑃,𝜀,𝐺∑∑𝑥𝑖𝑗𝜇𝑖𝑗−𝜂∑𝐺𝑖\n𝑖∈ℬ 𝑗𝜖𝑢 𝑖𝜖ℬ                                                                                   (12) \ns. t. 𝐶1:∑𝑥𝑖𝑗𝛾𝑖𝑗≥𝛾𝑚𝑖𝑛,∀𝑗∈𝑢,\n𝑖∈ℬ \n  𝐶2:∑𝑥𝑗𝑚=1,∀𝑗∈𝑢,\n𝑖∈ℬ \n  𝐶3:𝑃𝑖<𝐺𝑖+𝛽∑ 𝜀𝑖′𝑖 𝑖′∈ℬ,𝑖′≠i  −∑ 𝜀𝑖𝑖′+ 𝑖′∈ℬ,𝑖′≠i\n𝐸𝑖,∀𝑖∈ℬ, \nC4: ∑𝑞𝑓𝑖≤𝐿𝑖,∀𝑖∈𝐹\n𝑓=1ℬ ,𝑓∈ℱ, \nC5: 0≤𝑞𝑓𝑖≤1,∀𝑓∈ℱ,∀i∈ℬ, \nC6: 𝑥𝑖𝑗∈{0,1},∀i,∀j∈𝑢, \nC7: 𝐺𝑖≥0,𝜀𝑖𝑖′≥0,∀i∈ℬ, \nC8: 0≤𝑃𝑖≤𝑃𝑚𝑎𝑥𝑖,∀i∈ℬ,  \n 7 \nIn which, 𝐪=[𝑞𝑓𝑖], 𝐗=[𝑥𝑖𝑗], 𝐏=[𝑃𝑖], 𝛆=[𝛆i𝑖′], 𝐆=[𝐺i], 𝛾𝑚𝑖𝑛 \nillustrates the 𝑆𝐼𝑁𝑅 𝑚𝑖𝑛 to guarantee the reliability of the \nconnection between nodes and the base station. Also,  𝜂 \nrepresents a weighting factor for evaluation of  the power \nefficiency  index . The multi hop strategy of backhauling in \nthe presented mobile sensor network and the co nfiguration \nof the network has been exhibited in Figure 4.  \nFigure 4 shows a simple network structure to illustrate the \noperation process of the proposed connectivity approach . \n \nFig. 4. Operation process of the proposed connectivity approach  \n \nIn order to improve the reliability of the proposed model, \nwe applied a  path repair process with the branch node -based \nrouting algorithm which is shown  in Figure 8.  \n \nFig. 5. Reliable distributed transmission model . \n \nIn this paper, we present the cost of the L link, w hich has \nbeen optimized for multi -hop paths that minimize the \naverage power consumption of the mobile sensor network. \nOur distributed cross layer routing protocol uses a \nconnection  cost that can be added to the final goal function . \nUsing simulations, the paper  show s that the cost of our link \nis such that it maintains the anonymity of the base station \nwhile reducing the communication energy consumption.  \nThe problem of association and power allocation in this \napproach may be modelled as problem P2 which its elf can \nbe considered as the optimal solution for the primary \nproblem P1. Taking 𝑘𝑖=∑ 𝑥𝑖𝑗 j∈𝒰 , into account, this \nproblem is expressed as the following.  P2: \n \n max\n𝑞,𝑥,𝑃,𝜀,𝐺∑∑𝑥𝑖𝑗𝑙𝑜𝑔(𝑐𝑖𝑗)\n𝑗𝜖𝑢 𝑖𝜖ℬ\n+∑𝑘𝑖2𝑙𝑜𝑔 (∑𝑝𝑓𝑞𝑓𝑖𝐹\n𝑓=1)\n𝑖∈ℬ \n−∑𝑘𝑖𝑙𝑜𝑔(𝑘𝑖)\n𝑖𝜖ℬ\n−𝜂∑𝐺𝑖\n𝑖𝜖ℬ                                                     (13) \ns.t  C1 , C2 , C3 , C4 , C5, C6, C7, C8,  \n𝐶9:∑𝑥𝑖𝑗=𝑘𝑖,∀𝑖,\n𝑗∈𝑢 \nwhere,  𝑐𝑖𝑗=𝐵𝑙𝑜𝑔 (1+𝛾𝑖𝑗). \nD. Data Caching -based User Association Algorithm  \nIn this framework, P2 as a NL mixed integer programming \nproblem is not a convex problem and as Lemma 1 indicated, \nthe sub gradient method will be the best approach to solve \nit. Taking {𝑃,𝜀,𝐺} into account, the sensor association \nproblem will be mathemat ically modeled as follows.  \nP2.1 : \n max\n𝑞,𝑥∑∑𝑥𝑖𝑗𝑙𝑜𝑔(𝑐𝑖𝑗)\n𝑗𝜖𝑢 𝑖𝜖ℬ\n+∑𝑘𝑖2𝑙𝑜𝑔 (∑𝑝𝑓𝑞𝑓𝑖𝐹\n𝑓=1)\n𝑖∈ℬ\n−∑𝑘𝑖𝑙𝑜𝑔(𝑘𝑖)                                      (14)\n𝑖𝜖ℬ \ns. t.    C1, C2, C4, C5, C6, C9  \n \nFig. 4. Beamforming model in the mobile sensor architecture  \n \nLemma 1: considering 𝑝(1)≥⋯≥𝑝(𝑓)≥⋯≥𝑝(𝐹) as the \nprobability of demanded payload ( 𝑓), the optimal solution \nfor P2.1 is achievable as the following.  \n \n𝑞𝑓𝑖∗={1,𝑓𝑖=(1),….,(𝐿𝑖)\n0, 𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒    ,     ∀𝑖∈ℬ.      (15)                                                \n \n \n \n \n 8 \nProof 1: as mentioned before, P2.1 shows that obtaining the \noptimal value for ∑ 𝑝𝑓𝑞𝑓𝐹\n𝑓=1  is the target in which, the \ndemanded payload is itself consists of 𝐿𝑖 sections ℱ𝑙(𝑙=\n1,…,𝐿𝑖), and the probabi lity ℱ𝑙 is more than ℱ𝑙+1. \nTherefore, we have:  \n∑ 𝑞(𝑓)𝑖𝑙\n(𝑓)∈ℱ𝑙=1,∑𝑞(𝑓)𝑖𝑙=𝑞(𝑓)𝑖  𝑎𝑛𝑑   ⋃ℱ𝑙=\n𝑙=𝐿𝑖𝐿𝑖\n𝑙=1ℱ  \nAlso,  \n∑𝑝𝑓𝑞𝑓𝑖𝐹\n𝑓=1=∑ ∑ 𝑝(𝑓)𝑞(𝑓)𝑖𝑙 ≤\n(𝑓)∈ℱ𝑙𝐿𝑖\n𝑙=1∑𝑝(𝑙)(∑ 𝑞(𝑓)𝑖𝑙 \n(𝑓)∈ℱ𝑙)𝐿𝑖\n𝑙=1\n⟹∑𝑝𝑓𝑞𝑓𝑖≤∑𝑝(𝑙)𝐿𝑖\n𝑙=1,𝐹\n𝑓=1 \nConsequently, according to (15), this theory is confirmed. \nSo, we can conclude that  \nP̃2.1: max\n𝑥∑∑𝑥𝑖𝑗𝑙𝑜𝑔(𝑐𝑖𝑗)+∑𝑘𝑖2𝑙𝑜𝑔 (∑𝑝(𝑓)𝐿𝑖\n𝑓=1)\n𝑖∈ℬ 𝑗𝜖𝑢 𝑖𝜖ℬ \n−∑𝑘𝑖𝑙𝑜𝑔(𝑘𝑖)\n𝑖𝜖ℬ                                               (16) \ns. t. C1, C2, C6, C9.  \nFor simplicity of process to obtain the best solution for  P̃2.1 \nas a combination of several sub -problems, we work on its \ndual problem. Therefore, the target function should be \nreformulated as follows:  \nℒ(𝑥,𝑘,𝜇,𝜈)=∑∑𝑥𝑖𝑗𝑙𝑜𝑔(𝑐𝑖𝑗)+∑𝑘𝑖2𝑙𝑜𝑔(∑𝑝(𝑓)𝐿𝑖\n𝑓=1)\n𝑖∈ℬ 𝑗𝜖𝑢 𝑖𝜖ℬ \n− ∑𝑘𝑖𝑙𝑜𝑔(𝑘𝑖) \n𝑖𝜖ℬ −∑𝜇𝑗\n𝑗∈𝑢(𝛾𝑚𝑖𝑛−∑𝑥𝑖𝑗𝛾𝑢𝑗\n𝑖∈ℬ)− \n                             ∑𝜈𝑖\n𝑖∈ℬ(∑𝑥𝑖𝑗−𝑘𝑖\n𝑖∈u),    (17) \nWhere in this formulation,  𝜐=[𝜈𝑖],𝑘=[𝑘𝑖] 𝑎𝑛𝑑   𝜇=\n[𝜇𝑗]. It should be noted that 𝜈𝑖 and 𝜇𝑗 represent Lagrangian \nmultipliers. In continue, we can define the problem’s dual \nfunction 𝒟(.) as the following  \n𝒟(𝜇,𝜈)={max\n𝑥,𝑘ℒ(𝑥,𝑘,𝜇,𝜈)\n𝑠.𝑡.  𝐶2,𝐶6.                                                      (18)                                                                                     \n \nSubsequently, the dual problem of P̃2.1 (16) will be \nformulated as  \nmin\n      𝜇≥0,𝜈≥0𝒟(𝜇,𝜈).                                                                  (19)                                                                                                          \n𝜇𝑗 and 𝜈𝑖 are coefficients of the dual problem and solution \nof the goal function ca n be obtained as the following steps  \n𝑥𝑖𝑗∗={1,   𝑖𝑓    𝑖=𝑖∗\n0, 𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒 ,                                               (20) \nIn (20), 𝑖∗=arg𝑚𝑎𝑥 𝑖(log(𝑐𝑖𝑗)+𝜇𝑗𝛾𝑖𝑗−𝜈𝑖). Considering 𝑘𝑖, the second derivation of the goal function \nresults in  \n𝜕2ℒ\n𝜕𝑘𝑖2=2𝑙𝑜𝑔(∑𝑝(𝑓)𝐿𝑖\n𝑓=1)−1\n𝑘𝑖.                                                 (21) \n𝑘𝑖∗=−𝑊(−2𝑙𝑜𝑔(∑ 𝑝(𝑓)𝐿𝑖\n𝑓=1)𝑒𝑣𝑖−1)\n2𝑙𝑜𝑔 (∑ 𝑝(𝑓)𝐿𝑖\n𝑓=1),                        (22) \n \nAs it is obvious, ∑ 𝑝(𝑓)𝐿𝑖\n𝑓=1≤1, so, 𝜕2ℒ\n𝜕𝑘𝑖2 cannot be a positive \namount. With setting 𝜕2ℒ\n𝜕𝑘𝑖2 equal to zero, 𝑘𝑖∗ is achieved as \nthe optimum degree of 𝑘𝑖. \nIn (22), 𝑊(𝑧) shows the Lambert -W factor as a response \nfor 𝑧=𝑤𝑒𝑤. According to (20), the optimal solution \n(𝜇∗,𝜐∗) cannot be achieved by differentializing  of 𝒟(𝜇 ,𝜈 ). \nTherefore, a pplying the iterative gradient approach will be \nuseful.  \n𝜇𝑗(𝑡+1)=[𝜇𝑗(𝑡)−𝛿(𝑡)(∑𝑥𝑖𝑗\n𝑖∈ℬ(𝑡)𝛾𝑖𝑗−𝛾𝑚𝑖𝑛)]+\n,         (23) \n𝜈𝑖(𝑡+1)=[𝜈𝑖(𝑡)−𝛿(𝑡)(𝑘𝑖(𝑡)−∑𝑥𝑖𝑗\n𝑗∈𝑢(𝑡))]+\n,                          (24) \nIn this formulation, 𝑥𝑖𝑗(𝑡) and 𝑘𝑖(𝑡) can be renewed in an \niteration via (20) and (22). The step size was shown by  𝛿(𝑡) \nand we have [𝑎]+=max {𝑎,0}, 𝑡 also indicates the \niterations quantity.  \n \n5- Numerical Results  \nIn this section, we present the simulation results that \ndemonstrate the effec tiveness of the proposed Multi hop \nCooperative Beamforming Mobile Sensor Network (MCB -\nMSN) approach. For simplicity, we assume that the \nharvested  energy by base station during each time interval  \nis constant. Following the former schemes,  we modeled the \nenergy harvest at each base station as the s tationary \nstochastic process . In addition, we assume that the \npopularity of the content follows the introduced distribution \nmodel of [ 36] and that the contents of the F library have \nbeen sorted by popularity. Thus, the probability of the 𝑓𝑡ℎ \ndemand for popular content is calculated based on [ 37], \nwhere 𝛼 indicates the skewness of popularity. We compare \nthe performance of the association design and our proposed \npower o ptimization based on the signal strength received \nfrom the Reference Signal Received Power  (RSRP). In the \nsimulation, the sensors randomly move in the macrocellular \ngeographical area and the main simulation parameters have \nbeen shown in Table 1.  \nWe studie d the power optimization  in beamforming -\nbased multi -layer  heterogeneous networks. An association \nalgorithm and cooperative power control were proposed to \nfind the optimal data speed in addition to  decreasing  the \nnetwork total power utilization . These scheme s consider the  \n 9 \nsystem  security, data speed and energy consumption to be \nrelatively important. Also, in this paper, the effect of the \nnumber of sensors and the size of the cache was also \ninvestigated.  \n \nA- Simulation Environment  \nWe evaluate the effectiveness  of our approach using \nproprietary computer simulation with the Monte Carlo \nmethod. We analyze the results of implementing the \nproposed approach in MATLAB 2019 and CVX tool of \nPython programming language . The experiment \nenvironment was considered as a mult i-layer heterogeneous \nsystem with a number of small cells within a micro layer.  \nThe confidence interval of the results is 90%. The 𝑆𝑈 \nsensor nodes are evenly distributed on the grid at 880 ×\n880  𝑚2. The base station is fixed without any mobility . But \nactivated cells are able to submit packets  to the base station \nin cell 35 via multihop paths. This procedure divides the \ntarget network into a grid of 36 cells measuring 167 ×\n167 𝑚2. We considered the sensitivity of each network node \nto receive a signal eq ual to -100 dBm, which is the common \nvalue in mobile sensor networks and the signal -to-noise \n(SNR) required a function which are able to estimate the \nbase CSI and 𝑃𝑇𝑆𝑖,𝐷𝑖 required to reach the next hop \ndestination by observing the signal to noise (SN R). The \nmaximum transmi ssion  power of each node is limited to 30 \ndBm. Distributed beamforming is used for each hop and \noccurs each time when |𝐿𝐴|>1.  \nWe measure the performance of the protocol \nconsidering the context of base station anonymity (i.e., \nreducing the belief that the cell contains base station 𝐵(𝑢=\n35)). To assess MCB -MSN from the energy efficiency \npoint of view, we compared it with three other schemes of \nmobile sensor networks: Fixed Power Allo cation (FPA), \nRandom Power Allocation (RPA) and Cooperative NOMA \nSimultaneous Wireless Information and Power Transfer \n(CN-SWIPT) [ 38]. \nB- Numerical Results  \nFigure 5 compares the average throughput of the proposed \napproach, MCB -MSN and CN -SWIPT scheme with equal \nmaximum transmission power. Based on this figure, it is \nobvious that the average sum data rate increase s with \nincreasing the signal to noise ratio. It can also be seen that \nthe MCB -MSN algorithm performs much better than other \nalgorithms in terms of higher data rate. Because the MCB -\nMSN algorithm has the required flexibility to dedicate \nresources to the network entities.  \nTable 1 . Main implementation factors  \n \n \n \n  \n \n \n \n \n \n \n \n \n \nThis trend decreases slightly with an increase in N, because \nthe algorithm reduces the throughput available to each of \nthe nodes. In contrast, the demand for the throughput of \neach node is the same in all random power allocation, equal \npower allocation, CN -SWIPT algo rithms, because they all \nprovide the minimum throughput for each N. The \nthroughput decreases exponentially with increasing number \nof N. Because with increasing N, the demand for data rate \ndecreases. Because, the same MTP is shared equally \nbetween the nodes . \n \n \nFig. 5. Average throughput  vs. signal to noise ratio  \n \nBased on the achieved results in Figure 6, the average sum -\nrate increases almost linearly with increasing N in the \nMCB -MSN algorithm. While all three other two \nalgorithms, CN -SWIPT and RPA/PC show slight \nimprovement  \n 00.511.522.5\n0 5 10 15 20 25 30Average Throughput (bps/Hz)\nSNR[dB]RPA+PC\nCN-SWIPT\nMCB-MSN\nParameter  Value  \nConfiguration of the Network  Mobile Network, X -sectored BSs  \nsensor distribution model  uniform (U) and hotspot (Hs),  \ntransmit backoff  1.5 dB  \nBase Station MTP  43 dBm  \nCodec strategy  Adaptive multi -rate \n𝑹𝒙  𝒍𝒐𝒔𝒔  & 𝑻𝒙  𝒍𝒐𝒔𝒔  3 dB  \nPropagation model  Okumura -Hata  \nFairness Index  Security/ Throughput  \nUpper bound of iteration  2000  \n𝑳𝒎𝒂𝒓𝒈𝒊𝒏  5 dBm  \nLearning factor 𝒄𝟏=𝒄𝟐 1.1  \n \n \n 10 \n \nFig. 6. Average sum rate vs. number of sensors  \n \nFigure 7 shows the capacity of backhaul links and their \naverage traffic (link usage in percentage) for random power \nallocation, MSB -MSN and CN -SWIPT. Based on this \nfigure, it can be seen  that the MCB -MSN algorithm has the \nbest performance in terms of load balancing and link usage \nand capacity. So, it has the highest possible efficiency in \nusing backhaul links. Also, the high capacity of backhaul \nlinks reduces the potential for the backhau l link to be \ntrapped in the bottleneck while sending the traffic flow to \nthe central network.  \n \nFig. 7. Backhaul links capacity vs. signal to noise ratio  \n \nFigure 8 shows the performance of MCB -MSN and CN -\nSWIPT algorithms according to different number s of nodes. \nAs can be seen from the graph, energy efficiency is obtained \nwhen broader constraints on the total number of nodes are \nconsidered. Because, the feasible range of the problem increases and the algorithm has more freedom to maximize \nthe throughput  and minimize the energy consumption.  \nBut as the upper bound of demand decreases, the \nfeasible range becomes narrower and energy efficiency \ndecreases. Further lowering the upper bound to 𝑦𝑚𝑖𝑛 =\n𝑦𝑚𝑎𝑥 =𝐶𝑢𝑒 (where 𝑐𝑢𝑒 is the user equipment demand \nthat must be met) results in identical efficiency of both the \nMCB -MSN and CN -SWIPT algorithms. Therefore, the \nMCB -MSN algorithm, which uses dynamic power \noptimization, performs better than the CN -SWIPT, which \nhas strict constraints procedure.  \nFigure  9 shows  that the MCB -MSN algorithm uses \npower sources better than other algorithms. Increasing the \nMTP to saturation increases the energy efficiency. After \nreaching saturation, increasing MTP does not affect the \nenergy efficiency. Increasing MTP in Equivalent Pow er \nAllocation (EPA), Random Allocation (RA) and CN -\nSWIPT algorithms does not improve energy efficiency. In \nthis case, the performance of these algorithms is slightly \nworse. In the form of 1000 independent simulations, we \ninvestigated how many times each al gorithm successfully \ncalculates the solution. Our criterion is actually the possible \nvalues used to summarize the result of each simulation \nresult in CVX.  CVX as a linear programming method is a \npowerful optimizer for solving iterative problems like the \nintroduced main problem. Such convex -based tools can also \nbe applied to analyze for rapid prototyping of models and \nalgorithms incorporating convex optimization.  \n \n \n \nFig. 8. Network energy efficiency vs. total number of nodes  \n 012345\n0 10 20 30 40 50 60 70 80 90 100Average sum -rate (bps/Hz)\nNumber of Sensors [N]RPA+PC\nCN-SWIPT\nMCB-MSN\n02468101214\n0 5 10 15 20Capacity (bit per channel)\nSignal to noise ratio (dB)MCB-MSN CN-SWIPT RPA+PC FPA\n3456789\n15 25 35 45Network Energy Efficiency (bits/Joule)\nTotal number of Nodes (N)×106\nMCB-MSN\nCN-SWIPT\nFPA\nRPA+PC \n 11 \n \nFig. 9. Network energy efficiency vs. maximum transmission power  \n \n6. Conclusion  \nThis paper presents a novel approach to Multi hop \nCooperative Beamforming Mobile Sensor Network (MCB -\nMSN), which not only increases the anonymity of the base \nstation but also m aximizes the network energy efficiency in \nthe distributed beamforming by choosing the routes with \nhigher relay densities. In this paper, when the ℒ𝑖 link cost \nof the MCB -MSN algorithm is used, the mobile sensor \nnetwork maintains its level of anonymity significantly more \nthan in a state where anonymity enhancement techniques \nare not used. In future studies, more MCB -MSN energy \nconsumption should be eva luated in mobile sensor \nnetworks considering  non-ideal cooperative  beamforming \nconditions  so that  information needs to be transmitted \nfrequently.  In future, we plan to explore the potential of \nmulti -agent smart queuing in various HetNet scenarios such \nas p rivacy -aware recommendation and store cell \nrecommendation. We will also plan to examine how to \nexploit multi -modal data in the mobile sensor networks to \nfurther improve the proposed model.  \n ",
      "metadata": {
        "filename": "A Novel Approach for Establishing Connectivity in Partitioned Mobile Sensor Netw.pdf",
        "hotspot_name": "Aluminium_Die_Casting",
        "title": "A Novel Approach for Establishing Connectivity in Partitioned Mobile\n  Sensor Networks Using Beamforming Techniques",
        "published_date": "2023-08-09T08:35:00Z",
        "pdf_link": "http://arxiv.org/pdf/2308.04797v1",
        "query": "die casting aluminium radiator energy consumption reduction process optimization techniques"
      }
    },
    "A process planning system with feature based neural network search strategy for": {
      "full_text": " \n AIJSTPME Vol. 1, No. 1, January-June 2008  \n \n \n \nA Process Planning System with Feature Based Neural Network Search Strategy for \nAluminum Extrusio n Die Manufacturing  \n \n \nS. Butdee1, C. Noomtong1, S. Tichkiewitch2  \n1 IMSRC, Department of Production Engineering, Faculty of Engineering, KMUTNB, Bangkok, Thailand \n2 G-SCOP Laboratory, Grenoble Institute of Technology, Grenoble, France  \n \n \nAbstract \nAluminum extrusion die manufacturing is a critical ta sk for productive improvement and increasing potential \nof competition in aluminum extrusion industry. It causes  to meet the efficiency not only consistent quality but \nalso time and production cost reduction. Die manufacturing consists first of die design and process planning \nin order to make a die for extruding the customer’s re quirement products. The efficiency of die design and \nprocess planning are based on the knowledge and experience of die design and die manufacturer experts. This knowledge has been formulated into a computer system called the knowledge-based system. It can be reused to support a new die design and process planning. Such knowledge can be extracted directly from die geometry \nwhich is composed of die features. These features are stored in die feature library to be prepared for \nproducing a new die manufacturing. Die geometry is defined according to the characteristics of the profile so we can reuse die features from the previous similar profile design cases. This paper presents the CaseXpert \nProcess Planning System for die manufacturing based on feature based neural network technique. Die \nmanufacturing cases in the case library would be retrieved with se arching and learning method by neural \nnetwork for reusing or revising it to build a die design and process planning when a new case is similar with \nthe previous die manufacturing cases. The results of the system are dies design and machining process. The \nsystem has been successfully tested, it has been proved that the system can reduce planning time and respond high consistent plans. \n \n \nKeywords :  \nProcess Planning, Feature-Based Neural Network, Aluminum Extrusion Die Manufacturing. \n \n \n1 INTRODUCTION\nAluminum extrusion is a hot deformation process \nused to produce long, straight, semi finished metal \nproducts such as bars, solid and hollow sections, tubes and many shapes products. The hot aluminum \nextrusion process is pressed under high pressure and \ntemperature in a specific machine. A billet is squeezed from close container to be pushed through a \ndie to reduce its section [1]. A profile shape is \ndeformed in order to be adapted to the die orifice shape. The significant machine used to press \naluminum is called an aluminum extrusion press. The \nmain tooling of aluminum extrusion process is a die. It is used to form aluminum profile shapes. Hence, \ndie must be efficiently constructed to support \naluminum extrusion process to ensure obtaining a good extruded profile. Aluminum extrusion die manufacturing is the work significant activity in \naluminum extrusion industry in order to obtain an \nefficiency die. Die manufacturing is proceed of two main phases, including the die design and the process \nplanning for machining to make a die. Die design is \nbased on the skill of a die designer who has accumulated the working experiences for many \nyears. Die designer gives the die design concept and \ndetails to create the die geometry in CAD system. In addition, process planning for die machining is also \nbased on the experience of  a die process planner \nexpert. The knowledge base of die design and \nprocess planning should be captured from the \nexperts. It would be formulated, managed and stored \nin a computer system in order to use it and revise with new situation die design. At the present, \n \n27 \nS. Butdee  et al. \n \nintelligence system has been discussed in knowledge \nmanagement for industrial fields, such as design and \nmanufacturing. Intelligent systems can be knowledge based system, genetic algorithm, rule-based and \nframe-based expert system , fuzzy logic, artificial \nneural networks which are general tools using in \norder to solve the complex or specific problem in \nengineering works [2]. For aluminum extrusion die \ndesign and process planning of die manufacturing, we propose an artificial intelligent system to aid \nmanagement of the knowledge of design and process \nplanning with the feature based neural networks \ntechnique to search the si milar previous die design \ncase, in order to reuse the previous cases in the new \ncase design and manufacturing. Die geometry can be \nconstructed by the combination of geometrical \nfeatures. The principle features are holes, edges, grooves, pockets etc. These features are given by die \ndesigner experts. Therefore die features should be organized in feature library for reusing at a new die design and to decrease die design lead time. The \nknowledge of die design is translated from the human \nskill to the knowledge based system via feature \ndefinition. Hence, the knowledge base of die design \ncan be directly based on from die geometry design. \nThe geometrical features data of a die contains \nfeature shape, dimensions and so on. Moreover, each \nfeature of die geometry is employed to define the \npossibility of machining processes to fabric a die. The knowledge based system of designing of \nextrusion die is organized with frame-based and rule-\nbased system. This paper consists of five sections. \nFirstly, is to explain the fundamental of aluminum \nextrusion process and extrusion tools. The second section describes the main tool used for extrusion \nprocess (aluminum extrusion die). The third section \npresents the principle methods of die design process, the knowledge base of die design, and die geometry. \nThe fourth section addresses the process planning of \ndie manufacturing, including the knowledge base of die process planning.  The fifth section presents \nfeature based neural network for aluminum extrusion \ndie manufacturing.  Case study is illustrated in the sixth section. Finally, the summary section concludes \nthe application of the proposed system to support \naluminum extrusion die manufacturing. \n  \n2 ALUMINUM EXTRUSION \n2.1 Extrusion process \nAn extrusion press is used to extrude many materials \nsuch as lead, aluminum, copper, zinc, brass, etc. There are four characteristic differences among the various methods of extrusion and press used: \n• The movement of the extrusion relative to \nthe stem - direct and indirect process. \n• The position of the press axis – horizontal or vertical press. \n• Type of drive – hydraulic (water or oil) or mechanical press. \n• Method of load application – conventional \nor hydrostatic extrusion. \nNormally, the extrusion proce ss can be classified into \ntwo basic methods, direct and indirect extrusion. This paper we only discuss on the direct extrusion process. \nThere is the simplest production of aluminum extrusion process, which can be carried out without \nlubricant. The most important and principle method \nused in extrusion is the direct process as shown in Figure 1, which follows the step sequence as given \nbelow [3]:  \n• Loading the billet and piston into the press \n• Extrusion of the billet, press billet through \ndie \n• Decompression of the press and opening of \nthe container to expose the discard and the \npiston \n• Shearing the discard or backend or input a \nnew billet \n• Returning the shear, container and ram to \nthe loading position \n2.2 Equipment and tooling in aluminum \nextrusion process \nThe principle equipment and tooling in hot aluminum extrusion process are explained in [4], including: \n2.2.1  Press machine \nAn aluminum extrusion press is the main machine to extrude aluminum ingot (Billet) through a die; pressure from machine is generated by hydraulic \nsystem. Almost horizontal and hydraulic extrusion \npress is widely using in an aluminum extrusion industrial. Hydraulic extrusion press is illustrated in \nFigure 2. \n2.2.2  Run out table and puller \nThe extrusion emerges from the press onto the run out table, which supports the extrusion. A puller or \npullers guide the extrusion and keep it under constant tension. Run out table and puller equipments are \nshown in Figure 3.  \n2.2.3  Die oven \nDie oven is an equipment to preheat die temperature on appropriate state for extrusion. In hot forming \nmaterial tooling must be heated to increase\n \n \n28  \nCaseXPert A Process Planning System with Feature Based Neural Network Search Strategy for Aluminum \nExtrusion Die Manufacturing  \n \n \nFigure 1: Direct extrusion process \n \n \nFigure 2: Press machine and schematic of direct extrusion press \n \nFigure 3: Run out table and puller \n A1 P \nA1   P \nA2  P AC Main Ra m PistonContaine rDie \nStack Platen \nExtrusre \nSide Cion High \nPressu\nOil \nylinde r\n \n29 \nS. Butdee  et al. \n \nmechanical properties and to control temperature \nenvironment. Normally, an aluminum extrusion die is \nheated nearest billet temperature about 425-450°C. \nAnother, the significant parameter is die preheat \ntime. To prevent under heating or over heating of die, \ndie preheat time also needs to  be strictly controlled. \nThe recommended die preheating practices are as \nfollow: \n• Minimum soaking time: 1hr/inch of the die \nand backer. • Maximum allowance time after a die \nreaches specified temperature \n  300° C - 24 hours \n 370 °C - 10 hours \n 420 °C - 8 hours \n 480 °C - 2 hours \nIndeed, die preheat time ba sed on the efficiency of \ndie oven and any parameter, each company should \nhas its standard time. Die oven is illustrated in Figure 4. \n \n  \nFigure 4: Die oven \n \n2.2.4  Billet Oven \nBillet oven is used to preheat billet temperature 420- 450 \n°C. Most common are gas or induction ovens. \nBillet oven is shown in Figure 5. High billet \ntemperatures (Temperature more than 480 °C) will \nreduce the extrusion pressure, but decrease extrusion \nspeed of the extrusion in order to avoid surface \ndefects. Globally, the productivity of the press is \nreduced.  \n \n \n \nFigure 5: Billet oven \n 2.2.5  Stretcher \nStretcher is an equipment to extend aluminum profiles longitudinal direction. The purpose is to relieve residual stress in extruded profiles. Normally, \nstretch straight extrusion length about 0.5% and \ndecreases the cross-sectional dimensions correspondingly, over stretching can result in \ndistortion or dimensions out of tolerance. Stretching \nof 2% or more can lead to orange peel defects on the extrude surface. A high level of stretching is required \nfor straightening of distorted sections. A better \napproach is to control metal flow through the die to \nreduce distortion rather than to apply high levels of \nstretching. Aluminum extruded stretcher is shown in \nFigure 6. \n2.2.6  Extrusion cut-off saws \nA cut-off saw is an equipment to cut the aluminum profiles to preset length. Circular blades for cutting \naluminum extrusions are 16 to 20 inches and are \nmade from carbide. Modern saws have a self-contained hydraulic system and chip collector to trap \ncontinuously. Figure 7 illustrates extrusion cut-off \nsaws  \n \n \n30  \nCaseXPert A Process Planning System with Feature Based Neural Network Search Strategy for Aluminum \nExtrusion Die Manufacturing  \n \n \n \nFigure 6: Stretcher \n \n \n \nFigure 7: Extrusion cut-off saws \n \n2.2.7  Aging Oven   \nThe aging oven is given a heat treatment the extruded \nprofiles to improve the mechanical qualities of the \nproduct. The extruded profiles are heated to a set \ntemperature for an extended period ranging from 4-10 hours depending on mechanical properties \nrequired. Not all extrusions are artificially aged at the \nplant. Aging oven is demonstrated in Figure 8. \n \n \n \nFigure 8:  Aging oven  \n3 ALUMINUM EXTRUSION DIE  \nIn aluminum extrusion process, a die is an important \ntool to deform aluminum ingot (Billet) to get straight \naluminum profiles. An aluminum profile is formed \nby die orifice (die hole). In addition, the quality of products and extrusion productivity often depend on \ndie performance. An aluminum extrusion die can be \nclassified into three basic types:  solid, semi-hollow and hollow die. Its can produce aluminum profiles \nsolid, semi-hollow and hollow section respectively. \nThe normal type of die for solid and open semi-\nhollow shapes can be used for all metals that are \nextruded. Hollow sections in  a variety of shapes and \nsizes are reserved for alum inum alloys apart from the \nsimple hollow sections that can be produced in heavy metals or steel with a mandrel. The development of die design for simple and complicated aluminum \nsections is determined first of all because the \nextruded products have significant position in the world market. Aluminum extrusion tooling includes \ndie set, die holder, bolster/sub-bolster or pressing \nring, etc. The typical alum inum extrusion dies are \nillustrated in Figure 9. Die set of solid and hollow die \nare presented as in Figure 10 and 11 respectively. In \naddition, the table 1 explains the fundamental \nfunction of each die part component to play an \nimportant role in aluminum extrusion process. \n \n \n \nFigure 9: Aluminum extrusion dies \n \n31 \nS. Butdee  et al. \n \n \n \nFigure 10: Solid die components\n \n  \nFigure 11: Hollow die components\n \nTable 1: The functions of extrusion tooling \n \nDie    Form the section. A solid die \ntype is called die plate and for \nhollow die, we found its die cap \nand mandrel. \nBacker Supports the tongue of die to \nprevent collapse or distortion.   \nThe shape of backer orifice is often closely related to die orifice. Feeder \nplate  Balance aluminum flow through die orifice. \nBolster Supports extrusion load is \ntransmitted from die and backer. \nDie holder Holds the die and, to some \ndegree, the die backer. \nDie carrier Holds the die set in the press. \nBridge Divided metal flows and \nsupports the mandrel.    Typical solid die parts \nFeeder plate \nDie plate \nBacker plate \nBacker plate Solid die assembly view Product solid shape \nProduct \nhollow shape \nMandrel \nDie cap \nHolow die assemble view Bolster \n \n32  \nCaseXPert A Process Planning System with Feature Based Neural Network Search Strategy for Aluminum \nExtrusion Die Manufacturing  \n \nThe difficulty to obtain a good die is on the equal re petition of the velocity in each point of the final \nsection. It is the condition to obtain a good straight \nbar (see 4.1). A die for an aluminum extrusion needs properties follow as: \n• Accurate dimensions and product shape, to avoid the need for any corrective work and to decrease cost repair die. \n• Maximum possible die life, increase productivity and decrease extrusion cost per \nunit. \n• Maximum length of the extruded section, \nunless it is determined by calculating \nextrusion yields and selecting a proper billet \nsizes. \n• High extrusion speed, depend on die design \nand extruded operation. \n• A good-quality surface finish product. \n• Low manufacturing cost.  \nThese requirements are usually fulfilled with rod and \nsimple shapes. However, as the die more complex, it \nbecomes increasingly more difficulty to comply six \nrequirements. Many factors have to be considered in the design and construction of a die, including the \nflow pattern, maximum specific pressure, \ngeometrical shape of the s ection, wall thickness and \ntongue sizes, shape of bearing surface, and tolerance \nof the section.  Extrusion dies are essentially thick, \ncircular steel disks containi ng one or more orifices of \nthe desired profile. A die type and die geometry \ndepend on the characteristic s of each profile to be \nextruded. There are normal ly fabricated from hot \nworking steel, as H-13 (American Standard) and heated to the desired condition. Such difficult section \nof a bar served as example of what it formed to do \nfor forging exhibition. In a typical extrusion, the \nextrusion die will be placed  in the extrusion press \nalong with several supporting tools. These tools, also made from hardened tool steel, are known as backers, \nbolsters and sub-bolster. Its can provide support for the die during the extrusio n process to prevent die \nbroken and to extend the die life. In addition die \nsupport tools contribute to improved tolerance controls and extrusion speed.  A tool stack for a \nhollow die is similar to that used for a solid die. A \nhollow die is a two-piece construction, one piece \nforming the inside of the hollow profile and the other \npiece forming the outside of  the profile. It likewise \nrequires the use of additional support tools.  \n3.1 Solid dies \nSolid dies are used to produce profiles that do not \ncontain any voids. Various styles of solid dies are \nused, depending on the equipment and manufacturing \nphilosophy of the extruder. Some prefer to use recessed pocket or weld-plate style dies. A pocket die \nhas a cavity slightly larger than the profile itself, \napproximately to deep. This cavity helps control the metal flow and allows the billets to be welded \ntogether to facilitate the use of a puller. Both pocket \nand weld plate type dies provide for additional metal \nflow control, compared to that of the flat-face type \ndie. A weld plate is a steel disk that is placed (often pinned and/or bolted) in front of a solid die. It has an opening that controls the flow of metal to the orifice. \nWeld or feeder plates serve to control contour, and/or \nspread the aluminum. It is also important that the \nlayout of a multi-hole die is arranged to prevent \nextrusions from rubbing together or running on top of \neach other as they leave the press. A flat surface and \nnot the edge of a leg or rib should run along the run-\nout table to prevent small part of profiles to be bent. In Figure 12 illustrates typical solid dies one and four \nprofiles for a die plate. \n \nFigure 12: Typical solid dies\n \n33 \nS. Butdee  et al. \n \n3.2 Hollow dies \nA hollow die produces profiles with one or more \nvoids such as tube products. The profiles could be as simple as a tube with one void or as complex as a \nprofile with many detailed voids. The most common \ntype of hollow die is the porthole die, which consists of a mandrel and cap section; it may or may not have \na backer or back plate. The mandrel, also known as \nthe core, generates the internal features of the profile. The mandrel has two or more ports, it based on the \nshape of profiles or die design method. The \naluminum billet will be separated into each port and \nrejoins in the weld chamber prior to entering the \nbearing area and die orifice. Webs, also known as legs, which support the core or mandrel section, \nseparate the ports. The cap creates the external \nfeatures of the profiles. It is assembled with the mandrel. The die cap has a pocket or welding \nchamber, which is rejoin metal flow from porthole \nthrough die orifice. It is assembled with the mandrel. The backer or back plate, when used, provides \ncritical tool support and it is  immediately adjacent to, \nand in direct contact with the exit side of the cap. In \ncase the profiles have critical point a backer will necessary be supported to avoid die broken and to \nextend die life. Typical hollow dies are presented in \nFigure 13. \n \n \n \nFigure 13: Typical hollow dies \n \n3.3 Semi-hollow dies \nA semi-hollow die is used to produce profiles having \nsemi-hollow characteristics as defined in Aluminum Standard and Data (published by the Aluminum \nAssociation, Inc.). Semi-hollow dies have port holes, \nlegs, bridges as same as hollow die but without cores to make a void of section. It gives the extruded solid \nprofile shape with high tongue ratio. In practice, \nsemi-hollow profile, tongue ratio is grater than 5. \nThe semi-hollow classification derives from a mathematical comparison between the area of the \npartially enclosed void and the mathematical square \nof the size of the gap. This ratio (area/gap²) is called \nthe tongue ratio. Depending on the tongue ratio, semi-hollow dies can be constructed as flat, recessed-\npocket, weld-plate, or porthole design. Porthole dies \nare more prevalent in the production of semi-hollow profiles.  \n 4 ALUMINUM EXTRUSION DIE DESIGN \n4.1 Die design process \nDie design is one of crucial task in aluminum extrusion process. The traditional die design is based \non skills and experiences of a die designer. He/she \nlearns and accumulates knowledge of die design from the previous die design cases. The success and failure \ncases are studied and analyzed in order to use their \ninformation from these cases for improving a new die \ndesign to avoid failure case. In practice, a die may be \ntested many times before the extrusion profile is \nsatisfactory [5]. The first die test points out the \nefficiency of die design. If  the extruded profile shape \nis not perfect, the tester will modify die geometry \nsuch as bearing length to balance extrusion speed of \nthe section for obtaining profile shape perfectly.  \nThe quality factors of die design are: decrease the \nnumber of tests, increase die life, produce a good \nproduct and provide high productivity. This part \ndescribes the fundamental of aluminum extrusion \n \n34  \nCaseXPert A Process Planning System with Feature Based Neural Network Search Strategy for Aluminum \nExtrusion Die Manufacturing  \n \ndie design process. In general, die design process cons ists of laying out, selecting type of die and size, \ncalculating shrinkage all point on profile section, \ndetermining feed or porthole shapes (will be defined just after) and sizes for controlling aluminum flow, \ndetermining dimensional die orifice and tongue \ndeflection, calculating the bearing lengths, designing tooling support. Furthermore, a die designer has to \nconsider the size of the containers of the machines \nand the number of extruded profiles graved in the same die for each size of c ontainer. Each die design \nprocess should be focused on the parameters that have effect to die efficien cy such as shape factor, \nextrude ability, extrusion ratio, etc. In fact, die design or die geometry will be modified following the profile characteristics, esp ecially, profile shape and \ndimensions. To obtain high extrusion productivity, die design must be optimized to increase the capability of the extrusion process and to overcome \ndie fail. In addition, a tester should have a through \nunderstanding of the different  functions of each die features (i.e. leg, sink in, porthole, bearing etc.). These features will be modified to interact with the process parameters and each of the product \ncharacteristics. The complex flow of material in the \nextrusion die or container often creates different \ndeformation conditions in various regions of an \nextruded product. Die design has to imagine the \nmetal flow pattern in die and container in order to make decisions for the defi nition of each die feature. \nEspecially, die orifice a nd bearing length are the \nimportant features, which have directly effect with \nmetal flow velocity. In theo ry, flow velocity of each \npoint on the section should be balanced to avoid the \nextruded profile twist. Figure 14 illustrates the \ntypical profile, which is incompletely deformed \nshape because die bearing lengths are unbalance then \nthe velocities of two sides of the section are large \ndifferent. \n \n \n \nFigure 14: Typical profiles unbalance shape \n \nAs already described, bearing length is used to \ncontrol the extruded profile shape and extrusion speed. The suitable given bearing length is required \nin aluminum extrusion die design. If bearing length is \ntoo short, the profile shape may be twist or wave. On the other hand, great bearing length will reduce \nextrusion speed and productivity. In addition, the \nextruded surface quality is not satisfactory. \n4.2 The knowledge of die design \nIn general, die design is based on skills and experiences from a die designer, who accumulates the knowledge of die design for using this knowledge \nto solve the die design problems. The problems in die \ndesign work are: the selection of the suitable press, \nthe type of die, the choice of material, the laying out \nthe profile(s) on the die in order to optimize the extrusion yield, the different of the contraction \ncooling of each sections, the unbalance velocity of \nthe metal flow on each point of the sections, the consolidation of the tooling to withstand in extrusion process, etc. These problems have to use the \nknowledge base of die design in order to find the best \nsolution for solving each problem and to approach a \ngood die. \nFigure 15 shows the basic flat die for solid sections, \nconsisting of feeder plate, die plate, backer plate, \nbolster, and sub-bolster. Naturally, the metal flows from the container into a die by passing feeder, die orifice, back opening, bolster, and sub-bolster \nrespectively. The feeder pl ate contains one or more \ncavity to be used to weld the metal flow together \nbefore through into die orifice and to balance the \nmetal flow to the die. Sometime, flat die design is \nwithout feeder plate; if we can sink the face of the die \naround the cavity in the die plate. \nHowever, the feeder plat e can permit to enable \nsuccessive billets to weld together and to enable \nwider sections to be extruded than is normally \npossible from any given container size. The die plate is used to deform the material in order to take the \n \n35 \nS. Butdee  et al. \n \nshape of die orifice (see Figure 15). The backer provides the immediate support to the die to reduce \ndie deflection and to minimize the die stresses. In \ncase of no long tongues, the simply sections are usually extruded by using one of a standard range of \nbackers. If the shape contai ns critical sections that need good support, then a custom backer with an \naperture closet the die orifice can be used for \nsupporting the tongue necessary. \n   \nMetal flow  \nFeed \nOrific e \nBack \n \nFigure 15: Solid die geometry and features \n \nFurthermore, bolster and sub-bolster are used to \nsupport consolidation of a die. The bolster supports \nthe backer and usually sufficient support can be \nachieved by using standard bolster.  \nPorthole dies are primarily used to produce hollow \nextrusions as shown in Figure 16. The basic tooling \nin hollow extrusions composes of mandrel, die cap, \nbolster, and sub-bolster. The porthole dies are \nsuitable for multi-holes dies and can also be used \nwith the maximum section circumscribing circle \ndiameter relative to the container diameter. A \nfavourable extrusion ratio can be selected by using the optimum material flow; heating to relatively low \ntemperatures should then be sufficient. Moreover, \nthis type of die is used for sections with very critical \ntongues and cross sections. However, the die is \nmanufactured from a single piece of steel, die \ncorrection and die cleaning by polishing are difficult. Mandrel is the projection, fixed or floating that is \npositioned in front of the die cap in order to make the \nempty part of the sections. In porthole die the aluminum flow is split by legs, which support the \ncore of the mandrel. The material flows around the \nlegs, through the feeder holes and is welded together in the welding chamber as shown in Figure 16. The \nprofile shape is formed by  the clearance between the core on the mandrel and th e die opening in the die \ncap. The wall thickness of the extrusion is determined by the difference  in the diameters of the \ndie apertures in die cap and the mandrel. Die cap \ncontains weld chamber, die orifice, bearing length. \nThe hollow shapes are fo rmed by the core and \nbearing surface on die cap. In practice, the distance between core and die orifice is given by determining \nthe contraction cooling of the sections. Bolster and sub-bolster for hollow dies have the functionality \nthan for solid dies, in order to protect the die \ncollapse. \nBesides, the fundamental of die design, a die \ndesigner needs the explicit and implicit knowledge \nduring die design stages in order to obtain the efficiency of the die design. This knowledge \nencompasses the selection the suitable press and \ntooling, the determination number of die opening and the laying out of the die orifices, the calculation \nshrinkage allowances on section dimensions, the \nproviding of additional dimensional allowances for die dishing and tongue deflection, the determination \nof the bearing length to control the profile speed in \neach point on the sections, and the checking of the \nadequate support for critical sections. The knowledge \nabout die design is described the next part.\n \n36  \nCaseXPert A Process Planning System with Feature Based Neural Network Search Strategy for Aluminum \nExtrusion Die Manufacturing  \n \n \n \n Metal flow direction \n Weld \nchambe r\nFeeder \nhole\nCore \nLeg \nBearing \nsurface  \nFigure 16: Hollow die geometry and features \n \n4.3 Die geometry \nDie can be distinguished into three categories as \nsolid, hollow and semi-hollow die as described \nabove. The difference die geometries provide the \nvarious shapes of the profiles. Each die part has different machining processes in order to perform the \nfinal die shape. We proposed that the features of die \nparts are constructed based on OO (Objected-Oriented). Classes describe the feature objects and it \ncan be inherited by deriving a feature subclass from feature head class. Each class has methods and \nattributes to describe their class entity. These feature \nclasses are managed in database system. By example we have a holes feature class and then it can be \nderived to blind hole, through hole, counter bore, \ncountersink, etc. A user defines these features data and translates geometry data from CAD into feature \nclasses for using in process planning. For instance, \nclass hole has methods to calculate holes surface and volume, and attributes of holes feature includes \ndiameter, depth, centre point, axis, tolerance, and \nsurface finishing. Each attribute is used to define holes entity and to calculate holes surface and \nvolume respectively. Moreover, it is used to select \nmachining tools, such drill, centre drill, tap, and so on to make the holes. Figure 17 illustrates the typical \nfeatures of die geometry for principle solid die, for \nthe It has three parts: feeder, die, and back plate. \nA feature is a basic entity or information that has \nproperties to involve in engineering design and \nmanufacturing. It also represents engineering significance, not only geometric and topologic information. Non-geometric property such as, roughness, hardness of material, and so on are \npresented [6]. Die geometry  features are used to \nsupport die design in order to generate die geometry. \nIt can decrease die design lead-time and to facilitate \nfor creating a new die geometry by reusing or \nrevising the die features from library. The die features can be easily modified in order to use the \ninformation from features for supporting the process \nplanning of die manufacturing. This research die \nfeatures can be divided into two categories; die part \ngeometry and die feature lib rary. Die part features \ncompose and describe the f eeder plate, die plate, \nbacker plate, mandrel, die ca p, back plate, bolster and \nsub-bolster. These part feat ures can be reused for \nnew die design by adding the required feature from \ndie feature library. Die pa rts feature library are \nillustrated in Figure 18.  \nThese systems geometry are created by computer-\naided design such as SolidWorks ®. It has feature \nlibrary to facilitate and reuse the standard features in \norder to modify geometry. Thus feature library in \nCAD software can be applied to support die design \nbased on feature base design. Feature library of die \ngeometry can be used to modify feed feature, die orifice, backer opening, porthole, welding chamber, \nand so on. Moreover, each feature has entities to be \nused in machining process or computer aided \nmanufacturing.  \n \n37 \nS. Butdee  et al. \n \n \n Feature classes Die parts Die assembly \nOPEN POCKET CIRCULAR \nEDGE CHAMFER TAP HOLE \nCLOSED POCKET PLANE \nOPEN POCKET \nCIRCULAR EDGE CHAMFER \nTHROUGH HOLE \nPROFILE HOLE \nCLOSED POCKET PLANE \nOPEN POCKET \nCIRCULAR \nDEGE CHAMFER THROUGH HOLE \nCOUNTERBORE \nCLOSED POCKET \nPLANE Part no. \nFPAC245-01 \nPart no. \nBKAC245-01 Part no. DPAC245-01 \n \nFigure 17: Feature class library \n \n Die parts feature\nDie Solid die parts Hollow die parts \nFeede r Backe r Bolste r Mandrel Dieca p Bac k \n \nFigure 18: Die part feature library \n \n \n \n \n38  \nCaseXPert A Process Planning System with Feature Based Neural Network Search Strategy for Aluminum \nExtrusion Die Manufacturing  \n \n4.3.1 Feeder feature \nFeeder has function to control the metal flow through \ndie orifice. For solid die, feed  is in the feeder plate or \ndie plate whereas for hollow die, feed is the internal \npart of the mandrel and is called porthole.  Feeder \nshape affects the flow pattern, the extrusion pressure, the metal flow velocity, and the consequence the \nbearing length. Bearing length is adapted, based on \nthe local wall thickness and the velocity of the metal flow just at the entry thro ugh the die orifice. So the \nbearing length relates directly with feeder shape. For instance in Figure 18 sh ows typical feed feature \nlibrary and simple holes feat ure. Die part feature as \nfeeder plate can be subtracted with feeder feature as dog bone shape as shown in Figure 19. A user can modify feature dimensions to attain a new feed \ngeometry for each design case based on profile \ngeometry.  \n \n \n \nFigure 19: Typical feed and simple holes feature \n \n4.3.2 Welding chamber feature \nMetal flows from each porthole of mandrel in hollow die and joins again before to be pushed through die orifice in a welding chamber. Welding chamber \ngeometry is designed depending on porthole design \nand welding line allocation on the section. In practice, welding line should be avoided allocation \non the exposed surface. A die designer can select the welding chamber feature in die feature library. Welding chamber feature can be modified in order to \nobtain the required shape and dimensions. Moreover, the welding chamber depth is given according to the \nwall thickness of the section and is determined from \nthe total die depth. Figure 20 illustrates typical welding chamber features for using it to create the \nwelding chamber in die cap of hollow die geometry.  \n \n \n \nFigure 20: Typical welding chamber features \n \n39 \nS. Butdee  et al. \n \n5 PROCESS PLANNING OF ALUMINUM \nEXTRUSION DIES \n5.1 Process planning reviews \nProcess planning is defined as the planning and \ndevelopment of detailed instructions for the \nconversion of a raw materi al into a finished part \nbased on some feasible engineering design [7]. \nProcess planning encompasses the activities and \nfunctions to prepare a detailed set of plans. It includes the sequence of operations, selection raw \nmaterial, machines, tools, cutting tools, cutting \nparameters, etc. In general, process planning is based \non the skill and experience of a process planner who \ncan provide the knowledg e of process planning. \nHowever, the human memory may be lost and it is \nnot easy to collect enormous data. Knowledge based \nsystem has been determined to manage the implicit and explicit knowledge fr om any sources in a \ncomputer system. In recent years, artificial intelligence has been discussed and used in order to develop in a computer system for supporting process \nplanning tasks. In general process planning can be \ndistinguished into three methods: \n• The VPP (Variant Process Planning) method                                                                                                                  The hybrid method approach attempts to exploit knowledge in existing plans while generating a \nprocess plan for a new design [8]. Computer aided \nprocess planning can eliminate many of the decisions \nrequired during planning. It has the following \nadvantages: reduces the demand on the skilled \nplanner, reduces the planning time, reduces process planning and manufacturing costs, creates consistent \nplans, produces accurate plans, and increases \nproductivity [9]. The knowledge-based expert systems for process planni ng are reviewed in [10]. \nBy example, it is used to be application for in research [11] and [12] . In addition, Stryczek \nreviewed the application of computational \nintelligence in computer aided process planning [13].  \n• The generative method \n• The hybrid method \nThe variant process planning involves to retrieve an \nexisting plan for similar part and to modify the previous plan for the new part. Variant method is \nbased on a Group Technology (GT), coding and \nclassification approach to id entify a larger number of \npart attributes or parameters. The structure of VPP \nbegins by coding the part by a user and then searches \nthe similar part from part database. Each part of die set has a standard machine routing and retrieve the \nstandard operations from operations sequence file for \nreusing and modifying in order to apply with a new \npart. Finally, the system gives process plan. \nHowever, the machining routing of standard parts \ncan be generated from the knowledge of a die process planner.  \nThe generative method is based on useful knowledge base, production rules, or artificial intelligence to generate process plans. This method approaches \ndeals with generation of new process plans by means \nof decision logics and process knowledge. Data from \nCAD is extracted to recogni ze the feature shapes and \nretrieve the dimensions for generating the process \nplans with an inference engine. The inference engine \ndecides the selection process and machine by \nderiving from the knowledge base. In fact, \nknowledge base for process planning comes from the acquisition of the knowledge from experts, from the \nmachine specifications and capabilities, and so on. In \naddition, knowledge base provides the operations sequence, machines, cutting tools, cutting \nparameters, machining times, etc via the inference \nengine. To realize a generative process-planning module, it is necessary to have a knowledge base \nwhich includes three main components: the part \ndescription, the knowledge base and database, and the decisional logics and calculus algorithm.  \n5.2 The knowledge of process planning \nThe knowledge base is the central component of an \nexpert system. It contains generic knowledge for solving specific domain-related problems. In general, we can classify the knowledge base into three main \ncategories. There are procedural knowledge, \ndeclarative know ledge, and control knowledge. \nProcedural and declarative knowledge are stored in \nthe knowledge base. The control knowledge is used to construct the inference mechanism. Procedural \nknowledge can be sometimes also called production \nrules which represent the relation structure and problem oriented hierarchies of the knowledge stored \nor the production rules. The declarative knowledge \nmay be called the knowledge details or problem facts, represents the factua l part of the knowledge or \nthe specific features or th e problems. The knowledge \nmay be a group of data or a symbolic structure. Additionally, the declarative knowledge can be \nstored in a database. The control knowledge is the \nknowledge about a variety of processes, strategies, and structures used to coordinate the entire problem–\nsolving process, is not stored in the knowledge base. \nThe knowledge base for process planning is used to store the production rules, an inference engine, a \ndatabase, and so on. It can be used in order to \nperform the process planning for aluminum extrusion \n \n40  \nCaseXPert A Process Planning System with Feature Based Neural Network Search Strategy for Aluminum \nExtrusion Die Manufacturing  \n \ndie. The knowledge base of process planning and cost evaluation contains  rules and techniques for \nknowledge representation. It includes cutting tools, \ncomponent features and machining process [14]. The \nproposed knowledge-based system utilizes (rules, \ntables, equations, etc.) for: component specification, tool material selection, machining process, cutting \ntools selection, and cutting conditions. Martin and \nD’Acunto [15] presented a procedure for the design of a production system, based on part modeling and \nformalization of technological knowledge by using \nproduction features, and finally they can calculate \nproduction costs by determining from the multi-\ncriteria analysis. Grabowik and Knosala [16] presented a method of representation of the \nknowledge about the body construction and \ntechnology in an expert sy stem that aids the process \nof designing the machining technology of bodies. \nExpert system is focused on the improvement of the \nproposed method of object representation on the technological knowledge and body construction in \norder to increase the level of the generated \ntechnological documentation by increasing the number of the problems. For this research we \nemployed frame-based system and rule-based in \norder to manage the knowledge base of die process planning and cost estimating.  \n5.3 Die machining process \nMachining processes for making a part of die consist of various processes depe nding on geometry of each \npart as following: turning, drilling, milling, EDM (wire cutting, drilling, sparking), grinding, and \nassembling. Moreover, heat-treatment is the \nimportance method to impr ove mechanical properties \nof die material for supporting hot working and high \npressing.  \n5.3.1 Turning process \nTurning process is the early stage to form raw material cylindrical shape to obtain die disc before \nmachining with another processes. Facing, turning, \nand chamfering are the operations for machining \neach die part. Turning operati on of a die is shown in \nFigure 21. A measurement of  how fast material is \nremoved from a work piece can be calculated by \nmultiplying the cross section area of the chip by the \nlinear travel speed of the tool along the length of the \nwork piece. Material remova l rate in turning can be \ncalculated in the form:   \n MRR = π  × D × d × f × N            (1)   \nWhere: \n  D  is outer (or average) work piece diameter    d  is depth of cut \n f   is feed rate (ipr) \n N   is spindle speed (rpm)  \nThe spindle speed is given by: \n \n   DvN××=π12 (2) \n \nMaterial removal rate becomes: \n \n MRR = 12 × d × f × V            (3)  \n \n \n \nFigure 21: Turning operation \nWhere: \n  V   is cutting speed (fpm) \nIn straight turning, chip width is  ⎟⎟\n⎠⎞\n⎜⎜\n⎝⎛−\n20 fD D\n \nWhere:  \n D o and Df are outer and inner work piece \ndiameters, respectively \nCross section area is  ()\n42 2\n0 fD D−π\nhence the \nmaterial removal rate is  \nN fD DMRRf× ×−=4) (2 2\n0π\n (4) \n5.3.2 Milling process \nMilling is widely applied in machining process due \nto it includes a number of highly versatile machining \noperations capable of producing a variety of configurations. The basic types of milling such as \nslab milling, face milling, end milling, and so on. \nMilling process can be machined feed, porthole, weld chamber, etc. for die part  machining as shown in \nFigure 22.  \n \n41 \nS. Butdee  et al. \n \nMaterial removal rate in milling process can be \ncalculated by determination from milling parameters \nas the cutting speed, V (m/min) , is given by: \n \n V= π ×D V   (5) ×\n \nWhere: \n D is the cutter diameter (mm) \n N is the rotational speed of the cutter (rpm) \nFeed per tooth, mm/tooth can be calculated by: \n    n Nvf×=        (6)  Where: \n v   is linear speed of the work piece or feed \nrate, mm/min \n n  is number teeth on cutter \nThen material removal rate is given by the expression \n MRR = w × d × v (7)  \nor            \n MRR = w × d ×(f × N × n) (8) \nwhere:  \n w  is width of cut (mm) \n d  is depth of cut (mm)  \n \n \n \nFigure 22: Milling operation \n \n5.3.2 Grinding process \nGrinding process is a chip-removal process that uses an individual abrasive grain as cutting tool. The \ngrinding process is used to flatten the die surface in \norder to assemble and to correct die distortion. Die face must be flatness to avoid unbalance of the metal \nflow. Material removal rate in grinding machining is \ngiven by: \n   MRR = d × w × v   (9) \nWhere:  \n d  is depth of cut (mm) \n w is width of cut or grinding wheel \nthickness (mm) \n v   is linear speed of the work piece or feed \nrate, mm/min  \n5.3.3 Wire EDM machining process \nElectrical discharge wire cutting is used to cut contour as thickness plates, punches, dies, and small or complex holes. It will be cut die orifice or deep \nhole in die manufacturing process. The wire is a \ncutting tool that is usually made of brass, copper, or tungsten; zinc or brass coated and multi-coated wires are also used. The wire diameter is typically about \n0.30 mm for roughing cuts and 0.2 mm for finishing \ncuts. The cutting speed is generally given in terms of \nthe cross-sectional area cut per unit time. Typical \nexamples are: 18,000 mm\n2/hr for 50 mm thick D2 \ntool steel, and 45,000 mm2/hr for 150 mm thick \naluminum. These removal rates indicate a linear \ncutting speed of 18,000/50 is 360 mm/hr or 6 mm/min, and 45,000/150 is 300 mm/hr or 5 mm/min, \nrespectively. Typical EDM wire cutting process is \nillustrated in Figure 23. \n5.3.3 Electro discharge machining process \nElectro-discharge or spar k-erosion machining, is \nbased on the erosion of metals by spark discharges. The basic EDM system consists of a shaped tool \n(electrode) and work piece, the metal surface is \nremoved from a transient spark discharges through the dielectric fluid. Elect rodes for EDM are usually \nmade of graphite, brass, copper, and copper-tungsten is also used. EDM sparking is generally used to make bearing length of an extr usion die. Metal removal \nrates usually range from 2 to 400 (mm\n3/min). \n \n42  \nCaseXPert A Process Planning System with Feature Based Neural Network Search Strategy for Aluminum \nExtrusion Die Manufacturing  \n \nMoreover, EDM can be used to make the core shape of mandrel as shown in Figure 24.   \n \n \nFigure 23: EDM wire cutting process \n \n \n \nFigure 24: EDM sparking process \n \n5.3.4 Heat treatment \nIn addition, the important process in die \nmanufacturing is heat treatment. This process has played a role to increase die strength property.  \nFrom machining process as described above for fabrication an extrusion die, we proposed machining process classes as turning process class, drilling \nprocess class, milling pro cess class, and so on are \nderived from machining process class. These classes \nhave methods to calculate machining parameters and \nmetal removal rate in order to calculate machining \ntime of each process.  6 THE KNOWKEDGE BASE OF DIE \nDESIGN AND PROCESS PLANNING \n  \n6.1 The structure of knowledge based system for \ndie manufacturing  \nFor our application, we employed frame-based system and rule-based in order to manage the \nknowledge base of die process planning and cost \nestimating. Figure 25 presen ts the flow chart of the \nknowledge base process planning and cost estimating \nof a die. The knowledge is managed in frame-based \nsystem. \n \n \n \n43 \nS. Butdee  et al. \n \nTo approach process planning of a die, we need form \nfeatures and material type of die part in order to \nselect the feasible machinin g processes. Die features are retrieved by two ways: from die part geometry or \ndie features database.  \n \n F2 Die part Geometr\ny \nFrom features \n& material \nDie features \ndatabase \nCost estimation \ndatabasePossible machining process &  \nparameters\nCalculate machining \nTimes & costs\nSelect process \nOperation processes Constraints \n- cost \n- time - … \n- … \n Machining \nProcesses & \nCutting \nParameters \ndatabase Machines & \nTools database \nFeature -type \n-dimensions -tolerances \n-Finishing \nSurface -Volume Frame-based and rule-based system \nOp: Operation process F: Feature \nFOP\n \nF1 \nF3 OP 1 OP 2 OP 3 OP n \nFn \nFigure 25: Process planning and cost estimating flow chart of an aluminum extrusion die \n \nOn one hand, a user has to define all features of their parts into a database of a new die part case. On the other \nhand, we can reuse die features by reusing previous di e part features from data base. Die feature includes \nfeature class type, dimensions, tolerances, finishing surface , volume, etc. These features are then used to select \nmachining process by frame-based, rule -based system and decision table. Each feature has different operation \nprocesses to be machining. The syst em gives the possible machining processes by frame-based and select the \nsuitable those processes based on rule-based system a nd decision table. The sel ected process composes of \noperation method, machine type, machine tools, and cutting parameters. Machines and tools data can also be \nretrieved from machine and tools database. Machining process and cutting parameters database provide die operation method in the process and cutting conditions in order to calculate cutting time. Each machining \nprocess will be evaluated cutting time by formulas and machining time is then used to calculate machining \ncost. Finally, the system selects the process accord ing to constraints as machining time and cost. \nThis knowledge base system for process planning of the extrusion dies is used to train the data sets of input \nand output in neural network architecture. The next section describes the structure of die process planning with \nartificial neural network technique. \n6.2 The structure of die design and process pla nning system with arti ficial neural network \nThe structure of die design and process planning is illustrated in Figure 26. This structure consists of two main parts, including die design and process planning. The fi rst part is design die features based on the neural \nnetworks as defined in 7.1. The process starts by reading product data from database or from customer’s \nproduct. The product data of aluminum extrusion process is the characteristics of profile. It contains profile shape, section area, dimensions, tongue ratio. This data is  input data layer of the neural network in order to \n \n44  \nCaseXPert A Process Planning System with Feature Based Neural Network Search Strategy for Aluminum \nExtrusion Die Manufacturing  \n \nsearch the previous die design cases from die design case library. The output layer is the similar case which is \nretrieved from library to be reused the features of die design geometry. The machining processes for making \nthe die features can be given by the knowledge base of die machining process planning. \nThe frame based and ruled based systems are managed and derived the machining processes by determining \nthe machined features of die geometry. Tools database can support the system to select the machine tools for \neach machining operation. Process parameters are give n from process database. These machining parameters \nare cutting speed, cut of depth, feed rate, and so on. They can be defined with the knowledge base of the \nsuggestion from tool’s manufacturer and shop floor data  of die maker. Moreover, die machining process \nsequencing is ordered according to di e making routes standard. Each machining process route depends on the \ntype of die part. Eventually, machining process plan is generated by the proposed structure of die manufacturing based on neural network technique. \n \nDIE DESIGN\nInput\nlayer\nHidden\nlayer\nOutput\nlayer\nPROCESS PLANNINGProfile carateristics\nMachining processProduct\ndata\nDie design\ncase library\nTool & process\nparameters\nProcess sequencingTools &\nprocess\ndatabase\nProcess planDie features\nKnowledge\nbase process\nplanningDie design case\n \nFigure 26: The structure of die manufacturing with artificial neural network \n \n \n45 \nS. Butdee  et al. \n \n7 FEATURE BASED NEURAL NETWORK FOR ALUMINUM EXTRUSION DIE DESIGN AND \nMANUFACTUTING \n7.1 Artificial neural network in CAPP \nIn last decade, artificial neural network has been widely used in engineering application domain, especially for \ndesign and manufacturing. Artific ial neural network is a mathematical model for parallel computing \nmechanisms as same as biological brain. They consist of nodes, linked by weighted connections. Neural \nnetworks are constructed by hierarchical layers, which are input, hidden, and output layer respectively.  Neural \nnetworks learn relationships between input and output by iteratively changing interconnecting weight values \nuntil the outputs over the problem domain represent the desired relationship. Furthermore, neural networks \nperform a variety of functions such as pattern matchi ng, trend analysis, image recognition, and so on. CAPP \n(Computer Aided Process Planning) is the important task to couple CAD and CAM by interpreting feature model to machining process. Knapp et al [17] presented the ability of neural network in the process selection \nand within feature process sequencing. In this work, tw o co-operating neural networks were utilized: the first \none, a three layer back propagation neural network, takes in as input the attributes of a feature and proposes a \nset of machining alternatives; another fixed weight ne ural network selects exactly  one of the alternatives. \nParameters of the features are modified by the results of the operation until the final state of the feature has \nbeen reached. Yahia & al [18] proposed a feed forward neural network based intelligent system for computer aided process planning methodology. This methodology suggests the sequence of manufacturing operations to \nbe used, based on the attributes of a feature of the component. By integrating this methodology with computer \naided design (CAD), process planning can be generated, and tested, which helps in realizing concurrent engineering. Yue and al [19] presented a stat e of the art review of research in computer  integrated \nmanufacturing using neural network techniques. Neural network-based  methods can eliminate some \ndrawbacks of the conventional approaches, and  therefore have attracted resear ch attention particularly in \nrecent years. The  four main issues related to the neural network-based techniques, namely the  topology of the \nneural network, input representation, the training method and  the output format are di scussed with the current \nsystems. The outcomes of  this research using neural network techniques are studied, and the limitations and  \nfuture work are outlined.  Praszkiewicz  [20] purposed of this article is to  present the application of neural \nnetwork for time per unit determination in small lot production in machining. A set of features considered as input vector and time consumption in manufacturing process was presented and treated as output of the neural net. A neural network was used as a machining model. Sensitivity analysis was made and proper topology of \nneural network was determined.   \n7.2 Structure of artificial neural networ ks for die design and process planning \nThe proposed structure of neural network for die design and process planning is shown in Figure 27. The input \nparameters consist of the characteristics of aluminum profile including type of profile, profile shape, \ndimensions, cross-section area, extrusion ratio, CCD (Circumscribing Circle Diameter), tongue ratio etc. The \noutput layer contains the type of die, the number die orifice, extrusion ratio, die stack set, and die machining \nprocess routes.  \nShape\nSection area\nDimensions\nTongue ratioDie type\nNo die orifice\nInput\nlayer\nHidden layerOutput\nlayerProfile caracteristic\nDie machining\nprocess routeExtrusion ratio\nDie stack set\nFigure 27: The structure of neural networks for die \ndesign and process planning \n \nThe mathematical model of the biological neuron, there are three basic components as presented in Figure 28.  \n \n46  \nCaseXPert A Process Planning System with Feature Based Neural Network Search Strategy for Aluminum \nExtrusion Die Manufacturing  \n \n∑ ()ϕ\u00000x\n1x\n2x\npx0kw\n1kw\n2kw\nkpwkθ\nThresholdOutput\nSumming\njunctionkνActivation\nfunction\nky\nInput signals\n \nFigure 28: A perceptron neuron model \n \nFirst, the synapses of the neuron are modeled as weight s. The value of weight can present the strength of the \nconnection between an input and a neuron. Negative weight values reflect inhibitory connections, while positive values designate excitatory connections. Second component is th e actual activity within the neuron \ncell. This activity is referred to as linear combination. Finally, an activation function controls the amplitude of \nthe output of the neuron. An acceptable range of output is usually between 0 and 1, or -1 and 1.  \nEach neuron calculates two functions. The first is propagation function as shown in equation 10,  \n \n     (10)  kk wxν=∑ j j\n \nWhere wkj is the weight of the connection between neuron k and j, yk is the output from neuron k. The second is \nan activation function. The output of a neuron in a neural network is between certain values (usually 0 and 1, \nor -1 and 1). In general, there are three types of activation functions, denoted by ϕ(•) as illustrated in equation \n10. Firstly, there is the treshold function which takes on a value of 0 if the summed input is less than a certain \nthreshold value ( v), and the value 1 if the summed input is greater than or equal to the threshold value. \n \n {1  if  ν\n0  if  ν < (ν)=k\nkθ\nθ ϕ≥     (11) \n \nTraining of the networks will be discussed in the next section.  \n \n8 DATASET FOR NETWORK TRAINING \nThe key issues of the developed neural network based methodology for aluminum extrusion die design and \nprocess planning of die manufacturing will be discussed in the following sections: \n8.1 Formulate the knowledge based system of die design and process planning \nA set of rules has been generated to define the feature of die features and machining processes for each feature. These rules have been used to be the set of input and output layer of the neural network structure. These rules \nare captured from the successful die design and machinin g cases which are stored in die design case library. \nThe rules of thumbs may be gathered with the knowledge from die design and die making experts and other \nsource. The rules are formed in IF-THEN.  \n8.2 Design topology of the neural network model \nThe neural network has been trained by using the standard propagation algorithm. This work uses supervised learning, which is one of three categories of the training method. Supervised learning may be called \nassociative learning, is trained by providing with inpu t and matching output patterns. These input-output pairs \ncan be contributed by an external teacher, or by the system which co ntains the neural networks (self-\nsupervised). The learning process or  knowledge acquisition takes place by representing the network with a set \nof training examples and the neural network via the learning algorithm implicitly rules. The topology of the \nproposed neural network model applies feed forward archit ecture. Each variable is th e input value at a node of \n \n47 \nS. Butdee  et al. \n \nthe input layer. The input layer of neuronal node is designed in such a way that one node is allocated for the \nfeature type, and one node is allocated to  each of the above sets of feature a ttributes. Also the values of all the \ninput layer neurons are normalized to lie between 0 and 1.  The number of nodes in the input layer is equal to \none plus the number of all the possible different ranges of feature attributes, encountered in the antecedent part of the rules. The types of profile are represented by 3 nodes to denote solid, semi-hollow and hollow type \nrespectively. Profile shape is also given 20 principle sh apes which are addressed with 20 nodes. In addition, \ngeneral profile thicknesses are designed with 20 nodes, and each node has the difference value ranges in 0.5 \nmm. Moreover, dimensions (maximum width and height), CCD, cross section area, press machine capacity, \nextrusion ratio, perimeter, external perimeter and tongue  ratio define the numbers of the nodes in input layer \nwhich are 15, 15, 15, 15, 3, 30, 15, 17 and 12 respectively in our case. Consequently, the total number of nodes \nin input layer is 170. For example, the typical heat si nk profile is shown in Figure 29. The characteristics of \nprofile consist of: solid profile with rectangular shap e, general wall thickness 2.3 mm, width 24 mm, height \n15.3 mm, CCD 28.5 mm, section area 1.7 cm\n2, perimeter 20.32 mm, without external perimeter, and tongue \nratio 4.0. \n \nFigure 29: Typical aluminum profile \n \nWe can transform the characteristics of the typical profile to input layer of the network in vector format as \nillustrated in Table 2. \n \n \n \nTable 2: The typical vector of input layer \nColumn 1 2 3 4 5 6 7 8 …170 \nValue 1 0 0 0 0 0 0 0  0 \n \nIn the above vector, the column numbe rs [1-3] addresses the type of profile, [4-23] stand the profile shapes, \nand [24-33] stand for the sets corresponding to the diffe rent ranges of general wall profile thickness. Column \nnumbers [34-48], [49-63], and [64-78] are addressed according to the different ranges of maximum width, \nheight dimensions, and CCD respectively. Column numb ers [79-93] are the ranges  of cross section area. \nColumn numbers [94-96] address the press machine capacity as 660, 880, and 1800 tones respectively. Column numbers [97-126] stand for the sets corresponding to extrusion ratio. Column numbers [127-141] and [142-158] are presented the ranges of perimeter and external perimeter. The last column numbers [159-170] \nstand the ranges of tongue ratio.  \nThe output decision variables for the die design and process planning comprise of the various feature of die \ngeometry and die machining process plan. In addition, process sequencing and machining operations are given \nin the output layer. Die features and machining process are derived from the knowledge based system of die design and process planning. The first part of output network is a kind of dies, including 3 nodes as solid, \nsemi-hollow, and hollow die as same as the third 3 nodes input. The next two groups are the nodes of the \nnumber die orifice 15 nodes and the nodes of extrusion ratio  15 nodes respectively. Die set thicknesses are also \ndesigned corresponding to die stack dimensions of each pr ess machine. Die thickness co nsists of feeder plate, \n \n48  \nCaseXPert A Process Planning System with Feature Based Neural Network Search Strategy for Aluminum \nExtrusion Die Manufacturing  \n \ndie plate, backer plate thickness for solid or semi-hollow die, and mandrel, die cap thickness for hollow die. The number of die thickness group is 50 nodes.  Die features are defined following the main types of die \ncomponents, including feeder, die, back, mandrel and die cap part features. These features consist of four main feature categories, are hole, edge, gr oove, and pocket. Hole features have blind, through, tap hole, counter \nsink, counter bore, and deep hole. Edge features includ e edge chamfer and edge fillet. Groove features are v \ngroove, round groove, and rectangular groove. The last group is pocket which comprises open pocket plane, open pocket circular, open pocket scul ptured, closed pocket pl ane, closed pocket circular, and closed pocket \nsculptured. The number node of die features can be classified into 5 nodes following as die part components. The first node is feeder features which have open pocket  circular, open pocket plane,  edge chamfer, tap, and \nclose pocket plane. The second node addresses die featur es, including open pocket ci rcular, open pocket plane, \nedge chamfer, through, deep hole, and close pocket plane. The third node stands back features which are open \npocket circular, open pocket plane, counter bore, and close pocket plane. The fourth node presents mandrel \nfeatures, comprising open pocket circular, open pocket plane, edge chamfer, tap, open pocket sculptured, and \nclose pocket sculptured. The last node of die feature group nodes is die cap featur e, including open pocket \nplane, open pocket circular, edge chamfer, co unter bore, close pocket plane and deep hole.  \nThe last part of output layer is die parts machining processes (turning, drilling, milling, heat treatment, grinding, EDM (sparking, wire cutting, drilling) process) . Turning process includes rough turning, semi-finish, \nfinish turning, round chamfering, and round grooving operation. In addition, facing process is discussed to \nremove the material of open pocket circular in order to control die part thickness. Rough facing, semi-finish \nfacing, and finish facing are operations of facing proce ss. Drilling is the simple process to make a hole in die \ngeometry. There are seven operations: centering, drilli ng, boring, reaming, tapping, counter boring, and \ncountersinking. The important machining process to be cut the open pocket plane, and open pocket sculptured at mandrel, feeder plate, and back orifice of back plat e is milling process. This process comprises rough, semi-\nfinish, and finish milling operation. These machining processes and operations are also grouped in five nodes \naccording to die machining process pl anning routes which have feeder , die, back, mandrel, and die cap \nmachining process routes respectively.  \nIn addition, for aluminum extrusion manufacturing, heat treatment process is applied to enhance die strength property. After heat treatment, die may be distorted then  it must be machined with grinding process to flatten \ndie face. EDM sparking process is used to make beari ng lengths only die plate and di e cap. The last process is \nEDM wire cutting which includes rough and finish wire cutting. Die part of solid die and die cap of hollow die are used wire cutting process to cut die orifice. So the number of nodes in the output layer is 93. The output \nlayer vector is shown in Table 3. \n \nTable 3: The typical vector of output layer \nColumn 1 2 3 4 5 6 7 8 …93 \nValue 1 0 0 1 1 1 1 1  0 \n \nTolerance and surface finish are not se riously determined in aluminum extr usion die manufacturing due to the \nmachining process selection are not ba sed on the tolerance and surface finish value. The process routes of die \nmachining operation are consistency, it can be set the standard process.    \nIn the above vector of output layer,  the column numbers [1-3] stand the type of die. The column numbers [4-\n18] address the number of die orifice and extrusion ratio is presented in column numbers [19-33]. The column \nnumbers [34-43], [44-53], [54-63], [64-73], and [74-83] stand the part thickness of feeder, die, back, mandrel and die cap respectively. The next group column numbers [84-88] are the feature group of die part features. \nFinally, the last 5 nodes as shown in column number [89-93] stand the machining process routes in order to \nmake die parts. They are feeder, die, backer , mandrel, and die cap process planning routes respectively.                 \n8.3 Training the neural network \n \n \n49 \nS. Butdee  et al. \n \n \nFigure 30: Learning curve  \n \nThe standard back-propagation algorithm is used as th e learning mechanism for the neural network. The \ntraining data set has been prepared by translate die design cases data fro m cases library to input and output \nlayer format. Neural network tool box of MATLAB 2008 is employed to simulate the neural network \noperation. There are two alternative training modes possible, depending on the particular way of presenting the \ntraining patterns to the neural network and depending on when the network weights are updated, either after presentation of each training pattern or after presentation of the entire se t of examples. For this paper, we \nfound that the number of hidden layers is 1. The mode of training is pattern. The number of hidden layer is 5. Learning rate is o.1 and momentum rate is 0.7 respectively. The numbers of nodes in input and output layer are 170 and 93. The learning curve is shown in Figure 30.  \n \n9 CASE STUDY \nThis case study is tested with the aluminum extrusion die design and process planning to make a die from the \nexample case in an aluminum extrusion industry. The samp le product is selected to test the developed system. \nThe sample product is created by CAD system in order to  extract the characteristics of  this profile for use in \ndie design and process planning a die. The product is shown in Figure 31.  \nThe characteristics of this profile are including hollow profile with rectangular  shape, cross section area is 3.4 \ncm2, profile width is 50 mm and height is 14.7 mm, perimeter is 30.37 cm, and external perimeter is 19.24 cm, \nand tongue ratio is 1.4 respectively. This product data is transformed to input layer format in order to find out the die features and process planning. The result of this case is illustrated in Table 4 that has been translated \nfrom the codes of the output layer to features of a die and process planning.  \n \n \nFigure 31: The sample product in CAD system \n \n \n50  \nCaseXPert A Process Planning System with Feature Based Neural Network Search Strategy for Aluminum \nExtrusion Die Manufacturing  \n \nThe type of die is hollow die with one cavity. Extrusion ratio is 40. Die components are mandrel and die cap. There fore, the machining process plan s are mandrel, and die cap process plan ning route as shown in Table 4.  \nTable 4: The features and machining pro cess plan of the die design case study \nDie part Features Machining process and operations \nOpen pocket \ncircular \n Rough turning  \nSemi-finish turning \nFinish turning \nRough facing \nSemi-finish facing \nFinish facing \nEdge chamfer Round chamfering \nBlind hole \n Centering  \nDrilling \nTap Tapping Mandrel \nClosed pocket \nsculptured Rough milling \nSemi-finish milling \nFinish milling \nOpen pocket \ncircular \n Rough turning  \nSemi-finish turning \nFinish turning \nRough facing \nSemi-finish facing \nFinish facing \nBlind hole \n Centering  \nDrilling \nCounter bore Centering \nDrilling \nCounter boring  \nClosed \npocket \nsculptured EDM Sparking \nClose pocket plane Rough milling \nSemi-finish milling \nFinish milling Die cap \nDeep hole Rough wire cutting \nFinish wire cutting \n10 SUMMARY \nThis paper presents the result of application feature based method with artificial neural network in order to \nsearch the die design and process planning case from aluminum extrusion die manufacturing library. The \noutputs are die design features and possible die machining process plan. The die machining process plan consists of machining operations and sequences of each process. The detailed description of the neural \nnetwork based methodology includes formulating the knowledge base system for die design and process \nplanning, designing topology of the neural network model, and training the neural network have been given. The knowledge based system of die design and manufacturing are acquired from the die design and \nmanufacturer experts with other sources such as textbooks, researches, and so on. This knowledge is used to \ntrain in the neural network structure. The potential for application of the developed feature based neural network model has been presented with the help to design and plan the machining process of an aluminum \nextrusion die. Die design case stud y is successfully tested with the de veloped system. The number of die \n \n51 \nS. Butdee  et al. \n \nmanufacturing cases from the aluminum extrusion industry is more than 150 cases. The lead time of die design \nand process planning are decreased, and increase the effi ciency of die design and manufacturing to make a die \nto support extrusion process. It can enhance the competitive in order to produce a good aluminum extruded \nprofile.  \nIn addition, the fundamental of aluminum extrusion process, die design and process planning are described to understand the principal knowledge of aluminum extrusion process.   \n11 ACKNOWLEDGMENTS \nWe would like to thank MTAlumet Co., Ltd. Thailand for supporting the information and the resources to be used for this research. In addition, the authors thank Thai research fund and Fren ch government for funding \nour project.  \n12 REFERENCES \n[1] Laue K., Stenger H. (1981). Extrusion Processes,  Machinery, Tooling, American society for metals, \nThird edition, USA. \n[2] Negnevitsky M. (2002). “Artificial Intelligence: A Guide to Intelligent Systems”. Person Addison \nWesley, England.  \n[3] Sheppard T. (1999). “Extrusion of Aluminum Alloys”, Kluwer Academic Publishers, USA. \n[4] Noomtong C. (2006). “Aluminum Extrusion Die Design in A context of Integrated Design”. Thesis, \nInstitut National Polytechnique de Grenoble. \n[5] Noomtong C., Butdee S., Tichkiewitch S. (2008). “The case-based system for aluminum extrusion die \ndesign in a contex t of integrated”. Ninth International Aluminum Extrusion Technology & Exposition , \nVol. 1, pp. 371-380. \n[6] Lee S. F., Huifen W., Youliang Z., Jian C., Kwong W. C. (2003). “Feature-based collaborative design”. \nJournal of Materials Processing Technology , Vol. 139, pp. 613-618. \n[7] Kamrani A.K., Sferro P., Hanelman. (1995). “Critical Issues in Design and Evaluation of Computer \nAided Process Planning Systems”. Computers Industrial Engineering , Vol. 29, No. 1-4 \n[8] Elinson A., Herrmann J.W., Minis I.E., Nau D., Singh G. (1997). “Toward hybrid variant/generative \nprocess planning”.  Proceedings of DETC’97: 1997 ASME design Engineering Technical Conferences \nSeptember 14-17 , Sacramento, California. \n[9] Chang T. C. (1990). “Expert Process Planning for Manufacturing”, Addison-Wesley Publishing \nCompany, Inc, USA \n[10] Kiritsis D. (1995). “A Review of Knowledge-Based Expert Systems for Process Planning Methods and \nproblems”. International Journal of Manufacturing Technology , Vol. 10, pp. 240-262.  \n[11] Shi X., Chen J., Peng Y., Ruan X. (2002). “Development of a Knowledge -Based Process Planning \nSystem for an Auto Panel”. The International Journal of Advanced Manufacturing Technology , Vol. 19, \npp. 898-904. \n[12] Dequan Y., Rui Z., Jun C., Zhen Z. (2006). “Research of knowledge-based system for stamping process \nplanning”. The International Journal of Advanced Manufacturing Technology , Vol. 29, pp. 663-669. \n[13] Stryczek, R. (2007). “Computa tional intelligence in computer aided process planning-A review”, \nAdvance in manufacturing science and technology , Vol. 31, pp. 77-92. \n[14] Abdalla H.S., Edalew K.O., Nash  R.J. (2001). “A computer-based intelligent system for automatic tool \nselection”. Materials and Design , Vol. 22, pp. 337-351. \n[15] Martin P., D’Acunto A. (2003). “Design of a production system: an application of integration product- \nprocess”. Int. J. Computer Integrated Manufacturing , Vol. 16, pp. 509-516. \n[16] Grabowik C., Knosala R. (2003). “The method of knowledge representation for a CAPP system”. Journal \nof Materials Processing Technology , Vol. 133, pp. 90-98. \n[17] Knapp G.M., Wang H.P. (1992). “Neural networks in acquisition of manufacturing knowledge”. \nIntelligent Design & Manufacturing, Edited by Andrew Kusiak, John Wily & Sons Inc. \n \n52  \nCaseXPert A Process Planning System with Feature Based Neural Network Search Strategy for Aluminum \nExtrusion Die Manufacturing  \n \n[18] Yahia N.B., Fnaiech F., Abid S., Sassi B.H. (20 02). “Manufacturing process pl anning application using \nartificial neural networks, System”. Man and Cybernetics, IEEE International Conference , Oct 6-9, Vol. \n5. \n[19] Yue Y., Ding L., Ahmet K., Painter J., Walters M. (2002). “Study of neural network techniques for \ncomputer integrated manufacturing”. Engineering Computations , Vol. 19, pp. 136-157.  \n[20] Praszkiewicz I.K. (2008). “Appli cation of artificial neural network for determination of standard time in \nmachining”. Journal of Intelligent Manufacturing, Vol. 19, pp. 233-240. \n \n \n \n \n \n53",
      "metadata": {
        "filename": "A process planning system with feature based neural network search strategy for.pdf",
        "hotspot_name": "Aluminium_Die_Casting",
        "title": "A process planning system with feature based neural network search\n  strategy for aluminum extrusion die manufacturing",
        "published_date": "2009-07-03T12:08:13Z",
        "pdf_link": "http://arxiv.org/pdf/0907.0611v1",
        "query": "die casting aluminium radiator energy consumption reduction process optimization techniques"
      }
    },
    "An Innovative Line Balancing for the Aluminium Melting Process": {
      "full_text": " International Journal of Mechanical and Industrial Technology        ISSN 2348 -7593 (Online)  \nVol. 12, Issue 2, pp: (73 -84), Month: October 2024 - March 2025, Available at: www.researchpublish.com  \n \nPage | 73  \nResearch Publish Journals  Innovative Line Balancing for the Aluminium  \nMelting  Process  \nProf. Dr. Ray Wai Man Kong1, Ding N ing2, Theodore Ho Tin Kong3 \n1Adjunct Professor, S ystem Engineering Department, City University of Hong Kong, China  \n1Eagle Nice International (Holding) Ltd., Hong Kong, China  \n2Engin eering Doctorate Student, S ystem Engineering Department, City University of Hong Kong, China  \n3 Graduated Student, Master of Science in Aeronautical Engineering,  Hong Kong University of Science and Technology, \nHong Kong \n3 Thermal -acoustic (Mechanical) Design Engineer at Intel Corporation in Toronto, Canada  \nDOI: https://doi.org/ 10.5281/zenodo.15050721  \nPublished Date: 19 -March -2025  \nAbstract:   This research article explores the optimization of aluminium  extrusion processes through advanced line \nbalancing techniques, focusing on maximizing marginal profit by increasing melting and casting outputs. By \nemploying mixed integer linear programming (MI LP), we identify strategies to minimize idle costs and enhance \nproduction efficiency. The study demonstrates that increasing the daily cycle rate from 2 to 4.36 cycles results in a \nsignificant rise in daily marginal profit, calculated at USD67,786, after a ccounting for additional labour  costs. This \noptimization is achieved by expanding the workforce from 8 to 12 operators across two shifts, leading to a 50% \nincrease in labour  expenses. The findings reveal a remarkable 117.6% growth in marginal daily profit,  underscoring \nthe potential of automation and intelligent manufacturing in transforming the aluminium  extrusion industry.  \nInsights from cross -industry research, including Lean Methodology in the Modern Garment Industry, further \nillustrate the broader appl icability of these advancements. This study highlights the critical role of automation in \ndriving productivity and profitability in manufacturing sectors, paving the way for future innovations in aluminium  \nextrusion and beyond.  \nKeywords : Line Balancing, Pr oduction Plan, Aluminium  Extrusion , Automation, Aluminium  Manufacturing, Lean \nPractice .  \nI.   INTRODUCTION  \nAs a global market research company, the Aluminium  Extrusion Market Size was valued at USD 83.9 Billion in 2023 from \nMarket Research Future (MRFR) [1]  in Fig . 1, which is from the MRFR Database and Analyst Review. The Aluminium  \nExtrusion industry is projected to grow from USD 90.77 Billion in 2024 to USD 170.53 Billion by 2032, exhibiting a \ncompound annual growth rate (CAGR) of 8.20% during the forecast period (2024 - 2032).  The Aluminium  Extrusion \nMarket will be driven by a ris e in the need for strong, lightweight extruded items with high corrosion resistance, and \neconomic expansion, accelerated urbanisation  and expanding infrastructure projects are the key market drivers enhancing \nmarket growth.  \nThe aluminium  extrusion market h as been driven by the increasing demand for lightweight and durable extruded products. \nAluminium  extrusions are widely used in vari ous industries, including construction, automotive, aerospace, electronics, and \nconsumer goods, among others. One of the main  advantages of aluminium  extrusions is their lightweight nature. Aluminium  \nhas a high strength -to-weight ratio, making it an ideal material for applications where weight reduction is a critical factor, \nsuch as in the automotive and aerospace industries. Aluminium  is highly durable and corrosion -resistant, which makes it an \nattractive choice for products that need to withstand harsh environments or extreme weather conditions.  The aluminium   International Journal of Mechanical and Industrial Technology        ISSN 2348 -7593 (Online)  \nVol. 12, Issue 2, pp: (73 -84), Month: October 2024 - March 2025, Available at: www.researchpublish.com  \n \nPage | 74  \nResearch Publish Journals  frame can be used for the construction of the required material  for the iPad, iPhone and window.  The major materials of \nthe auto vehicles are the aluminium  alloy frame.  \nThe aluminium  alloy manufacturer has the most critical role in  the extrusion process , which  allows for the creation of \ncomplex shapes and designs, which ca n be used to meet the specific needs of different industries and applications.  \nII.    MANUFACTURING PROCESS OF ALUMINIUM ALLOY  \nManufacturing Process of Melting for Aluminium  Alloy Manufacture  \nAluminium  Alloy Melting is a major process in which a solid heats up and becomes a liquid state of aluminium . After the \nfeeding is completed, melting starts. During the smelting process, it is necessary to ensure that the melting is fast and \nuniform and that the te mperature near the flame is high, reaching more than 1200 ℃. The melting temperature is reaching \nover 700℃. To avoid local overheating and excessive temperature, the aluminium  alloy oxidation causes  a serious \ninconvenience to the later product refining. St irring should be carried out, and the unmelted furnace charge should be \nsqueezed into the aluminium  liquid so that almost all of it is immersed in the stirring to avoid local overheating. For the \nfinal immersion of Magnesium (Mg) and other metallic element s for various aluminium  alloys , the flame cannot be directly \nheated and melted, and because the aluminium  liquid is immersed in the raw material, the temperature is reduced, and the \nMg and other metallic elements are melted at a relatively low temperature,  which reduces the burning loss and improves \nthe combustion efficiency.  \nThe melting process chart in Fig. 2 is shown below to relate to the melting process. The machinery standard time and labour  \nstandard time are the critical time measurement s for the line balancing of the manufacturing process.  The input substance \nis the raw aluminium  and recycled aluminium , and the output is the aluminium  rod.  The aluminium  rod is a large and heavy \nstate of aluminium  to direct extrusion for reforming the va rious shapes of aluminium  to the related manufacturing processes \nfor the automobile, the frame of iPad & iPhone and window frame.  \n \nFigure 1 : Aluminium  Extrusion Market Research from MRFR Database and Analyst Review  \nThe problem  of Line Unbalancing in Aluminium  Melting  \nLine balancing in the aluminium  manufacturing industry is the technique of levelling the output of every operation in an \naluminium  alloy production line, so the output from the upstream operation can be optimized to pass through the \ndowns tream operation. There should neither be an accumulation of work between two processes (operation) nor a shortage \nof workpieces from the upstream workstation (previous work step) between the melting line and its inter -process. It is \nimportant to maintain t his balance because in a melting process line, the output of one process, as an aluminium  rod, is the \ninput of the next aluminium extrusion.  \n International Journal of Mechanical and Industrial Technology        ISSN 2348 -7593 (Online)  \nVol. 12, Issue 2, pp: (73 -84), Month: October 2024 - March 2025, Available at: www.researchpublish.com  \n \nPage | 75  \nResearch Publish Journals  A melting furnace and melting tank for stabilisation are not balanced; hence, there would be the following producti on \nproblems:  \n Reduced Efficiency:  \nIt means that an upstream aluminium  melting output is a downstream operation input. Because of this reason, the worker \nafter the melting process will not get the loading input as per their capacity of producing output, hence they will be \nunderutilized. In this case, it is to make matters wo rse, more machines and manpower will be allocated to increase \naluminium  rod sewing and cutting, but efficiency will fall even more.  \n Reduced the utilisation and efficiency of the melting furnace : \nThe melting furnace has poured the aluminium  liquid into the melting tank, and the melting furnace  is idle to wait for \nthe melting tank for several processes to the casting.  \n Energy wastage for keeping the high temperature in the melting furnace:  \nThe melting furnace is required to keep the high temperature and stir t he residue of the melting furnace. Once the melting \nfurnace is lower than the melting point of aluminium , the aluminium  solidifies to a solid state as steady stabilization of \nthe alloy structure.  It is not easy to melt and mix the new lot of aluminium  or recycled aluminium . \n \nFigure 2: Melting Process Chart for Aluminium  Alloy Manufacturing  \nIn the melting process, the operations in the melting furnace in Fig. 3 are defined as the melting workstation.  The operations \nin the melting tank are defined as the melting stabilization workstation  in Fig . 4.  The line balancing is required to make the \nbalance of the melting workstation and the stabilization workstation because these are related to the separated facility and \nequipment.  The workstation of stabilizatio n workstation includes the aluminium  rod casting in Fig. 4. \n \n International Journal of Mechanical and Industrial Technology        ISSN 2348 -7593 (Online)  \nVol. 12, Issue 2, pp: (73 -84), Month: October 2024 - March 2025, Available at: www.researchpublish.com  \n \nPage | 76  \nResearch Publish Journals   \nFigure 3 : Melting Furnace for Aluminium  Alloy Manufacturing  \n \nFigure 4 : Melting Stabilization and Casting for Aluminium  Alloy Manufacturing  \nThe traditional setup of the melting furnace line with the melting tank ratio is one-to-one, which is not followed by the line \nbalancing concept. The study of an aluminium  extrusion company is proposed for the use of the standard time ratio between \nthe melting furnace and the melting tank.  The below sect ion 4 shows  the case study how an aluminium  company refer to \nthe publication Journals from Prof Dr Ray WM Kong et al [ 2]. The imbalanced line balance  in the melting line  affects  the \nshortage of supply of aluminium  rods to the  aluminium  frame machining assembly.  For the aluminium  assembly, the \nupstream operation  is the supply of aluminium  rod and the downstream operation  is the aluminium  frame machining \nprocess .  It is not balanced the line balance; hence, the following shortage of supply causes  these  problems have  exist  as \nshown above points . \nIII.   LITERATURE REVIEW  \nProf. Dr Ray Wai Man Kong [3] has discussed strategies to optimize the mixed integer linear programming for line \nbalancing  and balance the capacity of machines, machine centres , and work centres  in the initial stages of line balancing to \nenhance output rates. In the context of aluminium  extrusion, merely increasing the capacity of individual machines or \nassembly lines does not necessarily improve overall production output and productivity due to line-balancing  challenges. \nThe article \"Lean Methodology for Lean Modernization\" provides insights into applying lean technology to develop future \nstate value stream mapping (VSM) and identify bottlenecks in the aluminium  extrusion process, thereby enhancing capacity \nand achieving balanced production.  \nIn the article on mixed integer linear programming in the Garment Line Balancing, there is a focus on the mathematical \noptimization method. For line balancing, Ocident Bongomin [ 4], the Assembly Line Ba lancing Problem (ALBP), also \n International Journal of Mechanical and Industrial Technology        ISSN 2348 -7593 (Online)  \nVol. 12, Issue 2, pp: (73 -84), Month: October 2024 - March 2025, Available at: www.researchpublish.com  \n \nPage | 77  \nResearch Publish Journals  known as assembly line design, is a family of combinatorial optimization problems widely studied for its simplicity and \nindustrial applicability. ALBP is NP -hard, encompassing the bin packing problem as a special case. ALBPs ar ise whenever \nan assembly line is configured, redesigned, or adjusted. In aluminium  extrusion, these problems are particularly pronounced \ndue to the unique balancing challenges posed by the extrusion process compared to other manufacturing lines, such as th ose \nfor trucks, buses, or machinery.  \nThe task involves distributing the total workload for producing aluminium  profiles among workstations along the line, \nadhering to strict or average cycle times. The general principles of line balancing include considera tions for machining, \nassembly, and disassembly environments; the number of product models (single -model, mixed -model, multimodal lines); \nand line layouts (basic straight lines, U -shaped lines, circular transfer lines).  \nThe assembly line balancing (ALB) pro blem has been extensively studied, as noted by Gary Yu -Hsin Chen [ 5]. The ALB \nmodel ensures that staff assignments balance the entire production process, effectively reducing production time or idle \ntime. In aluminium  extrusion, the mastery of skills by em ployees at each task is a critical indicator for achieving ALB. \nHowever, there is limited research on multifunctional workers with varying skill levels at workstations. Our research \nincorporates the Toyota Production System (TPS) principles, adapted for aluminium  extrusion, to optimize floor space, \nflexibility, and working conditions. This approach features U -shaped assembly lines and teams of workers managing \nextrusion processes on a single -piece flow basis.  \nChen et al. [ 6] address a multi -skill project sc heduling problem, which is relevant to aluminium  extrusion where projects \nare divided into tasks completed by skilled employees. Their multi -objective nonlinear mixed integer programming model \nconsiders employees' skill proficiency, multifunctional roles, and cell formation to minimize production cycle time. This \napproach, which accounts for real -world skill variations, effectively reduces production time through optimal personnel \nassignment and preferred production modes. The human factor introduces uncert ainty affecting actual cycle time, \nemphasizing the need for real -time dynamic line balancing in aluminium  extrusion.  \nHaile Sime & Prabir Jana (2018) [7] demonstrated the use of Arena simulation software to design and evaluate alternative \nproduction systems , optimizing resource utilization through effective line balancing. Markus Proster & Lothar Marz (2015) \nhighlighted the importance of dynamic balancing for high productivity in mixed -model assembly lines, applicable to \naluminium  extrusion where varying extrusion times for different profiles require adaptive strategies. Simulation tools can \nvisualize these methods, reducing complexity and enhancing transparency in planning extrusion lines.  \nGhosh and Gagnon (1989), along with Erel and Sarin (1998), provided detailed reviews on these topics. Configurations of \nextrusion lines for single and multiple products can be divided into single -model, mixed -model, and multi -model types. \nSingle -model lines extrude one product type, mixed -model l ines handle multiple products, and multi -model lines produce \nsequences of batches with intermediate setup operations (Becker & Scholl, 2006).  \nIV.   METHODOLOGY  \nA. Industrial Engineering  and Lean Technology to  Study the Line Balance of Aluminium  Extrusion  \nFollowing the Lean Methodology from the Lean Methodology for the Modern Garment Industry.  Industrial  engineering \napplies to the lean methodology and technology  in the line balancing of aluminium  extrusion manufacturing.  An industrial  \nengineer is working for the Here's how it is utilized:  \n(a) Work Measurement: Industrial engineering study involves conducting time and motion studies to measure the time \ntaken to perform each operation in the melting, stabilisation and casting processes . This cycle time data is related to \nthe machinery for calculating each operation's cycle time and relates  to the capacity . \n(b) Refining Process  in the Melting Analysis: the entire refining  process  is required to get the analysis report of aluminium  \nalloy comp osition  during the melting process at high temperatures . If the refinement cannot achieve the aluminium  \nalloy requirement, the refinement operation is required to be done again, repeated more times, to achieve the required \naluminium alloy composition and s trength. The simple  process  bottlenecks, inefficiencies, and areas of improvement \nin the refinement process can be identified by Value Stream Mapping. By understanding the process flow, the future \nstate of VSM can identify  opportunities for line balancing to optimize the line balancing between the melting furnace \noperation and the melting tank operation and optimize the  refinement time . \n  International Journal of Mechanical and Industrial Technology        ISSN 2348 -7593 (Online)  \nVol. 12, Issue 2, pp: (73 -84), Month: October 2024 - March 2025, Available at: www.researchpublish.com  \n \nPage | 78  \nResearch Publish Journals  (c) Capacity and Manpower Resource Allocation: Industrial engineers assess the aluminium  melting and casting workforce \nand eq uipment available in the manufacturing facility. They determine the number of operators required for each \noperation based on the involved machinery cycle time and capacity. The melting process relies on a melting furnace \nand machinery, but the refining and stirring operations are required for the skilful  operators to control the machine and \ncollect the sample of aluminium  alloy liquid for the metallic composition and strength te st. For the heavy machinery \nand aluminium  industry, it applies more than 1 operator to operate the facility and machinery to stir the aluminium  \nliquid and add the magnesium and other metals to mix the aluminium  alloy in the melting furnace.  The crows of \noperators are not fully loaded to operate the furnace and machine.  The lean practice with the industrial engineering \nconcept is a good tool to optimize the operators’ working time for sharing their time to involve more steps, reducing \ntheir waiting time an d idle time. This helps in allocating resources effectively and achieving a balanced line.  \n(d) Layout Design: Industrial engineers consider the layout of the time study of the melting process, stabilization process, \nrefinement process and casting,  which  has an  impact on efficiency  and output . They analyze the flow of materials, \nequipment placement, and operator movement. Optimizing  the layout , including batch layout  for setting the \noptimization of the aluminium  tank and the casting process.  The one melting fur nace to one melting tank and casting \nfacility is not an optimization.  To ensure the safety of the facility and not solidification of aluminium  liquid from the \nmelting furnace to the melting tank, the maximum ratio of the melting furnace to the melting tan k is 2 to 1 because of \nthe length of the connection pipe and the liquid flow rate between melting furnace to the melting tank  in Fig. 5.  \n(e) Continuous Improvement: Industrial engineering study emphasizes continuous improvement in line balanc e as referred \nto the Mixed Integer linear programming for Garment Line Balancing  and Lean Methodology . Industrial engineers \nmonitor the bottleneck operation of the refinery performance, collect data, and analyze  it to identify areas for further \noptimization. They implement changes, conduct follow -up studies, and refine the line-balancing  process to achieve \nhigher efficiency and productivity.  \n \nFigure 5 : Melting Line Comparison Diagram  \nV.   CASE STUDY FOR THE LINE BALANCING FOR MELTING PROCESS OPTIMIZATION  \nBy utilizing an industrial engineering study in line balancing aluminium  extrusion manufacturing, Compan y A in the case \nstudy  can optimize its production processes, reduce lead times, improve resource utilization, and enhance overall efficiency. \nThis results in increased productivity, cost savings, and improved customer satisfaction.  \nThe traditional melting process uses the melting furnace to li nk with the melting tank and the casting facility in series with \na ratio of 1:1. Aluminium  liquid in the melting furnace is poured into the melting tank by direct pipe connection. The time \nstudy of the traditional method was 2 times a day as shown in Fig. 6. \n International Journal of Mechanical and Industrial Technology        ISSN 2348 -7593 (Online)  \nVol. 12, Issue 2, pp: (73 -84), Month: October 2024 - March 2025, Available at: www.researchpublish.com  \n \nPage | 79  \nResearch Publish Journals  Figure 6: Traditional Time Schedule Melting Furnace and Melting Tank & Casting Chart  \nIn Fig. 7, the Time Schedule to Optimize the Melting Furnace and Melting Tank & Casting Chart is shown the Fig. 7, which \nmust be continuous, with no gaps in between. The materials flow through the layout in a loop shape. The hanger line is \nrequired to construct the hanger system and equipment.  The system is modernized to set up the control device to move the \nhanger between workstations and provide the just -in-time information to the manufacturing system.  The line balancing for \nthe hanger line can be optimised to increase production efficiency by increasing the throughput time based on increasing \nthe capacity of the bottleneck workstations in the process as the Lean Methodology for Garment Modernization that Prof \nDr Ray WM Kong mentioned [ 2]. \n \nFigure 7: Time Schedule to Optimize the Melting Furnace and Melting Tank & Casting Chart  \nThe plant  layout for optimization is  required to restructure the melting furnace to the melting tank with the casting facility.  \nThe plant layout has added the switch from the melting furnace to the melting tank with the casting facility. Aluminium  \nliquid can be poured  from two sets of meltin g furnaces to one set of melting tanks  and a casting facility.  \nProduction output before optimisation: 2 times/day  \nProduction output after optimisation: 4.36 times/day  \nThe increase in output  percentage : (4.36 times/day - 2 times/day) / 2 times/day x 100%  \nThe growth output percentage: 118%  \n International Journal of Mechanical and Industrial Technology        ISSN 2348 -7593 (Online)  \nVol. 12, Issue 2, pp: (73 -84), Month: October 2024 - March 2025, Available at: www.researchpublish.com  \n \nPage | 80  \nResearch Publish Journals  There is a n optimization in simple line balancing. The labour headcount is increased from 8 workers to 12 workers for 2 \nshifts a day. The labour cost has increased by 50%.  The maximum profit margin of each casting time o n 36 rods is over \nUSD28,800 (USD800 /rod). The additional profit margin from 2 times to 4.36 times a day is USD67,968.  To reduce the \nadditional labour cost fo r 4 workers , the total amount is USD182/day, so the net profit margin is USD67,786/day.  \nVI.   LINE  BALANCING MODEL ANALYSIS  \nThe main objective  of balancing the line is to maximize the marginal profit,  which requires reducing the idle cost of melting \nfurnaces. The increased daily output rates of aluminium alloy from the melting tank and the casting facility can provide \nmarginal profit, which also reduces the idle time of melting furnaces. The idle melting furnace is required to provide the \nenergy to keep the temperature of th e melting furnace because the drop temperature causes the aluminium  liquid to solidify \nat the bottom of the aluminium  residue. The change in temperature causes the melting furnace to crack based on the metallic \nstructure.  To keep the temperature from slow ing down to drop the temperature, the furnace is not destroy ed by \nthermoshocking, the energy is required to control and keep the expected temperature when the melting furnace is idle.  \nReferring to the mixed integer  linear program (MILP) for the garment lin e balancing, the optimization of linear \nprogramming is to find the optimized solution  from  Prof Dr Ray WM Kong [ 3], the number of melting cycle s in the melting \nprocess can be optimi zied to maximise the marginal profit.  \nBalancing in th e stage  of a single model finds a locally optimized solution in an iteration. The objective of the stage is to \nfind a solution(s) with a specified number of stations with a minimum cycle  time.  Solutions are considered locally optimized \nas the principal  objective is to  find a  solution which will define a smooth production by minimizing the objective function \nof melting workstation balance from Waldemar Grzechca [ 8]. The concept of ALBP, where the aim is to optimize the \nnumber of workstations with a predefined fixed cycl e time , is utilized in the formulation.  \nLinear  programming is adopted  to resolve the line balancing with constraints. The formula of linear programming is shown  \nin the following  standard format : \n𝑚𝑎𝑥 𝑋1,𝑋2,…,𝑋𝑛  (𝑧)= 𝑝1𝑥1+ …+ 𝑝𝑛𝑥𝑛  (1) \nsubject to   𝐴11𝑥1…+ 𝐴1𝑛𝑥𝑛 ≤ 𝑏1, \n ⋮  ⋮ \n  𝐴𝑚1𝑥1…+ 𝐴𝑚𝑛𝑥𝑛 ≤ 𝑏𝑚, \n 𝑥1,𝑥2,…,𝑥𝑛 ≥0 \nBy grouping the variables 𝑥1,𝑥2,…,𝑥𝑛 into a vector  x and constructing the following matrix and vectors from the problem \ndata, we can restate the standard form compactly as follows:  \n𝑚𝑎𝑥 𝑥  (𝑧)=𝑝′𝑥 \nsubject to 𝐴𝑥 ≤ 𝑏 ,   𝑥≥0 \n \nThe maximum  profit and cycle time is shown in the formula below : \n𝑃𝑟𝑜𝑓𝑖𝑡 𝑚𝑎𝑥 =max ( 𝑃𝑋𝑌−∑ 𝐼𝐶𝑥𝑥=𝑛\n𝑥=1 − ∑ 𝐼𝐶𝑦)𝑦=𝑛\n𝑦=1   (2) \n𝑀𝑎𝑥  (𝑃𝑟𝑜𝑓𝑖𝑡 )= max ( 𝑃𝑋𝑌−∑ 𝐼𝐶𝑥𝑥=𝑛\n𝑥=1 − ∑ 𝐼𝐶𝑦)𝑦=𝑛\n𝑦=1          ∀𝐶𝑇𝑥∈+𝑅 ，∀𝐶𝑇𝑦∈+𝑅  (3) \n𝑃𝑋𝑌= 𝑇𝐶𝑋𝑌∗𝑅𝑟 (4) \n𝑇𝐶𝑋𝑌=( 𝐶𝑇𝑥∗𝐸𝑓𝑓 𝑥%+𝐶𝑇𝑦∗𝐸𝑓𝑓 𝑦% )∗ 𝐸𝑓𝑓 𝑐𝑎𝑠𝑡% (5) \nWhere 𝑃𝑋𝑌 is the marginal profit of melting tank and casting  in USD,  \n 𝑇𝐶𝑋𝑌 is the output quantity of melting tank and casting in tons,  \n 𝐶𝑇𝑥 is the cycle time of melting  furnace  X per cycle in minutes,  \n 𝐶𝑇𝑦 is the cycle time of melting  furnace  Y per cycle in minutes,   International Journal of Mechanical and Industrial Technology        ISSN 2348 -7593 (Online)  \nVol. 12, Issue 2, pp: (73 -84), Month: October 2024 - March 2025, Available at: www.researchpublish.com  \n \nPage | 81  \nResearch Publish Journals   𝑅𝑟 is the marginal profit rate of melting tank and castin g in USD per ton,  \n𝐼𝐶𝑥 is the idle  cost of melting  furnace  X in minutes,  \n    𝐼𝐶𝑦 is the idle cost of melting  furnace  Y in minutes,  \n𝐸𝑓𝑓 𝑥% is the efficiency of output of melting  furnace  X after reduction of lose  \nduring manufacturing,  \n𝐸𝑓𝑓 𝑦% is the efficiency of output of melting  furnace  Y after reduction of lose  \nduring manufacturing,  \n𝐸𝑓𝑓 𝑐𝑎𝑠𝑡% is the efficiency of output of melting tank and casting after reduction  \nof lose during manufacturing,  \n \n∑ 𝐼𝐶𝑥𝑥=𝑛\n𝑥=1 = ∑(𝐶𝑎𝑝 𝑥− 𝐶𝑇𝑥𝑛\n1  𝑅𝑇𝑥) 𝐶𝑅 𝑥        ∀𝐶𝑇𝑥∈+𝑅 (6) \nwhere  𝐶𝑎𝑝 𝑥 is the daily available hours of melting  furnace  X in minutes,  \n    𝐶𝑇𝑥 is the cycle time of melting  furnace  X per cycle in minutes,   \n𝑅𝑇𝑥 is the number of time of melting  furnace  X in minutes per day,  \n𝐶𝑅 𝑥 is the idle cost rate of melting  furnace  X in USD per minute,  \n \n∑ 𝐼𝐶𝑦𝑦=𝑛\n𝑦=1 = ∑(𝐶𝑎𝑝 𝑦− 𝐶𝑇𝑦𝑛\n1  𝑅𝑇𝑦) 𝐶𝑅 𝑦      ∀𝐶𝑇𝑦∈+𝑅 (7) \nwhere  𝐶𝑎𝑝 𝑦 is the daily available hours of melting  furnace  Y in minutes,  \n    𝐶𝑇𝑦 is the cycle time of me lting furnace  Y per cycle in minutes,   \n𝑅𝑇𝑦 is the number of time of melting  furnace  Y in minutes per day,  \n𝐶𝑅 𝑦 is the idle cost rate of melting  furnace  Y in USD per minute,  \nsubject to  \n𝐶𝑇𝑥 𝑅𝑇𝑥≤  𝐶𝑎𝑝 𝑥  (8) \n𝐶𝑇𝑦 𝑅𝑇𝑦≤  𝐶𝑎𝑝 𝑦 (9) \nFirstly, the 𝑃𝑟𝑜𝑓𝑖𝑡 𝑚𝑎𝑥 is required to maximize the daily marginal profit as our goal. The  𝑃𝑋𝑌 is the profit of aluminium  rod \nper day,  which is 𝑃𝑋𝑌= 𝑇𝐶𝑋𝑌∗𝑅𝑟 , the 𝑇𝐶𝑋𝑌 total casting ouput per tank and casting equipment multiple the number of \ncycle in 𝑅𝑟 (number of cycle). The profit margin ( 𝑇𝐶𝑋𝑌) for each casting time means that the casting equipment can produce \nthe 36 rods each cycle, so the 𝑇𝐶𝑋𝑌 is USD800/ro d multiple 36 rods to be USD28,800 (USD800/rod)  per cycle of casting .  \nThe 𝑇𝐶𝑋𝑌 in the formula (5) is the discount the efficiency percentage in the casting facility and equipment,  𝐸𝑓𝑓 𝑐𝑎𝑠𝑡% . The \nmelting furnace x and the melting furnace x and f urnace  y have their  efficiency of output of melting furnace X after reduction \nof loss during the manufacturing process.  \nThe ∑ 𝐼𝐶𝑥𝑥=𝑛\n𝑥=1  is the sum idle labor cost of melting  furnace  x.  The melting  furnace  x is 1 set in the case study.  If the melting  \nfurnace  x is more than 1 set of melting  furnace s x to n sets , the total idle labor cost is summarized to n sets of melting  \nfurnace s x. The ∑(𝐶𝑎𝑝 𝑥− 𝐶𝑇𝑥𝑛\n1  𝑅𝑇𝑥) 𝐶𝑅 𝑥 means that the available of daily capacity , 𝐶𝑎𝑝 𝑥 in the melting  furnace  x \nminutes the utilized the metling furnace cycle time of melting furnace x  per cycle in minutes  𝐶𝑇𝑥  is multiplied by  the \nnumber of metling furnace x, 𝑅𝑇𝑥 and then the result  𝐶𝑎𝑝 𝑥− 𝐶𝑇𝑥𝑅𝑇𝑥 is utilizated total idle daily minutes to multiply  by \nthe idle cost 𝐶𝑅 𝑥 for the furnace x. The summarized the total idle cost for all furnace x in the  n sets. \nThe idle cost rate of melting furnace x,  𝐶𝑅 𝑥 is the standard idle labour  cost rate in USD per minute when it finds out the \nidle time in minutes to calculate the idle cost of furnace x.  International Journal of Mechanical and Industrial Technology        ISSN 2348 -7593 (Online)  \nVol. 12, Issue 2, pp: (73 -84), Month: October 2024 - March 2025, Available at: www.researchpublish.com  \n \nPage | 82  \nResearch Publish Journals  In the constraint, the total daily utilization time is the cycle time of furnace x to multiply  the number of time of melting \nfurnace x in minutes per day, 𝑅𝑇𝑥.  The daily total utilizatio n time is less than daily capacity of furnace, so it calls the \n𝐶𝑇𝑥 𝑅𝑇𝑥≤  𝐶𝑎𝑝 𝑥. \nThe∑ 𝐼𝐶𝑦𝑦=𝑛\n𝑦=1  is the sum idle labor cost of melting furnace y.  The melting furnace y is 1 set in the case study.  If the melting \nfurnace y is more than 1 set of melting furnaces y to n sets , the total idle labor cost is summarized to n sets of melting \nfurnaces  y. The ∑(𝐶𝑎𝑝 𝑦− 𝐶𝑇𝑦𝑛\n1  𝑅𝑇𝑦) 𝐶𝑅 𝑦  means that the available of daily capacity, 𝐶𝑎𝑝 𝑦 in the melting furnace y \nminutes the  utilized the me lting furnace cycle time of melting furnace y  per cycle in minutes 𝐶𝑇𝑦 is multiplied  by the \nnumber of metling furnace y, 𝑅𝑇𝑦 and then the result  𝐶𝑎𝑝 𝑦− 𝐶𝑇𝑦𝑅𝑇𝑦 is utilizated total idle daily minutes to multiply  by \nthe idle cost 𝐶𝑅 𝑦 for the furnace y. The summarized the total idle cost for all furnace y in the  n sets. \nThe idle cost rate of melting furnace y,  𝐶𝑅 𝑦 is the standard idle labour  cost rate in USD per minute when it finds out the \nidle time in minutes to calculate the idle cost of furnace y. \nIn the constraint, the total daily utilization time is the cycle time of furnace y multiplied by the number of times  of melting \nfurnace y in minutes per day, 𝑅𝑇𝑦.  The daily total utilization time is less than daily capacity of furnace, so it calls the  \n𝐶𝑇𝑦 𝑅𝑇𝑦≤  𝐶𝑎𝑝 𝑦. \nHence, the above formula from (1) to (9) has shown the detail of calculation for the linear progamming in the line balancing.  \nIn the case study, the increase in daily marginal profit is attributed to the enhanced output from melting and casting \nprocesses, as demonstrated by the following calculation:  \nThe increase in daily marginal profit is calculated as:  \nIncrement in daily marginal profit =  (New optimized daily cycles - Original daily cycles) *Output cost per cycle  per \nday - Additional operators' cost  \n \nSubstituting the values:  \n= (4.36 cycles  – 2 cycles) * USD28,800/cycle/day – (USD182/day)  \n= USD67,968/day  – USD182/day  \n= USD67,786/day  \n \nThe growth in marginal daily profit is calculated as:  \n = (USD125,386 – USD57,600) / USD57,600  \n =  117.6%  \nIn this optimization scenario, simple line balancing was achieved by increasing the labour  headcount from 8 to 12 workers \nacross two shifts per day, result ing in a 50% increase in labour  costs. The maximum profit margin per casting cycle of 36 \nrods exceeds USD28,800 (USD800 per rod). The additional profit margin from increasing production from 2 to 4.36 cycles \nper day amounts to USD67,968. After accounting f or the additional labour  cost of USD182 per day for the 4 extra workers, \nthe net profit margin is USD67,786 per day, reflecting a 117.6% growth in marginal daily profit.   \nVII.   CONCLUSION  \nLinear programming, particularly mixed integer linear programming (MILP), offers several benefits for line balancing in \naluminium  extrusion processes. Here are the key benefits and conclusions of applying linear programming to this research \nand development  of the aluminium  extrusion industry:  \n1. Optimization of Resources:  \n Linear programming helps in optimizing the allocation of resources, such as melting furnaces and casting facilities, \nto maximize output and minimize idle time. This ensures that resources are  used efficiently, reducing waste and \noperational costs.   International Journal of Mechanical and Industrial Technology        ISSN 2348 -7593 (Online)  \nVol. 12, Issue 2, pp: (73 -84), Month: October 2024 - March 2025, Available at: www.researchpublish.com  \n \nPage | 83  \nResearch Publish Journals  2. Maximization of Marginal Profit:  \n By optimizing the number of melting cycles and balancing the line, linear programming can help increase daily output \nrates  and greater utilization , thereby calculatin g the maximizing marginal profit. This is achieved by reducing idle \nlabour  costs and ensuring continuous operation of melting furnaces x and y. \n3. Reduction of Idle Time:  \n The approach minimizes idle time for melting furnaces  (furnaces x and y in n furnace) , which is crucial for maintaining \nthe necessary temperature to prevent aluminium  solidification and potential damage to the furnace  as a crash of furnace \nand melting tanks . This leads to more consistent production and less downtime.  \n4. Improved Production Flow:  \n Linear programming aids in achieving a smooth production flow by determining the optimal number of cycle times  \nof the melting process . This reduces bottlenecks and ensures a balanced workload across the production line.  \n5. Enhanced Decision -Making:  \n The use of linear programming provides a data -driven approach to decision -making, allowing aluminium  extrusion \nmanagers to evaluate different scenarios and choose the most efficient production strategy.  \n6. Flexibility and Scalability:  \n The methodology can be adapt ed to various production models, whether single -model, mixed -model, or multi -model \nlines, providing flexibility to accommodate changes in production demands or product types.  \nThe application of linear programming, particularly MILP, in aluminium  extrusion line balancing , offers significant \nadvantages in optimizing production processes. By focusing on maximizing marginal profit and minimizing idle costs, \nlinear programming ensures efficient resource utilization and enhances overall productivity. The approach  provides a \nstructured framework for balancing production lines, reducing idle time, and maintaining the operational integrity of melting  \nfurnaces. Ultimately, this leads to improved profitability, reduced operational risks, and a more agile production sys tem \ncapable of adapting to market demands and technological advancements.  In conclusion, effective line balancing in garment \nassembly operations is essential for resolving the issue of excessive work -in-process (WIP) inventory and improving the \nproduction output and efficiency, which often arises from an unbalanced production line. By systematically analyzing and \noptimizing the distribution of tasks among operators, organizations and factory planners can achieve a more synchronized \nworkflow that minimizes b ottlenecks and reduces idle time. This not only leads to a smoother production process but also \nsignificantly decreases the accumulation of WIP, thereby lowering storage costs and enhancing overall operational \nefficiency. Ultimately, implementing line -balancing techniques fosters a leaner manufacturing environment, improves \nresponsiveness to market demands, and contributes to higher levels of productivity and profitability in the garment industry.  \nThe research article on aluminium  extrusion can draw insights from cross -industry studies, highlighting the critical role of \nautomation and intelligent manufacturing in boosting productivity and output. The development of innovative technologies, \nsuch as the \"Design and Experimental Study  of Vacuum Suction Grabbing Technology to Grasp Fabric Piece\" by Prof. Dr. \nRay WM Kong et al. [9]  and in the K. M. Batoo (Ed.), Science and Technology: Developments and Applications, Innovative \nVacuum Suction -grabbing Technology for Garment Automation from  Prof. Dr. Ray WM Kong et al. [10] . Similarly, the \n\"Design of a New Pulling Gear for the Automated Pant Bottom Hem Sewing Machine\" by Prof. Dr. Ray WM Kong et al. \n[11] demonstrates how automation can enhance production rates in hem sewing machines. Looking  ahead, automation \npresents a significant opportunity to advance the aluminium  extrusion industry, paralleling its transformative impact on \ngarment and electronics manufacturing sectors.  ",
      "metadata": {
        "filename": "An Innovative Line Balancing for the Aluminium Melting Process.pdf",
        "hotspot_name": "Aluminium_Die_Casting",
        "title": "An Innovative Line Balancing for the Aluminium Melting Process",
        "published_date": "2025-03-29T14:31:33Z",
        "pdf_link": "http://arxiv.org/pdf/2504.02857v1",
        "query": "die casting aluminium radiator energy consumption reduction process optimization techniques"
      }
    },
    "Direct Reuse of Aluminium and Copper Current Collectors from Spent Lithium-ion B": {
      "full_text": " Direct Reuse of  Alumin ium and C opper  Current Collector s from Spent  \nLithium -ion Batteries   \nPengcheng Zhu a, c, Elizabeth H.  Driscoll b, c, Bo Dong a, c, Roberto Sommerville b, c, \nAnton Zorin b, c, Peter R. Slater a, c, Emma Kendrick b, c * \na School of Chemistry, The University of Birmingham, Birmingham, B15 2TT, United \nKindom  \nb School of Metallurgy and Materials, The University of Birmingham, Elms Rd, \nBirmingham B15 2SE, United Kingdom  \nc The Faraday Institution, Quad One, Harwell Campus, Didcot OX11 0RA, United \nKingdom  \n \nDr. P. Zhu  \nEmail: p.zhu@bham.ac.uk   \norcid.org/0000 -0002 -0197 -7054  \n \nProf. P. R. Slater  \nEmail: P.R.SLATER@bham.ac.uk  \norcid.org/0000 -0002 -6280 -7673  \n \n*Prof. E. Kendrick  \nEmail: E.Kendrick@bham.ac.uk   \norcid.org/0000 -0002 -4219 -964X  \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n Abstract  \nThe ever-increasing  number of  spent lithium -ion batter ies (LIBs) has presented a \nserious waste -management challenge . Aluminium  and copper  current collectors  are \nimportant components in LIBs  and take up a weight percentage of more than 15% . \nDirect reuse of current collectors can  effectively  reduce LIB waste  and provide an \nalternative renewable source of aluminium and copper . Besides, it also prevent s long \nmanufacturing processes and associated energy input and material consumption . \nHowever , there is a lack of  work on the direct reuse of current collectors in  the literature. \nHere in, alumin ium and copper  current collectors are reclaimed from commercial spent \nLIBs with different chemical treatments  and successfully reused  for LiNi 0.6Mn0.2Co0.2O2 \ncathodes and graphite anodes , respectively . The reclaimed current collectors  treated \nwith different processes show different surface compositions and  morphology  to \npristine ones , resulting in distinctive  wettability , adhesion and electrical conductivity. \nThe reused current collectors  show similar electrochemical performance to the pristine \none at low C rates, while extra caution should be taken at  high C rates  for aluminium \ncurrent collectors due to relatively low contact conductivity. This work provides \nsubstantial evidence that the direct reuse of aluminium and copper current collectors \nis possible and highlights the importance of the surface morphology of current \ncollectors .   \n \n \nKeywords: Lithium -ion battery, current collector, direct reuse, surface morphology , \nrecycling  \n \n \n  Introduction  \nSince the first commercial Lithium -ion battery (LIB) was released by Sony in 1991, the \npast three decades have witnessed an explosive growth of LIBs .1 The development of \nLIBs provides a green technology for energy storage, which  can help to address the \nglobal climate crisis  and build  a more sustainable society .2-3 Nevertheless, LIBs \ncontain many kinds of hazardous a nd valuable metals ,4 presenting a growing  waste -\nmanagement challenge when reaching  the end of their lives ,5-6 which is  approximately  \n10 years for commercial LIBs in electric vehicles ( EVs).7 The volume of spent LIBs \nwas around 2 60,000  tonnes in 201 9 and is predicted to increase to 1.4 million  tonnes \nin 20 30.8-9 It was reported that only 120,000 tonnes of spent LIBs were recycled in \n2019 ,10 lower than the EU target of 70% recycling rate .11 Therefore, how to deal with \never-increasing LIB waste has become an urgent  issue for our society.  \nRecycl ing spent LIBs has been regarded as a feasible and efficient strategy to address \nthe challenge of LIB waste .12 Most recycling studies have been focusing on electrode \nactive materials ,13-16 while less attention has been given  to inactive components, for \nexample , current collectors, separators, electrolyte s, and cases.  Current collectors are \nnormally aluminium  and copper foils and take up more than 15%  of the weight of \nLIBs.17 Recycling current collectors can effectively reduce  LIB waste and provide a \nlarge secondary source for Al and Cu.  In LIB recycling processes which use \nhydrometallurgy  processes  to recycle the active material, current collectors are \nnormally isolated from the black mass during the pre -treatment processes, and are \nreclaimed by comminution and sieving to produce Al and Cu concentrates which can \nenter Cu and Al refining processes. P yrometallurgical recycling processes will \nconsume Al as a reducing agent, and will produce a matte of Cu along with Ni and Co, \nwhich will require a further separation step involving  hydrometallurgically  processes .13, \n18-19 The obtained Al  and Cu need further melting, casting, and rolling  before reusing , \nwhich requires not only  high energy input and labour  cost but also environment al costs , \nsuch as CO 2 emission s, freshwater ecotoxicity  and marine eutrophication .20 \nFurtherm ore, Al and Cu may contaminate subsequent  waste stream s and lower the \nrecovery rate of other  valuable  metals , such as Co and Li . Complete removal of Al and \nCu is therefore  beneficial for improving subsequent recovery efficiency  with fewer  \npurification processes .21-22 With these  issues  in mind, d irect reus e of current col lectors \nprovides a n even better option than conventional recycling, which can not only solve LIB waste problems but also skip various recycling processes, retaining the embedded \nenvironmental and economic benefits . Some publications have proposed  the \npossibility of direct reuse of current collectors  in the literature ,23-24 however, there is a \nlack of experimental  work on the direct reuse of current collectors  to date .  \nCurrent collectors can strongly impact electrode performance although  they do not \nparticipate in reactions during cell cycling .17 Al and Cu current collectors transport  \nelectrons generated at electrodes to power external circuits and the connection \nbetween current collectors and electrodes is crucial for maintaining good electrical \ncontact. The surface composition and morphology of current collectors can be \nchanged by co rrosion after long LIB cycling ,25-26 or during the reclamation process, \nparticularly when alkaline or acidic solutions are used .27-28 The change in the current \ncollector surface can influence the contact between the current collector and \nelectrodes, which in turn affect s electrode properties and performance .17, 29 Therefore, \nit is necessary to understand the effects of current collector surface conditions when \nreusing current collect ors. \nHerein, Al and Cu  current collectors are reclaimed  from commercial  spent LIB cells  \nand directly  reused with LiNi 0.6Mn0.2Co0.2O2 (NMC622)  cathode s and graphite anode s, \nrespectively . Delamination mechanisms of the used Al and Cu current collectors with \ndifferent treatments and the corresponding surface composition and morphology are \ninvestigated . The effects of Al and Cu surface morphology on the wetting of electrode \nslurries , adhesion strength and electrical conductivity are elucidated. Furthermore, \nreclaimed Al and Cu current collectors are tested in LIB half cells and compared with \npristine Al and Cu current collectors  to verify  the feasibility of direct ly reusing  the \ncurrent collectors. We believe that this work can  provide  substantial  evidence for the \ndirect reuse of current collectors and also an insight into the effect of the surface \nmorphology of current collectors on LIB performance.  \nMethods  \nCell disassembly  \nEnd-of-life (EoL) LIB cells were automotive pouch cells from 1st generation Nissan \nLeaf. The cathode is a mixture of 75% lithium manganese oxide spinel (LMO) and 25% \nNickel Cobalt Aluminium oxide (NCA)  on an Al current collector , while the anode is \ngraphite on a Cu current collector. The cells were firstly discharged to 2.5 V before dismantling. The discharged EoL cells were then transferred to a n Argon -filled \nglovebox and  manually  opened using a ceramic scalpel . The components were then \nseparated to obtain cathode coatings on Al foils and anode coatings  on Cu foils. The \ncathode and anode coatings  were washed with anhydrous Dimethyl carbonate (DMC)  \nand dried  before subsequent treatments . A schematic illustration of cell disassembly \nis shown in Fig. 1a. More detailed informati on was reported in.30 \nCurrent collector  reclamation  \nThe delamination  of cathodic/anodic coatings from Al/Cu foils follows different \nprocedures , as illustrated in Fig. 1b . Two different  routes were investigated . Cathode \nsheets were immersed in N-methyl -2-pyrrolidone  (NMP ) solution at 60 ˚C overnight  to \ndissolve polyvinylidene fluoride (PVDF)  binder . The cathode coating was then washed \nusing  an eraser sponge  until no obvious black coating can be seen on the surface . \nThe obtained Al foil is named  ‘washed Al ’. Alternatively , cathode sheets were \nimmersed in 0.5  M oxalic acid  at 50 ˚C  under sonication ( 40 Hz, 50 W ) for 5 mins.  The \ncathode coatings separated  from Al foils  during the process  since the Al surface is \netched by oxalic acid . The obtained Al foil is named ‘etched Al ’. As for anode s, anode \nsheets were firstly soaked  in distilled water. Graphite coa tings quickly separated  from \nCu foils  in a few seconds . Then, the obtained Cu foils were further washed by 3 M HCl  \nat 30 ˚C  for 5 mins to remove surface ox ides (washed Cu) . The HCl -washed Cu foils \nwere then transferred to  2 M HNO 3 at 30 ˚C for 30 mins to  etch the Cu to  get rough \nsurface morphology  (etched Cu) . All foils were rinsed with distilled water and dried at \n60˚C before use.   \nElectrode making  \nNMC622 cathodes and graphite anodes  were coated on  the reclaimed  Al and Cu \ncurrent collectors, respectively . The cathode contains 96 wt% NMC622 (Targ ray), 2 \nwt% PVDF (Solvay) and 2 wt% C 65 (Imerys) . The anode contains 95.25 wt% artificial \ngraphite (S360 E3 Artificial Graphite, BTR), 1.5 wt% CMC ( BVH8, Ashland ), 2.25 wt % \nSBR ( BM451 -B, Zeon ), and 1 wt% C45 ( Imerys ). The mixing procedures are detailed \nin the supplementary information. The mixed slurries were then coated on Al and Cu \ncurrent collectors with an areal capacity of around 2 mAh/cm2. NMC622 and graphite \ncoatings  were initially dried at 80 and 50 ˚C , respectively,  to remove most solvents  \nfollowed by overnight  drying in a vac uum oven at 120 ˚C . The dried electrodes were calend ered to porosities of around 40% for electrochemical tests. Pristine Al and Cu \ncurrent co llectors (X iamen  TMAX) were also used for comparison.  \nContact angle measurement  \nThe contact angle of electrode slurries on Al and Cu current collectors was measured \nby a c ontact angle goniometer  (Ossila). Current collectors were cut into a rectangular \npiece of 20 x 60 mm2 and placed on a glass slide. A volume of around 40 µL of the \nslurry was dropped on the current collectors using a pipette. The contact angle was \nmeasured 30 s after the droplet was dropped on the current collector.  \nAdhesion  testing  \nThe a dhesion force between current collectors  and electrode coating  was measured \nby a 180 -degree peel method  using a modified Netzsch Kinexus Pro+ Rheometer . A \n25 mm -wide piece of double -sided tape was attached to a section of the top surface \nof the electrode coating , with the free end s of the coating and the tape attached to the \nupper and lower rheometer geometry, respectively . The two rheometer geometries \nthen vertically move up and down, respectively, to enact a 180 -degree peel  at a speed \nof 10 mm/s  with axial force measurement from the rheometer.  The force was recorded \nin a region where the axial force measurement was stable and peeling was visually \nconsistent.  The experimental setup can be seen in the inserts in Fig. 5. The adhesion \nforce was deter mined by taking the average value of five samples for each current \ncollector.  \nElectr ical conductivity measurement  \nThe electrical conductivity of NMC622/graphite electrodes on Al/Cu current collectors \nwas measured by a four-point probe (Ossila). All sample s were cut in a cylindrical \nshape  with a constant diameter of 14.8 mm . The samples were placed with the \nelectrode coating upwards on a glass slide for testing . The probe spacing is 1.27 mm. \nThe maximum applied voltage was 5V and the maximum current was set to be 1mA \nfor NMC622 on Al and 100 mA for graphite on Cu . This was repeated on  five samples \nfor each current collector  to take an average . \nElectrochemical testing 2032 type coin cell s were constructed for electrochemical te stes, with electrodes on \nthe re used or pristine current collectors (14.8 mm in diameter), tri-layer 2025 separator \n(Celgard, 16 mm in diameter), Li -metal disc (15 mm in diameter) and 70 µL of 1 M \nLiPF 6 in EC: DMC 3:7 v/v + VC 1%wt (PuriEL) . After the assembly of coin cells, a \nformation step composed of two charge –discharge cycles at a C -rate of 0.05 C \n(1C=175 mAh/g for NMC622 , 1C=350 mAh/g for graphite)  was conducted within a \nvoltage window of 2. 75 to 4. 3 V vs. Li/Li+ for NMC622  half-cells and 0 .005 to 1.5 V vs. \nLi/Li+ for graphite  half-cells, using a BCS -805 Biologic battery cycler (Biologic, France). \nThe NMC622 half cells were then charged  and discharged  at different C rates of 0.1  \nto 5 for 5 cycles at each C rate. The graphite  half cells were discharged and charged \nat different C rates from 0.1 to 10 for 5 cycles at each C rate . The capacity fade was \ntested for 50 cycles.  \n \nFig. 1 a) reclamation of Al and Cu current collectors from end -of-life lithium -ion battery \ncells; b) different treatment conditions for current collectors  \n \n3. Result  and discussion  \n3.1. Surface composition and morphology  \nThe mechanism of the delamination of  the electrodes from current collectors  differs \nunder different treatments , as illustrated in Fig. 1b. For Al, NMP can dissolve PVDF \nbinder but does not react with  Al current collectors .23, 31 NMP soaking can therefore \neffectively remove the PVDF binder, and subsequent scrubbing can therefore easily \nremove the cathode from the  surface of  Al current collectors.  Oxalic acid  can react  \nwith Al foil  and generate s hydrogen .21 The hydrogen bubbles generated at the \nAl/cathode  interface can exert a force  to separate the Al current collector  and the \ncathode .32 The dissolution of Al in oxalic acid is evidenced by the ICP -OES  test in \nTable S1. The delamination of the anode  from Cu current collectors mainly relies on \nlithium leaching and hydrogen bubbles generated at the Cu/graphite interface, \nexerting a force to separate the Cu current collector and the anode. The water solution \nbecomes very basic after lithium leach ing and can oxidise the Cu current collector, as \nevidenced by a trace amount of Cu in the solution  in Table S2 . Dilute HCl then removes  \nthe surface oxides but does n ot dissolve the copper foil .33 Further etching with 2M \nHNO 3 roughens  the surface of the Cu current collector .34 \nFig. 2 a and c show survey -scan XPS spectra of Al and Cu current collectors. All of \nthe three Al current collectors have very similar compositions on the surface except \nthe washed and etched Al have more fluorine ( ∼686 eV)  on their surfaces . A zoom -in \nXPS spectr um for F 1s is shown in Fig. S 1.  The appearance of fluorine  can be \nattributed to incomplete  removal of binder  or a thin film of AlF 3 formed on the Al surface \nduring LIB cycling .35 After oxalic acid etching , the peak intensity of fluorine  was \nreduced, indicating a reduced quantity . Fig. 2b further displays a high -resolution XPS \nspectrum for Al 2p. T wo Al  2p peaks at 72.9 and 7 5.2 eV are assigned to Al and Al \noxide, respectively .36 The pristine Al has a thick surface oxide layer which is formed \nduring production processes, while the etched Al has a thinner Al oxide layer due to \nthe action of the  oxalic acid  and a thin layer of Al oxid e is formed . The washed Al has  \na much weak er signal of Al oxide and Al  than the pristine and etched Al , which is \nindirect evidence that the surface is covered by some residuals . As for Cu, all three \nCu current collectors contain very similar elements,  mainly  including  Cu, O and C , as \nillustrated in Fig. 2c. Fig. 2d further demonstrates that the washed and etched Cu has \na much thinner oxide layer than the pristine Cu because HCl washing can easily \nremove the oxide layer and a thin oxide layer is subsequently formed when exposed \nin air.   \nFig. 2 Survey -scan  XPS spectra for a) Al current collectors and c) Cu current collectors, \nhigh-resolution XPS spectra for b) Al 2p and d) O 1s.  \nFig. 3 shows the surface morphology of  the Al and C u current collectors under different \ntreatments. To facilitate comparison, p ristine Al and Cu current collectors are also \nshown  in Fig. 3a and d, respectively . For Al current collector s, the washed Al  shows  \nsome features  including craters, and rolling or calend ering trace s on the surface,  \ngiving a slightly rough er surface than the pristine Al .37 Given that NMP  can only \ndissolve PVDF and  does not react with Al, the washed Al is expected to  maintain its \noriginal surface morphology  after treatment . Etched  Al shows a much rougher surface \nthan the washed  Al and pristine Al. T he surfa ce of the etched Al is full of etched pits \nand the rolling trace is  almost invisible in Fig. 3c due to the reaction between oxalic \nacid and Al . The size of the etched pits is up to  7 microns  in diameter  (Fig. S 2a). As \nfor Cu current collector s, the washed Cu shows a relatively flat and feature -free \nsurface except  for some corrosion pits  and cracks (Fig. 3e) as a result of  oxidative \ndissolution  during cycling and/or over-discharging prior to cell disassembly .38 The size \nof the corrosion pits on  the washed Cu is  at a submicron scale  (Fig. S 2b). Etched Cu  \nshows the roughest surface among all Cu current collectors, with numerous etched \ngrooves  formed during HNO 3 soaking, as shown in Fig. 3f. The width  of the etched \ngrooves is below  1 µm  (Fig. S 2c), much smaller than the etched features on the Al \ncurrent collector . It is worth mentioning that the corrosion pits and cracks on  the \nwashed Cu  (Fig. 2e)  facilitate further HNO 3 etching . A reference group  that a pristine \nCu with a flat surface cannot achieve the same surface roughness even with the same \nHNO 3 treatment ( Fig. S2d).  \n \nFig. 3 SEM micrographs of Al and Cu current collectors. a) pristine Al, b) washed Al, \nc) etched Al, d) pristine Cu, e) washed Cu and f) etched Cu (scale bar: 10 µm).  \n3.2. Wettability  \nThe wettability can be significantly affected by surface morphology .39-40 Fig. 4 plots  the \ncontact angle of NMC622  and graphite electrode slurr ies on Al  and Cu current \ncollectors.  The contact angle is an indicator of wettability,  and a lower contact angle  \nindicates  better wettability. The contact angle s of NMC622 slurry on the pristine, \nwashed, etched Al current collectors are 47.38, 44.02, 39.66  degrees , respectively, \nsuggesting that the etched Al exhibits the best wettability,  and the pristine Al exhibits \nthe worst wettability. The wettability increases with increasing surface  roughness. This \nis because the surface features on the washed and etched  Al current collectors are  \nseveral micron s-size, which are big enough to allow NMC622 slurry to enter , giving \nrise to an enhanced contact area and therefore better wettability  (as evi denced by the \ncross -section view in Fig. S3i).40 On the contrary,  the reverse is true for the Cu current \ncollectors, where  the etched Cu current collector shows the largest contact angle of \n102.4 degrees, followed by the washed Cu (87.04 d egrees) and pristine Cu (73.94 \ndegrees), revealing that the pristine Cu is the best and the etched Cu is the worst in \nterms of wettability. In this case, the holes and grooves on the washed and etched Cu \nare at a submicron scale , which can easily trap air and prevent the graphite  slurry from \nentering the surface features. The surface of the washed and etched Cu actually \nbecomes a Cu -air hydrophobic composite surface and therefore leads to worse \nwettability .39 It should be noted that different electrode ink solution systems for making \nNMC622 and graphite slurries are also a reason for the different wettab ility \nperformance of Al and Cu current collectors.  \n \nFig. 4 Wettability of Al and Cu current collectors under different treatments. Contact \nangle of NMC622 slurry on pristine, washed and etched Al (top), graphite slurry on \npristine, washed and etched Cu (b ottom).  \n3.3. Adhesion strength  \nFig. 5 display s the adhesion force at the interface s of Al/NMC622 and Cu/graphite \nmeasured by the 180-degree  peel off test.  The electrodes  are completely peeled off \nfrom the current collectors after the test, as demonstrated in the inserts in Fig. 5. For \nAl current collectors, the etched Al shows the highest adhesion force of 79.57 N/m, \nabout twice as much as the washed Al ( 41.71 N/m) and more than five times higher \nthan the pristine Al ( 15.26 N/m). It is interesting to see that the adhesion force \ndecreases linearly with increasing contact angle for Al (also shown in Table S3), \nsuggesting a strong correlation between wettability and adhesion. The adhesion force \nbetween Al current collectors and NMC622  electrodes is directly related to the binder  \ndistribution .41 Fig. S3 shows the image and elemental map of the cross -section of \nNMC622 on Al current collectors. The distribution of fluorine represents the distribution \nof PVDF binder. The pristine Al shows an obvious binder distribution  gradient , with \nmore PVDF located at the top of the NMC622 electrode , which  decreases on moving  \nclose r to Al. At the Al/NMC622  interface, the quantity of fluorine is very low,  indicating \nlittle binder exists at the interface and therefore leads to low adhesion. The low content \nof PVDF binder at the interfaces between the pristine Al and NMC622 electrode can \nbe directly observed from the cross -section image in Fig. S3a. Though t he washed Al \nshows a  slightly more uniform PVDF distribution than the pristine Al, a distribution \ngradient is still visible , with less PVDF at the washed Al/NMC622 interface . The \nnonuniform binder distribution is a common issue with electrode manufacturing  that \nresults from binder migration during electrode drying and has been systematically \nreported in a recent review .42 By contrast, the etched Al shows a more uniform \ndistribution of PVDF throughout  the cross -section, with a high PVDF content even at \nthe etched Al/NMC622  interface . The uniform PVDF distribution on the etched Al could \nbe ascribed to the rough Al surface wh ich is easily wetted by NMC622 slurry and traps \nmore PVDF binder at the interface, significantly alleviating binder migration and \nresulting in higher adhesion strength . It can be directly observed in Fig. S3i that  \nsubstantial amount of PVDF binder  is locat ed at the etched Al/NMC622 interface. It is \nalso worth mentioning that carbon and fluorine show almost the same distribution in \nFig. S3, suggesting a  high affinity of the carbon black to the PVDF to easily clump \ntogether to form a well -known carbon binder domain (CBD)  [7]. \nUnlike the Al current collectors, the three Cu current collectors show very similar \nadhesion forces , with the etched Cu contributing to  slightly high adhesion forces of \n2.67 N/m, followed by  the washed Cu ( 2.47 N/m) and the pristine Cu ( 2.26 N/m). The \nadhesion force between Cu current collectors and graphite electrodes can also be \nunderstood by the binder distribution. Fig. S4 show the  image and elemental map of \nthe cross -section of graphite electrode s on Cu current collectors . The distri bution of \nsodium  is an indicator of the distribution of  CMC.  For all Cu current collectors, the \nCMC distributes evenly within the graphite electrode but with little at the Cu/graphite \ninterface s, as demonstrated  in Fig. S4 d, h, l. The more uniform distribution of the CMC than PVDF might  be ascribed to the lower initial drying temperature  (50 ˚C for the CMC \nand 80 ˚C for the PVDF ). The low binder content at the interface s mainly results from  \nthe poor wettability  of the aqueous slurry on the Cu surface . In this respect, it may be \nof future interest to introduce s urfactants , which  can be used to decrease the surface \ntension of electrode slurr ies and enhance wettability .43 Apart from the poor wettability , \nthe shape of graphite flakes also influences the stacking and may affect the binder \ndistribution  and adhesion.  \nThe magnitude of the adhesion forces between Al/NMC622  and Cu /graphite  is \nsignificantly different. Comparing Fig. 5a and b, the adhesion at the Al/NMC622 \ninterface is one order of magnitude  higher than that at the Cu/graphite interface . It is \nsuspect ed the difference in adhesion force comes from  the different binders being \nused. To facilitate a direct comparison of the adhesion strength of PVDF and \nCMC/SBR, the  CMC/SBR  was replaced  with the same weigh t percentage of PVDF \n(3.75% ) to make the graphite slurry and was coated on to the same  Cu current \ncollectors . By using PVDF binder , the adhesion forces  increase to 20.34 , 69.82  and \n101.06  N/m for the pristine, washed and etched Cu  (Fig. S 5), respectively, about the \nsame level as the Al current collectors, thus indicating that PVDF is a much stronger  \nbinder  than CMC/SBR  in terms of adhesion . \n \nFig. 5 Adhesion strength between electrodes and current collectors. Peel off force \nbetween a) Al current collectors and NMC622 electrode and b) Cu current collectors \nand graphite electrode.  \n \n3.4. Electrical conductivity  \nFig. 6 plots the electrical conductivity of  NMC622 electrodes on Al current collectors \nand graphite electrodes  on Cu current collectors measured by a four-point probe test. \nThe electrical conductivities of NMC622 electrodes on the pristine, washed, etched Al \ncurrent collectors are 49.07, 20.98, 18.37 S/m, respectively . The electrical conductivity \ndecreases with increasing Al surface roughness, which is opposite to the adhesio n \nstrength. This is likely  due to Al current collectors with a rougher surface trap ping more \nPVDF binder at the Al/NMC622 interface and block ing the electrical conduction \npathway ,44 reducing  the contact conductivity. Thus, the rougher the Al surface, the \nlower the electrical conductivity.  The electrical conductivit ies of  graphite electrodes on  \nthe pristine, washed , etched  Cu current collectors are 1.18, 1.12, 1.20 x 106 S/m, \nrespectively.  The effect of the different treatments of the Cu current collectors on \nelectrical conductivity is therefore negligible . As discussed in section 3. 3, Cu surface \nmorphology does not affect the distribution s of CMC binder and carbon very much . \nThus, the electrical conductivities of graphite anodes on three different Cu current \ncollectors  are very similar . \nThe electrical conductivity of NMC622  electrodes on Al current collectors  is five orders  \nof magnitude lower than that of graphite  electrodes on Cu current collectors . The main \nreason is that graphite  is an electrical ly conductive material but NMC622 has very poor \nelectrical conductivity .45-46 In addition,  as noted above, the binder can also affect \nelectrical conductivity. Fig. S 4 shows that the electrical conductivities of graphite \nelectrode s made with 3.75% PVDF and coated on the pristine, washed, etched Cu \ncurrent collectors are 2.70, 1.89, 2.31 x 105 S/m, respectively . By replaci ng CMC/SBR \nwith the same amount of PVDF, the electrical conductivity of graphite electrodes  on \nCu current collectors can be reduced by about five times, indicating that CMC/SBR is \nbetter than PVDF in terms  of maximising electrode  electrical conductivity .   \nFig. 6 Four -point probe test for electrodes on current collectors. Electrical conductivity \nof a) NMC622 electrodes on Al current collectors, b) graphite electrodes on Cu current \ncollectors.  \n3.5. Electrochemical test ing \nReclaimed  Al and Cu current collectors were evaluated  with NMC622 and graphite \nelectrodes in LIB half cells, respectively, and compared with the pristine current \ncollectors  to verify the feasibility of direct reusing  current collectors . Fig. 7a shows the \nrate c apab ility of NMC622 electrodes on Al current collectors.  At 0.1C, NMC622 \nelectrodes on the pristine, washed and etched Al current collectors deliver similar \ncapacities of around 170 mAh/g which is very close to the theoretical value of 175 \nmAh/g, indicating ne arly full utilisation of the active material .47-48 In the C rate range \nfrom 0.1 to 0.5C,  the capacities of NMC622 electrodes on all Al current collectors are \nsimilar . As the C rate further increases to 1, 2 and 5C,  NMC622  on the pristine Al \ndelivers  higher capacities  than the counterpart  on the washed and etched Al. At 5C, \nNMC622 on the pristine Al delivers a capacity in the range of 80 -100 mAh/g, w hile \nNMC622  on the washed and etched Al deliver s low specific capacities, le ss than 15 \nmAh/g . This is attributed to  the NMC622 electrode on the pristine Al ha ving the highest \nelectrical conductivity. As the C rate increases  from 0.1  to 1C and even higher, the \ncurrent density for charging and discharging accordingly increases  by ten times and \neven higher, thus the electrical conductivity becomes a limiting factor. A higher \nelectrical conductivity contributes to a lower voltage drop and therefore a higher \ncapacity. Fig. 7b shows that  the NMC622  electrodes  on all the pristine, w ashed and \netched Al current collectors have stable  cycling over 100 cycles at 0.2C , indicating that \nthe reused Al current collectors have good stability.   \nFor the  Cu current collectors, the graphite electrodes on the pristine, washed, etched \nCu current collectors deliver similar capacities at a wide range of C rate s from 0.1 to \n10C, as shown in Fig. 7c. The highest capacity achieve d is about 350 mAh/g at 0.1C.  \nConsidering that the electrical conductivity of graphite electrodes on Cu current \ncollectors is  five orders of magnitude higher than that of NMC622 electrodes on Al \ncurrent collectors, the electrical conductivity should not be a limiting factor  for graphite \nelectrodes  even at high C rates.  Fig. 7d further displays that the capacity of the \ngraphite e lectrodes on different Cu current collectors is stable after 100 cycles at 0.2C. \nTherefore, both the washed and etched Cu can be directly reused and possibly  replace \npristine Cu current collectors . \n \nFig. 7 Electrochemical performance Al and Cu current collectors in half cells. Rate \ncapability of a) NMC622 on Al current collectors and c) graphite on Cu current \ncollectors; cyclability of b) NMC622 on Al current collectors at 0.2C and d) graphite on \nCu current collectors at 1  \n4. Conclusion s  \nAl and Cu current collectors reclaimed from spent commercial LIBs have been \nsuccessfully directly reused with some simple treatments , including NMP washing  and \noxalic acid etching for Al, water soaking with subsequent HCl washing and optional \nHNO 3 etching for Cu. As summarised in Table S3, the surface composition  of current \ncollector s change s slightly but the surface morphology varies distinctively with diff erent \ntreatments. The roughness of the surface morphology of both Al and Cu follows the \nsame order , etched > washed > pristine. The reused Al with higher surface roughness \nresults in better wetting of NMC622 slurry, enhanced adhesion at the Al/NMC622 \ninterface, more uniform PVDF binder distribution, but reduced electrical co nductiv ity. \nIn contrast to  Al, the effects of Cu surface morphology on the adhesion at Cu/graphite \ninterface and electrical conductivity , CMC binder distribution are not pronounced . The \nreused Al current collectors deliver very similar capacities at 0.1 – 1C but lower \ncapacities at higher C rates when compared with the pristine Al, while the reused Cu \ncurrent collectors exhibit  almost the same capacities as the pristine Cu at a wide C \nrate range from 0.1 -10C. This work details substantial evidence of direct reusing of \ncurrent collectors, providing an alternative renewable source of current collectors and \npreventing traditional long manufacturing processes and associated energy input  and \nmaterial consumption for new current collectors . \nAlthough the feasibility of reusing Al and Cu current collectors has been verified and \nthe effect of surface morphology has been investigated in this work, future effort is still \nnecessary, particularly for tw o areas below: 1) Conflict between interfacial adhesion \nand contact conductivity. Considering that polymeric binders are normally non -\nconductive, high er adhesion strength requires  more binders at the curre nt \ncollector/electrode interface, which inevitably reduces the contact conductivity. \nDeveloping new binders with improved adhesive performance or with good electrical \nconductivity is therefore a key target . 2) Scaling up for application in industry . Current \nwork was conducted at a lab scale, the scalability needs to be further demonstrated. \nFurthermore , it is easy to cause wrinkles on current collectors during  the reclamation \nprocesses , and so strategies  to maintain flatness while treating a large qua ntity of \ncurrent collector s will be a cha llenge.  Conflict of Interest  \nThe authors declare no conflict of interest.  \nAcknowledgements  \nThis work is supported by the Faraday Institution -funded Nextrode (FIRG015) . \nCATMAT (FIRG016)  and R eLIB (FIRG2 7) projects.  The x -ray photoelectron (XPS) \ndata collection was performed at the EPSRC National Facility for XPS (“HarwellXPS”), \noperated by Cardiff University and UCL, under Contract No. PR16195.  \n \nReference  \n(1) Li, M.; Lu, J.; Chen, Z.; Amine, K. 30 years of lithium ‐ion batteries. Adv. Mater. 2018,  30 (33), \n1800561.  \n(2) Goodenough, J. B. Electrochemical energy storage in a sustainable modern society. Energy Environ. \nSci. 2014,  7 (1), 14 -18. \n(3) Tollefson, J. Can the world kick its fossil -fuel addiction fast enough. Nature 2018,  556 (7702), 422 -\n425.  \n(4) Yang, Y.; Okonkwo, E. G.; Huang, G.; Xu, S.; Sun, W.; He, Y. On the sustainability of lithium ion \nbattery industry –A review and perspective. Energy Storage Mater. 2021,  36, 186 -212.  \n(5) Mrozik, W.; Rajaeifar, M. A.; Heidrich, O.; Christensen, P. Environmental impacts, pollution sources \nand pathways of spent lithium -ion batteries. Energy Environ. Sci. 2021,  14 (12), 6099 -6121.  \n(6) Winslow, K. M.;  Laux, S. J.; Townsend, T. G. A review on the growing concern and potential \nmanagement strategies of waste lithium -ion batteries. Resources, Conservation and Recycling 2018,  \n129, 263 -277.  \n(7) Armand, M.; Axmann, P.; Bresser, D.; Copley, M.; Edström, K.; Ek berg, C.; Guyomard, D.; Lestriez, \nB.; Novák, P.; Petranikova, M. Lithium -ion batteries –Current state of the art and anticipated \ndevelopments. J. Power Sources 2020,  479, 228708.  \n(8) Melin, H. E.; Rajaeifar, M. A.; Ku, A. Y.; Kendall, A.; Harper, G.; Heidri ch, O. Global implications of \nthe EU battery regulation. Science 2021,  373 (6553), 384 -387.  \n(9) Melin, H. The Lithium -Ion Battery Life Cycle Report. Circular Energy Storage: London, UK 2021 . \n(10) Melin, H. E. State -of-the-art in reuse and recycling of lithium -ion batteries –A research review. \nCircular Energy Storage 2019,  1, 1-57. \n(11) HALLEUX, V. New EU regulatory framework for batteries: Setting sustainability requirements. \n2021 . \n(12) Harper,  G.; Sommerville, R.; Kendrick, E.; Driscoll, L.; Slater, P.; Stolkin, R.; Walton, A.; Christensen, \nP.; Heidrich, O.; Lambert, S. Recycling lithium -ion batteries from electric vehicles. nature 2019,  575 \n(7781), 75 -86. \n(13) Sommerville, R.; Zhu, P.; Rajaeif ar, M. A.; Heidrich, O.; Goodship, V.; Kendrick, E. A qualitative \nassessment of lithium ion battery recycling processes. Resources, Conservation and Recycling 2021,  \n165, 105219.  \n(14) Ciez, R. E.; Whitacre, J. Examining different recycling processes for lit hium -ion batteries. Nature \nSustainability 2019,  2 (2), 148 -156.  \n(15) Mohr,  M.; Peters, J. F.; Baumann, M.; Weil, M. Toward a cell ‐chemistry specific life cycle \nassessment of lithium ‐ion battery recycling processes. Journal of Industrial Ecology 2020,  24 (6), 1310 -\n1322.  \n(16) Natarajan, S.; Aravindan, V. Recycling strategies for spent Li -ion battery mixed cathodes. ACS \nEnergy Letters 2018,  3 (9), 2101 -2103.  \n(17) Zhu, P.; Gastol, D.; Marshall, J.; Sommerville, R.; Goodship, V.; Kendrick, E. A review of current  \ncollectors for lithium -ion batteries. J. Power Sources 2021,  485, 229321.  (18) Li, J.; Shi, P.; Wang, Z.; Chen, Y.; Chang, C. -C. A combined recovery process of metals in spent \nlithium -ion batteries. Chemosphere 2009,  77 (8), 1132 -1136.  \n(19) Makuza, B.; Ti an, Q.; Guo, X.; Chattopadhyay, K.; Yu, D. Pyrometallurgical options for recycling \nspent lithium -ion batteries: A comprehensive review. J. Power Sources 2021,  491, 229622.  \n(20) Tao, Y.; Rahn, C. D.; Archer, L. A.; You, F. Second life and recycling: Energy and environmental \nsustainability perspectives for high -performance lithium -ion batteries. Science advances 2021,  7 (45), \neabi7633.  \n(21) Zeng, X.; Li, J.; Shen, B. Novel app roach to recover cobalt and lithium from spent lithium -ion \nbattery using oxalic acid. Journal of hazardous materials 2015,  295, 112 -118.  \n(22) Gastol, D.;  Marshall, J.; Cooper, E.; Mitchell, C.; Burnett, D.; Song, T.; Sommerville, R.; Middleton, \nB.; Crozier , M.; Smith, R. Reclaimed and Up ‐Cycled Cathodes for Lithium ‐Ion Batteries. Global \nChallenges 2022 , 2200046.  \n(23) Contestabile, M.; Panero, S.; Scrosati, B. A laboratory -scale lithium -ion battery recycling process. \nJ. Power Sources 2001,  92 (1-2), 65 -69. \n(24) Natarajan, S.; Akshay, M.; Aravindan, V. Recycling/Reuse of Current Collectors from Spent \nLithium‐Ion Batteries: Benefits and Issues. Advanced Sust ainable Systems 2022,  6 (3), 2100432.  \n(25) Ma, T.; Xu, G. -L.; Li, Y.; Wang, L.; He, X.; Zheng, J.; Liu, J .; Engelhard, M. H.; Zapol, P.; Curtiss, L. A. \nRevisiting the corrosion of the aluminum current collector in lithium -ion batteries. The journal of \nphysical chemistry letters 2017,  8 (5), 1072 -1077.  \n(26) Dai, S.; Chen, J.; Ren, Y.; Liu, Z.; Chen, J.; Li, C. ; Zhang, X.; Zhang, X.; Zeng, T. Electrochemical \ncorrosion behavior of the copper current collector in the electrolyte of lithium -ion batteries. Int. J. \nElectrochem. Sci 2017,  12 (10).  \n(27) Chen, J.; Li, Q.; Song, J.; Song, D.; Zhang, L.; Shi, X. Environme ntally friendly recycling and effective \nrepairing of cathode powders from spent LiFePO 4 batteries. Green Chemistry 2016,  18 (8), 2500 -2506.  \n(28) Toma, C. -M.; Ghica, G.; Buzatu, M.; Petrescu, M. -I.; Vasile, E.; Iacob, G. In A recovery process of \nactive cat hode paste from spent Li -ion batteries , IOP Conference Series: Materials Science and \nEngineering, IOP Publishing: 2017; p 012034.  \n(29) Wu, J.; Zhu, Z.; Zhang, H.; Fu, H.; Li, H.; Wang, A.; Zhang, H.; Hu, Z. Improved electrochemical \nperformance of the Silic on/Graphite -Tin composite anode material by modifying the surface \nmorphology of the Cu current collector. Electrochim. Acta 2014,  146, 322 -327.  \n(30) Marshall, J.; Gastol, D.; Sommerville, R.; Middleton, B.; Goodship, V.; Kendrick, E. Disassembly of \nLi ion cells—Characterization and safety considerations of a recycling scheme. Metals 2020,  10 (6), \n773.  \n(31) Li, L.; Zhai, L.; Zhang, X.; Lu, J.; Chen, R.; Wu, F.; Amine, K. Recovery of valuable metals from spent \nlithium -ion batteries by ultrasonic -assisted leac hing process. J. Power Sources 2014,  262, 380 -385.  \n(32) Liu, K.; Yang, S.; Lai, F.; Wang, H.; Huang, Y.; Zheng, F.; Wang, S.; Zhang, X.; Li, Q. Innovative \nelectrochemical strategy to recovery of cathode and efficient lithium leaching from spent lithium -ion \nbatteries. ACS Appl. Energy Mater. 2020,  3 (5), 4767 -4776.  \n(33) Zhu, P.; Wu, Z.; Zhao, Y. Hierarchical porous Cu with high surface area and fluid permeability. Scr. \nMater. 2019,  172, 119 -124.  \n(34) Al -Nami, S.; Fouda, A. E. -A. S. Corrosion Inhibition Effec t and Adsorption Activities of methanolic \nmyrrh extract for Cu in 2 M HNO3. Int. J. Electrochem. Sci 2020,  15, 1187 -1205.  \n(35) Myung, S. -T.; Sasaki, Y.; Sakurada, S.; Sun, Y. -K.; Yashiro, H. Electrochemical behavior of current \ncollectors for lithium batter ies in non -aqueous alkyl carbonate solution and surface analysis by ToF -\nSIMS. Electrochim. Acta 2009,  55 (1), 288 -297.  \n(36) Park, K.; Yu, S.; Lee, C.; Lee, H. Comparative study on lithium borates as corrosion inhibitors of \naluminum current collector in lithium bis (fluorosulfonyl) imide electrolytes. J. Power Sources 2015,  \n296, 197 -203.  \n(37) Zhang, X.; Winget, B.; Doeff, M.; Evans, J. W.; Devine, T. M. Corrosion of aluminum current \ncollectors in lithium -ion batteries with electrolytes containing LiPF6. J. Electrochem. Soc. 2005,  152 \n(11), B448.  (38) Myung, S. -T.; Hitoshi, Y.; Sun, Y. -K. Electrochemical behavior and passivation of current collectors \nin lithium -ion batteries. Journal of Materials Chemistry 2011,  21 (27), 9891 -9911.  \n(39) Khranovskyy, V.; Ekblad, T.; Yakimova, R.; Hultman, L. Surface morphology effects on the light -\ncontrolled wettability of ZnO nanostructures. Applied Surface Science 2012,  258 (20), 8146 -8152.  \n(40) Murakami, D.; Jinnai, H.; Takahara, A. Wetting transition from the Cassie –Baxter state to the \nWenzel state on textured polymer surfaces. Langmuir 2014,  30 (8), 2061 -2067.  \n(41) Haselrieder, W.; Westphal, B.; Bockholt, H.; Diener, A.; Höft, S.; Kwade, A. Measuring the coating \nadhesion strength of electrodes for lithium -ion batteries. Int. J. Adhes. Adhes. 2015,  60, 1-8. \n(42) Zhang, Y. S.; Courtier, N. E.; Zhang, Z.; Liu , K.; Bailey, J. J.; Boyce, A. M.; Richardson, G.; Sh earing, \nP. R.; Kendrick, E.; Brett, D. J. A Review of Lithium ‐Ion Battery Electrode Drying: Mechanisms and \nMetrology. Adv. Energy Mater. 2022,  12 (2), 2102233.  \n(43) Li, J.; Daniel, C.; Wood, D. Materials  processing for lithium -ion batteries. J. Power Sources 2011,  \n196 (5), 2452 -2460.  \n(44) Babinec, S. J.; Tang, H.; Meyers, G.; Hughes, S.; Talik, A. Composite cathode structure/property \nrelationships. ECS Transactions 2007,  2 (8), 93.  \n(45) Yazici, M.; Krasso wski, D.; Prakash, J. Flexible graphite as battery anode and current collector. J. \nPower Sources 2005,  141 (1), 171 -176.  \n(46) Amin, R.; Chiang, Y. -M. Characterization of electronic and ionic transport in Li1 -xNi0. 33Mn0. \n33Co0. 33O2 (NMC333) and Li1 -xNi0. 50Mn0. 20Co0. 30O2 (NMC523) as a function of Li content. J. \nElectrochem. Soc. 2016,  163 (8), A1512.  \n(47) Quilty, C. D.; Bock, D. C.; Yan, S.; Takeuchi, K. J.; Takeuchi, E. S.; Marschilok, A. C. Probing sources \nof capacity fade in LiNi0. 6Mn0. 2Co0. 2O2 (NM C622): an operando XRD study of Li/NMC622 batteries \nduring extended cycling. J. Phys. Chem. C 2020,  124 (15), 8119 -8128.  \n(48) NMC Battery Material (LiNiMnCoO2) Specification. http s://www.targray.com/li -ion-\nbattery/cathode -materials/nmc  (accessed 20 May).  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Supplymentary Informatio n \n \nDirect Reuse of Aluminium and Copper Current Collectors from Spent Lithium -\nion Batteries  \nPengcheng Zhu a, c, Elizabeth H. Driscoll b, c, Bo Dong a, c, Roberto Sommerville b, c, \nAnton Zorin b, c, Peter R. Slater a, c, Emma Kendrick b, c * \na School of Chemistry, The University of Birmingham, Birmingham, B15 2TT, United \nKindom  \nb School of Metallurgy and Materials, The University of Birmingham, Elms Rd, \nBirmingham B15 2SE, United Kingdom  \nc The Faraday Institution, Quad One, Harwell Campus, Didcot OX11 0RA, United \nKingdom  \n \n \nElectrode slurry mixing  \nFor cathode mixing, PVDF was pre -dissolved in NMP to make a PVDF solution with a \nconcentration of 8 wt%. Half the 8 wt% PVDF solution was mixed with C 65 using a \nTHINKY mixer (ARE -20, Intertronics) at 500 rpm for 1min and 2000 for 5 mins. \nNMC622 and the other half of the PVDF solution were added to the mixture and mixed \nagain at 500 rpm for 1 min and 2000 rpm for 10 mins. The mixture was subsequently \ndegassed in the THINKY mixer at 2200 rpm for 3 mins. The obtained slurry was \nhomogenous and had a solid content of around 60%. For anode mixing, CMC was \npre-dissolved in distilled water to make CMC solution with a concentration of 1.5 wt%. \nHalf of the 1.5 wt% CMC solution was firstly mixed with C 45 in THINKY mixer at 500 \nrpm for 1 min and 2000 rpm for 5 mins. Graphite and the remaining CMC solution were \nadded and mixed again at 500 rpm for 1 min and 2000 rpm for 10 mins. The mixture \nwas then degassed in the THINKY mixer at 2200 rpm for 3 mins. 2.25 wt% SBR \nsolution with a concentration of 40% was added at the end and mixed at 500 rpm for \n5 mins. The prepared graphite slurry was homogenous and had a solid content of \naround 50%.   \nInductively coupled plasma - optical emission spectrometry (ICP -OES) analysis  \n10 cm2 discs were cut from each current collector using a calibrated James -Heal \nsample cutter (James -Heal, UK). These samples were digested for 45 minutes with \nan av erage temperature of 170 ℃ in 10 mL of Aqua Regia (4:1 ratio of HCl : HNO 3 \n(70% aqueous)) using a microwave digester (Anton -Paar, Austria). The resulting \nmixtures were filtered using quantitative filter paper (Fisher Scientific, UK) and made \nup to 250 mL i n volumetric flasks, using distilled water (all glassware was washed \nthrice with distilled water). A sample was taken from this dilution for analysis (100% \nbaseline samples). A 5 mL aliquot was taken from the 250 mL volumetric flask and \ndiluted up to 50 mL  in volumetric flasks - again a sample was taken from this dilution \n(10% samples).  \nThe samples were analysed using an Agilent 5110 ICP -OES with an argon plasma \ntorch (Agilent Technologies, USA). Three repeat measurements were taken for each \nindividual samp le by the instrument with a rinse step between each sample. A re -slope \nof the calibration line was performed every 20 samples using the 0 ppm, 5, 20 and 70 \nppm standards. An error of 20% and an r2 value of 0.995 were chosen for the \ncalibration lines.  \nFor t he axial standards (0 - 15 ppm), a 50 ppm working solution consisting of 9 \nelements was prepared from 1000 ppm single element standards (Al, Co, Cu, Fe, Li, \nMn, Na, Ni, and P). This working solution was appropriately diluted down to produce \nthe desired sta ndards using volumetric flasks. These standards were acidified using \naqua regia.  \nFor the radial standards (20 - 100 ppm), these standards were prepared individually \n(rather than previous, where a working solution of 50 ppm was used) using 1000 ppm \nelement  standards (Al, Co, Cu, Fe, Li, Mn, Na, Ni, and P) to create the desired \nconcentrations. These were also acidified using aqua regia.   \nThe 20 ppm solution was used in the calibration of both the axial and radial \nmeasurements.  \nX-ray photoelectron spectrosco py analysis  XPS Analysis was performed using a Thermo NEXSA XPS fitted with a \nmonochromated Al kα X -ray source (1486.7 eV), a spherical sector analyser and 3 \nmultichannel resistive plates, 128 channel delay line detectors. All data was recorded \nat 19.2W an d an X -ray beam size of 400 x 200 µm. Survey scans were recorded at a \npass energy of 200 eV, and high -resolution scans were recorded at a pass energy of \n40 eV. Electronic charge neutralization was achieved using a Dual -beam low -energy \nelectron/ion source ( Thermo Scientific FG -03). Ion gun current = 150 µA. Ion gun \nvoltage = 45 V. All sample data were recorded at a pressure below 10-8 Torr and a \nroom temperature of 294 K. All Al and Cu current collectors were sputtered for 20 and \n30s using Ar 4000+ eV monato mic mode with a raster size of 2x2 mm^2 (etching rate \n0.57 nm/s ref. Ta2O5), respectively, to remove surface contamination. Data were \nanalysed using CasaXPS v2.3.20PR1.0. Peaks were fitted with a Shirley background \nprior to component analysis. Line shapes of LA(1.53,243) were used to fit components.  \nScanning electrode microscopy with energy -dispersive X -ray (SEM -EDX) \nspectroscopy analysis  \nThe surface morphology of the reclaimed and pristine current collectors was \ninvestigated by scanning electron microscop y (SEM, Philips XL30 FEG)  under an \nacceleration voltage of 10 kV. Magnifications of 2000x and over were utilised for the \nobservation of surface features on all current collectors. The elemental distributions \nwere measured by energy -dispersive X -ray spectro scopy (EDX, Oxford Inca 300).   \nFigure S1. Zoom in XPS spectra for F 1s.  \n \nFigure S2. Size of etched pits on etched Al surface measured by Image J.  \n \n \nFigure S3 SEM micrographs and EDS mapping of the cross -section of NMC622 \nelectrodes on pristine (a -d), washed (e -h) and etched (i -l) Al current collectors. \nMicrographs of NMC622 on pristine (a), washed (e) and etched (i) Al current collectors ; \nEDS mapping of Al (b, f, j), carbon (c, g, k) and fluorine distribution (d, h, l) (scale bar: \n20 µm).  \n \n \nFigure S4 SEM micrographs and EDS mapping of the cross -section of graphite \nelectrodes on pristine (a -d), washed (e -h) and etched (i -l) Cu current collectors. \nMicrographs of graphite on pristine (a), washed (e) and etched (i) Cu current collectors; \nEDS mapping of Cu (b, f, j), carbon (c, g, k) and sodium distribution (d, h, l) (scale bar: \n20 µm).  \n \n \nFigure S5. Adhesion force of graphite electrodes made with PVDF binder on Cu \ncurrent collectors  \n \nFigure S6. Electrical conductivity of graphite electrodes made with PVDF binder on \nCu current collectors  \n \n \n \n \n \nTable S1: ICP -OES test for Al current collector in oxalic acid  \nElement  Al Co Cu Li Mn Ni \nConcentration \n(ppm)  109.27  27.01  0 26.23  347.93  109.44  \n \nTable S2: ICP -OES test for Cu in water  \nElement  Al Co Cu Li Mn Ni P \nConcentration \n(ppm)  0.04 0.03 0.04 6.23 0.07 0.04 3.28 \n \n \n \n 29 \n \n Table S3: Reused and pristine Al and Cu current collectors  \nMaterial  Treatment  Surface \ncomposition  Surface \nroughness  Wettability  \n(degree)  Adhesion  \n(N/m)  Electrical \nconductivity \n(S/m)  Binder \ndistribution  Capacity \n@ 0.1 C \n(mAh/g ) Capacity \n@ 5 C \n(mAh/g ) \nP_Al  None  Al oxide  Low 47.38  15.26  49.07  Gradient  171.48  101.64  \nW_Al  NMP  Al oxide & \nresidual PVDF  Medium  44.02  41.71  20.98  Gradient  172.07  4.39 \nE_Al  Oxalic acid  Thin Al oxide  High  39.66  79.57  18.37  Uniform  173.52  13.10  \nP_Cu  None  Cu oxide  Low 73.94  2.26 1.18E6  Uniform  330.75  117.72  \nW_Cu  HCl Very thin Cu \noxide  Medium  87.04  2.47 1.12E6  Uniform  340.54  118.39  \nE_Cu  HCl&HNO 3 Very thin Cu \noxide  High  102.4  2.67 1.20E6  Uniform  323.89  104.43  \nNote: P stands for pristine, W stands for washed, E stands for etched  \n \n \n ",
      "metadata": {
        "filename": "Direct Reuse of Aluminium and Copper Current Collectors from Spent Lithium-ion B.pdf",
        "hotspot_name": "Aluminium_Die_Casting",
        "title": "Direct Reuse of Aluminium and Copper Current Collectors from Spent\n  Lithium-ion Batteries",
        "published_date": "2022-10-14T10:01:44Z",
        "pdf_link": "http://arxiv.org/pdf/2210.07678v1",
        "query": "die casting aluminium radiator energy consumption reduction process optimization techniques"
      }
    },
    "Information-Theoretic Study of Time-Domain Energy-Saving Techniques in Radio Acc": {
      "full_text": "DRAFT 1\nInformation-Theoretic Study of Time-Domain\nEnergy-Saving Techniques in Radio Access\nFranc ¸ois Rottenberg\nAbstract\nReduction of wireless network energy consumption is becoming increasingly important to reduce\nenvironmental footprint and operational costs. A key concept to achieve it is the use of lean transmission\ntechniques that dynamically (de)activate hardware resources as a function of the load. In this paper,\nwe propose a pioneering information-theoretic study of time-domain energy-saving techniques, relying\non a practical hardware power consumption model of sleep and active modes. By minimizing the\npower consumption under a quality of service constraint (rate, latency), we propose simple yet powerful\ntechniques to allocate power and choose which resources to activate or to put in sleep mode. Power\nconsumption scaling regimes are identified. We show that a “rush-to-sleep” approach (maximal power in\nfewest symbols followed by sleep) is only optimal in a high noise regime. It is shown how consumption\ncan be made linear with the load and achieve massive energy reduction (factor of 10) at low-to-medium\nload. The trade-off between energy efficiency (EE) and spectral efficiency (SE) is also characterized,\nfollowed by a multi-user study based on time division multiple access (TDMA).\nIndex Terms\nEnergy consumption, radio access technologies, physical layer, channel capacity.\nI. I NTRODUCTION\nA. Motivation\nAs of 2022, yearly data volume has gone up to more than 3 Zettabytes ( 1021bytes) and the\ntraffic continues to rise at a rate of about 25%/year [1]. Energy efficiency has improved over\nthe years but not fast enough, which results in an annual energy consumption growth of 2.5%\nfor the ICT sector [1]. It is becoming increasingly important to reduce energy consumption of\nwireless communication networks to reach climate ambitions and reduce operational expenses,\nin other words, “break the energy curve” [2], [3]. Most energy of wireless networks is consumed\nby the radio access network (RAN) and more specifically at base stations [4], [5]. Moreover, the\ntraffic load at a base station is highly varying across the day and most often lightly loaded, with\nFranc ¸ois Rottenberg is with ESAT-DRAMCO, Ghent Technology Campus, KU Leuven, 9000 Ghent, Belgium (e-mail:\nfrancois.rottenberg@kuleuven.be).arXiv:2303.17898v2  [cs.IT]  4 Sep 2023DRAFT 2\ntraffic at night being about 10 times lower than during the day [6]. This opens a big potential\nfor energy reduction through the use of lean transmission techniques that dynamically activate\nor deactivate resources as a function of the load, letting the system dynamically switch from\nfully active to deep sleep mode.\nB. State of the Art\nEnergy-saving techniques are a popular topic. A large effort has been made to integrate these\ntechniques into industrial products and standards. The 5G standard was for instance designed\nwith a lean paradigm in mind which resulted in, e.g., less reference signaling to increase\nsleep duration [7]–[9]. At low-to-medium load, a popular scheduling technique is a “rush-to-\nsleep” approach which compacts transmission in as few symbols as possible. These symbols\nare transmitted at maximal power, leaving the remaining symbols in the frame free, so that the\nsleep duration is maximized. Many studies have been performed to evaluate the gains of such\ntechniques based on standardized power models [5], system-level evaluations [10]–[14] and aided\nby actual measurements [15]. The use of machine learning was also identified as an interesting\ntool to predict traffic and/or to optimize energy-saving features [16], [17]. We refer to [8] for a\nreview of these techniques.\nDespite much work in the domain, there remains a fundamental gap to be filled by establishing\nan information-theoretic study of time-domain energy-saving techniques, even for basic systems\nsuch as single input single output (SISO) transceivers. Going back to the underlying physics\nof energy consumption of base stations and with a proper mathematical formulation of the\ncommunication link, a lot of additional understandings and improvements can be obtained: opti-\nmization of algorithms, finding optimal power scaling regimes as a function of load, guarantees\nof optimality for energy-saving features and/or finding the gap from it with existing techniques...\nAs an example, it is not clear if or when a rush-to-sleep approach is optimal or not. We should\nmention that many recent works have performed this kind of energy-saving studies but they have\nfocused on the spatial domain and more specifically the optimal operation of massive MIMO\nsystems (number of active antennas, served users, power allocation) as a function of the load [18]–\n[22]. On the other hand, information-theoretic time-domain studies are lacking. The fundamental\nstudies on energy-efficient communications have mainly focused on a stationary transmission of\nsymbols in time at a constant rate and average transmit power PT[23]–[27]. Considering an\nideal consumption model and a quasi-static channel, this choice seems intuitive. To clarify it,DRAFT 3\nlet us formalize the problem. The channel capacity of a complex discrete memoryless additive\nwhite gaussian noise (AWGN) channel, under an average transmit power constraint PT, is\nR= log2\u0012\n1 +PT\nLσ2\nn\u0013\n= log2\u0012\n1 +PT\nσ2\u0013\n[bits/channel use] ,\nwhere σ2=σ2\nnLis the noise power at the receiver σ2\nnnormalized by the path loss L. If the\ntransmission is divided in frames of Nsymbols, the average rate and transmit power are\nR=1\nNN−1X\nn=0log2\u0010\n1 +pn\nσ2\u0011\n, PT=1\nNN−1X\nn=0pn\nwhere pnis the transmit power of the n-th symbol. Considering a rate constraint R, let us find\nthe power allocation that minimizes the consumed power Pcons. Under an ideal consumption\nmodel, we have Pideal\ncons=PTand the problem can be written as1\nmin\np0,...,pN−11\nNN−1X\nn=0pns.t.1\nNN−1X\nn=0log2\u0010\n1 +pn\nσ2\u0011\n=R.\nGiven the concavity of the log(.)function, we can write using the Jensen’s inequality\nR≤log2 \n1 +1\nσ21\nNN−1X\nn=0pn!\n↔(2R−1)σ2≤1\nNN−1X\nn=0pn=PT\nand the bound is tight if uniform power allocation is used, i.e.,pn=PT= (2R−1)σ2, for\nn= 0, ..., N −1. Intuitively, the log dependence of the rate implies diminishing returns. Starting\nfrom a non-uniform allocation, it can always be improved by reallocating some power from the\ntime interval with the highest allocated power to the one with the lowest power.\nIn practice however, the consumed power Pcons is far from being equal or even linearly\nproportional to the transmit power PT. This is due to two main reasons, namely: i) as soon as a\ngiven time slot is active, a static load-independent power consumption is present due to activation\nof hardware components such as radio-frequency chains and baseband processing units; ii) the\nload-dependent power consumption, i.e., the dependence of Pconsinpn, is typically concave as\npower amplifiers (PAs) are more energy-efficient close to their saturation. Intuitively, this implies\nthat the “cost” of using more power decreases when a large output power is transmitted. These\ntwo effects will counterbalance the log penalty and push towards using a reduced number of\nactive time slots, especially in low-to-medium load scenarios.\n1This problem can be seen as a conventional waterfilling problem where water/noise levels are the same at each time slot.DRAFT 4\nC. Contributions\nThis paper presents a pioneering information-theoretic study of time-domain energy-saving\ntechniques, using a realistic power consumption model. The transmission model considers single-\nantenna base stations and users. Even for such a basic system, a comprehensive study of energy-\nsaving features is lacking, which is the gap this paper is aiming to fill. The investigated techniques\nprovide drastic energy reduction by dictating how to dynamically (de)activate hardware resources\nas a function of the load. The optimization problems are formalized as the minimization of Pcons\nfor a given rate. More specifically, the structure of our paper and our contributions are structured\nas follows. Section II presents the hardware power consumption model used in this work, with\ntwo distinct contributions: active and sleep energy consumption. The active power consumption\nmodel is shown to address a large variety of PA classes. Section III considers the optimal\nallocation of time resources in a single-user scenario. The solution is approached step by step\nthrough lemmas to get more insight on its nature. Linear and exponential scaling regimes of Pcons\nas a function of the load Rare identified. Asymptotic results for large Nare provided that greatly\nsimplify the analysis while having negligible performance penalty. We prove that a rush-to-sleep\napproach is optimal in a noise limited regime but not otherwise. The optimal trade-off EE-SE\nis also derived from previous results and we show that a maximal SE does not always provide\na maximal EE. Section IV then extends previous results by considering successive sleep modes,\nresulting in drastic energy reductions. Section V considers the extension to a multi-user scenario\nwhere users are multiplexed using TDMA. The optimal allocation is provided for the most\npromising regime in terms of energy-savings, i.e., the low-to-medium-load scenario where Pcons\nlinearly scales with the rate of each user and the system is not fully active. Finally, Section VI\nconcludes the paper.\nNotations : The operators ⌈.⌉,⌊.⌋and[.]are the ceil, floor and round operators, respectively.\nThe operator ⌊x⌉which we refer to as the ceil-floor operator selects among the upper and\nlower bounding integers of xthe one that optimizes the cost function. The function W(z)is the\nLambert W function, i.e., the solution of z=W(z)eW(z). We use the notation f(x) =O(g(x)),\nasx→a, if there exist positive numbers δandλsuch that |f(x)| ≤λg(x)when 0<|x−a|< δ.\nII. P OWER CONSUMPTION MODEL\nAs described in the introduction, we consider the transmission of Nsymbols, each of duration\nT[s]. The full frame has thus a duration NT [s]. Out of the Nintervals, a number Naare activeDRAFT 5\nand actually transmitting information while the remaining N−Naare inactive and in sleep mode.\nThis implies that 0≤Na≤N. For minimizing latency and maximizing the sleep duration which\nallows entering a deeper sleep mode [28], the Naactive intervals are grouped together at the\nbeginning of the transmission. We define as Eactive the energy consumed during active time slots,\nwhich is assumed to depend on the transmit power at each time interval, i.e.,p0, ..., p Na−1. On\nthe other hand, Esleeprepresents the energy consumed during sleep modes, which is non zero as\nall hardware components cannot be switched-off. It is assumed to depend on (N−Na)Tas a\nlonger sleep duration allows to enter a deeper sleep mode [28]. The average consumed power\nover the frame duration is thus given by\nPcons=Eactive(p0, ..., p Na−1) +Esleep((N−Na)T)\nNT. (1)\nIn the following, we detail the models of the sleep and active energy consumption. As a\nbenchmark, we also introduce the ideal consumption model\nPideal\ncons=1\nNNa−1X\nn=0pn, (2)\nimplying that the consumed power is equal to the transmit power. In other words, no losses are\npresent.\nA. Active Energy Consumption\nUsing the well-established model from [29], the active power consumption can be modelled\nas\nPactive =PPA+PRF+PBB\n(1−σDC)(1−σMS)(1−σcool)\nwhere PPA,PRFandPBBare the powers consumed by the PAs, the radio-frequency chains and\nthe baseband unit respectively. The coefficients σDC,σMSandσcoolare the loss factors related\nto DC-DC power supply, mains supply and active cooling respectively.\nWe are interested in modelling the dependence of the active consumed power Pactive in the\noutput power at each active time slot p0, ..., p Na−1. The base station (BS) power consumption\nanalysis of [29] showed that mainly the PA consumed power PPAscales with the output power.\nThe other terms are thus considered load-independent. The active energy consumed across the\nframe duration can thus be written as\nEactive(p0, ..., p Na−1) =T\nη \nNa˜P0+Na−1X\nn=0PPA(pn)!DRAFT 6\nTABLE I\nVALUES OF LOSS FACTORS AND EFFICIENCY USED IN EVALUATIONS [29].\nDC-DC σDC 7.5%\nMains supply σMS 9.0%\nCooling σcool 10.0%\nEfficiency η= (1−σDC)(1−σMS)(1−σcool)75.8%\nwhere ˜P0=PBB+PRFandη= (1−σDC)(1−σMS)(1−σcool). Values of loss factors and\nefficiencies used in evaluations are shown in Table I. To model the PA consumption, we use the\nfollowing model\nPPA(p) =PPA,0+βpα,0≤p≤Pmax (3)\nwith α∈]0,1]andβ≥0. The first term PPA,0represents the load-independent consumption\nwhile the second is load-dependent. This load dependency does not typically scale linearly with\np. The fact that α∈]0,1]implies concavity of PPA(p). This concavity comes from the fact that\na typical PA efficiency is improved when moving closer to saturation [30]. The constant Pmax\ndenotes the maximal transmit power.In practice, Pmaxis (much) lower than the PA saturation\npower, that we denote by Psat. The use of the so-called back-off Pmax/Psatis required as\nrecent technologies, e.g., orthogonal frequency division multiplexing (OFDM), have high peak-\nto-average power ratio (PAPR). A back-off (typically from -12 dB to -6 dB) prevents the PA\nto enter the saturation region, which would otherwise create nonlinear distortion impacting the\nsignal quality and creating out-of-band emissions. The authors of [31] have justified in details\nthe use of a similar model as (3) through their own measurements and a literature review [32],\n[33]. Model (3) is more general as it also includes a load-independent component, which is\nuseful for particular PA architectures.\n1) Ideal Power Amplifier: PA consumed power is linearly proportional to the output power\ngiving PPA,0= 0,β=α= 1 and\nPideal\nPA(p) =p.\n2) Class A Power Amplifier: PA consumed power is independent of the load and has a\nmaximal efficiency of 1/2giving PPA,0= 2Psat,β= 0 and\nPA\nPA(p) = 2 Psat\nwhere Psatis the saturation power of the PA.DRAFT 7\n3) Class B Power Amplifier: The PA consumed power has a load dependence that scales with\nthe square root of the output power giving PPA,0= 0,β=4\nπ√Psatandα= 1/2\nPB\nPA(p) =4\nπp\nPsat√p\nThis model has much relevance for typical base stations working with a significant back-off\nfrom saturation [31]. Some authors sometimes call it the “traditional” PA model [34]. Therefore,\nby default, we will use it in following evaluations, with a 8 dB back-off.\n4) Envelope Tracking Power Amplifier: According to the curve fitted model proposed in [35],\nPA consumption was shown to be modelled as\nPET\nPA(p)≈aPsat\n(1 +a)ηmax+1\n(1 +a)ηmaxp,\nwhere a= 0.0082 . This model can again be seen as a special case of (3).\n5) Doherty Power Amplifier: Theℓ-way Doherty PA consumed power is given by [25]\nPDoherty\nPA (p) =4Psat\nℓπ\n\n√ξ 0< ξ≤1\nℓ2\n(ℓ+ 1)√ξ−11\nℓ2< ξ≤1\nwhere ξ=p/P sat. The class B model is obtained as a special case when ℓ= 1. Except in such\nparticular cases, the model proposed in (3) cannot exactly represent such PAs but can provide\nan approximation depending on the operating range of the Doherty amplifier.\nRemark 1. We previously described how model (3) can address typical theoretical PA models.\nHowever, it can also be fitted for practical PAs based on measurements and/or datasheets. The\nPA was identified as the main load-dependent contribution. However, more generally, the model\nproposed in (3) can take into account other load-dependent terms.\nIn the light of this remark, we formalize the underlying assumption about the load-dependent\nactive energy consumption model used throughout this work.\n(As1): The active energy consumed across the frame is\nEactive(p0, ..., p Na−1) =T \nNaP0+γNa−1X\nn=0pα\nn!\n(4)\nwhere γ=β\nη≥0andP0=PBB+PRF+PPA,0\nη≥0,α∈]0,1],0≤pn≤Pmaxforn= 0, . . . , N a−1,\n0≤Na≤Nandpn= 0 forn=Na, . . . , N −1. The averaged consumed power is then\nPcons=Na\nNP0+γ\nNNa−1X\nn=0pα\nn+Esleep((N−Na)T)\nNT. (5)DRAFT 8\n1001011021030204060\nTime [ms]Psleep(t)[W]Piecewise constant sleep (As3)\nConstant sleep (As4)\nFig. 1. Two classical sleep power models are shown: constant or successive sleep modes based on values in Table II.\nB. Sleep Energy Consumption\nIn the ideal consumption model (2), the sleep energy consumption is exactly null. In practice,\nnot all hardware can be switched off as always-on reference signals are required to allow\nusers to access the network. Moreover, different hardware components have different activa-\ntion/reactivation latencies. Hence, depending on the sleep duration, more or less components\ncan be switched off. Therefore, power models have been proposed that consider successive sleep\nmodes as a function of the sleep depth. We define the power consumption in sleep mode as\nPsleep(t), where t= 0 is used a reference for the system entering sleep and tis the sleep\nduration so that Esleep(0) = 0 . Given the fact that increasing sleep duration allows to switch off\nmore hardware components, we introduce the following assumption.\n(As2):Psleep(t)is monotonically non-increasing.\nProposition 1. Under (As2), the sleep energy consumption Esleep(t)is a concave function of\nthe sleep duration t.\nProof. Directly follows from (As2).\nRemark 2. (As2)does not imply continuity of Psleep(t), which can have jump discontinuities.\nSwitching-off components might lead to a non-continuous drop of Psleep(t), as shown in Fig. 1.\nPopular models for Psleep(t)include the use of different sleep modes. The model in [28] has\nbeen used as a qualitative and quantitative reference for several years by companies [36]. MoreDRAFT 9\nTABLE II\nNUMERICAL VALUES OF POWER AND POWER MODELS USED IN EVALUATIONS .\nMaximal transmit power Pmax= 20 W\nDeep sleep power P3= 1 W\nLoad-independent active power P0= 110 P3\nSuccessive sleep power model\n(As3)[5]Psleep(t)\nMicro sleep Light sleep Deep sleep Hibernating sleep\nT1= 0 T2= 6 ms T3= 50 ms T4= 1 s\nP1= 50P3P2= 25P3 P3 P4= 0.1P3\nConstant sleep power model (As4) Psleep(t) =Psleep=P1\nrecently, 3GPP has introduced an improved model, expressed in relative units with respect to\nthe deep sleep mode, that better reflects current trends [5], [37]. The model contains four sleep\nmodes and numerical values are given in Table II for one configuration described in [5]. These\nvalues will be used as an example in evaluations.\nWe can formalize this model mathematically. Let us define as Sthe number of sleep modes,\nstarting from mode 0 (no sleep) to mode S(deepest sleep mode). The start and end of each\nsleep mode are denoted by TsandTs+1, with T0= 0 andTS+1= +∞. The sleep power\nconsumption during mode sis denoted by Ps. This notation is consistent with the definition of\nP0which denotes the load-independent active power consumption. This corresponds to “sleep\nmode zero”, taking place until T1, where no hardware components are actually switched off.\n(As3): the sleep power consumption is piecewise constant implying that Psleep(t) =Psand\nEsleep(t) =Zt\n0Psleep(t′)dt′=s−1X\ns′=0Ps′(Ts′+1−Ts′) + (t−Ts)Ps\nwhere sis the index such that Ts< t≤Ts+1andPs+1≤Psaccording to (As2).\nWe also introduce another popular sleep power model widely used in the literature and shown\nto be accurate to characterize 4G long term evolution (LTE) macro base stations [29].\n(As4): the sleep power consumption is constant implying that Psleep(t) =PsleepandEsleep(t) =\nPsleeptwithP0≥Psleep. The averaged consumed power is then\nPcons=Na\nNP0+γ\nNNa−1X\nn=0pα\nn+N−Na\nNPsleep. (6)\nRemark 3. (As4)is a particular case of (As3)with a single sleep mode: S= 1,Psleep=P1\nandT1= 0. In the following, we use the term “ (As3)−(As4)”, when both assumptions hold.DRAFT 10\nIII. O PTIMAL ALLOCATION OF TIMERESOURCES\nThis section considers the solution of minimizing the average consumed power under (As1)\nand a rate constraint\nmin\nNa,p0,...,pNa−1Pcons s.t.1\nNNa−1X\nn=0log2\u0010\n1 +pn\nσ2\u0011\n=R. (7)\nWe define a constant that will be useful throughout this section. Under (As1)−(As4),Rais\nthe constant that minimizes the convex problem\nRa= arg min\nx≥0P0−Psleep+γσ2α(2x−1)α\nx,\nwhere Psleep is the constant sleep power consumption defined in (As4). When P0−Psleep= 0,\nit is given by\nRa= (W(−α−1e−α−1) +α−1)/log(2) (8)\nwhere W(z)is the Lambert W function, i.e., the solution of z=W(z)eW(z). In the following,\nthe general solution of (7) is approached step by step, introducing several lemmas, which provide\ninsight on its form and scaling as a function of R. Finally, the trade-off SE versus EE will be\ncharacterized.\nA. Optimal Allocation for Load-Dependent Consumed Power\nThe following lemma provides the power allocation that minimizes the load-dependent part of\nthe average consumed power under a rate constraint and without a maximal per-time slot power\nconstraint. Under (As1), the load-dependent part of Pconscan be identified as\nPld(p0, ..., p N−1) =γ\nNN−1X\nn=0pα\nn, (9)\nwhich equals PconsforP0= 0 andEsleep(t) = 0 .\nLemma 1. Under (As1), forP0=Esleep(t) = 0 ,γ >0andPmax→+∞, the minimum of (7),\nis achieved by uniformly allocating power among Natime slots\nNa=⌊min(NR/R a, N)⌉,\npn=\n\n\u0010\n2RN\nNa−1\u0011\nσ2ifn= 0, ..., N a−1\n0 otherwise.DRAFT 11\n0 2 4 6 802468\np0p1R=1\n2(log2(1 +p0) + log2(1 +p1))\n1\n2(p0+p1) = 3\n1\n2(√p0+√p1) = 0 .75\n1\n2(√p0+√p1) = 2\n(a)0 0.2 0.4 0.6 0.8 10246810\nδ\nαRaδ= 0\nδ= 1\nδ= 5\n(b)\nFig. 2. (a) Contour plot of i) rate constraint and ii) load-dependent power consumption ( γ= 1) for a frame of N= 2 time slots\nand two load-dependent power exponents α:1and1/2. (b) Constant Raas a function of the load-dependent power exponent\nαand the ratio δ= (P0−Psleep)/(γσ2α).\nProof. See Appendix VII-A.\nRemark 4 (Frame of N= 2 symbols) .To illustrate Lemma 1, Fig. 2a considers the particular\ncaseN= 2,γ= 1. It shows contour plots of the constraint Rand the cost function Pld, as a\nfunction of p0andp1. In the case α= 1, implying linearity of consumed and transmit power,\nthe objective function curve (p0+p1)/2 = 3 is a straight line. As shown in the introduction,\nutilization of the two time slots is always optimal ( Na= 2), with uniform power allocation.\nHowever, for α= 1/2, this is not anymore the case. For low values of R, it is better to only\nuseNa= 1 slot while for large R,Na= 2 is optimal, again with uniform power.\nRemark 5 (Scaling of Nafor general Nandα).For an arbitrary value of Nandα∈]0,1],\nthe number of activated time slots Nascales approximately linearly with the rate Rup to the\npoint where the maximal number Nis allocated, i.e., when R > R a. When R≤Ra, the rate\nper activated time slot is approximately equal to Ra. The “approximate” nature comes from the\nrounding operation. This error disappears for large N, as will be formalized properly in the\nfollowing. The constant Rais independent of Rand is a function of α. As shown in Fig. 2b for\nthe case δ= 0 (P0=Psleep= 0),Ramonotonically decreases as a function of α, implying thatDRAFT 12\nmore time slots are activated for a fixed value of R. In the asymptotic cases of αapproaching\n0 or 1, a single ( Na= 1) or all time slots ( Na=N) are allocated, respectively.\nB. Optimal Allocation with no Maximal Power Constraint\nThe following lemma gives the power allocation that minimizes the averaged consumed power\nunder a rate constraint and without a maximal per-time slot power constraint. Removing this\nconstraint provides the solution when it is not binding, i.e., when the user experiences a good\nchannel (low normalized noise variance σ2=Lσ2\nn) and its target rate is not too high. The general\ncase will be addressed in next subsection.\nLemma 2. Under (As1)-(As2)and for Pmax→+∞, the minimum of the problem (7) is\nachieved by uniformly allocating power among Natime slots\npn=\n\n\u0010\n2RN\nNa−1\u0011\nσ2ifn= 0, ..., N a−1\n0 otherwise(10)\nwhere Nais the argument that minimizes\nmin\n0≤Na≤NNa\nN\u0010\nP0+γσ2α\u0010\n2RN\nNa−1\u0011α\u0011\n+Esleep((N−Na)T)\nNT. (11)\nUnder (As3)−(As4), the solution is\nNa=⌊min(NR/R a, N)⌉.\nProof. See Appendix VII-A.\nRemark 6 (Relation with Lemma 1) .Lemma 2 extends the result of Lemma 1 by considering a\nnon-zero sleep power Psleepand load-independent active power consumption P0. Under a constant\nsleep power model (As4), the problem has a similar solution.\nRemark 7 (Scaling of Ra).As shown in Fig. 2b, Raincreases with δ= (P0−Psleep)/(γσ2α).\nThis implies that less time slots should be activated if normalized noise power σ2,Psleep andγ\nare small and if the active load-independent power consumption P0is high. A major difference\nwith Lemma 1 is that Radoes not go to zero as αapproaches 1 when δ > 0. This implies\nthat, given nonzero static power consumption, not all time slots should always be activated. The\nparticular load-independent case γ= 0 implies that Ra= +∞andNa= 1. This makes sense\nasPconsthen only depends on Na. As shown in next subsection, considering a finite maximal\ntransmit power per time slot will change this result.DRAFT 13\nAlgorithm 1 Iterative resource allocation\nRequire: σ2, R, N, P max, Rmax, Ra\nNa←(11) ▷Init. by sol. of Theor. 2\np0, ..., pN−1←(10)\nNmax←0\nˆN←N\nwhile pNa−1> P maxdo ▷Check max power constraint\nNmax←Nmax+ 1 ▷Set one more time slot to max power\nR←(NR−Rmax)/(N−1) ▷Adapt rate constraint\nN←N−1\nNa←(11) ▷Update with sol. of Theor. 2\np0, ..., pN−1←(10)\nend while\npNa, ..., pNa+Nmax−1←Pmax\npNa+Nmax, ..., p ˆN−1←0\nC. Optimal Allocation with Maximal Power Constraint\nWe now consider the additional constraint of a finite maximal per-time slot power Pmax. This\nconstraint may render the problem unfeasible. Therefore, we introduce the following assumption.\n(As5) : Problem (7) is feasible, i.e.,\nR≤Rmax= log2\u0012\n1 +Pmax\nσ2\u0013\n.\nThe exact solution of problem (7) generally requires an iterative solution.\nProposition 2. Under (As1)−(As2),(As5), the solution of the problem (7) can be obtained\nby using Algorithm 1.\nProof. See Appendix VII-B.\nTo avoid the need of an iterative solution and the ceil-floor operator, we use the fact that\nthe problem greatly simplifies by considering the asymptotic case of a large N. Then, the ratio\nNa/Ncan be considered asymptotically continuous instead of only taking discrete values.\nRemark 8 (Large Nassumption) .The large Nassumption is realistic in practice as frames are\ntypically made of many symbols. Moreover, the assumption of having a sufficiently large Nis\ncentral and implicit in this work. This article investigates the gain of activating only a share\nof transmission time slots. To be able to do this, some flexibility in the number of activated\nresources should be available, implying a sufficiently large N.DRAFT 14\n0 2 4 6 8 10050100150200\nRmax RaPcons,max\nP0\nPsleepLinearExponential\nR[bits/channel use]Pcons [W]N= 10 symbols, σ2= 10 mW\nUniform\nAsympt. opt.\nOpt.\nRush-to-sleep\n(a)0 0.5 1 1.5 2 2.5050100150200\nRmax RaPcons,max\nP0\nPsleepLinear\nR[bits/channel use]Pcons [W]N= 50 symbols, σ2= 5 W\nUniform alloc.\nAsympt. opt. alloc.\nOpt. alloc.\nRush-to-sleep\n(b)\nFig. 3. Power consumption versus load using optimal/uniform power allocation with constant sleep mode (As4).\nTo provide the closed-form asymptotic solution, we define\n˜R= min( Ra, Rmax), Pa= (2Ra−1)σ2,˜P= min( Pa, Pmax).\nTheorem 1. Under (As1)−(As5), the solution of problem (7) and two scaling regimes of\nPconsas a function of Rcan be found:\n□(Linear ) IfR≤˜R, asN→+∞, the allocation\nNa=h\nNR/ ˜Ri\n, pn=\n\n˜Pifn= 0, ..., N a−1\n0otherwise\nis asymptotically optimal and achieves a consumed power\nPcons=Psleep+RP0−Psleep+γ˜Pα\n˜R+ϵ\nwhere ϵis the gap from the optimum which asymptotically vanishes: |ϵ|=O(1/N).\n□(Exponential ) IfR > ˜R:\nNa=N, p n=\u0000\n2R−1\u0001\nσ2forn= 0, ..., N −1,\nPcons=P0+γσ2α\u0000\n2R−1\u0001α.\nProof. See Appendix VII-C.\nRemark 9. For large N, the solution has a simple form: the ceil-floor operator is replaced by a\nrounding operator. The power and number of bits per-activated time slot ( ˜Pand˜R) are constantDRAFT 15\nin the linear regime. As shown in Fig. 3a and 3b, the approximation error can barely be seen\nand is already negligible for small values of Nsuch as 10 or 50.\nRemark 10 (Scaling regimes) .Lemma 1 puts forward two scaling regimes: linear and exponen-\ntial. They can easily be identified in Fig. 3a. In Fig. 3b, a higher noise regime is considered so\nthat only the linear regime is present ( ˜R=Rmax).\nRemark 11 (Gain with respect to uniform allocation) .Fig. 3a and 3b also plot the gain with\nrespect to a uniform allocation, i.e.,pn=\u0000\n2R−1\u0001\nσ2forn= 0, ..., N −1. As expected, the\ngain is larger at low load (rate) where the optimal allocation only activates few resources.\nRemark 12 (Rush-to-sleep) .IfRmax≤Ra(high noise regime, Fig. 3b), a rush-to-sleep approach\nis optimal: active time slots transmitting at full power Pmaxand rate Rmax. This minimizes Na\nand maximize sleep duration. If Ra< R max(low noise regime, Fig. 3a), reduced transmit power\nshould be used instead and not even using sleep for R > R a. It is then better to fully activate\nthe system with a uniform allocation.\nRemark 13 (Converse problem) .We here consider the minimization of Pconsfor a fixed R.\nThe maximization of Rfor a fixed Pconscan also be considered. It could occur if power is\navailable and should directly be used, e.g., a solar panel or wind turbine without battery and/or\nnot connected to the grid. The solution can be found using same methodology or directly by\n“reverting” the result of Theor. 1. Indeed, the allocation that minimizes Pconsfor a fixed rate\nRis also the allocation that maximizes Rfor the minimal value of Pconsof the inital problem.\nScaling regimes of Ras a function of Pconswill be linear and logarithmic. It is here omitted\ndue to space constraints.\nD. Trade-Off: Spectral Efficiency versus Energy Efficiency\nConsidering that the transmission occupies a bandwidth B= (1 + αrol)/T, where αrol∈[0,1]\nis the roll-off factor, so that the SE and the EE are\nSE =R\nTB=R\n1 +αrol[bits/s/Hz] ,EE =BSE\nPcons=R\nTPcons[bits/Joule] . (12)\nGiven these relationships, the trade-off SE-EE can be easily identified from previous results.\nCorollary 1. Under (As1)−(As5), the optimal EE for a given SE is:DRAFT 16\n□IfSE≤˜R/(1 +αrol)(implying R≤˜R), asN→+∞,\nEE =BSE\nPsleep+ SE1+αrol\n˜R\u0010\nP0−Psleep+γ˜Pα\u0011+O(1/N).\n□IfSE>˜R/(1 +αrol)(implying R > ˜R):\nEE =BSE\nP0+γσ2α(2SE(1+ αrol)−1)α.\nProof. From the definition of the EE in (12), it is clear that its maximization is equivalent to\nthe minimization of Pcons. As a result, the optimal allocations and the results of Theor. 1 can\ndirectly be used, which lead to the above results. The two cases correspond to the linear and\nexponential scaling regimes of Theor. 1, respectively.\nCorollary 2. Under (As1)−(As5), the SE that maximizes the optimal EE given in Corollary 1\nis:\n□IfRa≥Rmax:\n¯SE = SE max=Rmax\n1 +αrol,EEmax=BSEmax\nP0+γPα\nmax.\n□IfRa< R max:\n¯SE =¯R\n1 +αrol,EEmax=B¯SE\nP0+γσ2α\u0000\n2¯R−1\u0001α\nwhere ¯Ris the constant that minimizes the convex problem\nmin\n¯R∈[Ra,Rmax]P0+γσ2α\u0000\n2¯R−1\u0001α\n¯R.\nProof. See Appendix VII-D.\nRemark 14 (Scaling of EE-SE) .Fig. 4a and 4b plot the EE as a function of the SE for the\noptimal and uniform allocation. Most gain in terms of EE (not absolute energy) is obtained at\nmedium SE. The plots are obtained by varying the rate R, all other parameters being fixed.\nRemark 15 (Optimal EE) .As shown in Fig. 4a, for a low normalized noise, the maximal EE\nis not obtained at maximal SE while it is for a higher noise, as shown in Fig. 4b.DRAFT 17\n0 2 4 6 802004006008001,000 EEmax\n¯SE\nSE[bits/s/Hz]EE[kbits/Joule]σ2= 10 mW,B= 20 MHz, αrol= 0.1\nUniform\nOptimal\nRush-to-sleep\n(a)0 0.5 1 1.5 2050100150200 EEmax\n¯SE = SE max\nSE[bits/s/Hz]EE[kbits/Joule]σ2= 5 W,B= 20 MHz, αrol= 0.1\nUniform\nOptimal\nRush-to-sleep\n(b)\nFig. 4. Energy efficiency versus spectral efficiency for optimal/uniform allocation with constant sleep mode (As4).\nIV. O PTIMAL ALLOCATION FOR PIECEWISE CONSTANT SUCCESSIVE SLEEP MODES\nFig. 3a and 3b have shown promising gains to reduce consumed power at low load. However,\nthey are still limited by the relatively high sleep power consumption Psleep. Similarly, Fig. 4a\nand 4b can seem limited as one would hope to obtain an EE that is approximately flat as a\nfunction of the SE. The reason is the same: a too high Psleep.\nTo drastically reduce energy consumption at low-to-medium load, it is of paramount impor-\ntance to implement successive sleep modes and use a frame duration long enough so that the\nsystem can enter these sleep modes. To find the optimal allocation, the iterative algorithm of\nProp. 2 can be used, which requires to solve problem (11) at each iteration. This is an integer\nprogramming problem which can have a significant complexity. If the problem is relaxed by\nconsidering Nacontinuous, it remains challenging to solve as it implies the minimization of the\nconcave function Esleep(t)(Prop. 1).\nIf the sleep power consumption is assumed piecewise constant, according to (As3), a simple\nallocation can still be found. This sleep power model was detailed in Section II-B and Fig. 1.\nFor a given Na, only sleep modes such that Ts≤(N−Na)Tcan be entered. We define\nN+\na,s=N−⌊Ts/T⌋so that sleep mode scan be used if Na≤N+\na,s. Depending on the target rate\nR, it might be unfeasible to use a given sleep mode because of the maximal power constraint\nper time slot. A minimum of active time slotsNR\nRmaxis required to satisfy it. Otherwise, the power\nper time slot would have to be higher than Pmaxto satisfy the rate constraint. Mode sis thusDRAFT 18\n0 2 4 6 8 10050100150200\nRmaxPcons,max\nR[bits/channel use]Pcons [W]σ2= 10 mW,NT= 200 ms\nUniform\nAsympt. opt.\nOpt.\nRush-to-sleep\n(a)0 0.5 1 1.5 2 2.5050100150200\nRmaxPcons,max\nR[bits/channel use]Pcons [W]σ2= 5 W,NT= 200 ms\nUniform\nAsympt. opt.\nOpt.\nRush-to-sleep\n(b)\nFig. 5. Power consumption versus load with successive sleep power model (As3).\nfeasible only ifNR\nRmax≤N+\na,s. As a result, deepest sleep modes are only possible for low values of\nR, which intuitively makes sense. For a target rate R, we define the set of feasible sleep modes\nasSR=n\ns|s∈ {0,···, S−1}andNR\nRmax≤N+\na,so\n. Moreover, we generalize the definition of\nRaper sleep mode as the constant Ra(s)that minimizes the convex problem\nRa(s) = arg min\nx≥0P0−Ps+γσ2α(2x−1)α\nx.\nWe also define ˜Rs= min( Ra(s), Rmax).\nTheorem 2. Under (As1)−(As3),(As5), asN→+∞, the solution of problem (7) is found\nby computing for all feasible sleep modes s∈ S R\nNa,s=h\nmin\u0010\nNR/ ˜Rs, N+\na,s\u0011i\n, pn,s=\n\n(2RN\nNa,s−1)σ2ifn= 0, ..., N a,s−1\n0 otherwise\nPcons,s=Na,s\nN\u0010\nP0+γσ2α\u0010\n2RN\nNa,s−1\u0011α\u0011\n+Esleep((N−Na,s)T)\nNT+O(1/N)\nand choosing among these modes the one that has the minimal Pcons,s.\nProof. See Appendix VII-E.\nRemark 16. Fig. 5a and Fig. 5b are plotted based on the same simulation parameters as Fig. 3a\nand 3b, but with a different sleep model. Using successive power modes has a drastic impact on\nthe consumed energy at low load. A key parameter to allow drastic savings is to have a largeDRAFT 19\n0 2 4 6 802004006008001,000\nSE[bits/s/Hz]EE[kbits/Joule]σ2= 10 mW,B= 20 MHz, αrol= 0.1,NT= 200 ms\nUniform\nOptimal\nRush-to-sleep\n(a)0 0.5 1 1.5 2050100150200\nSE[bits/s/Hz]EE[kbits/Joule]σ2= 5 W,B= 20 MHz, αrol= 0.1,NT= 200 ms\nUniform\nOptimal\nRush-to-sleep\n(b)\nFig. 6. Energy efficiency versus spectral efficiency with successive sleep power model (As3).\nenough frame duration so that deepest sleep modes can be used. For these figures, it was fixed\nto 200 ms. Hence, deep sleep power mode can be entered but not hibernating sleep power mode.\nRemark 17. Similarly, Fig. 6a and 6b can be compared to Fig. 4a and 4b, where only the\nsleep model differs. As ideally expected, the EE quickly reaches a plateau. To still improve this\nbehaviour, a longer frame duration can be used.\nV. O PTIMAL ALLOCATION FOR TDMA S YSTEM\nWe now extend previous results by considering a downlink transmission from the BS to K\nusers. We consider the constant power sleep model so that (As1)−(As4)hold. The users\nare multiplexed using TDMA. In the frame of Nsymbols, each symbol is allocated to the\ntransmission towards at most one user so that no inter-user interference is present. Out of the N\nsymbols, Nksymbols are allocated to user k, fork= 0, ..., K −1. The power associated to the\nn-th symbol transmitted to user kis denoted by pk,n, where n= 0, ..., N k−1andk= 0, ..., K−1.\nThe normalized noise variance σ2\nk=σ2\nnLkis considered different at each user, as each can have\na specific path loss Lk. The consumed power is then\nPTDMA\ncons =Na\nNP0+γ\nNK−1X\nk=0Nk−1X\nn=0pα\nk,n+N−Na\nNPsleepDRAFT 20\nwhere Na=PK−1\nk=0Nkso that 0≤Na≤N. We consider the generalization of problem (7) of\nminimizing the power consumption, under (As1)−(As4)and per-user rate constraints Rk. The\nproblem can be formulated as\nmin\nNk,pk,n\nn=0,...,N k−1\nk=0,...,K−1PTDMA\ncons s.t.1\nNNk−1X\nn=0log2\u0012\n1 +pk,n\nσ2\nk\u0013\n=Rk∀k,K−1X\nk=0Nk≤N. (13)\nWe assume in the following that the problem has a feasible solution, which generalizes (As5) .\n(As6) : Problem (13) is feasible. Defining the maximal per-user rate Rk,max= log2\u0010\n1 +Pmax\nσ2\nk\u0011\n,\nit implies that\nK−1X\nk=0⌈NR k\nRk,max⌉ ≤N.\nMoreover, we define the constant Rk,athat minimizes the convex problem\nRk,a= arg min\nx≥0P0−Psleep+γσ2α\nk(2x−1)α\nx.\nWe also define ˆRk= min( Rk,a, Rk,max)andˆPk= min( Pk,a, Pmax). Two regimes can be consid-\nered for Problem (13), depending if the constraintPK−1\nk=0Nk≤Nis binding or not.\nIn terms of power savings, the most promising case is the low-to-medium load regime where\nthe constraint is not binding. In that case, the problem fully decouples per-user and the linear\nregime solution of Theor. 1 can directly be used on a per-user basis.\nTheorem 3 (TDMA - linear regime) .Under (As1)−(As4),(As6), asN→+∞, ifPK−1\nk=0Rk\nˆRk≤\n1 +O(1/N), the allocation\nNk=h\nNR k/ˆRki\n, pk,n=ˆPk+O(1/N)forn= 0, ..., N k−1\nfor user k= 0, ..., K −1is an asymptotic solution of Problem (13) and\nPcons=Psleep+K−1X\nk=0RkP0−Psleep+γˆPα\nk\nˆRk+O(1/N).\nProof. See Appendix VII-F.\nRemark 18 (K= 2- TDMA) .ForK= 2, Fig. 7a and 7b plot Pconsas a function of R1andR2.\nThe optimal solution of Theor. 3 is plotted where it is valid, i.e., the asymptotic linear regime\nR0\nˆR0+R1\nˆR1≤1. As already observed in the single-user case (Fig. 3b), this regime can cover the\nwhole feasible rate region, as shown in Fig. 7b, characterized by a relatively higher noise powerDRAFT 21\n(a)σ2\n0=σ2\n1= 10 mW\n (b)σ2\n0=σ2\n1= 5 W\nFig. 7. Power consumption versus load in a K= 2 user TDMA system using optimal/uniform power allocation with constant\nsleep mode (As4). The optimal allocation is valid in the asymptotic linear regime, i.e., whenR0\nˆR0+R1\nˆR1≤1.\nimplying that ˆRk=Rk,max,∀k. As a benchmark, a fully active uniform allocation was plotted\nwhere a share Nk/N=Rk/(R0+R1)of time slots was allocated to each user.\nRemark 19 (rush-to-sleep - TDMA) .IfPK−1\nk=0Rk\nˆRk≤1, a rush-to-sleep allocation is asymptoti-\ncally optimal for each user such that Rk,max≤Rk,a. On the other hand, ifPK−1\nk=0Rk\nˆRk>1, it is\noptimal to use a fully active system and no sleep.\nIf the constraintPK−1\nk=0Nk=Nis binding, Problem (13) is coupled between users and\nchallenging to solve. As the sleep duration is zero, the power consumption becomes\nPTDMA\ncons =P0+γ\nNK−1X\nk=0Nk−1X\nn=0pα\nk,n.\nAs this regime does not allow switching-off components, relatively small energy reduction\npotentials are expected. Given space constraints, we do not provide a more detailed solution.\nOne possibility is to reduce the target rates of the users to make the constraint non-binding and\nthen use Theor. 3. Another possibility is to use conventional scheduling policies, that are not\naware of sleep capabilities, which makes sense as the system is fully active.\nVI. C ONCLUSION\nIn this work, we have proposed a fundamental study of time-domain energy-saving techniques\nin radio access. The results provide key novel insights from an information-theoretic perspective.DRAFT 22\nConsidering equal gain parallel communication channels, conventional information-theoretic\nresults state that all channels (time slots in this study) should be equally used to minimize\ntransmit (not consumed) power under a rate constraint. On the contrary, popular energy-saving\ntechniques steer towards an extreme opposite “rush-to-sleep” approach: compact transmission in\nas few time slots as possible, at maximal transmit power, to maximize sleep duration.\nUsing a realistic power consumption model, our information-theoretic study bridges the gap\nbetween these two extremes. Simple allocations are provided that allow drastic energy savings\nreaching factors of 10 at low load. At low-to-medium load, the optimal number of active time\nslots is linearly proportional to the rate, resulting in a power consumption which linearly scales\nwith the rate. At a higher load, all time slots become allocated. The rush-to-sleep approach is\nshown to be optimal in a high-noise regime but not otherwise. In a low-noise regime, it might\nbe better to use a fully active system. Moreover, the fundamental trade-off between EE and SE\nis revisited leveraging the time-domain hardware sleep capabilities. Transmitting at maximal SE\nmaximizes the SE in a high-noise regime while, in a low-noise regime, a reduced SE maximizes\nthe EE. Considering a sleep model with increasing depth complicates the study but also greatly\nincreases the energy-saving gains. For a piecewise constant model, simple allocations can still be\nfound. Finally, for a multi-user TDMA system, single-user results are applicable on a per-user\nbasis in the low-to-medium load regime, where the system should not be fully active and where\nsleep-aware energy-saving gains can be achieved.\nVII. A PPENDIX\nWe start by introducing two lemmas that will be useful in the following.\nLemma 3. An optimal solution of the following problem\nmin\np0,...,pN−1γ\nNN−1X\nn=0pα\nns.t.1\nNN−1X\nn=0log2\u0010\n1 +pn\nσ2\u0011\n=R (14)\nmust have a uniform allocation among active time slots: ∀n, n′, ifpn>0,pn′>0thenpn=pn′.\nProof. The case α= 1 was already treated in the introduction.For α= 0, the cost function\nonly depends on the number of active time slots so that a single time slot must be allocated\npower. In the following, we will use a proof by contradiction for the case 0< α < 1. Any\nnon-uniform power allocation has at least two time slots, say n= 0andn= 1(potentially using\na re-indexing), that are such that p0>0,p1>0andp0̸=p1. We consider the power allocatedDRAFT 23\nto the other time slots as fixed and optimize the cost function with respect to p0andp1only.\nFor the sake of clarity, we define ρn=pn/σ2. The reduced problem is\nmin\nρ0,ρ1˜f(ρ0, ρ1) =1X\nn=0ρα\nns.t.1X\nn=0log (1 + ρn) =Z\nwhere Z=NRlog 2−PN−1\nn=2log (1 + ρn). The constraint implies that\nρ1=eZ\n1 +ρ0−1 (15)\nso that ρ0andρ1take values only in the domain [0, eZ−1]and have a one-to-one relationship. As\nρ0→0,ρ1→eZ−1and vice versa. Moreover, the problem is symmetrical so that ˜f(ρ0, ρ1) =\n˜f(ρ1, ρ0). Using (15), the problem can be rewritten as a monovariable unconstrained problem\nmin\nρ0˜f(ρ0) =ρα\n0+ρα\n1=ρα\n0+\u0012eZ\n1 +ρ0−1\u0013α\n.\nThe derivative of ˜f(ρ0)and its limit at the bounds of its domain are given by\n˜f′(ρ0) =α\nρ1−α\n0−eZ\n(1 +ρ0)2α\nρ1−α\n1,lim\nρ0→0˜f′(ρ0) = +∞,lim\nρ0→eZ−1˜f′(ρ0) =−∞. (16)\nTo find the critical points, we set f′(ρ0) = 0 and combining with (15), we find the condition\n0 =ρ1−α\n1(1 +ρ0)−ρ1−α\n0(1 +ρ1) (17)\nwhich shows that there is always one critical point in ρ0=ρ1=eZ/2−1. Moreover, the fact that\nthe problem is symmetric implies an odd number of critical points: if there is a critical point in\n˜ρ0, there is one ineZ\n1+˜ρ0−1. Using again (15), we can rewrite the condition (17) as\n0 = ( eZ−1−ρ0)1−α(1 +ρ0)1+α−ρ1−α\n0eZ.\nThe point ρ0= 0 is not a critical point. Hence, we can restrict to ρ0>0and divide by ρ1−α\n0\n0 = ( eZ−1−ρ0)1−α(1 +ρ0)1+αρα−1\n0−eZ\n| {z }\n˜g(ρ0).\nThe function ˜g(ρ0)is infinite in ρ0= 0 and−eZforρ0=eZ−1. The number of roots\nof˜g(ρ0)and thus critical points of ˜f(ρ0)is at most equal to one plus the number of critical\npoints/alternations of ˜g(ρ0). Setting ˜g′(ρ0) = 0 gives the condition\n0 =−(1−α)(1 + ρ0)ρ0+ (eZ−1−ρ0)(1 + α)ρ0+ (eZ−1−ρ0)(1 + ρ0)(α−1)DRAFT 24\nDomain [0, eZ−1]\nIntercept ˜f(0) = ( eZ−1)α\nSymmetry ˜f(ρ0) =˜f\u0010\neZ\n1+ρ0−1\u0011\nDerivatives ˜f′(0) = + ∞,˜f′(eZ−1) =−∞\nConcavity ˜f′′(0) = ˜f′′(eZ−1) =−∞\nCritical points Always 1 in ρ0=ρ1=eZ/2−1\nPotentially 2 others symmetrical\nFig. 8. Sketch of function ˜f(ρ0).\nwhich is a quadratic equation in ρ0. It has thus at most two solutions. As a result, ˜g(ρ0)has max\ntwo alternations and ˜f(ρ0)has at most 3 critical points. The second order derivative of ˜f(ρ0)\nand its limit at the bounds of its domain are given by\n˜f′′(ρ0) =−α(1−α)1\nρ2−α\n0+1\nρ2−α\n1αeZ\n(1 +ρ0)3\u0012\n−(1−α)eZ1\n1 +ρ0+ 2ρ1\u0013\nlim\nρ0→0˜f′′(ρ0) =−∞,lim\nρ0→eZ−1˜f′′(ρ0) =−∞. (18)\nAs shown in Fig. 8, three cases can be distinguished. In case (a), there is a single critical\npoint in ρ0=ρ1=eZ/2−1which is a maximum. In cases (b) and (c), there are three critical\npoints: the middle one in ρ0=ρ1=eZ/2−1will now be a minimum (local in (b), global in (c))\nwhile the two on its sides are maxima. Hence, global minima can only be obtained for either\nρ0= 0, ρ1=eZ−1orρ1= 0, ρ0=eZ−1orρ0=ρ1=eZ/2−1. Hence, it is impossible to\nfind an optimal allocation such that p0>0,p1>0andp0̸=p1.\nLemma 4. Under (As1)−(As4), the following function is convex for x >0\nf(x) =P0−Psleep+γσ2α(2x−1)α\nx.\nProof. Under (As4),P0−Psleep≥0and thus (P0−Psleep)/xis convex. Given that the sum of\ntwo convex functions is convex, it is sufficient to show that\nγσ2α(2x−1)α\nxor equivalently g(y) =(ey−1)α\ny\nis convex for y >0(using y=xlog 2 ). Its second derivative is\ng′′(y) =(ey−1)α−2(e2y(α2y2−2αy+ 2)−ey(αy2−2αy+ 4) + 2)\ny3.\nGiven that y >0, we have directly that y3>0,(ey−1)α−2≥0and it is sufficient to show that\nh(y) =e2y(α2y2−2αy+ 2)−ey(αy2−2αy+ 4) + 2 ≥0.DRAFT 25\nUsing the Taylor series expansion ey=P+∞\nr=0yr\nr!, which converges for all y, we find\nh(y) =+∞X\nr=0(2x)r\nr!(α2y2−2αy+ 2)−+∞X\nr=0yr\nr!(αy2−2αy+ 4) + 2\n=+∞X\nr=2yr\u0012\nαα2r−2−1\n(r−2)!+α2−2r\n(r−1)!+2r+1−4\nr!\u0013\n.\nTo show that h(y)≥0fory >0andα∈[0,1], it is sufficient to show that for all r≥2\nα2r−2α−1\n(r−2)!+α2−2r\n(r−1)!+2r+1−4\nr!≥0\n↔α22r−2r(r−1) +αr(3−2r−r) + 2r+1−4≥0.\nForr= 2, this is verified as 2α2−6α+ 4 = 2( α−1)(α−2)is always positive for α∈[0,1].\nForr= 3, this is also verified as α212−α24 + 12 = 12( α−1)2is again positive for α∈[0,1].\nForr≥4, we have r(r−1)≥r2/2,3−2r−r≥ −2r+1and−2−r+4≥ −1so that\nα22r−2r(r−1) +αr(3−2r−r) + 2r+1−4≥2r−2(α2r2/2−αr8 + 8−2−r+4)\n≥2r−2(α2r2/2−αr8 + 7)\nwhich roots are in 8r±√\n50r. Given that 8r−√\n50r≥0.92r >1forr≥2, both roots are\nstrictly larger than 1 and the term is positive for α∈[0,1], which concludes the proof.\nA. Proof of Lemmas 1 and 2\nOne can first note that Lemma 1 is a particularization of Lemma 2 when P0=Esleep(t) =\n0. Hence, the result will be found as a specific case in the following. Under (As1)-(As2),\nProblem (7) can be rewritten as\nmin\nNaNa\nNP0+\"\nmin\np0,...,pNa−1γ\nNNa−1X\nn=0pα\nn#\n+Esleep((N−Na)T)\nNT(19)\nwhich shows that only the second term depends on the power allocation, i.e., the term defined\nasPld. From Lemma 3, an optimal allocation needs to be uniform in the number of activated\ntime slots. For a given Na, the rate constraint fixes the transmit power per active time slot\npn=\u0010\n2RN\nNa−1\u0011\nσ2ifn= 0, ..., N a−1.\nHence, the problem can be reformulated as finding the optimal number of active slots Nathat\nminimizes the consumed power, i.e., Problem (11) in Lemma 2. Moreover, under (As3)−(As4),\nthe problem becomes\nmin\nNaPsleep+Na\nN\u0010\nP0−Psleep+γσ2α\u0010\n2NR\nNa−1\u0011α\u0011\n.DRAFT 26\nWe define x=RN\nNaand relax the problem by considering xas continuous\nmin\nxf(x) =P0−Psleep+γσ2α(2x−1)α\nx.\nFrom Lemma 4, we know that f(x)is convex. Given the definition of xand the integer nature\nofNa,xcan only take discrete values in practice. Given the fact that f(x)is convex, it is\nguaranteed that one of the neighboring possible values of Ra= arg min f(x)is optimal. As a\nresult, the solution is given by either Na=⌈RN\nRa⌉,Na=⌊RN\nRa⌋orNifRN\nRa> N . Using the\nceil-floor notation concludes the proof of Lemma 2. The above result can also be particularized\nto the problem of Lemma 1 by setting P0=Esleep(t) = 0 and the problem simplifies to\nmin\nxf(x) =(2x−1)α\nx↔min\nyg(y) =(ey−1)α\ny.\nwhere y=xlog 2 . Setting its derivative to zero, we find\nαeyy=ey−1↔y=W\u0010\n−α−1e−α−1\u0011\n+α−1↔Ra=W\u0010\n−α−1e−α−1\u0011\n+α−1/log 2,\nwhich concludes the proof of Lemma 1.\nB. Proof of Proposition 2\nThe algorithm is initialized by the solution of the relaxed problem assuming that no max\npower constraints are binding. The solution is then given by Theor. 2. If the solution is such\nthatpn≤Pmax,∀n, the problem is solved. On the other hand, if, for at least one time slot\npn> P max, at least one of the constraints must be binding. Hence, the algorithm allocates the\nmaximal power Pmaxto one additional time slot. The rate constraint on the remaining time slots\nis then adapted. The power allocation is re-computed assuming that no max power constraint is\nbinding on the remaining time slots not yet set to Pmax. Again, the solution is given by Theor. 2.\nAgain, the algorithm checks if the max constraint is verified. If yes, the algorithm has converged.\nIf not, it enters a novel iteration and so on until convergence.\nC. Proof of Theorem 1\nLet us consider one by one four different cases. On the one hand, the exponential regime\nmentioned in the theorem where R > ˜Rand i) ˜R=Rmaxor ii) ˜R=Ra. On the other hand, the\nlinear regime mentioned in the theorem where R≤˜Rand iii) ˜R=Raor iv) ˜R=Rmax.\nCase i) : This case is not applicable according to (As5)as it is unfeasible to have R > R max.DRAFT 27\nCase ii) : The case together with (As5)implies Rmax≤R > R a. From Lemma 2, we can\nfind that Na=Nand the corresponding power allocation, which is well feasible as it does not\nviolate the Pmaxconstraint. The exponential regime result of Theor. 1 is then found.\nCase iii) : This case implies R≤Ra≤Rmax. Let us first consider that the maximal power\nconstraint per time slot is not active such that we can use the result of Lemma 2. If R≤Ra,\nthe optimal number and ratio of active time slots are\nNa=⌊RN/R a⌉=\u0014RN\nRa\u0015\n+ϵ1=RN\nRa+ϵ2\nNa\nN=\u0014RN\nRa\u0015\n/N+ϵ1\nN=R\nRa+ϵ2\nN.\nwhere |ϵ1|<1and|ϵ2|<1. The optimal ratio Na/Nasymptotically converges to R/R aand the\nsame occurs if the optimal number of time slots Nais approximated using a rounding operator\ninstead of the ceil-floor operator. Using this result, as N→+∞, the optimal power allocation\nper active time slot of Lemma 2 can be rewritten as\npn=\u0010\n2RN\nNa−1\u0011\nσ2=Pa+O(1/N) (20)\nwhere Pa=\u0000\n2Ra−1\u0001\nσ2andNacan be the ideal value ⌊RN/R a⌉or its approximation using\nthe rounding operator. Given that Ra≤Rmax, this allocation does not violate the Pmaxconstraint\nand the result is feasible. The power consumption becomes\nPcons=Psleep+Na\nN(P0−Psleep+γPα\na+O(1/N)) =Psleep+RP0−Psleep+γPα\na\nRa+O(1/N).\nCase iv) : This case implies R≤Rmax≤Raand thus Pa> P maxsuch that allocation (20) is not\nfeasible. As an alternative, the iterative Algorithm 1 can be used and simplified in the asymptotic\nregime. Indeed, as N→+∞, at each iteration, the algorithm allocates a constant power Pa\n(independent of R) to active time slots, not yet set to Pmax. At the convergence of the algorithm,\nthe allocation will have approximately RN/R maxactive time slots with maximal power Pmax\nand rate Rmax. As a result, as N→+∞, at the optimum, Na/N= [R/R max] +O(1/N) =\nR/R max+O(1/N)and the allocation\nNa=\u0014RN\nRmax\u0015\n, pn=Pmaxforn= 0, ..., N max\nis asymptotically optimal and achieves a consumed power\nPcons=Psleep+RP0−Psleep+γPα\na\nRmax+O(1/N).\nCases iii) and iv) can be written more compactly using the definitions of ˜Rand˜P, giving the\nlinear regime result of Theor. 1.DRAFT 28\nD. Proof of Corollary 2\nThe results of Corol. 2 can be found by minimizing the EE expression in the two regimes\nof Corol. 1. In the regime where SE≤˜R/(1 +αrol), it is clear that the EE is maximized for\nthe largest SE, i.e., when SE = ˜R/(1 + αrol). Moreover, if Ra≥Rmax, we have ˜R=Rmax\nand the second regime of Corol. 1 is not feasible. The optimal SE corresponds to the maximal\nSE,SEmax=Rmax/(1 + αrol). On the other hand, if Ra< R max,˜R=Raand the regime\nSE>˜R/(1 +αrol)can be entered. The optimization over ¯Rthen provides the optimum and can\nonly improve the optimum as ¯Ris allowed to take value Ra.\nE. Proof of Theorem 2\nIt is direct to see that one should choose the optimal allocation among feasible sleep modes.\nUnder (As3), the sleep energy consumption at time tif sleep mode sis used is Esleep,s(t) =\nEsleep(Ts) + (t−Ts)Pswhere Esleep(Ts) =Ps−1\ns′=0Ps′(Ts′+1−Ts′).2The consumed power using\nsleep mode scan then be written as\nPcons,s=˜Es\nNT+Na\nNP0+γ\nNNa−1X\nn=0pα\nn+N−Na\nNPs\nwhere ˜Es=Esleep(Ts)−TsPs. This form is similar to the one given in (6), under (As4). The\nsole differences are the presence of Psinstead of Psleepand the constant ˜Es/(NT), which affects\nthe cost function but does not impact the optimization. The result of Theorem 1 can then be\nused: uniform allocation among Na,sactive mode is optimal. The only difference is the fact that\nthe maximal value of Na,sisN+\na,sinstead of N, so that the sleep duration is sufficient to enter\nmode s. As a result, we find Na,s=h\nmin\u0010\nNR/ ˜Rs, N+\na,s\u0011i\n.\nF . Proof of Theorem 3\nIf the constraintPK−1\nk=0Nk≤Nis not binding, Problem (13) is fully decoupled between users\nand can be solved by solving for k= 0, ..., K −1an independent per-user problem\nmin\nNk,pk,n\nn=0,...,N k−1Nk\nNP0+γ\nNNk−1X\nn=0pα\nk,n+N−Nk\nNPsleep s.t.1\nNNk−1X\nn=0log2\u0012\n1 +pk,n\nσ2\nk\u0013\n=Rk\nso that the asymptotic solution of Theorem 1 can be used giving Nk=h\nNR k/ˆRki\n. If the\nconstraintPK−1\nk=0Nk≤Nis not violated, this is the asymptotic solution of Problem (13). From\n2No deeper sleep mode than sis considered even if Ts+1< t, which could decrease sleep energy consumption. Still, this\ndoes not affect the optimization result a deeper sleep mode will perform better and will be chosen instead.DRAFT 29\nSection VII-C, we know that, as N→+∞,Nk/N=Rk/ˆRk+O(1/N)and the constraint can\nthus be equivalently written asPK−1\nk=0Rk/ˆRk≤1 +O(1/N).\nACKNOWLEDGMENT\nThe author would like to thank his colleagues from the DRAMCO-KU Leuven lab and Dr. P ˚al\nFrenger for many fruitful discussions and valuable comments.",
      "metadata": {
        "filename": "Information-Theoretic Study of Time-Domain Energy-Saving Techniques in Radio Acc.pdf",
        "hotspot_name": "Aluminium_Die_Casting",
        "title": "Information-Theoretic Study of Time-Domain Energy-Saving Techniques in\n  Radio Access",
        "published_date": "2023-03-31T08:57:20Z",
        "pdf_link": "http://arxiv.org/pdf/2303.17898v2",
        "query": "die casting aluminium radiator energy consumption reduction process optimization techniques"
      }
    },
    "MEDPNet_ Achieving High-Precision Adaptive Registration for Complex Die Castings": {
      "full_text": "MEDPNet: Achieving High-Precision Adaptive\nRegistration for Complex Die Castings\nYu Du1, Yu Song1, Ce Guo2, Xiaojing Tian1*, Dong Liu2,\nMing Cong2\n1*School of Mechanical Engineering, Dalian Jiaotong University, 794\nHuanghe Road, Dalian, 116028, Liaoning, China.\n2School of Mechanical Engineering, Dalian University of Technology, 2\nLinggong Road, Dalian, 116024, Liaoning, China.\n*Corresponding author(s). E-mail(s): tzy@djtu.edu.cn;\nAbstract\nDue to their complex spatial structure and diverse geometric features, achiev-\ning high-precision and robust point cloud registration for complex Die Castings\nhas been a significant challenge in the die-casting industry. Existing point cloud\nregistration methods primarily optimize network models using well-established\nhigh-quality datasets, often neglecting practical application in real scenarios.\nTo address this gap, this paper proposes a high-precision adaptive registration\nmethod called Multiscale Efficient Deep Closest Point (MEDPNet) and intro-\nduces a die-casting point cloud dataset, DieCastCloud, specifically designed to\ntackle the challenges of point cloud registration in the die-casting industry. The\nMEDPNet method performs coarse die-casting point cloud data registration\nusing the Efficient-DCP method, followed by precision registration using the\nMultiscale feature fusion dual-channel registration (MDR) method. We enhance\nthe modeling capability and computational efficiency of the model by replacing\nthe attention mechanism of the Transformer in DCP with Efficient Attention\nand implementing a collaborative scale mechanism through the combination of\nserial and parallel blocks. Additionally, we propose the MDR method, which\nutilizes multilayer perceptrons (MLP), Normal Distributions Transform (NDT),\nand Iterative Closest Point (ICP) to achieve learnable adaptive fusion, enabling\nhigh-precision, scalable, and noise-resistant global point cloud registration. Our\nproposed method demonstrates excellent performance compared to state-of-the-\nart geometric and learning-based registration methods when applied to complex\ndie-casting point cloud data.\n1arXiv:2403.09996v1  [cs.CV]  15 Mar 2024Keywords: Complex Die Castings, point cloud registration, efficient Attention,\nmultiscale feature fusion\n1 Introduction\nComplex Die Castings are critical components in industries such as manufacturing,\ntransportation, and defense, characterized by intricate structures and diverse forms.\nHigh-quality three-dimensional reconstruction of their overall surfaces through point\ncloud registration plays a vital role in enhancing product molding quality and ensuring\nsafety in subsequent use. Recent work has made substantial progress in fully automatic,\n3D feature-based point cloud registration. At first glance, benchmarks like 3DMatch\n[1] appear to be saturated, with multiple state-of-the-art (SoTA) methods reaching\nnearly 95% feature matching recall and successfully registering over 80% of all scan\npairs [35]. However, due to the complexity of the spatial structure of Die Castings and\ntheir susceptibility to complex background interferences such as casting reflections,\noil contamination, machining marks, etc., there is currently no effective method to\nachieve high-precision point cloud registration of die-casting data. We believe that a\nhigh-precision adaptive method is the key to addressing this issue.\nCurrently, representative point cloud registration methods can be broadly catego-\nrized into two main types: those based on geometric properties[24][29][6][7]] and those\nbased on deep learning[35][3][36][27][8]. The method for point cloud registration, as\nshown in Fig 1, aims to calculate the optimal transformation parameters (R, T) (three\nrotation angles in R and three translation components in T) from the common parts\nof the data known as correspondences[49]. In recent years, with the rapid development\nof deep learning[4][12], it has found widespread application in point cloud registration\ntasks. Deep learning-based registration algorithms, including DCP[8], PointNetLK[32],\nGeoTransformer[18], etc., have significantly improved the speed and performance of\npoint cloud registration tasks. However, these methods often require more computa-\ntional resources, and their performance is frequently constrained by the quality of the\ndataset, often leading to suboptimal results in practical applications and difficulty in\nachieving stable high-precision point cloud registration effects.\nRepresentative point cloud registration algorithms based on geometric proper-\nties include Iterative Closest Point (ICP)[24] and Normal Distributions Transform\n(NDT)[29]. Such methods have low hardware requirements, are easy to implement,\nexhibit strong interpretability, and do not involve time-consuming training processes.\nHowever, they face challenges such as sensitivity to local minima or poor generaliza-\ntion, reliance on manually crafted features to distinguish corresponding relationships,\nand significant impact from the designer’s experience and parameter tuning capabil-\nities. Additionally, these methods often consume considerable time, posing potential\nbottlenecks in real-time applications.\nIn addressing this challenge, given the intricate nature of die-cast components, we\nhave devised a highly efficient adaptive registration method and created the DieCast-\nCloud point cloud dataset to validate the efficacy of the approach.DCP method\nperforms well in point cloud registration tasks, but it still has certain stability issues\n2when faced with complex die-cast point cloud data with diverse surface feature vari-\nations. To address this problem, we introduce Efficient Attention[14] to replace the\nTransformer Attention[48] in DCP. By combining serial and parallel blocks, Efficient\nAttention efficiently captures global feature information and improves computational\nefficiency. Efficient Attention differs from traditional self-attention mechanisms in\nterms of implementation. Traditional self-attention mechanisms[42][22][41] generate\nan attention map for each position to aggregate input values and produce outputs. In\ncontrast, Efficient Attention does not generate separate attention maps for each posi-\ntion. Instead, it interprets the keywords of attention as global attention maps, with\neach global attention map corresponding to a semantic aspect of the entire input.\nEfficient Attention uses these global attention maps to aggregate values and generate\na global context vector. Then, each position uses a set of coefficients to weight the\nglobal context vector and adjust its own representation. This approach gives Efficient\nAttention advantages in terms of memory and computational efficiency, as it does\nnot require calculating similarities between each pair of positions, thereby reducing\ncomputational and storage complexities. Although the improved DCP network pro-\nvides initial registration for casting point clouds, it has certain limitations in terms of\naccuracy and stability, restricting its practical feasibility in industrial applications.\nTo address this issue, we propose a multi-scale adaptive fine registration method.\nBuilding upon a favorable initial pose obtained from the DCP network, we further\nenhance the precise registration of point cloud data through the fusion of multi-scale\nfeature information. In order to ensure the robustness and verifiability of fine regis-\ntration, we improve and integrate ICP and NDT. Specifically, we initially perform\nmulti-scale feature extraction on the die-casting point cloud data to avoid the impact\nof feature loss or noise interference on point cloud registration. Subsequently, we use\na dual-channel approach to obtain transformation matrices from NDT and ICP that\nhave undergone multi-scale feature fusion. We then apply nonlinear weighting to the\nobtained transformation matrices, endowing them with good adaptability and stable\nregistration accuracy.\n•We replaced the Transformer’s Attention in DCP with Efficient Attention and imple-\nmented a collaborative scale mechanism through a combination of serial and parallel\nblocks to improve both the modeling capability and computational efficiency of the\nmodel.\n•We propose a Multiscale feature fusion dual-channel precision registration(MDR)\nmethod and supported our experimental details through ablation experiments under\nvarious scenarios.\n•We established the point cloud dataset DieCastCloud to address the challenge of\nscarce high-quality point cloud data in the die casting industry.\n2 Related Work\nDCP (Deep Closest Point): DCP[8] is a representative learning-based method\nfor point cloud registration[32][18][9]. The primary objective of the DCP method is\n3Fig. 1 Illustrates the point cloud registration process for a die-cast part . **”View 1” and\n”View 2” correspond to point clouds X and Y under different views.** Through an encoder, the point\ncloud data of the die-cast part are converted into data in feature space. Then, from the common\nparts of the unaligned point cloud pairs, the optimal transformation parameters R, t are calculated\nto obtain the best transformation matrix T∗, where R represents the rotation angles, t represents the\ntranslation components, and T∗is the best transformation matrix.\nto address issues encountered by traditional methods such as 4PCS(4-points congru-\nent sets for robust pairwise surface registration)[2] and CPD(Coherent point drift)[7],\nwhich are prone to noise interference and susceptible to getting trapped in local\noptima. DCP utilizes deep neural networks[51] to learn representations of point clouds\nand employs these learned representations for point cloud registration. Specifically,\nthe DCP[8] method comprises two sub-networks: a feature extraction network and a\ntransformation prediction network. The feature extraction network is responsible for\nextracting local and global geometric features[38] from the input point clouds, while\nthe transformation prediction network predicts the rigid transformation required to\nalign two point clouds.\nTo tackle the matching problem in point cloud registration[1][19][9][2][50], DCP[8]\nadopts the approach of Pointer Networks[31]. Pointer Networks use attention mecha-\nnisms to select positions in the input sequence, addressing the challenge of predicting\ndiscrete labels. By predicting a position distribution at each output step, Pointer Net-\nworks can be considered as ”soft pointers” for selecting matching positions. The entire\nnetwork is differentiable, allowing for end-to-end training[55]. HSGM [21][16] proposes\na hierarchical similarity graph module to relieve the conflict of backbone networks\nand mine the discriminative features. Additionally, Transformer[48] models are uti-\nlized to learn contextual information of point clouds, enabling the model to capture\nglobal feature information. However, DCP still exhibits certain limitations when deal-\ning with point cloud data with significant initial pose differences. In the presence of\nadded Gaussian noise[52], although DCP[8] demonstrates better robustness compared\nto methods like FGR[15], it is still subject to some degree of influence.\nAn Adaptive Registration Method Based on Multimodal Data: The cur-\nrent trend in point cloud tasks is the increasing popularity of multimodal data[5][39].\nGeometry-based methods[23][43][45], in the context of point cloud registration, involve\nutilizing geometric features such as point positions, distances, and orientations to\nestablish correspondences and achieve alignment between point clouds. These meth-\nods typically aim to find the optimal rigid transformation to minimize geometric\n4disparities between point clouds, ensuring accurate registration. ICP (Iterative Clos-\nest Point)[24] and NDT (Normal Distributions Transform)[29] are two of the most\nrenowned geometry-based point cloud registration methods, particularly suitable for\nscenarios with local overlap and small-scale rigid transformations.\nICP (Iterative Closest Point)[24] is a classical method for point cloud registration,\nwith the primary goal of iteratively finding the optimal rigid transformation between\ntwo point clouds to align them as closely as possible[57][58]. The core idea of ICP\ninvolves iteratively mapping points from the target point cloud to the reference point\ncloud and updating the rigid transformation based on the corresponding mappings.\nThis iterative process continues until convergence is achieved, ultimately realizing\nthe best possible alignment between the two point clouds. NDT (Normal Distribu-\ntions Transform)[29] is a method used for point cloud registration, and its core idea\ninvolves describing the local structure of each point cloud by modeling the normal\ndistribution of points. By mapping each point in the point cloud to its corresponding\nGaussian distribution[53], NDT represents the point cloud as a set of probability den-\nsity distributions[56]. During the registration process, the method adjusts the rigid\ntransformation[54] by minimizing the disparity in probability density distributions\nbetween the two point clouds, thereby achieving optimal point cloud alignment[59][60].\nWhile both geometric-based methods[24][29][2][45] and their variants[50][23] have\nbecome increasingly mature, integrating them into practical engineering applications\nremains a highly challenging task. The ICP method performs well in scenarios with\nlocal overlap and small-scale rigid transformations but is sensitive to noise and prone to\ngetting stuck in local optima. The NDT method demonstrates advantages in handling\nlarge-scale[20], sparse, or point clouds with complex geometric structures; however, its\nperformance is constrained by the choice of parameters.\n3 Proposed Method\n3.1 Overview\nGiven two sets of point clouds X={xi∈R3|i= 1, . . . , N }andY={yi∈\nR3|i= 1, . . . , M }, the objective is to estimate rigid transformations Ti={Ri, ti}for\ni= 1, . . . , N to align these two point clouds. In Fig 2, we present the architecture\nof MEDPNet. In brief, we embed the acquired die-casting part point cloud data into\na high-dimensional space using DGCNN[10] and encode the contextual information\nwith the Efficient Attention[14] module, finally estimating the alignment using a dif-\nferentiable SVD layer][47]. In which, xP\niis the embedding of point iin the P-th layer,\nandhP\nθis a nonlinear function in the P-th layer parameterized by a shared multilayer\nperceptron (MLP). The forward mechanism is given by:\nxP\ni=hP\nθ(xP−1\ni) (1)\nWe input the collected unaligned input point clouds X and Y into the same space,\nwhere we embed each point of the two input point clouds individually, and iterate over\nthe features of each point in the input point clouds, represented by the aggregation\n53D SensorRobot𝑿\n𝒀DGCNNSharedParameterTransformerℱ𝒳ℱ𝒳ℱ𝒴ℱ𝒴Φ𝒳Φ𝒴Pointer𝑌!𝑚(𝒙\",𝒴)𝑹#$=𝑽𝑼!𝑡#$=−𝑹#$𝒙0+𝒚0\t(a) Efficient DCP\n(b) Transformer(b) DGCNN\nTransfor-mation Matrix(b) Efficient Transformer\n(c) MDRFig. 2 The architecture of MEDPNet. In the diagram, part (a) shows the structure\nof Efficient DCP, part (b) illustrates the composition of Efficient Attention, and part (c)\noutlines the framework of the MDR method, with a detailed exposition of its details. The\nMEDPNet method collects point cloud data of the same die-casting part under different\npostures through a robotic arm equipped with a 3D sensor, inputs the unaligned point cloud\npairs into Efficient DCP for preliminary registration, and then refines the alignment through\nMDR to preserve essential feature information.\nfunction as:\nFx={xP\n1, xP\n2, . . . , xP\ni, . . . , xP\nN}, (2)\nand\nFy={yP\n1, yP\n2, . . . , yP\ni, . . . , yP\nN} (3)\nDGCNN constructs a k-NN (k-nearest neighbor) graph M, nonlinearly acquires edge\nvalues at edge endpoints, and performs per-vertex aggregation at each layer. Unlike\nPointNet[33], which extracts independent information from each point, DGCNN\nexplicitly incorporates local geometric shapes into its representation. This is achieved\nthrough the forward mechanism:\nxP\ni=f({hP\nθ(xP−1\ni, xP−1\nj)∀j∈Ni}) (4)\nwhere Nirepresents the set of neighbors of point iin the k-NN graph, ensuring that\nlocal geometric features are considered during the aggregation process. In the task of\ndie-casting part point cloud registration, DGCNN achieves higher quality registration\nperformance by leveraging these local geometric information.\n6Softmax\nQ K VOutput Feature Map\nInput Feature MapN × CN × C N × C C × NC × CN × CFig. 3 Illustration of the architecture of efficient attention .Where the input feature map\nundergoes a transformation into three distinct components: Queries ( Q), Keys ( K), and Values ( V).\nThese components facilitate a self-attention schema by computing attention scores between Qand\nK, followed by a softmax normalization to acquire a probabilistic weight distribution. The weighted\nsum of these probabilities with Vculminates in the output feature map, encapsulating a dynamic\nrepresentation of salient features pivotal for subsequent layers of the network to process. This mech-\nanism underpins the network’s capacity to accentuate pertinent information within the feature space\nselectively.\n3.2 Efficient Attention\nBefore introducing Efficient attention, let’s first discuss the concept of dot-product\nattention. Dot-product attention is a fundamental attention mechanism commonly\nused in models like Transformers. For a given query vector Q, key vector K, and value\nvector V, the computation of dot-product attention is as follows:\nAttention( Q, K, V ) = softmax\u0012QKT\n√dk\u0013\nV (5)\nHere, dkis the dimensionality of query/key vectors, QKTrepresents the dot product\nbetween query vector Qand key vector K, and it is scaled by√dkto stabilize gra-\ndient magnitudes. The softmax function[46] normalizes the dot product results into\nattention weights, which are then used to weight the value vector Vto generate the\nfinal output. This mechanism allows the model to dynamically allocate attention based\non the similarity between queries and keys, capturing relationships between different\npositions in the input sequence.\nThe principle of efficient attention is to optimize the traditional attention mecha-\nnism, particularly in addressing the challenges of high computational complexity and\nsignificant memory consumption when processing long sequence data. By reducing\nredundancies in computation and employing more efficient computational strategies,\nsuch as low-rank factorization and kernel techniques, it approximates the key operation\nQKTin the standard self-attention mechanism, as shown in Fig 3.\n7In the standard self-attention mechanism, the input sequence X∈Rn×dundergoes\nlinear transformations to obtain queries Q, keys K, and values V, where Q=XW Q,\nK=XW K,V=XW V, and WQ,WK,WV∈Rd×dkare the corresponding weight\nmatrices, with dkbeing the feature dimension.\nTraditionally, attention weights are obtained by computing QKTand applying the\nsoftmax function, i.e.,\nAttention( Q, K, V ) = softmax\u0012QKT\n√dk\u0013\nV (6)\nThis step has a computational complexity of O(n2dk), which for long sequence data,\nresults in significant computational burden and memory requirements.\nEfficient attention introduces an approximation technique to reduce this complex-\nity, specifically, it uses a form\nAttention( Q, K, V )≈softmax\u0012ϕ(Q)ϕ(K)T\n√dk\u0013\nV (7)\nfor approximation, where ϕ(·) is a nonlinear function mapping to a lower-dimensional\nfeature space, effectively reducing the required computational power and storage space.\nThis mapping not only reduces the need for direct computation of QKTbut also, by\nselecting an appropriate ϕfunction, can lower the computational complexity of the\nattention mechanism from O(n2dk) toO(nmd k), where mis the dimension of the\nmapped lower-dimensional space, significantly smaller than the length of the input\nsequence n.\nPrecisely for these reasons, Efficient attention significantly enhances computational\nand storage efficiency in die-cast parts point cloud registration tasks by optimizing\nthe attention mechanism. It effectively manages long-sequence dependencies, accu-\nrately captures changes and spatial relationships between point clouds, achieves\nhigh-precision registration, and broadens the application scope in resource-constrained\nenvironments.\n3.3 Adaptive Multi-scale Patch Matching for Registration\nFirst, let’s review the geometric point cloud registration method. Its core idea involves\niteratively optimizing the rigid transformation {R, t}, where Ris the rotation matrix\nandtis the translation vector. The objective is to minimize the distance Tbetween\ntwo point clouds, achieving their alignment. The expression for this can be given as\nfollows:\nT∗= arg min\nTNX\ni=1∥f(xi)−yi∥2(8)\nour approach primarily adopts the ICP (Iterative Closest Point) method and the NDT\n(Normal Distributions Transform) method. The core idea of the ICP method is to\nachieve point cloud registration by iteratively optimizing the rigid transformation {R,\n8Fig. 4 Adaptive parameter optimization . First, we input the formula to be optimized into\nan MLP and use backpropagation to optimize the weights. Next, through a self-updating filtering\nmechanism, we iterate to find the smallest registration error, iteratively updating the adaptive hyper-\nparameter ε.\nt}to minimize the distance Tbetween point clouds. Its expression is:\nT∗= min\nR,tX\ni∥Rxi+t−yi∥2(9)\nHere, ICP primarily aligns point clouds through iterative optimization. In the process\nof minimizing the objective function, adjustments to Randtare made to bring the\ntwo point clouds as close together as possible in space.\nThe NDT method describes the local structure of point clouds by modeling the\nnormal distribution of each point. Its optimization objective is formulated as:\nT∗= min\nR,tX\ni1\n2(µxi−µyi)T\n2Σ−1\ni(µxi−µyi) (10)\nµxiandµyidenote the mean of the normal distribution corresponding to points in\ntwo point clouds, and Σ irepresents the covariance matrix of the normal distribution.\nThe goal is to adjust Randtto make the two point clouds as consistent as possible\nin terms of normal distribution, minimizing the objective function.\nMulti-scale Feature Fusion Module: In both ICP (Iterative Closest Point) and\nNDT (Normal Distributions Transform), we introduced a multiscale feature fusion\nmodule aimed at addressing key challenges in point cloud registration, such as local\nminima and sensitivity to initial values. Our approach first utilizes the Feature Pyra-\nmid Network (FPN) [17] method to generate multiple scales of point clouds through\n9downsampling. At each scale, the ICP algorithm is independently applied to find the\noptimal rigid transformation. Subsequently, the coarse-scale registration results are\npropagated to finer scales, resulting in the final registration outcome and transforma-\ntion matrix. We have successfully implemented feature fusion on die-cast point cloud\npairs at kdifferent scales.\nThe multiscale ICP method aims to minimize the registration error across all scales\nkand corresponding points iby finding the optimal rigid transformation Rkandtk.\nThis approach allows us to comprehensively tackle registration challenges at multiple\nscales, thereby enhancing the algorithm’s robustness in diverse scale environments.\nThe expression for this method is given by:\nT∗= min\nRk,tkX\nkX\ni∥Rkxk\ni+tk−yk\ni∥2(11)\nsimilarly, multiscale Normal Distributions Transform (NDT) minimizes the cumulative\nerror of the normal distribution for corresponding points at each scale, utilizing the\nMahalanobis distance metric (measured by the difference between the inverse covari-\nance matrix and the normal distributions). This optimization seeks to find the optimal\nrigid transformation Rkand translation vector Tkat each scale. This enables precise\nregistration of point cloud Xwith target point cloud Yin the normal distribution\nacross multiple scales through adjustments in rigid transformation and translation. It\ncan be expressed as:\nT∗= min\nRk,TkX\nkX\ni1\n2(µk\nxi−µk\nyi)TΣ−1\ni(µk\nxi−µk\nyi) (12)\nDual Channel Fusion Module: Previous studies have primarily focused on\nenhancing the performance of the ICP (Iterative Closest Point) and NDT (Normal Dis-\ntributions Transform) methods, yet they have overlooked the importance of ensuring\nstability under conditions of high precision. This issue becomes particularly evident\nwhen dealing with complex die-cast point cloud data, where the ICP algorithm may\nperform excellently on point cloud data ”a,” while the NDT algorithm shows superior\nperformance on point cloud data ”b.” To overcome this challenge, we propose a novel\ndual-channel fusion module. Through multi-scale feature fusion, this module enables\nthe ICP and NDT methods to obtain rigid transformation matrices T1andT2, respec-\ntively. We input 300 pairs of rigid transformation matrices from the ICP and NDT\nmethods into an MLP for learnable self-feedback weighting and iterate the weights of\nthe optimal registration results by minimizing the registration error, as shown in Fig\n4. The MLP consists of three fully connected layers, with neuron counts of 32, 64, and\n32, respectively. Here, we opt for the Huber loss function, expressed as:\nLδ(a) =(\n1\n2a2for|a| ≤δ,\nδ(|a| −1\n2δ) otherwise.(13)\n10Where a=l−ˆlrepresents the prediction error, i.e., the difference between the actual\nvalue land the predicted value ˆl.δis a threshold parameter that determines the point\nat which the loss function transitions from squared error to linear error. When the\nabsolute value of the error is less than or equal to δ, the loss function behaves like the\nsquare of the error (similar to MSE), imposing a heavier penalty for smaller errors\nto encourage more precise fitting. Conversely, when the absolute value of the error\nexceeds δ, the loss function becomes linear (similar to MAE), reducing the penalty for\nlarger errors and enhancing the model’s robustness.\nAlthough the merged matrix Tobtained at this point demonstrates certain reli-\nability, repeated tests have shown that the model’s accuracy decreases when facing\nunfamiliar samples. We hypothesize that this instability might be due to the limited\nsample size, making it difficult for the model to learn the complete features of die-\ncast part point clouds. However, due to the irreplicability of die-cast samples and\nthe industrial production cycle’s inability to accommodate the training duration for a\nlarge volume of samples, we introduced the hyperparameter ε. Initially, we hoped to\ndirectly obtain optimal weights and εthrough the MLP mechanism, but the limited\ntraining samples and excessive number of parameters to be optimized led to unsatis-\nfactory results. To address this, we added a self-updating filtering mechanism on top\nof the MLP. With the determination of optimal weights W∗\n1andW∗\n2, we use the root\nmean square error (RMSE) feedback from each iteration to determine the correspond-\ningε, choosing the εassociated with the minimum RMSE as the input for the next\niteration, as shown in Fig 4.The formula for RMSE is as follows:\nRMSE =vuut1\nNNX\ni=1(li−ˆli)2 (14)\nWhere Nis the number of samples, liis the actual value of sample i, and ˆliis the\npredicted value for sample i.Finally,we can obtain:\nT∗=W∗\n1·T1+W∗\n2·T2+ε∗(15)\nIn this setup, we achieved learnable adaptive registration through a multilayer per-\nceptron and a self-updating filtering mechanism, obtaining desirable results on the\ndie-cast part point cloud dataset DieCastCloud.\n4 Experiment and Analysis\nIn this section, we will conduct comparative experiments to assess the effectiveness\nof our approach. We first introduce the details of the experiments in Section 4.1.\nIn Section 4.2, we evaluate the Efficient DCP method on our die-cast dataset and\nperform ablation experiments to ensure the method’s effectiveness. In Section 4.3, we\nintroduce the an adaptive registration method based on multimodal data, providing\ncorresponding experiments at each step. In Section 4.5, we will present an overall\nintroduction to our method, MEDPNet (Multimodal Efficient Deep Closest Point),\nand compare it with the current state-of-the-art methods.\n114.1 Implementation Details\nThis experiment utilizes Open3D[30] 1.2.0 and PCL 1.9.1 to implement algorithm\nexecution in Python and C++. The experimental platform is Ubuntu 18.04 system,\nwith PyTorch[37] version 1.8.1, CUDA version 11.1, GPU=RTX 3090 (24GB) * 1,\nCPU=15 vCPU AMD EPYC 7642 48-Core Processor. To test the generalization of\ndifferent models, we will split DieCastCloud into training and testing sets.\nDue to the complexity of the surface features of die cast parts, unlike the approach\nin PointNet[33] experiments of uniformly sampling 1024 points on the model’s outer\nsurface, we opted to sample 4096 points. This decision was based on an understanding\nof the complexity of die cast surface features, aiming to more comprehensively preserve\nthe point cloud’s feature information, thereby enhancing the accuracy and reliability\nof subsequent registration.\nTo ensure consistency and standardization in data processing, we performed a\nseries of preprocessing steps on the collected point cloud data. Initially, the point cloud\ndata was centered at the origin and scaled to fit within a unit sphere. Throughout\nthis process, we only used the three-dimensional coordinates (x, y, z) of the points as\ninput features, without introducing any additional attribute information, to accurately\nassess the model’s ability to recognize and process geometric shapes themselves.\nThe initial pose has a critical impact on point cloud registration. To better quantify\nthe performance of coarse registration, we employed multiple error metrics, including\nMean Squared Error (MSE), Root Mean Squared Error (RMSE), and Mean Absolute\nError (MAE), to ensure the reliability of the method. Moreover, considering prac-\ntical applications in the die casting industry, we primarily focused on Root Mean\nSquared Error (RMSE) and registration time (s) during fine registration, where all\nangle measurements were made in degrees ( °).\n4.2 Datasets\nThe DieCastCloud dataset contains 2,000 point cloud data, including 5 different types\nof die-cast parts. This dataset is randomly divided into a training set and a test\nset, with proportions of 0.8 and 0.2, respectively. In practical applications in the die-\ncasting industry, the purpose of point cloud registration is to enhance the completeness\nof the point cloud data while preserving key features, to ensure high-precision 3D\nreconstruction and facilitate product quality control. Unlike other datasets[28][25],\nhere, to ensure the practical feasibility of the method, the overlap rate of point cloud\ndata in DieCastCloud is set to be greater than 85%.\nWe utilized the UR16e robotic arm equipped with the high-precision 3D laser\nscanner CIRRUS 3D 300 to collect point cloud data of die-cast parts, and we named\nthe resulting dataset DieCastCloud. The point cloud data in DieCastCloud covers the\nmain external surfaces of the die-cast parts, including complex geometric features such\nas pipes and holes. Additionally, we processed and filtered the collected raw point\ncloud data to obtain a richer sample set.\nFinally, in the creation process of DieCastCloud, point cloud data was enhanced\nthrough techniques such as rotation, translation, scaling, and random erasure to\nincrease the diversity of the data and enhance the model’s generalization capabilities.\n12Specifically, we randomly rotated the point cloud data at random angles along any\naxis and translated it in any spatial direction, with the translation range controlled\nwithin [-800mm, 800mm]. The scaling ratio was constrained to between [0.95, 1.05] of\nthe original point cloud size.\n4.3 Efficient of DCP\nIn this experiment, we use DCP-v2, which incorporates a Transformer, as our\nbaseline. We compare the performance of Efficient DCP with other cutting-edge\ndeep learning-based point cloud registration methods, including PointNetLK[32],\nGeoTransformer[18], PRNet[36], DeepGMR[9], and DCP[8]. We randomly split 2000\ndie-cast component point clouds from DieCastCloud into validation and test sets,\nutilizing different point cloud data during the training and testing periods.During\ntraining, we sampled the point clouds and applied a random rigid transformation\nalong each axis, with rotations uniformly sampled within [0, 60 °] and translations in\nthe range of [-150mm, 150mm]. The source point cloud and the point cloud after the\nrigid transformation were used as the input to the network.\nTo ensure a fair comparison among these methods, we follow the convention and\nuse performance metrics including Mean Squared Error (MSE) for rotation angles\n(MSE(R)), Mean Squared Error for translation directions (MSE(t)), Root Mean\nSquared Error for rotation angles (RMSE(R)), Root Mean Squared Error for transla-\ntion directions (RMSE(t)), Mean Absolute Error for rotation angles (MAE(R)), and\nMean Absolute Error for translation directions (MAE(t)), to guarantee the reliability\nof the experiments.\nTable 1 assesses the performance of our method and its counterparts in this exper-\niment. In this study, Efficient DCP adopts a structure that integrates DGCNN with\nEfficient Transformer, with a learning rate of 0.001, 200 epochs, train batch sizes\nof 32 and train batch sizes of 10, and employs Stochastic Gradient Descent (SGD)\nas the optimizer. Across all evaluated performance metrics, Efficient DCP showcases\noutstanding performance.\nTable 1 Evaluating the performance of Efficient DCP against other advanced methods\nModel MSE(R) MSE(t) RMSE(R) RMSE(t) MAE(R) MAE(t)\nPointNetLK 51.271578 0.114432 6.842565 0.089526 7.664845 0.045454\nGeoTransformer 24.352470 0.001422 5.898481 0.002177 2.974119 0.084997\nPRNet 82.665150 0.014432 12.54549 0.114551 4.859481 0.072361\nDeepGMR 29.159647 0.008747 3.861095 0.084411 3.784151 0.048944\nDCP 24.372444 0.009330 4.851226 0.017721 2.311324 0.027983\nEfficient DCP(Ours) 4.822984 0.000231 2.196129 0.015187 1.350260 0.008338\nPerformance metrics include Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and\nMean Absolute Error (MAE) in both translation and rotation directions. Boldfaced numbers high-\nlight the best performance and the second best are underlined .\n134.4 Multiscale Feature Fusion Dual-channel Precision\nRegistration\nIn this section, we conducted comparative experiments to validate our choice of the\nIterative Closest Point (ICP) and Normal Distributions Transform (NDT) methods.\nIn real industrial scenarios, registration time needs to be controlled within 60 seconds.\nTo better evaluate the feasibility of the methods, we used Root Mean Square Error\n(RMSE) in millimeters and registration time in seconds as performance evaluation\nmetrics, with the results shown in Table 2. Additionally, to more intuitively understand\nthe impact of rotation angles ( °) and translation distances (mm) on various methods,\nwe conducted comparative experiments to test the performance of each method at\nspecific rotation angles and translation distances, with results presented in Table 3.\nFinally, we elucidated the advantages of our dual-channel precision registration method\nbased on multi-scale features.\nTable 2 Performance on clean and noisy samples\nData clean Data noisy\nMethod RMSE(mm) Time(s) RMSE(mm) Time(s)\nSAC-IA 0.128 47.64 0.206 53.76\n4PCS 0.168 13.32 0.273 16.11\nNDT 0.158 4.33 0.182 5.64\nICP 0.153 12.47 0.197 18.12\nMDR(ours) 0.092 25.92 0.148 29.41\nPerformance metrics primarily include Root Mean Square Error (RMSE) and registration time (s).\nBoldfaced numbers highlight the best performance and the second best are underlined .\nTable 3 Testing the impact of different rotation angles and translation distances on the\nexperimental results\nData Rotate ( °) Data Translation (mm)\nMethod 10 20 30 100 500 1000\nSAC-IA 0.004 0.008 0.015 0.003 0.007 0.015\n4PCS 0.002 0.007 0.011 0.010 0.014 0.022\nNDT 0.007 0.009 0.012 0.001 0.003 0.010\nICP 0.002 0.004 0.008 0.003 0.008 0.017\nMDR(ours) 0.001 0.001 0.003 0.001 0.002 0.002\nWe tested the Root Mean Square Error (RMSE) of each method when the rotation angles were\n10°, 30 °, and 90 °, and the translation distances were 100mm, 500mm, and 1000mm, respectively.\nBoldfaced numbers highlight the best performance and the second best are underlined .\nWe first evaluated the performance of SAC-IA[45], 4PCS[2], NDT[29], ICP[24],\nand MDR on the DieCastCloud dataset. To assess the robustness of these methods,\nwe introduced noise with an intensity of 0.1 into the DieCastCloud dataset and then\n14Input\nPointNetLK DCP Efficient DCP NDT MEDPNet(ours) ICP MDR\nClean\nNoisy\nInputPointNetLK NDT DCP MDR MEDPNet(ours) ICP Efficient DCP\nFig. 5 Registration result visualization . We visualized the registration results of various\nmethods under clean and noisy samples, ranking them in descending order according to the\nroot mean square error.\nconducted comparative experiments to evaluate the stability of each method under\nnoisy conditions. By incorporating a multi-scale feature fusion module, MDR pos-\nsesses more comprehensive feature information compared to other methods, thereby\nachieving higher registration accuracy and noise resistance. As shown in Table 2, MDR\ndemonstrates high performance while meeting the registration time requirements in\npractical applications.\nNext, we tested the effects of rotation angles and translation distances on several\nalgorithms. We rotated the point cloud data by 10 °, 30°, and 90 °around any spatial\naxis and translated it by 100mm, 500mm, and 1000mm in any spatial direction. As\ncan be seen from Table 3, ICP showed a notable performance for different angles\nof rotation, while NDT performed better in facing translation issues and exhibited\n15greater robustness in dealing with spatial position changes. Our method demonstrated\nthe best registration performance compared to the other methods.\nTable 4 The registration performance of\nvarious methods under clean and noisy samples\nMethod Data clean Data noisy\nPointNetLK 6.84315 15.9744\nDCP 4.85126 5.87391\nNDT 4.27784 9.76239\nEfficient DCP (Ours) 2.19618 3.71646\nICP 2.07729 6.73248\nMDR (Ours) 1.94877 4.67442\nMEDPNet (Ours) 1.17245 1.32954\nHere, we arrange the methods in descending order\naccording to their root mean square error (RMSE)\non clean samples. Boldfaced numbers highlight\nthe best performance and the second best are\nunderlined .\n4.5 Influence of MEDPNet\nOverall, the MEDPNet method achieves high-quality registration results by initially\napplying Efficient DCP for coarse registration of unaligned point cloud pairs, followed\nby fine-tuning through MDR. To assess the accuracy and robustness of our method,\nwe conducted tests on both clean and noisy samples, with our experimental results\npresented in Table 4. We selected root mean square error (RMSE) as the performance\nmetric to comprehensively account for both variance and bias. The experimental out-\ncomes indicate that our method not only ensures high accuracy but also maintains\nrobustness across different conditions.\n4.6 Visualization\nIn this section, we present a visual comparison of the performance of various advanced\nmethods against our MEDPNet approach, as illustrated in Fig 5. We selected three\ntypical types of die casting samples and conducted experiments under both noise-\nfree and noisy conditions to ensure the practicality of our method. Our comparison\nmainly includes PointNetLK[32], DCP[8], ICP[24], NDT[29], and our improved meth-\nods Efficient DCP, MDR, and MEDPNet, arranged in descending order according to\nroot mean square error (RMSE). As can be observed in Fig 5, MEDPNet achieves\nstate-of-the-art performance on both clean and noisy samples.\n5 Conclusion\nIn the intricate domain of die casting, where complex spatial structures and het-\nerogeneous geometric features prevail, the quest for precise and resilient point cloud\n16registration represents a formidable challenge. Traditional methodologies predomi-\nnantly hinge on high-caliber datasets, endeavoring to enhance registration fidelity\nthrough network model optimization, yet frequently neglecting the nuances of real-\nworld deployment. Addressing this lacuna, the present exposition delineates the\nMultiscale Efficient Deep Closest Point (MEDPNet) modality, coupled with the estab-\nlishment of DieCastCloud, a bespoke point cloud dataset specifically designed to\nmitigate the application impediments of point cloud registration within the die casting\nsphere.\nThe MEDPNet initially conducts coarse registration based on Efficient-DCP, sub-\nsequently transitioning to advanced precision registration via the Multiscale feature\nfusion dual-channel registration (MDR) method. By replacing the traditional Trans-\nformer’s attention mechanism with Efficient Attention, it introduces a Multiscale\nfeature fusion dual-channel precision registration (MDR) technique. This technique\nminimizes registration errors by adaptively optimizing the final transformation matrix\nusing multilayer perceptrons (MLP), resulting in an adaptive, scalable, and highly\nrobust global point cloud registration framework.\nAlthough our method achieves excellent registration results for die casting point\nclouds, there are still some shortcomings. Firstly, due to the large size of Die Castings,\nas well as the presence of occlusions, blind spots, and the influence of machining\nmarks, collecting high-quality point cloud data often requires a significant amount\nof time. Furthermore, for different Die Castings, it is necessary to adjust the data\ncollection strategy of the robot. We believe that an intelligent die casting point cloud\ngenerator is key to solving this problem. Secondly, in the step of precise registration,\nwe utilize unsubsampling point cloud data, where the parameter count of each point\ncloud reaches the tens of millions level. In actual industrial production, how to reduce\ncomputational costs is an urgent problem that needs to be addressed.\n6 Acknowledgements\nThis work was supported by both the Unveiling the Top Technical Research Project\nof Dalian City (2023JB11GX001) and the Key Special Projects of the National Key\nR&D Program (2022YFB3706802).",
      "metadata": {
        "filename": "MEDPNet_ Achieving High-Precision Adaptive Registration for Complex Die Castings.pdf",
        "hotspot_name": "Aluminium_Die_Casting",
        "title": "MEDPNet: Achieving High-Precision Adaptive Registration for Complex Die\n  Castings",
        "published_date": "2024-03-15T03:42:38Z",
        "pdf_link": "http://arxiv.org/pdf/2403.09996v1",
        "query": "die casting aluminium radiator energy consumption reduction process optimization techniques"
      }
    },
    "Optimization of Solidification in Die Casting using Numerical Simulations and Ma": {
      "full_text": "Optimization of Solidi\fcation in Die Casting using\nNumerical Simulations and Machine Learning\nShantanu Shahane1, Narayana Aluru, Placid Ferreira, Shiv G Kapoor,\nSurya Pratap Vanka\nDepartment of Mechanical Science and Engineering\nUniversity of Illinois at Urbana-Champaign\nUrbana, Illinois 61801\nAbstract\nIn this paper, we demonstrate the combination of machine learning and three\ndimensional numerical simulations for multi{objective optimization of low\npressure die casting. The cooling of molten metal inside the mold is achieved\ntypically by passing water through the cooling lines in the die. Depending\non the cooling line location, coolant \row rate and die geometry, nonuni-\nform temperatures are imposed on the molten metal at the mold wall. This\nboundary condition along with the initial molten metal temperature a\u000bect\nthe product quality quanti\fed in terms of micro-structure parameters and\nyield strength. A \fnite volume based numerical solver is used to determine\nthe temperature-time history and correlate the inputs to outputs. The objec-\ntive of this research is to develop and demonstrate a procedure to obtain the\ninitial and wall temperatures so as to optimize the product quality. The non-\ndominated sorting genetic algorithm (NSGA{II) is used for multi{objective\noptimization in this work. The number of function evaluations required for\n1Corresponding Author Email: shahaneshantanu@gmail.com\nPreprint submitted to Journal of Manufacturing Processes January 6, 2020arXiv:1901.02364v2  [cs.CE]  3 Jan 2020NSGA{II can be of the order of millions and hence, the \fnite volume solver\ncannot be used directly for optimization. Therefore, a multilayer perceptron\nfeed{forward neural network is \frst trained using the results from the numer-\nical solution of the \ruid \row and energy equations and is subsequently used\nas a surrogate model. As an assessment, simpli\fed versions of the actual\nproblem are designed to \frst verify results of the genetic algorithm. An in-\nnovative local sensitivity based approach is then used to rank the \fnal Pareto\noptimal solutions and select a single best design.\nKeywords: Die Casting, Deep Neural Networks, Multi{Objective\nOptimization\n1. Introduction\nDie casting is one of the popular manufacturing processes in which liquid\nmetal is injected into a permanent metal mold and solidi\fed. Generally, die\ncasting is used for parts made of aluminum and magnesium alloys with steel\nmolds. Automotive and housing industrial sectors are common consumers of\ndie casting. In such a complex process, there are several input parameters\nwhich a\u000bect the \fnal product quality and process e\u000eciency. With advances\nin computing hardware and software, the physics of these processes can be\nmodeled using numerical simulation techniques. Detailed \row and tempera-\nture histories, micro-structure parameters, mechanical strength etc. can be\nestimated from these simulations. In today's competitive industrial world,\nestimating the values of input parameters for which the product quality is\noptimized has become highly important. There has been extensive research\nin numerical optimization algorithms which can be coupled with detailed\n2numerical simulations in order to handle complex optimization problems.\nSolidi\fcation in casting process has been studied by many researchers.\nMinaie et al. [1] have analyzed metal \row during die \flling and solidi\fcation\nin a two dimensional rectangular cavity. The \row pattern during the \flling\nstage is predicted using the volume of \ruid (VOF) method and enthalpy equa-\ntion is used to model the phase change with convection and di\u000busion inside\nthe cavity. They have studied the e\u000bect of gate location on the residual \row\n\feld after \flling and the solid liquid interface during solidi\fcation. Im et al.\n[2] have done a combined \flling and solidi\fcation analysis in a square cavity\nusing the implicit \flling algorithm with the modi\fed VOF together with the\nenthaply formulation. They studied the e\u000bect of assisting \row and opposite\n\row due to di\u000berent gate positions on the residual \row. They found that the\nliquid metal solidi\fes faster in the opposite \row than in the assisting \row sit-\nuation. Cleary et al. [3] used the Smoothed Particle Hydrodynamics (SPH)\nto simulate \row and solidi\fcation in three dimensional practical geometries.\nThey demonstrated the approach of short shots to \fll and solidify the cavity\npartially with insu\u000ecient metal so that validation can be performed on par-\ntial castings. Plotkowski et al. [4] simulated the phase change with \ruid \row\nin a rectangular cavity both analytically and numerically. They simpli\fed\nthe governing equations of the mixture model by scaling analysis followed\nby an analytical solution and then compared with a complete \fnite volume\nsolution.\nRecently, there has been a growing interest in the numerical optimization\nof various engineering systems. Poloni et al. [5] applied neural network with\nmulti{objective genetic algorithm and gradient based optimizer to the de-\n3sign of a sailing yacht \fn. The geometry of the \fn was parameterized using\nBezier polynomials. The lift and drag on the \fn was optimized as a function\nof the Bezier parameters and thus, an optimal \fn geometry was designed.\nElsayed and Lacor [6] performed a multi{objective optimization of a gas cy-\nclone which is a device used as a gas-solid separator. They trained a radial\nbasis function neural network (RBFNN) to correlate the geometric param-\neters like diameters and heights of the cyclone funnel to the performance\ne\u000eciency and the pressure drop using the data from numerical simulations.\nThey further used the non-dominated sorting genetic algorithm (NSGA{II)\nto obtain the Pareto front of the cyclone designs. Wang et al. [7] optimized\nthe groove pro\fle to improve hydrodynamic lubrication performance in or-\nder to reduce the coe\u000ecient of friction and temperature rise of the specimen.\nThey coupled the genetic algorithm (GA) with the sequential quadratic pro-\ngramming (SQP) algorithm such that the GA solutions were provided as\ninitial points to the SQP. Stavrakakis et al. [8] solved for window sizes for\noptimal thermal comfort and indoor air quality in naturally ventilated build-\nings. A computational \ruid dynamics model was used to simulate the air\n\row in and around the buildings and generate data for training and test-\ning of a RBFNN which is further used for constrained optimization using\nthe SQP algorithm. Wei and Joshi [9] modeled the thermal resistance of a\nmicro-channel heat exchanger for electronic cooling using a simpli\fed thermal\nresistance network model. They used a genetic algorithm to obtain optimal\ngeometry of the heat exchanger so as to minimize the thermal resistance\nsubject to constraints of maximum pressure drop and volumetric \row rate.\nHusain and Kim [10] optimized the thermal resistance and pumping power of\n4a micro-channel heat sink as a function of geometric parameters of the chan-\nnel. They used a three dimensional \fnite volume solver to solve the \ruid\n\row equations and generate training data for surrogate models. They used\nmultiple surrogate models like response surface approximations, Kriging and\nRBFNN. They provided the solutions obtained from the NSGA{II algorithm\nto SQP as initial guesses. Lohan et al. [11] performed a topology optimization\nto maximize the heat transfer through a heat sink with dendritic geometry.\nThey used a space colonization algorithm to generate topological patterns\nwith a genetic algorithm for optimization. Amanifard et al. [12] solved an\noptimization problem to minimize the pressure drop and maximize the Nus-\nselt number with respect to the geometric parameters and Reynolds number\nfor micro-channels. They used a group method of data handling type neural\nnetwork as a surrogate model with the NSGA{II algorithm for optimization.\nEsparza et al. [13] optimized the design of a gating system used for gravity\n\flling a casting so as to minimize the gate velocity. They used a commercial\nprogram (FLOW3D) to estimate the gate velocity as a function of runner\ndepth and tail slope and the SQP method for optimization. Patel et al. [14]\nmodeled and optimized the wear behavior of squeeze cast products. Three\ndi\u000berent optimization methods (genetic algorithm, particle swarm optimiza-\ntion and desirability function approach) are combined with neural network\nas a surrogate model. The training data for neural network is obtained from\nexperiments and nonlinear regression models.\nIn this paper, we consider the heat transfer and solidi\fcation processes\nin die casting of a complex model geometry. The computer program [15]\nsolves the \ruid \row and energy equations, coupled with the solid fraction{\n5temperature relation, using a \fnite volume numerical method. When not\nsigni\fcant, natural convection \row is neglected and only the energy equa-\ntion is solved. The product quality is assessed using grain size and yield\nstrength which are estimated using empirical relations. The solidi\fcation\ntime is used to quantify the process e\u000eciency. The molten metal and mold\nwall temperatures are crucial in determining the quality of die casting. The\nwall temperature is typically nonuniform due to the complex mold geometries\nand asymmetric placement of cooling lines. This nonuniformity can be mod-\neled by domain decomposition of the wall and assigning single temperature\nvalue to each domain. Neural networks are trained using the data generated\nfrom the simulations to correlate the initial and wall temperatures to the\noutput parameters like solidi\fcation time, grain size and yield strength. The\noptimization problem formulated with these three objectives is then solved\nusing genetic algorithm. The procedure illustrated here can be applied to any\npractical mold geometry with a complex distribution of wall temperatures.\n2. Numerical Model Description\nThe numerical model incorporates the e\u000bects of solidi\fcation and heat\ntransfer in die casting. Since the common die casting geometries have thin\ncross-sections, the solidi\fcation time is of the order of seconds and hence,\nthe e\u000bect of natural convection has been found to be negligible. Thus, the\nmomentum equations of the liquid metal are not solved in this work. The\nenergy equation which can be written in terms of temperature has unsteady,\n6di\u000busion and latent heat terms.\n\u001aCp@T\n@t=r\u000f(krT) +\u001aLf@fs\n@t(1)\nwhere,Tis temperature, \u001ais density,Cpis speci\fc heat, kis thermal con-\nductivity,Lfis latent heat of fusion, fsis solid fraction and tis time. The\nGulliver-Scheil equation (2) [16] relates solid fraction to temperature for a\nbinary alloy.\nfs(T) =8\n>>>>><\n>>>>>:0 if T >Tliq\n1 if T <Tsol\n1\u0000\u0010\nT\u0000Tf\nTliq\u0000Tf\u0011 1\nkp\u00001otherwise(2)\nwhere,kpis partition coe\u000ecient, Tfis freezing temperature, Tsolis solidus\ntemperature and Tliqis liquidus temperature.\nSecondary Dendrite Arm Spacing (SDAS) is a microstructure parameter\nwhich can be used to estimate the 0.2% yield strength. The cooling rate\nat each point in the domain is computed by numerically solving the energy\nequation and solid fraction temperature relation (eqs. (1) and (2)). The\nfollowing empirical relations link the cooling rate to SDAS and yield strength.\nSDAS =\u00152=A\u0015\u0012@T\n@t\u0013B\u0015\n[in\u0016m]: (3)\nwhere,A\u0015= 44:6 andB\u0015=\u00000:359 are based on the model for microstructure\n7in aluminum alloys [17].\n\u001b0:2=A\u001b\u0015\u00001=2\n2+B\u001b (4)\nwhere,\u001b0:2is in MPa,\u00152(SDAS) is in \u0016m,A\u001b= 59:0 andB\u001b= 120:3 [18].\nGrain size estimation is based on the work of Greer et al. [19]. The grain\ngrowth rate is given by:\ndr\ndt=\u00152\nsDs\n2r(5)\nwhere,ris the grain size, Dsis the solute di\u000busion coe\u000ecient in the liq-\nuid andtis the time. The parameter \u0015sis obtained using invariant size\napproximation:\n\u0015s=\u0000S\n2\u00190:5+\u0012S2\n4\u0019\u0000S\u00130:5\n(6)\nSis given by\nS=2(Cs\u0000C0)\nCs\u0000Cl(7)\nwhere,Cl=C0(1\u0000fs)(kp\u00001)is solute content in the liquid, Cs=kpClis solute\ncontent in the solid at the solid-liquid interface and C0is the nominal solute\nconcentration. Hence, from the partition coe\u000ecient ( kp) and estimated solid\nfraction (fs), eqs. (5) to (7) are solved to get the \fnal grain size.\n8(a) Geometry\n (b) Mesh: 334000 Elements\nFigure 1: Clamp: 16.5 cm x 9 cm x 3.7 cm [15]\nEquations (1) to (7) are solved numerically using the software OpenCast\n[15] with a \fnite volume method on a collocated grid. The variations of\nthermal conductivity, density and speci\fc heat due to temperature are taken\ninto account. Most practical die casting geometries are complex and require\nunstructured grids. In our work, we have \frst generated a tetrahedral mesh\nusing GMSH [20] and then divided into a hexahedral mesh using TETHEX\n[21]. The details of the numerical algorithm and veri\fcation and validation of\nOpenCast are discussed in previous publications [15, 22]. A model geometry\nrepresenting a clamp [15] has been considered to illustrate the methodology.\nFigure 1 shows the clamp geometry with a mesh having 334000 hexahedral\nelements. It is important to assess the e\u000bects of natural convection. Hence,\nthe clamp geometry is simulated for two cases viz. with and without natural\nconvection. Figures 2 and 3 plot grain size and yield strength contours with\nidentical process conditions for both the cases. Since the solidi\fcation time\nis around 2.5 seconds, the velocities due to natural convection in the liquid\nmetal are observed to be negligible. It is evident from the contours that there\nis no signi\fcant e\u000bect of natural convection and hence, it is neglected in all\n9our further simulations.\n(a) Without Natural Convection\n (b) With Natural Convection\nFigure 2: Clamp: Grain Size ( \u0016m)\n(a) Without Natural Convection\n (b) With Natural Convection\nFigure 3: Clamp: Yield Strength (MPa)\n3. Optimization\nIn die casting the mold cavity is \flled with molten metal and solidi\fed.\nThe heat is extracted from the cavity walls by \rowing coolants (typically wa-\nter) through the cooling lines made inside the die. The quality of the \fnished\n10product depends on the rate of heat extraction which in turn depends on the\ntemperature at the cavity walls. Due to complexity in the die geometry, the\nwall temperature varies locally. An optimal product quality can be achieved\nif the temperature distribution on the cavity walls and initial \fll temperature\nare set properly. Thus, in this work, the following optimization problem with\nthree objectives is proposed:\nMinimizeff1(Tinit;Twall);f2(Tinit;Twall);f3(Tinit;Twall)g\nsubject to 900\u0014Tinit\u00141100 K and 500\u0014Twall\u0014700 K(8)\nwhere,f1= solidi\fcation time, f2= max (grain size) and f3=\u0000min(yield\nstrength). Minimizing the solidi\fcation time increases productivity. Re-\nduction in grain size reduces susceptibility to cracking [23] and improves\nmechanical properties of the product [24]. Thus, minimization of the max-\nimum value of grain size over the entire geometry is set as an optimization\nobjective. Higher yield strength is desirable as it increases the elastic limit of\nthe material. Hence, the minimum yield strength over the entire geometry is\nto be maximized. For convenience, this maximization problem is converted\nto minimization by multiplying by minus one. This explains the third ob-\njective function f3. All the objectives are functions of the initial molten\nmetal temperature ( Tinit) and mold wall temperature ( Twall). The initial\ntemperature is a single value in the interval [900 ;1100] K which is higher\nthan the liquidus temperature of the alloy. As discussed before, the mold\nwall temperature need not be uniform in die casting due to locally varying\nheat transfer to the cooling lines. Thus, in this work, the wall surface is\ndecomposed into multiple domains with each domain having a uniform tem-\n11perature boundary condition which is held constant with time during the\nentire solidi\fcation process. If the die design with cooling line placement\nand coolant \row conditions are included, the thermal analysis of the die can\nalso be done to identify these domains. Due to the lack of this information,\nthe wall is decomposed into ten domains using the KMeans classi\fcation al-\ngorithm from Scikit Learn [25]. Figure 4a shows the domain decomposition\nwith ten domain tags and \fg. 4b shows a random sample of the boundary\ntemperature with a single temperature value assigned uniformly to each do-\nmain. Thus, the input wall temperature ( Twall) is a ten dimensional vector\nin the interval [500 ;700] K which is lower than the solidus temperature of\nthe alloy. Hence, this is a multi{objective optimization problem with three\nminimization objectives which are a function of eleven input temperatures.\n(a) Domain Numbers\n (b) Randomly Assigned Values\nFigure 4: Domain Decomposition of the Boundary and Random Value Assignment\n12(a) Time: 0.037 s\n (b) Time: 0.294 s\n (c) Time: 0.731 s\n (d) Time: 1.88 s\nFigure 5: Clamp Temperature Contours (K) (Solidi\fcation Time: 3.02 s)\n(a) Time: 0.037 s\n (b) Time: 0.294 s\n (c) Time: 0.731 s\n (d) Time: 1.88 s\nFigure 6: Clamp Solid Fraction Contours (Solidi\fcation Time: 3.02 s)\nFigures 5 and 6 plot temperature and solid fraction for di\u000berent time steps\nduring solidi\fcation for the sample shown in \fg. 4b with Tinit= 986 K. It\ncan be seen that the temperature and solid fraction contours are asymmetric\ndue to non-uniform boundary temperature. For instance, domain number\n10 is held at minimum temperature and thus, region near it solidi\fes \frst.\nFigure 7 plots \fnal yield strength and grain size contours. The cooling rates\nand temperature gradients decrease with time as solidi\fcation progresses.\nHence, the core regions which are thick take longer time to solidify. As the\ngrains have more time to grow, the grain size is higher in the core region\nand correspondingly, the yield strength is lower. These trends along with the\nasymmetry are visible in \fg. 7.\n13(a) Yield Strength (MPa)\n (b) Grain Size ( \u0016m)\nFigure 7: Clamp Microstructure Parameters\n3.1. Genetic Algorithm\n3.1.1. Single Objective Optimization\nGenetic algorithms (GAs) are global search algorithms based on the me-\nchanics of natural selection and genetics. They apply the `survival of the\n\fttest' concept on a set of arti\fcial creatures characterized by strings. In\nthe context of a GA, an encoded form of each input parameter is known as\na gene. A complete set of genes which uniquely describe an individual (i.e.,\na feasible design) is known as a chromosome. The value of the objective\nfunction which is to be optimized is known as the \ftness. The population of\nall the individuals at a given iteration is known as a generation. The overall\nsteps in the algorithm are as follows:\n1. Initialize \frst generation with a random population\n2. Evaluate the \ftness of the population\n3. Select parent pairs based on their \ftness (better \ftness implies higher\nprobability of selection)\n144. Perform crossover to generate an o\u000bspring from each parent pair\n5. Mutate some genes randomly in the population\n6. Replace the current generation with the next generation\n7. If termination condition is satis\fed, return the best individual of the\ncurrent generation; else go back to step 2\nThere are multiple strategies discussed in the literature for each of the\nabove steps [26, 27]. A brief overview of the methods used in this work is\ngiven here. The population is initialized using the Latin Hypercube Sam-\npling strategy from the python package pyDOE [28]. Fitness evaluation is\nthe estimation of the objective function which can be done by full scale com-\nputational model or by surrogate models. The number of objective function\nevaluations are typically of the order of millions and thus, step 2 becomes\ncomputationally most expensive step if a full scale model is used. Instead, it\nis common to use a surrogate model which is much cheaper to evaluate. In\nthis work, a neural network based surrogate model is used, details of which\nare provided in section 3.2. Tournament selection is used to choose the parent\npairs to perform crossover. The idea is to choose four individuals at random\nand select two out of them which have better \ftness. Note that since the\noptimization is cast as a minimization problem, lower \ftness value is desired.\nUniform crossover is used to recombine the genes of the parents to generate\nthe o\u000bspring with a crossover probability of 0.9. Random mutation of the\ngenes of the entire generation is performed with a mutation probability of\n0.1. Thereafter, the old generation is replaced by this new generation. Note\nthat the elitist version of GA is used which passes down the \fttest individual\nof the previous generation to this generation as it is. Elitism was found help-\n15ful as it ensures that the next generation is at least as good as the previous\ngeneration.\n3.1.2. Multi{Objective Optimization\nThe simultaneous optimization of multiple objectives is di\u000berent than\nthe single objective optimization problem. In a single objective problem, the\nbest design which is usually the global optimum (minimum or maximum)\nis searched for. On the other hand, for multi{objective problem, there may\nnot exist a single optimum which is the best design or global optimum with\nrespect to all the objectives simultaneously. This happens due to the con-\n\ricting nature of objectives i.e., improvement in one can cause deterioration\nof the other objectives. Thus, typically there is a set of Pareto optimal solu-\ntions which are superior to rest of the solutions in the design space which are\nknown as dominated solutions. All the Pareto optimal solutions are equally\ngood and none of them can be prioritized in the absence of further informa-\ntion. Thus, it is useful to have a knowledge of multiple non-dominated or\nPareto optimal solutions so that a single solution can be chosen out of them\nconsidering other problem parameters.\nOne possible way of dealing with multiple objectives is to de\fne a single\nobjective as a weighted sum of all the objectives. Any single objective op-\ntimization algorithm can be used to obtain an optimal solution. Then the\nweight vector is varied to get a di\u000berent optimal solution. The problem with\nthis method is that the solution is sensitive to the weight vector and choosing\nthe weights to get multiple Pareto optimal solutions is di\u000ecult for a practical\nengineering problem. Multi{objective GAs attempt to handle all the objec-\ntives simultaneously and thus, annihilating the need of choosing the weight\n16vector. Konak et al. [29] have discussed various popular multi{objective GAs\nwith their bene\fts and drawbacks. In this work, the Non-dominated Sorting\nGenetic Algorithm II (NSGA{II) [30] which is a fast and elitist version of\nthe NSGA algorithm [31] is used. The NSGA{II algorithm to march from a\ngiven generation of population size Nto a next generation of same size is as\nfollows:\n1. Select parent pairs based on their rank computed before (lower rank\nimplies higher probability of selection)\n2. Perform crossover to generate an o\u000bspring from each parent pair\n3. Mutate some genes randomly in the population thus forming the o\u000b-\nspring population\n4. Merge the parent and o\u000bspring population thus giving a set of size\n2\u0002N\n5. Evaluate the \ftness of the population corresponding to each objective\n6. Divide the population into multiple non-dominated levels also known\nas fronts\n7. Compute the crowding distance for each individual along each front\n8. Sort the population based on front number and crowding distances and\nrank them\n9. Choose the best set of Nindividuals as next generation (i.e., Nindi-\nviduals with lowest ranks)\n10. If termination condition is satis\fed, return the best front of the current\ngeneration as an approximation of the Pareto optimal solutions; else\ngo back to step 1\n17Before iterating over the above steps, some pre-processing is required. A\nrandom population of size Nis initiated and steps 5{8 are implemented to\nrank the initial generation. The parent selection, crossover and mutation\nsteps are identical to the single objective GA described in section 3.1.1. The\nalgorithms for remainder of the steps can be found in the paper by Deb et al.\n[30]. Ranking the population by front levels and crowding distance enforces\nboth elitism and diversity in the next generation.\n3.2. Neural Network\nThe \ftness evaluation step of the GA requires a way to estimate the\noutputs corresponding to the set of given inputs. Typically, the number of\ngenerations can be of the order of thousands with several hundreds of popu-\nlation size per generation and thus, the total number of function evaluations\ncan be around hundred thousands or more. It is computationally di\u000ecult\nto run the full scale numerical estimation software. Thus, a surrogate model\nis trained which is cheap to evaluate. A separate neural network is trained\nfor each of the three optimization objectives (eq. (8)). Hornik et al. [32]\nshowed that under mild assumptions on the function to be approximated,\na neural network can achieve any desired level of accuracy by tuning the\nhyper-parameters. The building block of a neural network is known as a\nneuron which has multiple inputs and gives out single output by performing\nthe following operations:\n1. Linear transformation: a=Pn\ni=1wixi+b; where,fx1;x2;:::;xngaren\nscalar inputs, wiare the weights and bis a bias term\n2. Nonlinear transformation applied element-wise: y=\u001b(a); where,yis\na scalar output and \u001bis the activation function\n18Multiple neurons are stacked in a layer and multiple layers are connected\ntogether to form a neural network. The \frst and last layers are known as\ninput and output layers respectively. Information \rows from the input to\noutput layer through the intermediate layers known as hidden layers. Each\nhidden layer adds nonlinearity to the network and thus, a more complex\nfunction can be approximated successfully. At the same time, having large\nnumber of neurons can cause high variance and thus, blindly increasing the\ndepth of the network may not always help. The number of neurons in the\ninput and output layers is de\fned by the problem speci\fcation. On the other\nhand, number of hidden layers and neurons has to be \fne tuned to have low\nbias and low variance. The interpolation error of the network is quanti\fed\nas a loss function which is a function of the weights and bias.\nName of\nNetworkNo. of\nHidden LayersNo. of Neurons\nper Hidden LayerLearning RateNo. of\nEpochsL2\u0015Dropout\nFactor\nSol. Time 4 50 0.001 300 0.004 0\nMax. Grain 6 75 0.001 300 0.005 0.2\nMin. Yield 4 25 0.003 400 0.01 0\nTable 1: Neural Network Hyper{Parameters\nName of Network Training Error Tesing Error Validation Error\nSol. Time 0.90% 1.01% 1.08%\nMax. Grain 1.29% 2.04% 1.92%\nMin. Yield 0.25% 0.38% 0.43%\nTable 2: Average Percent Training, Testing and Validation Errors\n19(a) Solidi\fcation Time\n (b) Max. Grain Size\n (c) Min. Yield Str.\nFigure 8: Neural Networks: Error Estimates for 200 Testing Samples\nA numerical optimization algorithm coupled with gradient estimation by\nthe backpropagation algorithm [33] is used to estimate the optimal weights\nand bias which minimize the loss function for a given training set. This is\nknown as training process. An approach described by Goodfellow et al. [34]\nis used to select the hyper{parameters and train the neural network. Mean\nsquare error is set as the loss function. A set of 500 random samples is used\nfor training and two di\u000berent sets of 200 each are used for validation and test-\ning. The number of inputs and outputs of the problem specify the number\nof neurons in the input and output layers respectively. Here, there are three\nneural networks with 11 input neurons and one output neuron each (11 tem-\nperatures and three objectives mentioned in eq. (8)). The number of hidden\nlayers and hidden neurons, learning rate, optimizer, number of epochs and\nregularization constant are the hyper{parameters which are problem speci\fc.\nThey are varied in a range and then chosen so that both the training and\nvalidation errors are minimized simultaneously. The accuracy of prediction\nis further checked on an unseen test data. This overall approach is used for\ntraining the neural network with low bias and low variance. In this work,\n20the neural network is implemented using the Python library Tensor\row [35]\nwith a high level API Keras [36]. After testing various optimizers available\nin Keras, it is found that the Adam optimizer is the most suitable here. The\nparameters \f1= 0:9 and\f2= 0:999 are set as suggested in the original\npaper by Kingma and Ba [37] and the `amsgrad' option is switched on. The\nactivation functions used for all the hidden and output layers are ReLU and\nidentity respectively. This choice of activation functions is generally recom-\nmended for regression problems [34]. A combination of L2 regularization\nwith dropout is used to control the variance. The training is stopped when\nthe loss function changes minimally after a particular epoch. This is known\nas the stopping criteria. After varying the hyper-parameters in a range, it is\nfound that for the values listed in table 1, the training, testing and validation\nerrors (table 2) are simultaneously minimized. Moreover, since all the three\nerrors are low and close to each other, it shows that the bias and variance\nare low. Figure 8 plots percent relative error for 200 testing samples. It can\nbe seen that all the three neural networks are able to predict with acceptable\naccuracy.\n4. Assessment of Genetic Algorithm on Simpler Problems\nIt is di\u000ecult to visualize the variation of an output with respect to each\nof the eleven inputs. Hence in this section, two problems are considered\nwhich are simpli\fed versions of the actual problem. In the \frst case, the\nboundary temperature is set to a uniform value and hence, there are only\ntwo scalar inputs: ( Tinit;Twall). For the second case, the initial temperature\nis held constant ( Tinit= 1000 K) and the boundary is split into two domains\n21instead of ten. Thus, again there are two scalar inputs: ( T(1)\nwall;T(2)\nwall).T(1)\nwallis\nassigned to domain numbers 1{5 and T(2)\nwallto domain numbers 6{10 (\fg. 4a).\nThe ranges of wall and initial temperatures are the same as before (section 3).\nSuch a simpli\fed analysis gives an insight into the actual problem. Moreover,\nsince these are problems with two inputs, the optimization can be performed\nby brute force parameter sweep and compared to the genetic algorithm. This\nhelps to \fne tune the parameters and assess the accuracy of the GA.\n4.1. Single Objective Optimization\nIn this section, all the objectives are analyzed individually. A two dimen-\nsional mesh of size 40,000 with 200 points in each input dimension is used for\nthis analysis. The outputs are estimated from the neural networks for each\nof these points. Figure 9 plots the response surface contours for each of the\nthree objectives with their corresponding minima. The minima are estimated\nfrom the 40,000 points. The XandYaxes are initial and wall temperatures,\nrespectively. When the initial temperature is reduced, the total amount of\ninternal energy in the molten metal is reduced and thus, the solidi\fcation\ntime decreases. The amount of heat extracted is proportional to the tem-\nperature gradient at the mold wall which increases with a drop in the wall\ntemperature. Thus, the drop in wall temperature reduces the solidi\fcation\ntime. Hence, the minimum of solidi\fcation time is attained at the bottom\nleft corner (\fg. 9a). The grain size is governed by the local temperature\ngradients and cooling rates which are weakly dependent on the initial tem-\nperature. Thus, it can be seen that the contour lines are nearly horizontal\nin \fg. 9b. On the other hand, as wall temperature reduces, the rate of heat\nextraction rises and hence, the grains get less time to grow. This causes a\n22drop in the grain size. Thus, the minimum of the maximum grain size is on\nthe bottom right corner (\fg. 9b). The contour values in \fg. 9c are negative\nas maximization objective of the minimum yield strength is converted into\nminimization by inverting the sign. The minimum is at the top right corner\nof \fg. 9c. Figure 10 has similar plots for the second case of constant initial\ntemperature and split boundary temperature. Figure 10c shows the e\u000bect of\nnonuniform boundary temperature. The minimum is attained at wall tem-\nperatures of 500 K and 700 K since the local gradients and cooling rates vary\ndue to the asymmetry in the geometry. This analysis shows the utility of the\noptimization with respect to nonuniform mold wall temperatures.\n23(a) Solidi\fcation Time (s)\n (b) Max. Grain Size ( \u0016m)\n(c) Min. Yield Str. (MPa)\nFigure 9: Parameter Sweep: Uniform Boundary Temperature\n24(a) Solidi\fcation Time (s)\n (b) Max. Grain Size ( \u0016m)\n(c) Min. Yield Str. (MPa)\nFigure 10: Parameter Sweep: Split Boundary Temperature with Tinit= 1000 K\nTable 3 lists the optima for the single objective problems estimated from\nparameter sweep and GA. Note that since the 200 K range is divided into\n200 divisions, the resolution of the parameter sweep estimation is 1 K. For\nall the six cases, the outputs and corresponding inputs show that the GA es-\ntimates are accurate. The GA parameters are varied in the following ranges:\n[25{100] generations, [10{50] population size, [0.75{0.9] crossover probability\nand [0.05{0.2] mutation probability. Similar ranges have been used in the\n25literature [38{40]. Parameter values of 50 generations with population size\nof 25 and crossover and mutation probability of 0.8 and 0.1 respectively are\nfound to give accurate estimates as shown in table 3. The elitist version of\nGA is used which passes on the best individual from previous generation to\nthe next generation.\nProblem\nTypeOptim.\nObjectiveOptim.\nTypeParam. Sweep GA Estimates\nInputs (K) Output Inputs (K) Output\nUniform\nB.C.Solid. Time (s) Min. 900, 500 1.962 900.2, 500.2 1.962\nMax. Grain ( \u0016m) Min. 1100, 500 22.41 1099.1, 500.3 22.42\nMin. Yield (MPa) Max. 1100, 700 145.4 1099.9, 699.9 145.4\nTinit=\n1000 KSolid. Time (s) Min. 500, 500 1.982 500.6, 500.3 1.982\nMax. Grain ( \u0016m) Min. 500, 500 22.49 500.1, 500.1 22.50\nMin. Yield (MPa) Max. 500, 700 140.9 500.1, 699.9 140.8\nTable 3: Single Objective Optimization: Genetic Algorithm Estimates compared with\nParameter Sweep Values for Two Input Problems\n4.2. Bi-Objective Optimization\nIn this section, two objectives are taken at a time for each of the two\nsimpli\fed problems de\fned in section 4. As before, a two dimensional mesh\nof size 40,000 with 200 points in each input dimension is used for this analysis.\nThe outputs are estimated from the neural networks for each of these points.\nThe feasible region is the set of all the attainable designs in the output space\nwhich can be estimated by the parameter sweep. For a minimization problem,\na designd1is said to dominate another design d2if all the objective function\nvalues ofd1are less than or equal to d2. The design space can be divided\nin two disjoint sets SpandSdsuch thatSpcontains all the designs which do\nnot dominate each other and at least one design in Spdominates any design\n26inSd[41].Spis called as the Pareto optimal or non-dominated set whereas,\nSdis called as the non-Pareto optimal or dominated set. Since the designs\nin Pareto optimal set are non-dominated with respect to each other, they all\nare equally good and some additional information regarding the problem is\nrequired to make a unique choice out of them. Thus, it is useful to have a list\nof multiple Pareto optimal solutions. Another way to interpret the Pareto\noptimal solutions is that any improvement in one objective will worsen at\nleast one other objective thus, resulting in a trade-o\u000b [42].\n(a) Parameter Sweep\n (b) NSGA{II\nFigure 11: Uniform Boundary Temperature: Solidi\fcation Time vs Min. Yield Strength\n27(a) Parameter Sweep\n (b) NSGA{II\nFigure 12: Uniform Boundary Temperature: Max. Grain Size vs Min. Yield Strength\nThe blue region in the left parts of \fgs. 11 to 14 indicates the feasible\nregion. Using a pairwise comparison of the designs in the feasible region\nobtained by parameter sweep, the Pareto front is estimated which is plot-\nted in red. The right side plots of \fgs. 11 to 14 show the Pareto fronts\nobtained using NSGA{II. The NSGA parameters are varied in the following\nranges: [25{100] generations, [500{1500] population size, [0.75{0.9] crossover\nprobability and [0.05{0.2] mutation probability. The population size used\nin this work is kept higher than the literature [43] to get a good resolution\nof the Pareto front. It can be seen that both the estimates match which\nimplies that the NSGA{II implementation is accurate. A population size of\n1000 is evolved over 50 generations with crossover and mutation probability\nof 0.8 and 0.1 respectively. Existence of multiple designs in the Pareto set\nimplies that the objectives are con\ricting. This can be con\frmed from the\nsingle objective analysis. For instance, consider the two objectives solidi\fca-\n28tion time and minimum yield strength in the uniform boundary temperature\ncase. From \fgs. 9a and 9c it can be seen that individual minima are attained\nat di\u000berent corners. Moreover, the directions of descent are di\u000berent for each\nobjective and thus, at some points, improvement in one objective can worsen\nother. This e\u000bect is visible on the corresponding Pareto front plot in \fg. 11.\n(a) Parameter Sweep\n (b) NSGA{II\nFigure 13: Split Boundary Temperature with Tinit= 1000 K: Solidi\fcation Time v/s Min.\nYield Str.\n29(a) Parameter Sweep\n (b) NSGA{II\nFigure 14: Split Boundary Temperature with Tinit= 1000 K: Max. Grain Size v/s Min.\nYield Str.\n5. Results of Multi{Objective Optimization Problem with Eleven\nInputs\nAfter veri\fcation of the NSGA{II implementation on simpli\fed prob-\nlems, multi{objective design optimization with eleven inputs is solved. As\ndiscussed before, some additional problem information is required to choose\na single design from all the Pareto optimal designs. In die casting, there is a\nlot of stochastic variation in the wall and initial temperatures. Shahane et al.\n[15] have performed parameter uncertainty propagation and global sensitiv-\nity analysis and found that the die casting outputs are sensitive to the input\nuncertainty. Thus, from a practical point of view, it is sensible to choose a\nPareto optimal design which is least sensitive to the inputs. In this work,\nsuch an optimal point is known as a `stable' optimum since any stochastic\nvariation in the inputs has minimal e\u000bect on the outputs. A local sensitiv-\n30ity analysis is performed to quantify the sensitivity of outputs towards each\ninput for all the Pareto optimal designs. For a function f:Rn!Rmwhich\ntakes input x2Rnand produces output f(x)2Rm, them\u0002nJacobian\nmatrix is de\fned as:\nJf[i;j] =@fi\n@xj81\u0014i\u0014m;1\u0014j\u0014n (9)\nAt a given point x0, the local sensitivity of fwith respect to each input can\nbe de\fned as the Jacobian evaluated at that point: Jf(x0) [44]. Here, there\nare eleven inputs and two outputs. Thus, the 2 \u000211 Jacobian is estimated\nat all the Pareto optimal solutions evaluated using the neural networks with\na central di\u000berence method. Then, the L1norm of the Jacobian given by\nthe sum of absolute values of all its components is de\fned as a single scalar\nmetric to quantify the local sensitivity.\nTo begin with, two pairs of objectives are chosen: fsolidi\fcation time,\nminimum yield strength gandfmaximum grain size, minimum yield strength g.\nFor both of these cases, a population size of 500 with 5000 generations is set.\nFigure 15 plots the Pareto fronts colored by the value of Jacobian norm at\neach design. It can be seen that the norm varies signi\fcantly and thus, rank-\ning the designs based on the sensitivity is useful. The design with minimum\nnorm is chosen and marked on the Pareto fronts as a stable optimum. Note\nthat minimum norm is observed at the end of the Pareto front. However,\nthe norm is low on the entire left vertical side of the Pareto front. Hence,\nit may be a good idea to choose the design near shown `knee' region which\nhas similar value of the objective on the X{axis but much lower value of the\nobjective on the Y{axis.\n31(a) Solidi\fcation Time v/s Min. Yield\nStr.\n(b) Max. Grain Size v/s Min. Yield Str.\nFigure 15: Pareto Front for Two Objectives (Colored by Jacobian Norn)\nThe next step is to perform the complete multi{objective optimization\nanalysis as mentioned in eq. (8). NSGA{II is used with a population size of\n2000 evolved over 250 generations. Figure 16 plots the Pareto optimal de-\nsigns in a three dimensional objective space colored by the value of Jacobian\nnorm at each design. Since it is di\u000ecult to visualize the colors on the three\ndimensional Pareto front, a histogram of norm of the Jacobian at each of\nthese designs is also plotted in \fg. 17. It can be seen that the norm varies\nfrom 0.95 to 11.1. The histogram is skewed towards the left which implies\n32that multiple designs are stable. The stable optimum is:\nInputs:Tinit= 1015:8 K\nTwall=f500:7;502:8;500:0;501:5;500:5;\n503:6;643:6;508:8;502:3;500:7gK\nOutputs: Solidi\fcation Time = 1 :99 s\nMax Grain Size = 22 :39\u0016m\nMin Yield Strength = 137 :95 MPa(10)\nFigure 16: Pareto Front for Three Objectives (Colored by Jacobian Norn)\n33Figure 17: Histogram of Local Sensitivity of Designs on the Pareto Front for 3 Objectives\n6. Conclusions\nThis paper presents an application of multi{objective optimization of the\nsolidi\fcation process during die casting. Although the procedure is illus-\ntrated for a model clamp geometry, the process is general and can be applied\nto any practical geometry. Practically, it is not possible to hold the entire\nmold wall at a uniform temperature. The \fnal product quality in terms of\nstrength and micro-structure and process productivity in terms of solidi\fca-\ntion time depends directly on the rate and direction of heat extraction during\nsolidi\fcation. Heat extraction in turn depends on the placement of coolant\nlines and coolant \row rates thus, being a crucial part of die design. In this\n34work, the product quality is assessed as a function of initial molten metal and\nboundary temperatures. Knowledge of boundary temperature distribution in\norder to optimize the product quality can be useful in die design and process\nplanning. NSGA{II, which is a popular multi{objective genetic algorithm,\nwas used for the optimization process. Since the number of function evalua-\ntions required for a GA is extremely high, a deep neural network was used as\na surrogate to the full computational \ruid dynamics simulation. The training\nand testing of the neural network was completed with less than thousand full\nscale \fnite volume simulations. The run time per simulation using OpenCast\nwas about 20 minutes on a single processor i.e., around 333 compute hours for\n1000 simulations. All the simulations were independent and embarrassingly\nparallel. Thus, a multi{core CPU was used to speed up the process with-\nout any additional programming e\u000bort for parallelization. Computationally,\nthis was the most expensive part of the process. Subsequent training and\ntesting of the neural network took a few minutes. Implementation of GA is\ncomputationally cheap since the evaluation of a neural network is a sequence\nof matrix products and thus, was completed in few minutes. Hence, it can\nbe seen that the strategy of coupling the GA and neural network with \fnite\nvolume simulations is computationally bene\fcial.\nIn this work, the wall is divided into ten domains. Together with the\ninitial temperature, this is an optimization problem with eleven inputs. Both\nsingle and multi{objective genetic algorithms were programmed and veri\fed\nwith parameter sweep estimation for simpli\fed versions of the problem. The\nsingle objective response surfaces were used to get an insight regarding the\ncon\ricting nature of the objectives since the individual optimal solutions\n35were completely di\u000berent from each other. Moreover, the solidi\fcation time,\nmaximum grain size and minimum yield strength varied in the ranges [2, 3.5]\nseconds, [22, 34] microns and [134, 145] MPa respectively for the given inputs.\nThis showed the utility of the simultaneous optimization of all the objectives\nsince there was a signi\fcant scope for improvement. After estimating multiple\nPareto optimal solutions, a common question is to choose a single design. The\nstrategy of choosing the design with minimum local sensitivity towards the\ninputs was found to be practically useful due to the stochastic variations\nin the input process parameters. Overall, although die casting was used\nas an example for demonstration, this approach can be used for process\noptimization of other manufacturing processes like sand casting, additive\nmanufacturing, welding etc.\nAcknowledgments\nThis work was funded by the Digital Manufacturing and Design Inno-\nvation Institute with support in part by the U.S. Department of the Army.\nAny opinions, \fndings, and conclusions or recommendations expressed in this\nmaterial are those of the author(s) and do not necessarily re\rect the views\nof the Department of the Army. The authors would like to thank Beau Glim\nof North American Die Casting Association (NADCA) and Alex Monroe of\nMercury Castings for their insightful suggestions.\n36References",
      "metadata": {
        "filename": "Optimization of Solidification in Die Casting using Numerical Simulations and Ma.pdf",
        "hotspot_name": "Aluminium_Die_Casting",
        "title": "Optimization of Solidification in Die Casting using Numerical\n  Simulations and Machine Learning",
        "published_date": "2019-01-08T15:28:33Z",
        "pdf_link": "http://arxiv.org/pdf/1901.02364v2",
        "query": "die casting aluminium radiator energy consumption reduction process optimization techniques"
      }
    },
    "Optimizing IoT Energy Efficiency on Edge (EEE)_ a Cross-layer Design in a Cognit": {
      "full_text": "arXiv:1901.05494v2  [eess.SP]  10 Jan 2020This work has been submitted to the IEEE for possible\npublication. Copyright may be transferred without notice, after\nwhich this version may no longer be accessible.1\nOptimizing IoT Energy Efﬁciency on Edge (EEE):\na Cross-layer Design in a Cognitive Mesh Network\nJianqing Liu, Yawei Pang, Haichuan Ding, Ying Cai, Haixia Zh ang, Yuguang Fang\nAbstract —Battery-powered wireless IoT devices are now\nwidely seen in many critical applications. Given the limite d\nbattery capacity and inaccessibility to external power rec harge,\noptimizing energy efﬁciency (EE) plays a vital role in prolo nging\nthe lifetime of these IoT devices. However, a sheer amount of\nexisting works only focus on the EE design at the infrastruct ure\nlevel such as base stations (BSs) but with little attention t o the EE\ndesign at the device level. In this paper, we propose a novel i dea\nthat aims to shift energy consumption to a grid-powered cogn itive\nradio mesh network thus preserving energy of battery-power ed\ndevices. Under this line of thinking, we cast the design into a\ncross-layer optimization problem with an objective to maxi mize\ndevices’ energy efﬁciency. To solve this problem, we propos e\na parametric transformation technique to convert the origi nal\nproblem into a more tractable one. A baseline scheme is used\nto demonstrate the advantage of our design. We also carry out\nextensive simulations to exhibit the optimality of our prop osed\nalgorithms and the network performance under various setti ngs.\nIndex Terms —Energy efﬁciency, Cognitive radio network,\nOFDM, Cross-layer optimization, Fractional programming.\nI. I NTRODUCTION\nOver the last few years, the explosive growth of smart\ndevices is accelerating the advent of Internet-of-Things ( IoT).\nAmong these IoT devices, a signiﬁcant number of them run\non wireless radio and are powered by battery [1], [2]. It is\nsimply because they are deployed in hard-to-access locatio ns,\ninfeasible to be hard wired, or just designed to be user frien dly.\nSuch use cases sometimes require battery-powered IoT devic es\nto operate for a signiﬁcant amount of time without battery\nreplacement, 10 years for instance [3]. To achieve this goal , a\nfew low-power wireless technologies such as Bluetooth Low\nEnergy (BLE) [4], ZigBee (IEEE 802.15.4e) [5], WiFi (IEEE\n802.11ah) [6], low-power long-range (LoRa) [7] are develop ed\nwith the objective of optimizing a device’s radio interface for\nhigher energy efﬁciency (e.g., via better power control and\ntransmission scheduling). However, such optimization wit h\nJ. Liu is with the Department of Electrical and Computer Engi neering,\nUniversity of Alabama in Huntsville, Huntsville, AL 35899 U SA e-mail:\njianqing.liu@uah.edu\nY . Pang is with the Department of IoT, School of Computer and S oftware,\nNanjing University of Information Science and Technology, Nanjing 210044,\nChina e-mail: yaweipang@gmail.com\nH. Ding is with the Department of Electrical Engineering and Com-\nputer Science, University of Michigan, Ann Arbor, MI 48109 U SA e-mail:\ndhcbit@gmail.com\nY . Cai is with the Computer School, Beijing Information Scie nce &\nTechnology University, Beijing 100101, China e-mail: ycai @bistu.edu.cn\nH. Zhang is with the School of Control Science and Engineerin g, Shandong\nUniversity, Jinan 250100, China e-mail: haixia.zhang@sdu .edu.cn\nY . Fang is with the Department of Electrical and Computer Eng ineering,\nUniversity of Florida, Gainesville, FL 32611 USA e-mail: fa ng@ece.uﬂ.edu.energy efﬁciency as the objective usually comes with the\nsacriﬁce to other metrics. For instance, long duty-cycle in curs\nhigh latency [8]; low transmit power reduces data rate [9]\nand simpliﬁed radio even hampers security [10]. In light of\nthis, alternative solutions to increasing IoT devices’ ene rgy\nefﬁciency are worthwhile to investigate.\nIt has been well recognized that the attempt in increasing\nenergy efﬁciency is constrained by the spectrum efﬁciency\n[11]. The reason lies in the Shannon’s capacity theorem, whi ch\nreveals that link capacity increases only logarithmically with\npower but linearly with bandwidth, indicating that bandwid th\ncould more effectively bring down the power consumption.\nWith the observation that a large portion of licensed spectr um\nis not well utilized in certain areas [12], one can harvest\nand opportunistically access under-utilized spectrum. As an\nenabling technology, cognitive radio (CR) [13] promises to\nrealize dynamic spectrum access and thus increase spectrum\nefﬁciency.\nIn this paper, we propose a novel idea - by allowing battery-\npowered IoT devices to shift their energy consumption to gri d-\npowered CR-capable devices, IoT devices’ energy efﬁciency\ncan be increased. This energy-efﬁciency-on-edge (EEE) ide a\nis motivated by the observation that IoT devices are more\nsensitive to energy consumption than grid-powered devices\nare. In other words, it is worthwhile to spend a bit more\ngrid energy on grid-powered devices so as to somewhat, if\nnot signiﬁcantly, improve the energy efﬁciency of battery-\npower devices. To enable such “energy shift”, we leverage\na cognitive capacity harvesting network (CCHN) which was\nﬁrstly proposed in our previous works [14] as shown in\nFig.1. The essence of this architecture is that the routers,\nalso called CR routers, have CR capabilities and they can\nform a multi-hop mesh network to get closer to and help\nthe end devices with no CR capabilities for data exchange,\npotentially saving energy and increasing spectrum efﬁcien cy.\nUnder this structure, we intend to augment it with a centrali zed\nIoT system1(e.g., LoRAWAN, LTE) and let the CCHN help\nbattery-powered IoT devices to relay data, thus increasing their\nenergy efﬁciency.\nSpeciﬁcally, we consider the uplink transmission where IoT\ndevices can send data either directly to the BS/gateway or to\nCR routers. For the former case, it is a one-hop transmission\nwhile in the latter one, an IoT device is ﬁrstly connected\nto a CR router using its existing radio interface (e.g., LoRa ,\nWiFi) and its trafﬁc are then delivered to the BS via multi-ho p\n1In this work, a hypothetical model instead of a realistic sys tem is used to\nconvey the idea.2\ntransmissions using harvested bands. The rationale of stud ying\nthis problem is that IoT devices could use lower transmissio n\npower to associate with closer CR routers but may not be able\nto always receive satisfactory service through the CCHN due\nto the uncertainty of the harvested bands; while they could\nobtain reliable throughput via direct connection to the BS\nbut may need to apply higher transmission power. Obviously,\nthere is a tradeoff between service quality (i.e., reliabil ity\nand throughput) and power consumption. Therefore, we will\ninvestigate the energy efﬁcient design in this respect, wit h\nthe design dimension to be device association, uplink power\ncontrol and channel allocation, and multi-hop scheduling a nd\nrouting in the CCHN.\nTowards this design objective, there are several technical\nchallenges to be addressed. First, the weighted sum-of-rat ios\nform of the objective function is non-convex, which makes\nthe optimization problem difﬁcult to tackle. Second, the av ail-\nability of harvested spectrum is highly unpredictable and t he\nusable bandwidth in this regard should be naturally modeled\nas a random variable, which results in a stochastic constrai nt\nand makes the problem intractable. Third, the association\nvariable is in integer form and tightly coupled with other\ndecision variables so solving the problem via conventional\napproaches is highly prohibitive especially when the netwo rk\nsize is large. In light of these aforementioned challenges, we\npropose corresponding solution to each of them, which in tur n\ndemonstrates our technical contributions as follows:\n•We introduce auxiliary variables and transform the origi-\nnal objective function into a parametric subtractive form\nwhich bears the desired convexity property. Their equiv-\nalence in terms of ﬁnding the same solution is further\nproved.\n•We reformulate the stochastic constraint of the availabili ty\nof harvested spectrum as a chance constraint of ∆-\nconﬁdence level. Then, the feasible region of original\noptimization problem becomes a convex set.\n•We address the integer programming part through a two-\nstep procedure: relaxing and then rounding. To decouple\nthe decision variables, we further apply the dual-based\napproach to make the original problem more tractable.\nThe rest of the paper is organized as follows. Section II\nintroduces the most recent literature of this topic. Sectio n\nIII describes the system model. The problem formulation is\noutlined in Section IV . We propose solution algorithms in\nSection V and present the performance evaluation in Section\nVI. Finally, Section VII concludes the paper.\nII. R ELATED WORKS\nIoT wireless access technology can be broadly classiﬁed\ninto random access (or contention-based) and deterministi c\naccess (or connection-based) categories [15]. Exemplary s tan-\ndard of the former is LTE-RACH [16] while the latter could be\nTDMA-based (e.g., BLE) and OFDM-based (e.g., 802.11ah).\nThere has been a ﬂux of works on optimizing these standards\nto make them energy-efﬁcient, but the drawbacks are also\nevident in the sense that other performance metrics such\nas latency, throughput and security could be hampered [8],[9], [10]. Since our work is not constrained to any existing\nstandard, we will survey generic EE design in hypothetical\nwireless networks. Nevertheless, the number of related wor ks\nis still signiﬁcant so we limit the scope to the OFDM-\nbased access systems and future networks like cognitive rad io\nnetworks (CRNs) and the CRN-enabled networks.\nThe well-recognized EE design model is to maximize\nenergy efﬁciency (deﬁned as the ratio of rate to power\nconsumption) under wireless resource constraints [17]. Fo r\ninstance, in an OFDM-based radio access network, there are\nmany related works to optimize EE by jointly considering\npower control and subcarrier allocation [18], [19], [20], [ 21].\nCheung et al. [18] focused on a multi-relay assisted OFDM\ncellular network and studied the EE maximization problem\nby formulating EE as the ratio of total network throughput to\ntotal power consumption at a BS. Xiong et al. also targeted\nat the same wireless setting but investigated the EE maxi-\nmization constrained on users’ proportional rate fairness [19].\nHowever, these papers and their related ones bear two major\nshortcomings: (i) they modeled the network EE in a “sum-to-\nsum” form (instead of the “sum-of-ratios” form) which fails\nto capture each device’s EE; (ii) they used the infrastructu re’s\npower consumption in calculating EE, which lacks emphasis\non the battery-powered end devices. Unfortunately, there i s a\nlack of study when it comes to addressing these two issues.\nRecent works [20], [21] nonetheless have some merits along\nthis line. Zarakovitis et al. [21] studied the EE design in\na downlink OFDM cellular system by characterizing EE in\nweighted sum-of-ratios. They applied the Maclaurin series\nexpansion to transform the objective into a tractable form a nd\nthen solved it in polynomial time. He et al. [20] focused on a\nmulti-cell downlink OFDM cellular system with coordinated\nbeamforming. They introduced auxiliary variables to trans form\nthe weighted sum-of-ratios objective into a parametric sub trac-\ntive form, which afterwards became easier to address. These\nworks adopted the “sum-of-ratios” to deﬁne the EE objective\nbut still employed the infrastructure’s rather than the dev ice’s\npower consumption.\nThe EE design in CRNs and CRN-enabled networks is still\nin its infancy. Amongst the limited works, Wang et al. in\n[22] considered an OFDM-based CRNs and investigated the\nEE design by taking channel uncertainty into consideration .\nXie et al. [23] proposed a new cognitive cellular network\narchitecture consisting of macrocells and femtocells. The y\nutilized game theoretic approaches to investigate the ener gy\nefﬁcient resource allocation. There are other similar work s in\nthis context [24], [25], but they all bear two critical limit ations:\n(i) end devices are assumed to have CR capabilities which\nin nature is not energy efﬁcient due to the tedious spectrum\nsensing; (ii) the EE measurement is the ratio of overall rate\nto overall power consumption, which is in the “sum-to-sum”\nform. To cope with the ﬁrst challenge, a cognitive capacity\nharvesting network (CCHN) was proposed in [14] and its\nbasics are discussed in Section II. Under this architecture ,\nDing et al. [26], [27] and Liu et al. [28], [29] developed\nprotocols to achieve higher throughput and energy efﬁcienc y\nfor lightweight end devices.\nIn this work, we exploit the CCHN-enabled OFDM access3\nsystem to maximize energy efﬁciency of battery-powered IoT\ndevices. We characterize the network EE using the summation\nof end devices’ EE, and model the problem under a novel idea\n- “energy shift” from battery-powered devices to grid-powe red\nones.\nIII. S YSTEM MODEL\nA. Network Description\nIn this paper, we consider a CCHN-augmented network\nwhich consists of a secondary service provider (SSP), a BS,\nmultiple CR routers and end users2, as shown in Fig.1.\nSpeciﬁcally, SSP is a wireless service provider which has\nits own licensed bands, typically called the “basic bands”,\nfor reliable control signalling, handling handovers and so on.\nSSP could also harvest spectrum bands from other operators\nvia paradigms such as spectrum sensing or spectrum auction.\nAs the centralized coordinator, SSP observes and collects\nnetwork information (e.g., users’ trafﬁc demands, channel state\ninformation) in its coverage area and then performs network\noptimization (e.g., power control, channel allocation, li nk\nscheduling and routing) to determine the optimal approache s\nfor service provisioning. CR routers are grid-powered devi ces\nwith CR capabilities, and they form a mesh network that is\ncapable of using the harvested bands to transmit data. BS\nhas multiple radio interfaces and serves as the gateway to th e\nInternet for CR routers. In this architecture, end users do n ot\nhave to possess CR capabilities. CR routers could tune their\nradio interfaces to what end users use to make connections.\nDue to the close proximity between CR routers and end\nusers, the frequency reuse ratio and devices’ energy efﬁcie ncy\nare greatly enhanced. For more details of this architecture ,\ninterested readers are referred to [14].\nSSP \nBase Station End Users \n CR router \nCR transmission \nCellular transmission \nFigure 1: The CCHN-augmented IoT Network.\nB. Network model\nIn this paper, we focus on uplink data transmission from\nIoT devices to a BS. As shown in Fig.1, suppose a set\nof battery-powered devices/users, say, U={1,...,u,...,U},\n2We use “end users” and “IoT devices” interchangeably in the f ollowing\narticle to represent the same meaning.each of which initializes a session whose destination is the\nBS denoted by b. We index the set of sessions as L=\n{l1,...,lu,...,lU}and let s(lu)andd(lu)denote the source (i.e.,\ns(lu)=u) and destination (i.e., d(lu)=b) of session lu∈L,\nrespectively. Also consider the network consisting of KCR\nroutersK={1,...,k,...,K}and together with the BS b, we\ndenoteK=K∪{b}as the set of grid-powered wireless\ninfrastructures. Suppose the network applies single carri er-\nfrequency division multiplexing access (SC-FDMA) for end\nusers’ uplink transmissions, where the network’s basic ban d\nis divided into Ntotnumber of orthogonal sub-channels which\nare shared among end users. Denote the harvested band as\nmand it is allocated to the mesh network to form multi-hop\ntransmissions.\nOur model is applicable to a low mobility scenario. In such\nan environment, resource allocation can be conducted durin g\nthe channel coherence time when channel is regarded as stati c.\nIn light of this, we can just apply the line-of-sight (LOS)\nchannel model instead of fast fading ones. Moreover, althou gh\nsignaling and computation overhead is incurred when solvin g\nthe cross-layer optimization problem, the solution is appl icable\nover a relatively long time scale due to users’ low mobility\nand small variation of network parameters, which makes such\noverhead tolerable in the long run.\nIV. P ROBLEM FORMULATION\nGiven the system model described before, we target at the\nproblem of uplink end-to-end data delivery from end users\nto the BS, where the user association, uplink power control,\nchannel allocation, routing and link scheduling are jointl y\nconsidered so as to maximize the network wide end users’\nenergy efﬁciency.\nA. Uplink SC-FDMA\nIn practical systems, similar to speciﬁcations in the 3GPP\nLTE standard, the uplink SC-FDMA in our design also implies\nrestrictions on power and channel allocation [30]. First, a ny\nsub-channel can only be assigned to one user, called exclu-\nsiveness . Second, every user’s allocated sub-channels must\nbe continuous, called adjacency . Third, user’s transmit power\nshould be identical on any allocated sub-channel in order to\nretain a low peak-to-average power ratio (PAPR).\nDenote the indicator xu,kas the association variable, where\nxu,k=1implies that user uis associated with infrastructural\nnode k, and xu,k=0otherwise. Normally, the association is\nassumed to be performed in a large scale compared to the\nvariation of channel so the fast fading is averaged out over\nthe association time [31]. We also consider a relatively low\nmobility environment, where the resource allocation is car ried\nout during the channel coherence time so the channel can be\nregarded as static. Furthermore, in our network, to simplif y\nthe problem, we assume the Ntotsub-channels are not reused\nso as to avoid strong interference.4\n1) User Association: Any end user can only be associated\nwith the BS or a CR router, but not both. This physical\nconstraint can be expressed as follows\n/summationdisplay.1\nk∈Kxu,k=1, ∀u∈U, (1)\nxu,k∈{0,1}, ∀u∈U,∀k∈K. (2)\n2) Sub-channel Allocation: Suppose the end user uis allo-\ncated with Nunumber of sub-channels which are contiguous\nin nature3. Due to the property of exclusiveness , the total\nallocated sub-channels cannot exceed Ntot, which is stated in\nbelow /summationdisplay.1\nu∈UNu≤Ntot,Nu∈Z+(3)\nwhereZ+denotes the set of nonnegative integers.\n3) Link Capacity: Given the sub-channel allocation, ac-\ncording to Shannon-Hartley theorem, the capacity of the lin k\nbetween an end user uand the infrastructural node jcan be\ncalculated as follows\ncu,j=NuW·log2(1+pu|hu,j|2\nNuW·N0),∀u∈U,∀j∈K, (4)\nwhere Wis the bandwidth of each sub-channel, N0is the\npower spectrum density (of unit W/Hz) of the Additive White\nGaussian Noise (AWGN), and hu,kis the channel fading\ncoefﬁcient. Here, puis denoted as the transmit power of the\nend user uand as we know, pumust be evenly distributed\nacross the allocated sub-channels to retain a low PAPR, whic h\nis given bypu\nNu. Therefore, the link capacity is calculated as\nthe sum of all allocated sub-channels’ capacities, as shown in\n(4).\n4) Power Control: Due to the hardware constraint, the\ntransmit power of an end user cannot exceed its maximum\nallowable power level Pu,max. Since end users have different\npower capabilities, Pu,maxis a user-dependent variable. More-\nover, we do not consider the power control of CR routers so\ntheir transmit powers are assumed to be ﬁxed.\nB. The Cognitive Mesh Network\nWhen an end user is associated with a CR router, its trafﬁc\nis delivered to the BS via multi-hop transmissions in the\ncognitive mesh network. In other words, the SSP allocates\nthe harvested band mto the mesh network and performs link\nscheduling and routing optimization to determine how to ass ist\nthe end user uto complete its session luwhose destination is\nthe BS b. In what follows, we investigate the link scheduling\nand routing problem in the cognitive mesh network.\n1) Transmission Range and Interference Range: Following\nthe widely used model [26], we deﬁne the power propagation\ngain from the CR router i(∀i∈K) to another infrastructural\nnode (either a CR router or the BS) j(∀j∈K\\ i) asgi,j=\nζ·d−γ\ni,j, whereζis the antenna gain, di,jrefers to the Euclidean\ndistance between iand j, andγis the path loss exponent.\n3Note that our work can be naturally extended to incorporate t he case of\nﬁnding the optimal sub-channel allocation pattern (i.e., a speciﬁc chunk of\nNusub-channel collections) [32], [33], and we will leave it fo r the future\nwork.Let assume that CR routers apply the same constant transmit\npower Ptand deﬁne that the transmission is successful only\nwhen the received signal power exceeds a threshold Pth\nr, i.e.,\nPt·gi,j≥Pth\nr. Then, we can obtain the transmission range of\nCR router iasRT\ni=(Pt·ζ/Pth\nr)1/γ. Accordingly, we deﬁne\nthe set of infrastructural nodes being in the transmission r ange\nof the CR router i(∀i∈K) as\nTi={j∈K| di,j≤RT\ni,j/nequali}. (5)\nOn the other hand, to efﬁciently use harvested bands, the\nSSP should ensure the transmissions over different links do\nnot conﬂict with each other. In light of this, we deﬁne the\ninterference range in a similar way as before. Suppose the\nreceived interference can be ignored only when the received\npower is less than a threshold Pth\nI, i.e., Pt·gi,j<Pth\nI.\nTherefore, the interference range of the CR router i(∀i∈K)\ncan be obtained as RI\ni=(Pt·ζ/Pth\nI)1/γ. Accordingly, the set\nof infrastructural nodes being in the interference range of the\nCR router i(∀i∈K) is deﬁned as\nIi={j∈K| di,j≤RI\ni,j/nequali}. (6)\n2) Conﬂict Graph and Independent Sets [26]: Given the\nprior deﬁnition of the interference range, we can claim that\ntwo communication links conﬂict if the receiver of one link\nis within the interference range of the transmitter of the ot her\nlink. A conﬂict graph G=(V,E)is used to characterize the\ninterfering relationship among different infrastructura l links.\nSpeciﬁcally, each vertex vindicates a transmission link (i,j)\n(∀i∈K,∀j∈K\\ i) and two links are said to be conﬂicted if\nthere is an edge econnecting the two corresponding vertices.\nWith this conﬂict graph being created, we can deﬁne an\nindependent set (IS), which consists of a set of vertices I⊆V\nand any two of them do not share an edge. In this case, all the\ntransmission links in an IS do not interfere with each other a nd\nthus can be carried out successfully at the same time. If addi ng\none more vertex into the IS Iresults in a non-independent\none, the set Iis called the maximal independent set (MIS).\nWe can collect all the MISs of the conﬂict graph in a set\nQ={I1,...,Iq,...,IQ}, where Qrepresents the total number of\nMISs, i.e., Q=|Q|.\n3) Link Scheduling: In this paper, we consider different\nMISs are scheduled with certain time shares (out of unit\ntime) so that the links within each MIS can carry out the\ntransmissions simultaneously. From our previous discussi on,\nwe know only one of the MISs can be active at one time\ninstance and we denote the time share allocated to the MIS Iq\nasλq. Therefore, we have to satisfy the following constraint\nQ/summationdisplay.1\nq=1λq≤1, (7)\n0≤λq≤1,∀q∈{1,...,Q}. (8)\nOn the other hand, the link capacity of the link (i,j)can be\nobtained based on Shannon-Hartley theorem, which is\nci,j=Wm·log2(1+Pt·gi,j\nWm·N0),∀i∈K,∀j∈K\\ i,(9)\nwhere Wmis the bandwidth of the harvested band. According5\nto the link scheduling, the actual data rate over the link (i,j)\ncould be 0 if(i,j)is not scheduled at a time instance. In other\nwords, we use ri,j(Iq)to represent the achieved data rate over\nthe link(i,j)when Iqis scheduled, where ri,j(Iq)=ci,jif\n(i,j)∈Iqand 0 otherwise. Considering the link (i,j)could\nexist in all the MISs, the total achieved data rate over the li nk\n(i,j)can be expressed as\nRi,j=Q/summationdisplay.1\nq=1λqri,j(Iq). (10)\n4) Flow Routing: In this paper, we consider the network\nlevel end-to-end (from users to the BS) service provisionin g.\nSuppose the end user uinitiates a session lu, the SSP should\ndetermine whether to support it by associating udirectly to\nthe BS through one-hop transmission or connecting uto the\ncognitive mesh network and then arriving at the BS via multi-\nhop transmissions. Here denote fi,j(lu)as the supported ﬂow\nrate for the session luover the link(i,j)at the network level.\nIf node iis the source of session lu, which is the end user u\n(i.e., s(lu)=u), we have the following constraints\n/summationdisplay.1\nj∈{j|u∈Tj}fj,u(lu)=0, (11)\nfu,j(lu)·xu,j=r(lu). (12)\nThe constraint (11) means that the incoming ﬂow rate of any\nsession at the source node is zero since the end user is the\ninitiator of the session. The constraint (12) reﬂects the ﬁr st hop\nfrom the end user uto an infrastructural node j∈K, where\nr(lu)signiﬁes the achievable data rate of user u. Clearly, the\nassociation variable is coupled with ﬂow rate in (12), imply ing\nthat there only exists one wireless link from the end user u\nto one of the infrastructural nodes to support u′sdata rate.\nBesides, the ﬂow rate on a link should be constrained by the\nlink capacity according to (4), which is expressed as\n0≤fu,j(lu)≤cu,j. (13)\nFor any infrastructural node i∈K, which is the CR router,\nthe ﬂow conservation law (FCL) implies that for any session\nlu, the total ﬂow into imust be equal to the total ﬂow out of\ni. This can be expressed as\n/summationdisplay.1\nj∈{j|i∈Tj}fj,i(lu)+fu,i(lu)·xu,i=/summationdisplay.1\nk∈Tifi,k(lu). (14)\nClearly, if the CR router iis directly associated with end user\nu, constraint (14) could be rewritten as r(lu)=/summationtext.1\nk∈Tifi,k(lu)\naccording to (12); whereas if the CR router iis the interme-\ndiate infrastructural node to support lu, constraint (14) would\nbe equivalent to/summationtext.1\nj∈{j|i∈Tj}fj,i(lu)=/summationtext.1\nk∈Tifi,k(lu).\nMoreover, all the ﬂows in the cognitive mesh network are\ncompleted at the BS, which means that the BS is the common\ndestination, i.e., d(lu)=b,∀lu∈L. Thus, we have another\nconstraints for the destination node bdescribed as follows:\n/summationdisplay.1\nj∈Tbfb,j(lu)=0, (15)fu,b(lu)·xu,b+/summationdisplay.1\nj∈{j|b∈Tj}fj,b(lu)=r(lu). (16)\nNote that if the end user uis directly connected to the BS (i.e.,\nxu,b=1), (16) becomes/summationtext.1\nj∈{j|b∈Tj}fj,b(lu)=0, indicating that\nsession luis not supported through the CR routers. Instead, if\nxu,b=0, meaning user uis associated with the cognitive mesh\nnetwork, (16) could be rewritten as/summationtext.1\nj∈{j|b∈Tj}fj,b(lu)=r(lu).\nBesides, for the link from one CR router i∈K to another\ninfrastructural node j∈ Ti, the total ﬂow rate on that link\n(by aggregating all the rates of supported sessions) should not\nexceed the link capacity. From our previous discussion, we\nknow that the link capacity is dependent on the scheduled\ntime share on that link, which is described as (10). Thus, we\nhave the following constraint\n0≤/summationdisplay.1\nlu∈Lfi,j(lu)≤Ri,j. (17)\nC. User-centric Network-wide Energy Efﬁciency Optimizati on\nOur work aims to maximize the user-centric network-wide\n(i.e., the weighted sum of end users’) energy efﬁciency by\nconsidering user diversity in power capability (e.g., resi dual\nenergy, maximal allowable power). In light of it, we apply th e\nratio-based EE model, where the EE is measured in bit/s/Joul e\nand deﬁned as the ratio of data rate to power consumption\nwhich in this work are both with respect to the end users.\nThus, we coin it as the user-centric EE metric, in contrast to the\nprevious works that applied BS’s power consumption in the EE\ndeﬁnition [20], [21]. Furthermore, contrary to the convent ional\nEE deﬁnition as the ratio of the system sum rate to the sum\npower consumption [18], [34], our user-centric network-wi de\nEE measures the weighted sum of end users’ EE, such that the\nheterogeneous EE requirements from different users of vari ous\npower capabilities can be investigated.\nBefore presenting the optimization problem, we ﬁrst deﬁne\nthe power consumption model of end user uasηpu+Pc, where\nηis the efﬁciency of the transmit power ampliﬁer when it\noperates in the linear region, whereas Pcis the circuitry power\ndissipated in all other circuit blocks (e.g., mixer, oscill ator,\nDAC and etc.) which is independent of the transmit power pu\nand normally a constant value. Then, the EE for the end user\nucan be obtained asr(lu)\nηpu+Pc, where r(lu)denotes its achieved\ndata rate according to (12). Finally, we introduce a weighti ng\nfactorωuassociated with user u′sEE, which provides a means\nfor service differentiation as well as fairness. Particula rly, the\nweights could be determined inversely proportional to user s’\nresidual energy so that less power capable users are allocat ed\nwith higher EE priorities.\nBy considering the user association, power control, channe l\nallocation, routing and link scheduling constraints intro duced\npreviously, we can thus formulate the following optimizati on\nproblem to achieve the maximal U ser-centric N etwork-wide6\nEE(UNEE-Max)\nMax/summationdisplay.1\nu∈Uωur(lu)\nηpu+Pc\ns.t.(1)∼(3),(7)∼(8),(11)∼(17);\n0≤fi,j(lu),∀lu∈L,∀i∈K,∀j∈K\\ i;\n0≤pu≤Pu,max,∀u∈U(18)\nwhere xu,k,Nu,pu,fu,j(lu),fi,j(lu)andλqare optimiza-\ntion decision variables. Clearly, UNEE-Max is a cross-laye r\noptimization problem involving coupled variables from the\nphysical layer to the network layer. In the next section,\nwe elaborate several difﬁculties in addressing UNEE-Max\nproblem and introduce techniques to solve it accordingly.\nV. O VERVIEW OF THEUNEE-M AXPROBLEM\nA. Complexity of The UNEE-Max\nWe ﬁrst highlight several key difﬁculties in solving the\nUNEE-Max problem.\n1) NP-completeness for searching all MISs: Under con-\nstraint (7), we need to search all the MISs for link schedulin g.\nHowever, ﬁnding all the MISs in a conﬂict graph G=(V,E)\nis NP-complete, which is the common obstacle encountered\nin multi-hop wireless networks [35]. Although we can apply\nbrute-force search when the size of G=(V,E)is small, it\nis highly prohibitive when Gbecomes large. Therefore, it\nrequires a cost-effective approach to ﬁnd MISs so as to make\nthe problem tractable.\n2) Uncertainty of the harvested band: In CRNs, SUs are\nallowed to access PUs’ spectrum bands only when these bands\nare not occupied by PUs. SUs must immediately evacuate\nwhen PUs reclaim the spectrum. In practice, the availabilit y\nof these harvested bands is highly unpredictable due to the\nuncertainty of PUs’ activity and SSP’s statistical inferen ce\nmodel (i.e., false alarm / miss detection probabilities) [3 6].\nTherefore, the bandwidth Wmof the harvested band (deﬁned in\n(9)) is a random variable, whose probability distribution c ould\nbe derived from statistical characteristics of these PUs’ b ands\nfrom some observations and experiments [37]. However, the\nrandomness of Wmmakes (17) a stochastic constraint, which\ncauses the feasible region of UNEE-Max to be both random\nand nonconvex.\n3) Combinatorial nature of user association: The indicator\nvariable xu,kin constraints (1) and (2) enforces unique as-\nsociation, which makes the problem combinatorial. Althoug h\nthe classical branch-and-bound approach can be applied to\nsolve general integer programming problems, due to the tigh t\ncoupling between the association and the resource allocati on\n(i.e., power control, link scheduling and routing) in UNEE-\nMax, it is difﬁcult to solve using traditional approaches.\n4) Nonconcavity of objective function: The objective func-\ntion in the UNEE-Max problem is in the form of weighted\nsum of linear fractional functions (WSoLFF), which is gener -\nally nonconcave [38]. An immediate consequence is that the\npowerful tools from the convex optimization theory do not\napply to the UNEE-Max, and the KKT conditions are only\nnecessary conditions for optimality [39]. Therefore, we ne edto transform the UNEE-Max to a certain form from which\napproximate solution to the UNEE-Max can be found.\nB. The UNEE-Max Relaxation Algorithm\nAfter outlining the difﬁculties in solving UNEE-Max prob-\nlem, we introduce the relaxation or transformation techniq ues\nto make the UNEE-Max tractable.\n1) Critical MIS set: Although there exists exponentially\nmany MISs in a conﬂict graph, Li et. al. [35] proved that\nonly a small portion of MISs, termed as critical MIS set ,\ncan be scheduled in the optimal resource allocation. Instea d\nof searching all MISs, we thus apply the SIO-based ap-\nproach proposed in [35], [40] to return the critical MIS set\nQ′={I1,...,Iq,...,IQ′}in polynomial time, where Q′⊆Q .\nTherefore, we can replace Qwith Q′in constraint (1), and\n(17) to make the UNEE-Max problem more tractable. Note\nthat SIO-based approach may only give a fraction of Q′in a\nlimited searching time leading to a loss in solution optimal ity.\nIn light of this, we could deliberately allow a longer search ing\ntime as the SIO-based approach can be run ofﬂine.\n2)∆-conﬁdence level: To address the stochastic constraint\n(17), inspired by the concept of value at risk (VaR) in [41],\nwe reformulate it as a chance constraint of ∆-conﬁdence level\nrepresented as follows\nPr0≤/summationdisplay.1\nlu∈Lfi,j(lu)≤Q/summationdisplay.1\nq=1λqWmlog2(1+Pt·gi,j\nWmN0)≥∆,\nwhere∆∈[0,1]indicates the conﬁdence level for stochastic\nconstraint (17) to be satisﬁed and (i,j)∈Iq. Suppose FWm(·)\nrepresent the cumulative distribution function (CDF) of ra n-\ndom variable (r.v.) Wm. We could then obtain the Fci,j(·)as\nthe CDF for the r.v. ci,j=Wmlog2(1+Pt·gi,j\nWmN0), which is the\nlink capacity of(i,j). Thus, by integrating the critical MISs,\nthe above inequality could be reformulated as\n0≤/summationdisplay.1\nlu∈Lfi,j(lu)≤Q′/summationdisplay.1\nq=1λqF−1\nci,j(1−∆). (19)\nBy replacing (17) with (19), the original stochastic con-\nstraint is converted to a linear inequality constraint in fi,j(lu)\nandλq.\n3) Integer relaxation and rounding: In the ﬁrst phase, we\nassume that end users can be associated with the BS and CR\nrouters at the same time. In other words, we relax the integer\nassociation variable xu,kto the continuous domain of [0,1].\nUnder this assumption, we also introduce the sub-channel\nauxiliary variable Nu,k=Nu·xu,k, where Nu,k∈R+andR+\nrepresents set of all nonnegative numbers, the power auxili ary\nvariable pu,k=pu·xu,kand the ﬂow auxiliary variable\n/tildewidefu,k(lu)=fu,k(lu)·xu,j, so that/summationtext.1\nk∈KNu,k=Nu,/summationtext.1\nk∈Kpu,k=pu,\n/summationtext.1\nk∈K/tildewidefu,j(lu)=r(lu). Therefore, we can eliminate association\nconstraint (1) and (2) and rewrite the UNEE-Max problem as7\na Relaxed-UNEE-Max problem which is described as follows\nMax/summationdisplay.1\nu∈Uωu/summationtext.1\nk∈K/tildewidefu,k(lu)\nη/summationtext.1\nk∈Kpu,k+Pc\ns.t. 0≤/summationdisplay.1\nu∈U/summationdisplay.1\nk∈KNu,k≤Ntot,Nu,k∈R+;\n0≤/summationdisplay.1\nk∈Kpu,k≤Pu,max,∀u;pu,k∈R+;\n0≤/tildewidefu,k(lu)≤Nu,kWlog2(1+pu,k|hu,k|2\nNu,kWN 0),∀u,∀k;\n/summationdisplay.1\nj∈{j|i∈Tj}fj,i(lu)+/tildewidefu,i(lu)=/summationdisplay.1\nk∈Tifi,k(lu),∀u;\n/summationdisplay.1\nj∈{j|b∈Tj}fj,b(lu)=/summationdisplay.1\nk∈K/tildewidefu,k(lu),∀u;\n(7)∼(8),(11),(15),(19);\n0≤fi,j(lu),∀lu∈L,∀i∈K,∀j∈K\\ i.\n(20)\nIn the second phase, we develop a rounding scheme, as\nwhat will be discussed in Section VII, to convert the output\nof the Relaxed-UNEE-Max problem into a feasible value that\nsatisﬁes the constraints of original problem UNEE-Max in\n(18).\n4) Parametric subtractive transformation: It is clear that\nthe constraints in the Relaxed-UNEE-Max problem (20) form\na convex feasible set Xw.r.t. the optimization variable set\n(p,N,/tildewidef,f,λ) ∈X.4However, it is still challenging due\nto the sum-of-ratio form in the objective [42]. To overcome\nthis difﬁculty, we ﬁrstly transform the objective function in\n(20) into an intermediate form by introducing an auxiliary\nvariableα={α,...,αU}and reformulate the Relaxed-UNEE-\nMax problem as\nMax/summationdisplay.1\nu∈Uωuαu\ns.t./summationtext.1\nk∈K/tildewidefu,k(lu)\nη/summationtext.1\nk∈Kpu,k+Pc≥αu,∀u;\n(p,N,/tildewidef,f,λ)∈X.(21)\nAlthough the objective is an afﬁne function w.r.t. α, problem\n(21) is not a convex optimization yet due to the fractional\nconstraint. Thus, we further convert (21) into a parametric\nsubtractive form and show in the following theorem its equiv -\nalence to the weighted sum maximization problem (21) with\nfractional constraint.\nTheorem V .1. Suppose(p∗,N∗,/tildewidef∗,f∗,λ∗,α∗)is the solution\nto problem (21), there exist β∗such that for the parametric\nvariablesα=α∗andβ=β∗,(p∗,N∗,/tildewidef∗,f∗,λ∗)satisﬁes the\n4For the sake of brevity, we deﬁne the vectors of optimization variables as\np={pu,k},N={Nu,k},/tildewidef={/tildewidefu,k(lu)},f={fi,j(lu)}andλ={λq}.KKT conditions of the following problem\nMax/summationdisplay.1\nu∈Uβu/bracketleftBigg/summationdisplay.1\nk∈K/tildewidefu,k(lu)−αu(η/summationdisplay.1\nk∈Kpu,k+Pc)/bracketrightBigg\ns.t.(p,N,/tildewidef,f,λ)∈X.(22)\nAlso, the following system equations hold for the parametri c\nvariables (α∗,β∗) and the tuple(p∗,N∗,/tildewidef∗,f∗,λ∗):\n \nαu=/summationtext.1\nk∈K/tildewidefu,k(lu)\nη/summationtext.1\nk∈Kpu,k+Pc\nβu=ωu\nη/summationtext.1\nk∈Kpu,k+Pc.(23)\nOn the contrary, if (p∗,N∗,/tildewidef∗,f∗,λ∗)is the solution to\nproblem (22) while (23) system equations are met for α=\nα∗andβ=β∗, then(p∗,N∗,/tildewidef∗,f∗,λ∗,α∗)satisﬁes KKT\nconditions for problem (21), where β=β∗is the Lagrange\nmultiplier for fractional constraint in (21).\nProof. See Appendix A. /squaresolid\nBased on Theorem V .1, we can address the problem (21) by\nsolving (22) while guaranteeing (23), such that the solutio n of\nthe Relaxed-UNEE-Max could be obtained. Furthermore, it is\nworth noting that if the solution is unique, it is also the glo bal\nsolution [38]. Toward solving (22), we apply a dual-based\napproach, which has been widely adopted in various network\nsettings for its simplicity of implementation, and augment\nit with the parametric programming to form inner loop and\nouter loop iterative update processes. The detailed steps a re\ndescribed in the following section.\nVI. A LGORITHM FOR THERELAXED -UNEE-M AX\nBased on our prior discussion, the Relaxed-UNEE-Max\nproblem is equivalent to problem (21), whose solution is\nidentical to (22) when satisfying (23). Hence, we focus on\nsolving problem (22), and for the presentation clarity, we ﬁ rst\noutline the general idea of the solution algorithm.\nThe whole algorithm is split into an inner loop and an\nouter loop optimization problem. The algorithm starts with\ninitializing the parametric variables αandβ. For the given\nαandβ, (22) becomes a convex optimization problem with\nan afﬁne objective and a convex feasible set. The inner loop\napplies dual decomposition approach to solve this convex\noptimization problem, and each iteration of the dual-based\nmethod is termed as the inner loop iterations. Multiple inne r\nloop iterations are performed till the optimal dual and prim al\nsolutions are reached. The output of inner loop, which is\n(p∗,N∗,/tildewidef∗,f∗,λ∗), are then fed back to the outer loop to\nupdate the parametric variables αandβ. The overall algorithm\nterminates if the convergence condition for αandβ(we will\nelaborate it later) are met. Otherwise, the algorithm conti nues\nby solving the inner loop optimization problem again using\nthe updated αandβ.8\nA. Algorithm for The Inner Loop Optimization Problem\nSuppose the parametric variables are αtandβtat\nthe tthouter loop iteration, the inner loop procedure\nstarts with introducing a partial Lagrange multiplier v=\n{v1,1,...,v1,|K|,...,vU,|K|}w.r.t. the third constraint (nonlinear\ncapacity constraint) in problem (20) attempting to decoupl e\nthe decision variables. We denote the partial Lagrangian by\nL(v;p,N,/tildewidef,f,λ)and express it as\nL(v;p,N,/tildewidef,f,λ)=\n/summationdisplay.1\nu∈Uβt\nu/bracketleftBigg/summationdisplay.1\nk∈K/tildewidefu,k(lu)−αu(η/summationdisplay.1\nk∈Kpu,k+Pc)/bracketrightBigg\n−\n/summationdisplay.1\nu∈U/summationdisplay.1\nk∈Kvu,k/bracketleftbigg\n/tildewidefu,k(lu)−Nu,kWlog2(1+pu,k|hu,k|2\nNu,kW·N0)/bracketrightbigg\n.\n(24)\nThe dual function can be then obtained as\nD(v)=max\np,N,/tildewidef,f,λL(v;p,N,/tildewidef,f,λ).\nSince problem (22) is convex and Slater’s condition for\nconstraint qualiﬁcation is assumed to hold, it follows that there\nis no duality gap and thus the primal problem can be solved\nvia its dual\nRelaxed-UNEE-Max Optimal =min\nv/{ollowsequal0D(v).\n1) Dual problem: We solve the dual variables via the pro-\njected subgradient method. First, let us denote the primal v ari-\nables obtained at sthinner loop iteration as (ps,Ns,/tildewidefs,fs,λs).\nThen, the dual variables at sthinner loop iteration are updated\nas follows\nvs+1\nu,k=/bracketleftBigg\nvs\nu,k+δ(/tildewidestfs\nu,k(lu)−Ns\nu,kWlog2(1+ps\nu,k|hu,k|2\nNs\nu,kW·N0))/bracketrightBigg+\n(25)\nwhereδis the step size and [·]+denotes the projection into\nthe set of non-negative real numbers.\nIn what follows, we focus on solving the primal variables\ngiven the dual variables at each inner loop iteration.\n2) Primal problem: We now argue that the primal problem\narg max\np,N,/tildewidef,f,λL(vs;p,N,/tildewidef,f,λ)\ncan be reorganized into a routing subproblem and a physical\nlayer resource allocation subproblem. Thus, solving the pr imal\nproblem is equivalent to solving two independent subproble ms,\neach of which is fairly straightforward. Toward this end, we\nrewrite original partial Lagrangian as follows\nL(vs;·)=/summationdisplay.1\nu∈U/summationdisplay.1\nk∈K/bracketleftBig\nβt\nu/tildewidefu,k(lu)−vs\nu,k/tildewidefu,k(lu)/bracketrightBig\n/bracehtipupleft/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext /bracehtipdownright/bracehtipdownleft/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext /bracehtipupright\nrouting subproblem+/summationdisplay.1\nu∈U/bracketleftBig\n/summationdisplay.1\nk∈Kvs\nu,kNu,kWlog2(1+pu,k|hu,k|2\nNu,kWN 0)−βt\nuαt\nu(η/summationdisplay.1\nk∈Kpu,k+Pc)\n/bracehtipupleft/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext /bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehtipdownright/bracehtipdownleft/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext /bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehtipupright\nresource allocation subproblem/bracketrightBigand we can represent it as L(vs;·)=L(vs;/tildewidef,f,λ)rout+\nL(vs;p,N)res. Thus, we can separate the primal optimization\nproblem into the following subproblems\nMaxL(vs;/tildewidef,f,λ)rout\ns.t./summationdisplay.1\nj∈{j|i∈Tj}fj,i(lu)+/tildewidefu,i(lu)=/summationdisplay.1\nk∈Tifi,k(lu),∀u;\n/summationdisplay.1\nj∈{j|b∈Tj}fj,b(lu)=/summationdisplay.1\nk∈K/tildewidefu,k(lu),∀u;\n(7)∼(8),(11),(15),(19);\n0≤fi,j(lu),∀lu∈L,∀i∈K,∀j∈K\\ i.(26)\nwhich is the routing subproblem, while\nMaxL(vs;p,N)res\ns.t. 0≤/summationdisplay.1\nu∈U/summationdisplay.1\nk∈KNu,k≤Ntot,Nu,k∈R+;\n0≤/summationdisplay.1\nk∈Kpu,k≤Pu,max,∀u;pu,k∈R+,(27)\nwhich is the physical layer resource allocation (i.e., powe r\ncontrol and channel allocation) subproblem.\nIt is clear that with the given dual variables vsand the para-\nmetric variables αtandβt, the routing subproblem (26) is a\nlinear optimization problem w.r.t. the decision variables , which\ncan be easily solved by many softwares, such as CPLEX. On\nthe other hand, the resource allocation subproblem belongs\nto the general convex optimization problem with a concave\nobjective and a convex feasible region. Thus, it also can be\neasily solved via the interior point method, for instance.\nWith the primal variables obtained at each iteration, they a re\nfed back to the dual variable update process according to (25 ),\nand we keep iterating the inner loop iterations till a predeﬁ ned\nstopping criterion is met.\n3) Stopping criterion and step size: First, we deﬁne the\nstopping criterion for the inner loop algorithm as |vs+1−vs|≤\nε, whereεdenotes a predeﬁned threshold. On the other hand,\nthe choice of step size δaffects the convergence rate of\nthe solution. Normally, we could apply diminishing step siz e\nor constant but sufﬁciently small step size [43], which are\nboth guaranteed to converge to the optimal solutions. We wil l\nexamine the impact of step size on the convergence rate in the\nperformance evaluation section.\nB. Algorithm for The Outer Loop Optimization Problem\nThe outer loop optimization problem is in a parametric\nsubtractive form as the objective in problem (22). The goal\nis to iteratively obtain the parametric variables αandβ,\nwhere the iteration here is termed as the outer loop iteratio n.\nParameterαmay be intuitively viewed as the “price” of power\nconsumption while parameter βis introduced as the Lagrange\nmultiplier for the fractional constraint in (21). Here, we a pply\nthe gradient method [20] to update the parametric variables in\na following way:\nαt+1\nu=αt\nu−ξ(αt\nu−/summationtext.1\nk∈K/tildewidestft,s∗\nu,k(lu)\nη/summationtext.1\nk∈Kpt,s∗\nu,k+Pc),∀u, (28)9\nβt+1\nu=βt\nu−ξ(βt\nu−ωu\nη/summationtext.1\nk∈Kpt,s∗\nu,k+Pc),∀u, (29)\nwhere/tildewidestft,s∗\nu,k(lu)andpt,s∗\nu,kare the converged values of decision\nvariables after s∗inner loop iterations. Similar to the inner loop\noptimization, another small threshold value σis selected and\nthe stopping criterion is set to |αt+1−αt|≤σand|βt+1−βt|≤\nσ. The convergence of the outer loop optimization can be\nguaranteed by the gradient method and the step size ξshould\nbe selected to be sufﬁciently small. Later, we will give the\nconvergence analysis in the performance evaluation sectio n.\nDual update in \n(25)\nRouting prob. \nin (26)Res. alloc. prob. \nin (27)\nSub-problems Master Problem Inner Optimization \nParametric update \nin (28,29)Outer Optimization \n,\n,,\nFigure 2: Summary diagram for the solution algorithm\nof problem (22).\nFor the presentation clarity, we give a high level overview o f\nthe solution algorithm for optimization problem (22) as sho wn\nin Fig.2, which shows the necessary information exchange\nbetween solution processes. Besides, Algorithm 1 formally\ndescribes the solution algorithm for the Relaxed-UNEE-Max .\nAlgorithm 1 Algorithm for Solving Relaxed-UNEE-Max\nInput: Given network settings; Initialize all the variables\np0,N0,/tildewidef0to any feasible value; Let v0=α0=α0=0;\nSett=s=0; Initialize thresholds σ,ε and step size δ,ξ.\nOutput: p∗,N∗,/tildewidef∗,f∗,λ∗\n1:Calculateα1,β1andv1according to Eq.(23) and Eq.(25),\nrespectively.\n2:while|αt+1−αt|≥σor|βt+1−βt|≥σdo\n3: t←t+1;\n4: while|vs+1−vs|≥εdo\n5: s←s+1;\n6: Solve resource allocation sub-problem (27) and ob-\ntainps,Ns;\n7: Solve routing sub-problem (26) and obtain /tildewidefs,fs,λs;\n8: Update dual variable vs+1according to Eq.(25);\n9: end while\n10: Update parametric variables αt+1andβt+1according\nto Eq.(28) and Eq.(29), respectively;\n11:end while\nVII. U SERASSOCIATION AND INTEGER ROUNDING\nTo this end, the problem (22) is solved via the prior\nalgorithm whose solution is identical to the one in the Relax ed-\nUNEE-Max problem (20). However, due to the physicalconstraint that every user can only be associated with one\ninfrastructural node, the previously obtained solution sh ould\nbe converted to a feasible one for the original problem. Be-\nsides, the integer property of the number of allocated OFDM\nsub-channels also requires a further rounding procedure to\nthe obtained solution. Inevitably, this step could introdu ce\nperformance degradation, but in the performance evaluatio n\nsection, we will show that its impact on the performance is\nquite limited.\nFirst, we present the association rule as\nk=arg max\ni∈K/tildewidestf∗\nu,i(lu)\nηp∗\nu,i+Pc,∀u.\nThe above operation indicates that we associate the user wit h\nthe infrastructural node which provides the largest value o f\nEE. In other words, if end user uobtains the highest EE from\nnode k, we set the association variable xu,k=1while xu,i=0\nfori/nequalk. In so doing, we can ﬁx the association variables and\nthe original problem UNEE-Max in (18) could be simpliﬁed\nsigniﬁcantly. Here, we coin this simpliﬁed problem by ﬁxing\nthe association variables as Asso-UNEE-Max and it can be\nsimilarly addressed by the prior algorithm in Fig.2. In late r\nsection, the comparison between the network performance of\nAsso-UNEE-Max and the one obtained by solving Relaxed-\nUNEE-Max will be demonstrated.\nNext, we introduce the integer rounding function as\nRnd(Nu)=max{⌊Nu⌋,1},∀u,\nwhere the operator ⌊·⌋rounds the input to the greatest integer\nthat is less than or equal to the input. Besides, the reason we\napply max function is to guarantee that every end user can\nat least be assigned with one sub-channel for fairness. The\nrounding operation is applied to the solution obtained from\nsolving the Asso-UNEE-Max problem, so that the OFDM\nchannel allocation can be determined accordingly. However ,\nthe ﬂow variables obtained from Asso-UNEE-Max may not\nbe feasible anymore when doing integer rounding. Therefore ,\nwe need to re-solve the UNEE-Max problem (18) and get the\ncalibrated ﬂow values which are the feasible ones. Noticing\nthat for the ﬁxed channel allocation, sub-problem (27) can b e\neasily addressed by classical iterative water-ﬁlling algo rithm\n[44], which is just a one-dimensional (i.e., power) optimiz ation\nproblem. Here, we denote this solution as the one from a\nso-called Rnd-UNEE-Max problem. Its performance will be\ncompared with the ones obtained from Asso-UNEE-Max and\nRelaxed-UNEE-Max, respectively, in the evaluation sectio n.\nIn Algorithm 2, we formally give the detailed steps to de-\nscribe the algorithm for user association and integer round ing\nfor the outputs of Algorithm 1.\nVIII. P ERFORMANCE EVALUATION\nA. Simulation Setup\nWe consider a 500×500m2area served by one BS and\n12 CR routers, where the BS is put in the center while\nCR routers represented in squares are placed around it, as\nshown in Fig.3. We also randomly scatter 35 end users in\nthis area whose locations are shown by dots. The end users’10\nAlgorithm 2 Algorithm for User Association and Integer\nRounding of Outputs of Algorithm 1\nInput: Given the output of Algorithm 1.\nOutput: The calibrated variables p∗,N∗,/tildewidef∗,f∗,λ∗\n1:foru=1:U do\n2: Find ksuch that k=arg max\ni∈K/tildewidestf∗\nu,i(lu)\nηp∗\nu,i+Pc;\n3: Setxu,k=1;\n4:end for\n5:Update the problem (18) and solve the Relaxed-UNEE-\nMax according to Alg.1 to obtain p′,N′,/tildewidef′,f′,λ′;\n6:foru=1:U do\n7: LetNu∗=max{/floorleftbig\nNu′/floorrightbig\n,1};\n8:end for\n9:Update the problem (18) and solve the Relaxed-UNEE-\nMax according to Alg.1 to obtain p∗,/tildewidef∗,f∗,λ∗.\ndevices are assumed to have an identical circuitry power\nconsumption Pc=50mW and power ampliﬁer efﬁciency\nη=5.78. We assume the users’ allowable transmit power\nPu,maxmay vary and its impact on system performance will\nbe examined later. To provide fairness for end users, all the\nweighting factors ωare set to 1. On the infrastructure side,\nthe CR routers are assumed to employ ﬁxed power Ptfor\ntransmission and their antenna gain is set to ζ=4.63. The\npower interference threshold Pth\nIis set to 3.59×10−7Wwhile\nthe receiving power threshold Pth\nris set to 1.0×10−6W.\nThe transmission environment between infrastructural nod es\nare assumed to have path loss exponent n=3. Given these\nsystem parameters, the interference/communication range can\nbe calculated numerically and we could obtain the conﬂict\ngraph in this regard. We utilize the OFDM channel model for\nwireless link between end user uand infrastructural node kas\n128.1+37.6log10(ru,k)dBm where ru,kis in kilometers [45].\nFollowing the standard, we set the bandwidth of each sub-\nchannel as W=180KHz . The noise power spectral density\nis set to N0=1×10−12W/Hz. In addition, for the harvested\nband, we consider that the availability of it follows a unifo rm\ndistribution.\nAs for the algorithmic parameter settings, we set the stop-\nping threshold εandσas 0.01 and 0.8, respectively; while\nthe step size δ=ξ=1×10−5.\nB. Benchmark Setting\nTo demonstrate the advantage of our proposed ideology in\nimproving end users’ energy efﬁciency, we leverage the basi c\ncellular network (i.e., 4G/LTE) as the benchmark to compare\nwith. In other words, we consider the same end users within\nthis geographical area served by the small cell BS as shown in\nFig.3 (excluding CR routers). Similarly, the benchmark UNE E\nmaximization problem, coined as Ben-UNEE-Max, can be\n0\n-100 \n-200 200 \n100 \n0\n-100 \n-200 200 \n100 \n0-100 -200 100 200 0-100 -200 100 200 \nFigure 3: Evaluated network topology in an 500×500m2\narea: 35 end users in blue dots, 12 CR routers in yellow\nsquares and 1 BS in black diamond.\nproposed as follows:\nMax/summationdisplay.1\nu∈Uωur(lu)\nηpu+Pc\ns.t./summationdisplay.1\nu∈UNu≤Ntot,Nu∈Z+;\n0≤r(lu)≤NuWlog2(1+pu|hu,b|2\nNuW·N0),∀u∈U;\n0≤pu≤Pu,max,∀u∈U.(30)\nIn this benchmark setting, end users have to be served by\nthe BS so the design of user association, link scheduling and\nﬂow routing is eliminated. Rather, we only consider the powe r\ncontrol and channel allocation in this one-hop transmissio n\nscenario. To effectively solve (30), the same transformati on\napproach can be applied to ﬁrstly convert it into a tractable\none, which is then solved via the water-ﬁlling algorithm [44 ].\nC. Results and Analysis\nFirst, we examine the convergence behaviors for both inner\nloop and outer loop optimizations. Since the inner loop is\na dual-based (i.e., Lagrangian) algorithm, we also compare\nits convergence rate under different selection of step size s.\nThe results are shown in Fig.4. For demonstrative purposes,\nwe only randomly select 10 users for this simulation and\nuse their average EE as the metric to show the convergence\nperformance. Here, the user’s maximum allowable power\nPu,maxis set as 1.5W while CR routers’ transmit power Ptis\nset as 1W. The bandwidth of harvested band Wmis 100KHz\nand we set the conﬁdence level ∆=0.7, while the number\nof OFDM sub-channels is selected to Ntot=100. Moreover,\nthe data in Fig.4a is collected at the last iteration of the ou ter\nloop optimization.\nAs we can see from Fig.4a, the average EE monotonically\nincreases till the algorithm converges and the EE remains\nrelatively constant (i.e., the difference not exceeding th e\nthreshold) afterwards. It can be observed that the algorith m11\n0 50 100 150 200 250 3000.000.350.701.051.40Average EE (Kbits/J)\nIterationsδ= 1e-5\nδ= 5e-6\nδ= 1e-6\n(a) Inner loop convergence rate\nunder various step sizes0 100 200 300 400 5000.51.01.5\nAuxiliary Variable α\nAuxiliary Variable β−1\nIterationsAuxiliary Variable α(Kbits/J)\n0.20.40.60.8\nAuxiliary Variable β−1(W)\n(b) Outer loop convergence rate\nw.r.t. parametric variables\nFigure 4: Convergence analysis for Algorithm.1\ncan be guaranteed to converge to the same value under three\ndifferent step-size settings, but δ=1×10−5gives the fastest\nconvergence rate (around 48 iterations). This is because as long\nas the step size is sufﬁciently small to guarantee convergen ce,\nan even smaller step size is not necessary as it slows down the\nrate to the optimal value. On the other hand, Fig.4b illustra tes\nthe convergence performance for the outer loop algorithm. F or\nthe notational convenience, we take the reciprocal of βso that\nits unit now becomes W, while the unit of αis naturally being\nKbits/Jaccording to (23). It can be seen that the converged\noptimal value of αis exactly the same as the one in Fig.4a,\nwhich proves the overall convergence of Algorithm 1. On the\nother hand, we observe that the average transmit power for\nend users is around 0.28W at convergence, which is a small\nvalue compared to Pu,max.\nGiven the feasibility of Algorithm 1, we now conduct the\nperformance comparison from solving Relaxed-UNEE-Max,\nAsso-UNEE-Max and Rnd-UNEE-Max utilizing Algorithm 1\nand Algorithm 2, respectively. Besides, by solving (30), we ob-\ntain the network-wide energy efﬁciency in the 4G/LTE cellul ar\nnetwork, which is used as the benchmark. The evaluation is\nconducted under different network sizes in terms of the numb er\nof end users. We also employ the same values of Ntot,Pu,max,\nWmand conﬁdence level ∆as the previous simulation. The\nresults are shown in Fig.5. It can be seen that these curves\ndemonstrate the same relationship between the network size\nand the network-wide energy efﬁciency: as the number of\nusers increase linearly, the network-wide energy efﬁcienc y ﬁrst\ngrows exponentially and then increases slowly. The reason i s\nthat the network resources in terms of OFDM sub-channels\nand harvested band are sufﬁcient when the network size is\nsmall and introducing more users will increase the resource\nutilization efﬁciency, thus increasing the total network E E. As\nthe number of users keeps increasing, the network becomes\ncongested in the sense that scheduling and routing in the\ncognitive mesh network becomes the major bottleneck to\nfurther boost the network performance. In the later evaluat ion,\nwe will demonstrate this phenomenon.\nOn the other hand, we can see that the solution to the\nRelaxed-UNEE-Max problem yields the highest network-wide\nEE since every user can be associated with several infras-\ntructural nodes to take full advantage of network diversity .\nHowever, by ﬁxing the association variables and solving\nthe Asso-UNEE-Max problem does not sacriﬁce too much10 15 20 25 30 3501020304050Network-wide EE (Kbits/J)\nNumber of End UsersRelaxed-UNEE-Max\nAsso-UNEE-Max\nRnd-UNEE-Max\nBen-UNEE-Max\nFigure 5: Performance Comparison for different prob-\nlems under various network sizes.\n0.70 0.75 0.80 0.85 0.90 0.9502468101214Number of Users\nConfidence Level ∆BS\nCR mesh\n(a) Impact of harvested band on\nuser associations20 40 60 80 10012.513.013.514.0Network-wide EE (Kbits/J)\nNumber of OFDM Sub-channelsConfidence Level ∆= 0.70\nConfidence Level ∆= 0.75\nConfidence Level ∆= 0.80\n(b) Impact of number of OFDM\nchannels on network EE\nFigure 6: Impact of bandwidth on network performance.\nnetwork performance, as shown in Fig.5. Based on the solutio n\nof the Asso-UNEE-Max problem, further applying rounding\nprocedure and solving the Rnd-UNEE-Max problem, gives\nan even lower network-wide EE. Nevertheless, the optimalit y\ngaps between the solutions of Relaxed-UNEE-Max and Rnd-\nUNEE-Max reduces from 19.35% to 7.14% as the number\nof users increases from 10 to 35, which means our proposed\napproximation algorithm for association and rounding work s\nwell when the network size scales up.\nFurthermore, the network-wide energy efﬁciency in our\nnetwork is much higher (e.g., 143% more in the scenario of 25\nend users) than that in the traditional cellular network. Su ch a\nsigniﬁcant gain in the energy efﬁciency on one hand attribut es\nto the additional harvested spectrum while on the other hand is\ndue to the close proximity between end users and CR routers\nallowing lower transmit power for users.\nNext, we analyze how the number of OFDM sub-channels\nand uncertainty of harvested band could affect the user as-\nsociation decision and network performance. For the user\nassociation evaluation, we randomly select 13 users just fo r\ndemonstrative purposes and set Ntot=100andWm=20KHz ,\nwhile keeping other parameters the same as before. The resul t\nis shown in Fig.6a, which illustrates the number of users\nconnected to the BS and to the cognitive mesh network. It\ncan be seen that when the conﬁdence level increases, more12\nusers are switched from the cognitive mesh network to the\nBS. The reason is that higher conﬁdence level means more\nstrict requirement on constraint (17), which in other words\nmeans that the usable harvested band becomes more limited.\nTherefore, some users are re-associated with the BS so that\ntheir throughput would be higher although they may use highe r\ntransmit power.\nThe harvested band affects the backbone capability, while\nthe number of OFDM sub-channels impacts the capacity of the\nﬁrst hop from end users and infrastructural nodes. As shown\nin Fig.6b, we examine how different OFDM sub-channel\npatterns (e.g.,{6,15,25,50,75,100}) inﬂuences the network-\nwide EE. It can be observed that for the ﬁxed uncertainty\nof the harvested band (i.e., available bandwidth), network -\nwide EE increases in a decreasing rate as the number of\nOFDM sub-channels increases. The reason is that as the\nnumber of OFDM sub-channels becomes sufﬁciently large,\nthe available harvested band allocated to the cognitive mes h\nnetwork becomes the bottleneck to support the trafﬁc on the\nﬁrst hop links. This also explains the observation that the\nnetwork-wide EE increases as the conﬁdence level decreases\nfor the ﬁxed number of OFDM sub-channels.\n0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.581216Network-wide EE (Kbits/J)\nPmax(W)Pt= 2.0W\nPt= 1.5W\nPt= 1.0W\nFigure 7: Impact of users’ and CR routers’ transmit\npower on network performance\nAnother design dimension that could impact the network\nperformance is the transmit power. Here, we examine how\nthe users’ transmit power as well as CR routers’ transmit\npower could jointly affect the network-wide EE. The results\nare shown in Fig.7. First of all, we see that the user’s maximu m\nallowable power Pmax only affects the network-wide EE at\nits lower value while the network performance stays constan t\nasPmax continues to increase. The reason is that end users\ncan utilize very low power for connection and increasing the\nPmax would not give a higher transmit power in order to\noptimize the EE. On the other hand, the relationship between\nCR routers’ transmit power Ptand network performance\nis worth explaining. According to Eq.(5-6), Ptimpacts the\ncommunication/interference range, which further inﬂuenc es\nthe construction of the conﬂict graph. For instance, when\nPt=1.0W,RT\ni=166.7m and RI\ni=234.5m; while when\nPt=2.0W,RT\ni=209.9m and RI\ni=295.46m. From thenetwork topology shown in Fig.3, we can clearly see that\nthe number of reachable infrastructural nodes for each CR\nrouter becomes larger while each CR router’s interfered nod es\nremain the same. As a result, the size of each MIS qincreases\nand more links can be scheduled for transmission at the same\ntime, which enhances the achievable link capacity in the mes h\nnetwork. Therefore, the network-wide EE increases with the\nPtincreasing from 1W to 2W. It should be noted that this\nmay not always hold true if the power increase incurs more\ninterfered nodes. However, this general trend reﬂects the f act\nthat by sacriﬁcing the infrastructure’s power consumption , end\nusers’ EE will be improved, which indicates that the power\nconsumption burden is shifted from light-weighted end devi ces\nto the more powerful infrastructural nodes.\nIX. C ONCLUSION\nIn this work, we investigate the energy efﬁciency (EE) de-\nsign of battery-powered devices. Our ideology is to shift th eir\nenergy consumption to grid-powered devices, thus increasi ng\ntheir EE. This ideology is realized in a cognitive mesh net-\nwork, in which we model a cross-layer optimization problem\nto maximize end devices’ EE. Speciﬁcally, we propose an\nobjective function as the weighted sum of each device’s EE\nand characterize constraints including device associatio n, ﬂow\nrouting, link scheduling, channel allocation and power con trol.\nTo solve this complex problem, we propose parametric sub-\ntractive transformation, ∆-conﬁdence level, critical MISs and\ninteger relax-then-rounding to convert the original probl em\ninto a tractable one, and further decouple this large scale o pti-\nmization problem into a two-layer optimization problem. We\nconduct extensive simulations to demonstrate the optimali ty\nand feasibility of our proposed algorithms and also show how\nthe design variables impact the network performance.",
      "metadata": {
        "filename": "Optimizing IoT Energy Efficiency on Edge (EEE)_ a Cross-layer Design in a Cognit.pdf",
        "hotspot_name": "Aluminium_Die_Casting",
        "title": "Optimizing IoT Energy Efficiency on Edge (EEE): a Cross-layer Design in\n  a Cognitive Mesh Network",
        "published_date": "2019-01-11T02:33:01Z",
        "pdf_link": "http://arxiv.org/pdf/1901.05494v2",
        "query": "die casting aluminium radiator energy consumption reduction process optimization techniques"
      }
    },
    "Power Reduction in FM Networks by Mixed-Integer Programming. A Case Study": {
      "full_text": "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier xx.xxxx/ACCESS.2023.DOI\nPower Reduction in FM Networks by\nMixed-Integer Programming. A Case\nStudy\nPASQUALE AVELLA1, PAOLO NOBILI2, AND ANTONIO SASSANO.3\n1Dipartimento di Ingegneria, Università del Sannio, Piazza Roma 21, 82100 Benevento Italy (e-mail: avella@unisannio.it)\n2DAFNE, Università della Tuscia, Via San Camillo de Lellis 01100 Viterbo Italy(e-mail: nobili@unitus.it)\n3Dipartimento di Ingegneria Informatica, Automatica e Gestionale - DIAG, Sapienza Università di Roma, via Ariosto 25, 00185 Roma Italy (email:\nsassano@diag.uniroma1.it\nCorresponding author: Pasquale Avella (e-mail: avella@unisannio.it).\nABSTRACT The climate change emergency calls for a reduction in energy consumption in all human\nactivities and production processes. The radio broadcasting industry is no exception. However, reducing\nenergy requirements by uniformly cutting the radiated power at every transmitter can potentially impair the\nquality of service. A careful evaluation and optimization study are in order. In this paper, by analyzing the\nItalian frequency modulation analog broadcasting service, we show that it is indeed possible to significantly\nreduce the energy consumption of the broadcasters without sacrificing the quality of the service, rather, even\ngetting improvements.\nINDEX TERMS Broadcast networks, FM, Power reduction, Mixed-Integer Programming\nI. INTRODUCTION\nThe frequency-modulated analog radio (FM) service has\nbeen, for over 60 years, a faithful companion on long car\njourneys and, for many, the only alternative to loneliness in\ntheir homes. Now, the end of its natural life cycle is approach-\ning: many European countries (Switzerland, UK, Denmark,\nSweden, Germany) have devised and implemented plans\nfor its definitive shutdown; Norway already dismissed all\nnational networks in 2017. However, FM analog broadcasting\nstill resists digital radio and Internet streaming replacements\nand we can safely predict that it will be active in several\ncountries, including Italy, for quite a few years to come.\nIn fact, all shutdown projects have met with fierce resistance\nfrom users and (mostly local) broadcasters. Users do not want\nto abandon the FM. However, the reasons for the switch-off\nwere and are very valid. Digital radio and, above all, Internet\nstreaming are much more efficient and widespread vectors\nfor radio programs. Thus an Italian user (for example) can\nlisten to his favorite local radio in Denmark without any\nproblems. However, not everyone has a digital receiver or\na smartphone or, rather, not everyone is used to turn on a\nsmartphone to listen to radio broadcasting, and not all cars\nare equipped with DAB receivers (only since 2018 has thepresence of such receivers become mandatory for new cars in\nItaly). Therefore, technological evolution alone has not been\nable to change user’s habits.\nTo this technological evolution, more recently the need (by\nadministrations) has been added to avoid wasting energy and\nhope (by broadcasters) to stay in business in the face of the\nrising costs of electricity bills. To this end, many are asking\nto consider the FM broadcasting industry an energy-intensive\nsector and request state aid to continue broadcasting. How-\never, is it an energy-intensive sector? In 2020 the British\nBroadcasting Company (BBC) [3] published a study entitled\n\"The energy footprint of BBC radio services: now and in the\nfuture\" which evaluated the energy consumption associated\nwith the broadcasting and receiving of radio programs. The\nresults were astonishing: the energy requirements of the\nanalog component (essentially only the FM radio) computed\nconsidering both the receivers and the broadcasting infras-\ntructure, unquestionably emerged as the most demanding, as\nshown in Table 1.\nThe BBC estimates the total energy consumed by all forms\nof radio and TV broadcasting in 2018 at 325GWh, corre-\nsponding to a 37.1MW stove lit for 24hours a day and 365\ndays a year. Moreover, 100GWh ( 30.8%) was used by the\nVOLUME 4, 2016 1arXiv:2310.19492v1  [math.OC]  30 Oct 2023TABLE 1. Top five components with the highest energy consumption in the\nBBC radio system for 2018\nRadio System Component Annual Energy (GWh) % of total\nAnalogue Radio Set 82 25.2\nDAB Radio Set 55 17.0\nTV Set 33 10.1\nFM Broadcast Infrastructure 26 8.1\nAM Broadcast Infrastructure 17 5.2\nFM service alone, of which approximately 26GWh ( 8.1%)\nby the broadcast infrastructure. Analog (FM) is the most\nenergy-intensive method for broadcasting radio programs.\nHence the urge to “switch-off” analog broadcasting which,\nunfortunately, would have, at the moment, unsustainable\nsocial costs.\nUsually, the problem of reducing the power of FM networks\nis addressed by designing more efficient directional antennas\n[7]. Instead, an optimization-based approach would allow\na reduction in energy consumption without decreasing the\nquality of the service, being aware that at present there is a\nstrong waste of resources. Obviously, this solution does not\nhave the same effect for all countries but it is certainly more\neffective in countries where the \"waste\" is more significant,\nand Italy is one of them.\nIn the UK the BBC broadcasts 40programs while there\nare approximately 450 other local and national broadcast-\ners; there are approximately 1,500 transmitters; In France,\napproximately 2,200 transmitters broadcast 945 programs.\nIn Italy, according to the FM Frequency Register developed\nby the State Agency for Telecommunications (AGCOM),\nthere are approximately 16,381 transmitters broadcasting\nprograms of 1,133 networks [1]. These numbers alone do\nnot say much about the energy consumption. A few very\npowerful transmitters could consume the same energy as\nhundreds of small transmitters. However, Italy has almost\neight times more transmitters than France, compared with\na slightly higher number of broadcasters (1,133 vs. 945).\nThe number of frequencies used by Italian networks was\nat least twice that used by French networks. This situation\ntends to produce a significant \"national self-interference\"\n(i.e. Italian broadcasters disturbing other Italian broadcasters)\nwhich, in turn, is a push to increase the transmission power\nto counteract the interference.\nIn short, there are several signs supporting the hypothesis\nthat the average electromagnetic radiated power (and hence\nthe power consumed) by transmission plants in Italy may be\nparticularly high. In recent years, specialized online mag-\nazines have dealt with this problem seriously, underlining\nhow (for example) the power of the FM transmitters in the\ncity of Milan has had to increase by 10times to counter the\ninterference generated by other (Milanese) broadcasters and\nhow, due to the age of the transmission plants, the efficiency\nof the systems (the ratio between the electromagnetic power\nradiated at the antenna and the power consumed at the meter)\nis far from the ideal 70%.Finally, the FM broadcasters themselves declare that they\nmanage a network of transmitters that consume a large\namount of energy and that they are forced to decrease the\npower of their networks during the night to cut energy\ncosts. In short, everything would suggest that the Italian FM\n\"industry\" is certainly energy-intensive but also particularly\ninefficient in ensuring service to users.\nTo quantify the energy consumption of the 16,381 Italian FM\ntransmitters, we used the FM frequency register developed\nby AGCOM. The report only provides the amount of radiated\npower from each of the Italian plants, but we can infer a good\napproximation of the energy consumption by guessing the\n(mean) efficiency of the transmitters. Given the old age of\nmost Italian plants, a good estimate for such efficiency would\nbe approximately 50%, and we used such a hypothesis in our\ncomputations. The result of the calculation is that the energy\nconsumption of Italian FM plants is approximately 253GWh\nper year (in contrast to 26GWh reported by the BBC for the\nUnited Kingdom - see above) and with an average power of\napproximately 29 MW. In short, the consumption of Italian\nFM networks alone is close to the total consumption of\nBritish radio networks and is approximately 10 times higher\nthan that of British FM networks alone.\nII. LITERATURE\nThe scientific literature concerning planning and re-planning\nof Wireless Networks with the goal of energy sustainability\nhas flourished in recent years. 5G Networks ,sensor networks\nand the Internet of Things (IOT) environment ask for accu-\nrate service and low energy consumption. Consequently the\nproblem of maximizing the energy efficiency of the system\nby adjusting the minimum signal-to-interference plus noise\nratio to guarantee the required service has been studied by\nmany authors see [8], [4], and [9]. With the exception of\n[7], the FM Networks have received much less attention\nand our paper, to the best of our knowledge, is the first to\npose in an optimization format the problem of re-planning\nan FM network to reduce energy consumption, preserve and\nimprove the current service and reducing the interference\nproduced in the bordering Administrations.\nIII. DEFINITIONS AND A PRACTICAL EXAMPLE\nThe FM infrastructure is a set of transmitters Tbroadcasting\nradio programs to a set Rof geographically distributed loca-\ntions, represented by receiving points on a map. Let Adenote\nthe set of the radio broadcasting networks. We assume that\ntransmitter t∈Tcarries the programs of a single network\na∈A, being T(a)the set of transmitters of the network\na∈A. Transmitter toperates on a single frequency and\nemits a radio signal with power pt∈[0, Pmax]. The power\nprtreceived at a specific geographic location rfrom trans-\nmitter tis proportional to the emitted power ptby a factor\nart∈[0,1], that is prt=art·pt. The factor artis called\nthe fading coefficient and summarizes the reduction in power\nthat a signal experiences while propagating from ttor. The\n2 VOLUME 4, 2016value of the fading coefficient depends on many factors (e.g.,\nthe distance between the communicating devices, presence\nof obstacles, antenna patterns, and tropospheric effects) and\nis commonly computed using a suitable propagation model.\nLetT(r)denote the set of transmitters received at point r.\nTo be acceptably listened to in r, the received power must be\ngreater than a minimum value pmin, called background noise ,\nand overcome the interference of all the other transmitters\nreceived in r. Note that a transmitter tcan also interfere in r\nifprt< pmin. Usually, the interfering power of a transmitter\ninris computed by considering a different fading factor ¯art\nand is multiplied by a constant called the protection ratio PR,\nnamely ¯prt= ¯art·PR·pt. Hence, for each transmitter tand\nreceiver rwe have two different receiver powers, namely, the\nuseful power prtand the interfering power ¯prtwith, usually,\nprt≤¯prt. The effect of interfering signals on a useful\nsignal is a complex physical phenomenon that determines the\nreception quality.\nThe interference is stronger when useful and interfering\nsignals are transmitted at the same frequency ( co-channel\ninterference ) but it is non-negligible also if the signals are\nmodulated at different frequencies. In this paper, we will\nconsider only co-channel interference and we will denote\nbyI(rt)the set of transmitters, different from t, received\ninrat the same frequency used by transmitter t. Following\na standard technical practice [10] we assume that a useful\nsignal from tcan be received with acceptable quality at the\nreceiving point rif the ratio between the useful signal and\nthe sum of the interfering signals and background noise,\ncalled signal-to-interference-plus-noise ratio ( SINR ) is above\na threshold θ:\nprtP\nj∈I(rt)¯prj+pmin≥θ (1)\nIt is important to note that the useful signal is emitted by a\nunique transmitter, and all the other co-channel signals inter-\nfere. This is due to the fact that Frequency Modulation is an\nanalog technology and, contrary to the more advanced digital\ntechnologies like DAB, there is no positive composition of\nco-channel signals carrying the same content.\nA comment about our choice of studying the case in which\nthe interference is produced only by transmitters operating at\nthe same frequency ( Co-Channel Hypothesis ) is mandatory\nhere. This choice is strongly motivated by the main goal of\nour paper, namely showing that the reduction of the operating\npowers of the FM transmitters preserves the current service\nwhile reducing the energy consumption and the Italian in-\nterference in the bordering Administrations. The question\nis: How do we define the \"current service\"? A rigorous\ndefinition should take into account the interference generated\nby the adjacent channels but, in this case, we would reduce\nthe current service areas of the Italian broadcasters and hence\nwe would favor a more drastic reduction in operating powers.\nThe Italian broadcasters would certainly object that theirservice is \"acceptable\" even in the presence of adjacent chan-\nnel interference and this claim is strongly supported by the\nempirical evidence that the Italian FM operating frequencies\nare seldom separated by the 200 KHz suggested by ITU\n[6]. This has been the main motivation for adopting the Co-\nChannel Hypothesis: the maximization of the current service\nareas of Italian networks to protect .\nAnother important point underlines the importance of the\nchoice of the Italian networks in our case study. In general,\nmany transmitters of the same network, possibly modulated\nat different frequencies, are received at the same receiving\npoint r. This is usually a clear sign of a network that has\nbeen poorly designed or that has grown in a \"tragedy of the\ncommons\" environment. In a healthy environment in which\nfrequencies are properly used and accurately planned, the\naverage number of transmitters received from each network\nat each receiving point should be close to one. This is the case\nfor most FM networks in the world but not in Italy, making\nthe choice of our case study particularly interesting. It follows\nthat in our study we can have more than one transmitter\ntcarrying the same network and with a SINR above the\nthreshold.\nLetPrbe the population of the receiver point r. Let ytbe\nthe percentage reduction in the power of transmitter t∈T.\nFor each receiving point r∈Rand each network a∈A,\nletsrabe a binary variable that is 0if the best server of\nnetwork ainrhas a SINR above the threshold (covered)\nand1otherwise. Set Tcontains only the servers of some\nreceiving point r∈R. Let tra∈T(a)be the best server\nof the network Afor the receiving point rand let prabe the\nuseful power received in rfromtra. LetIrabe the set of co-\nchannel interfering transmitters for traandθthe threshold\nabove which a receiver point is regarded as covered. Finally,\nletZbe the set of pairs (r, a)with the property that the\nserver of the Italian network ain an Italian receiving point\nrsatisfies the service constraint (SINR ratio) in the current\nconfiguration. That is Zrepresents the current FM service to\nbe preserved.\nThe task of planning (and re-planning) FM networks is usu-\nally performed using powerful and effective simulation tools .\nIn a simulation environment, some of the restrictions we are\nforced to introduce into our optimization model can be re-\nlaxed, and the computation can be made more accurate. One\nof the features of the simulation models is that the quality of\nservice and the level of interference can be presented through\ninformative service maps. In this paper we use a simulation\ntool to verify the quality of the solutions produced by our\noptimization models. Before defining the models, we will\nuse our simulation tool to show what happens at a specific\nreceiving point, at a specific frequency when the power of\nthe main interfering transmitter is reduced.\nFigures 1 and 2 focus on the improvement obtained at a real\nreceiving point situated in the vicinity of the Slovenian city\nof Capodistria. The tables under the pictures report the rele-\nVOLUME 4, 2016 3vant data regarding the seven transmitters whose signals are\nreceived at this point, namely the identifier, useful power (in\ndB), and interfering power (in dB). Of the seven transmitters,\nthe first one is Slovenian (and is the server for this point of\na Slovenian network) and the remaining ones are Italian (and\nhence are interfering). Column SumInt reports the cumulative\ninterference provided by the background noise pminand by\nthe interfering transmitters listed in the preceding columns\n(note that values expressed in dB are not additive).\nFIGURE 1. Capodistria: Current Scenario.\nTo read the two service maps better we now explain our\ncolor code. We start by noticing that the \"pixels\" in the\nmap are the squares clearly visible at the resolution we have\nchosen for our example. We call Affected Administration the\nAdministration whose Quality of Service (QoS) we want\nto assess and Interfering Administration the Administration\nwhose networks disturb the service of the Affected Adminis-\ntration. Each \"pixel\" of the map belonging to the Affected\nAdministration has a color indicating the QoS of the cor-\nresponding receiving point. The possible colors are (blue,\nlight blue, green, yellow, and red) . The first four colors are\nassociated with four QoS levels, (Q4, Q3, Q2, Q1) . Each\ngrade on the quality scale is achieved when the SINR at the\nreceiving point is greater than or equal to a given threshold. In\nour case, the thresholds were (0,−6,−12,−15)dB. Finally\na red \"pixel\" indicates that the SINR at the corresponding\nreceiving point is below a threshold of −18dB and no service\nis available.\nFor the Interfering Administrations, the colors of the \"pix-\nels\" (red, brown, light brown, orange, yellow, green) have\ndifferent meanings. They represent the level of cumulative\nFIGURE 2. Capodistria; Post Power Reduction.\ninterference produced by the networks belonging to the In-\nterfering Administration and are determined by the value\nof the electric field at the receiving point. In our case, the\ncolor changes from red(severest interference) to yellow (light\ninterference) when the electric field is greater than or equal\nto the thresholds (70,50,40,30,20)dB(µV/m ). The green\ncolor indicates that the signal was received with an electric\nfield below 20dB(µV/m ) causing negligible interference.\nLet us now return to our example and observe that, in\nthis case, Slovenia is the Affected Administration and Italy\nthe Interfering Administration. The interfering power of the\nItalian transmitter 4500 before the optimization is 86.07 dB,\nmaking its signal the worst interfering transmitter received\nat this point. The cumulative interference experienced by\nthe point is at 86.08dB, overcoming the useful power of\nthe Slovenian transmitter (the server) by 14.15dB (see the\nlast row in the table) and rendering its reception very poor\n(i.e. the SINR of the Slovenian transmitter at the point is\n−14.15dB, corresponding to a QoS of Q1). In contrast,\nafter optimization, the interfering power of the interfering\ntransmitter 4500 is decreased to 73.07dB and consequently,\nthe cumulative interference is lowered to 73.30dB. Hence,\nafter optimization, the SINR of the Slovenian transmitter\nincreases to −1.36dB, corresponding to a QoS of Q3.\nThe question now is: What happens to the service of the\nItalian transmitter 4500 in Italy? Reducing its power could\nmake some other Italian transmitter, operating at the same\nfrequency, the best server in some important receiving point\nin Italy. Hence the network operating transmitter 4500 could\nlose an important portion of its users. In such a situation, the\n4 VOLUME 4, 2016power of the new best server should inevitably be decreased.\nWe would have the \"butterfly effect\" phenomenon, well\nknown to all the practitioners trying to resolve interference\nproblems by means of simulation tools, one transmitter at a\ntime. The answer is very simple, the power of 16,381Italian\ntransmitters must be changed simultaneously , by solving a\nglobal optimization problem that guarantees each broadcaster\nto preserve its original service. This is exactly what we have\ndone in the next section.\nIV. THE MILP MODEL\nIn this section, we introduce the optimization models aimed\nat solving the FM Power Reduction problem . In particular, we\nbegin with the following mixed-integer fractional program-\nming problem:\nminX\nr∈RPrX\na∈Asra\nprtraytraP\nj∈Ira¯prj+pmin+Msra≥θ r∈R, a∈A\nsra∈ {0,1}r∈R, a∈A\nsra= 0,(r, a)∈Z\n0≤yi≤1, i∈T(2)\nwhere\n- The objective function aims to minimize the total popu-\nlation not covered by FM networks.\n-Mis a very large value that enforces the SINR inequal-\nity associated with the pair (r, a)to satisfy exactly one\nof the following conditions: either i)the receiver r∈R\nis served by the transmitter tra;orii)srais set to 1and\nthe inequality is no longer active.\n- The constraint sra= 0 for(r, a)∈Zforces server\na∈Ato satisfy the SINR constraint in r∈Rand\nhence to preserve the original service of the network a\ninr.\nThe above model can be easily linearized as follows:\nminX\nr∈RPrX\na∈Asra (MILP )\nytra−θX\nj∈Ira¯prj\nprtrayj\n+Msra≥pmin\nprtraθ r∈R, a∈A\nsra∈ {0,1}r∈R, a∈A\nsra= 0,(r, a)∈Z\n0≤yi≤1, i∈T(3)\nThis choice has a positive algorithmic outcome because\neach SINR inequality involves only transmitters operating\nat the same frequency, and the model is decomposable into\nblocks, one for each frequency in the spectrum. This property\nmakes the model more easily solvable by the current MIP\nalgorithms, which can recognize blocks and solve each of\nthem as a sub-MIP.Usually, the LP-relaxation of a MILP containing a Big-M\ncoefficient does not provide good lower bounds and very\nrarely provides good quality feasible solutions to the original\nproblem. To contrast this bad behavior of the Big-M constant\nand produce more efficient lower and upper bounds, the\nauthors of [2] used a different (and provably lower) value\nfor the Big-M. Namely, the sum of all the power received\nfrom the interfering transmitters and the background noise\nor, more formally, Mra=θpmin\nprtra+θP\nj∈Ira¯prj\nprtra.\nIt is expected that this lower value of the constant Mracould\nproduce a better relaxation and more accurate upper and\nlower bounds. One of the surprising results of this paper is\nthat this does not happen and the results produced by the\nformulation in [2] lead to power reductions and FM service\nof inferior quality with respect to those obtained in our MILP\nmodel with M= 1040. We have observed above that this be-\nhavior could be motivated by the efficient management of the\nBig-M constraints ensured by Gurobi (while the lower values\nofMraand their high variability could have a masquerading\neffect for Gurobi). However, what is really surprising and\npromising for all practitioners interested in replicating our\nresults is that a special LP problem which is not the relaxation\nof our original MILP produces results, in terms of power\nreduction and service increase, which are even better than\nthose produced by the MILP model. The LP model is the\nfollowing:\nminX\nr∈RPrX\na∈Asra (LP)\nytra−θX\nj∈Ira¯prj\nptrayj\n+sra≥pmin\nprtraθ r∈R, a∈A\nsra≥0r∈R, a∈A\nsra= 0 (r, a)∈Z\n0≤yi≤1, i∈T(4)\nIn this model, the variable sracan achieve any positive value,\nand the numerical problems originating by M(or, worst,\nMra) are overcome. The first advantage of the model (4)\nis its scalability ; namely, it can be easily solved by any\ncommercial LP-solver in a very short time even if the number\nof transmitters and receiving points becomes very large. The\nsecond - surprising - advantage is that the quality of the\nresults produced by the LP model and in particular the values\nof the variables y, guarantee, as we will see in the next\nsection, a greater reduction in the transmitting powers and\na more significant growth in the service area of the Italian\nnetworks.\nV. RESULTS\nThe MILP (3) and LP (4) models were embedded via a\nPython interface into the MIP solver Gurobi 10 [5] to obtain\nprovably good solutions. The experiments were run on an\nVOLUME 4, 2016 5Intel Xeon W-10885 2.40GHz workstation with 128 GB\nRAM.\nThe test bed involves all the Italian transmitters listed in the\nFM Register managed by AGCOM [1] and all the foreign\nFM transmitters registered in the ITU Master Register of\nGeneva , whose service areas are interfered by the Italian FM\nnetworks. In particular, all the information was updated on\nMay 10, 2022 .\nWhen talking about service we refer to the service of a\ntransmitter of Administration A on a receiving point located\nin the territory of Administration A (e.g. the service of an\nItalian network in Italy). As stated above our goal is to\nreduce the power of the Italian transmitters (the transmitters\nbelonging to non-Italian administrations are fixed to their\ncurrent power) while guaranteeing that the service areas of\nthe Italian networks are preserved (at least one server of\nthe network must guarantee the service at a receiving point\nRP if RP is served in the current served in the current\nscenario ). The current scenario is summarized in Table 2,\nwhere Population is the sum over all the radio networks\nof the users that could be potentially served if there was\nno interference and Currently served population denotes the\nsum of the users currently covered by all the networks.\nTABLE 2. Current scenario\nTotal number of transmitters 21,805\nNumber of servers 20,558\nNumber of Italian servers 16,381\nNumber of foreign servers 4,177\nPopulation - Italy 1,834,629,759\nPopulation - abroad 202,050,318\nCurrently served population - Italy 1,377,115,927\nCurrently served population - abroad 169,894,991\nWith several thousands of active transmitters and only about\n200channels to share in the FM spectrum, some level of co-\nchannel interference is unavoidable. What is more peculiar to\nthe Italian context (and a sign of poor network design) is that\nseveral receiving points experience co-channel interference\neven from transmitters associated with the same network.\nWith the MILP model (3), Gurobi provided in 4,200 secs and\n758,237 branch-and-bound nodes, a lower bound LB and an\nupper bound UB for the optimal solution, with a percentage\ngap (computed as 100∗(UB−LB)/LB ) less than 1%.\nBefore starting enumeration, the lower bound given by LP-\nrelaxation was strengthened by the Gurobi built-in cutting\nplanes. The outcomes for the MILP model (3) are reported\nin Table 3, where Number of Italian plants shut down counts\nthe transmitters with zero potentially serviceable points (the\nreceiving points in which the transmitter is above pmin),∆\nPower is the percentage variation (decrease in this case) of\ntotal transmission power and ∆Served population is the\nvariation (increase) in the sum of served users for all the\nnetworks, distinct in Italian and non-Italian but belonging to\none of the aforementioned bordering Administrations.\nGurobi solved to optimality the LP model (4)in only 21\nseconds. The outcomes of the LP model are presented inTABLE 3. Summary of the outcomes of the MILP model\nNumber of Italian plants shut down 1,473\n∆Power (%) -53.23\n∆Served population - Italy +133,824,096\n∆Served population - abroad +16,531,361\nTable 4.\nTABLE 4. Summary of the outcomes of the LP model\nNumber of Italian plants shut down 1.704\n∆Power (%) -65.46\n∆Served population - Italy +101,666,064\n∆Served population - abroad +3,522,406\nA brief comment concerning the comparison of the results\nreported in Tables 3 and 4. The LP model has a significant\nimpact on the Italian networks. Power reduction is greater\nthan that produced by the MILP model ( 65.46% vs. 53.23%)\nand, analogously, the number of shut-down Italian plants\nis greater (1,704 vs. 1,473). By contrast, the MILP model\nproduces a greater number of incremental users of the Ital-\nian (133,824,096 vs. 101,666,064) and non-Italian networks\n(16,531,361 vs. 3,522,406). In both cases, we achieve the\ngoals of reducing the power and increasing the service, but\nwith an inverse ratio between power reduction and incremen-\ntal users.\nIn the most favorable case of the LP model, the annual energy\nused by the Italian FM networks would decrease, if the\nreduction obtained by the optimization were implemented,\nfrom 253 to just over 152 GWh ( 40% reduction) and the\nItalian FM \"stove\" would reduce its power from the current\n28,800 KW to only 17,460 KW. Obviously, the saved power\ncould be used elsewhere: Seen from the perspective of clean\nenergy production, it is as if we had activated a new, small,\n\"green\" power plant of over 11MW.\nFinally, in Tables 5 and 6 we provide some examples of\nthe increment of the service areas both in Italy and in the\nbordering Administrations. In particular, we have listed the\n20networks of Italian and non-Italian administrations with\nthe greatest increase in users produced by the MILP and LP\nmodels. The Italian networks are anonymized, but is evident\nthat the increment of users is uniformly distributed, which is\nanother feature of our model.\nVI. SIMULATION RESULTS\nFigures 3 and 4 show the service maps produced by the\nsimulation tool. The color code is described in Section III\nand the maps show the effect of implementing the solution\nproduced by the LP model for a typical Italian network.\nIn this case, Italy is the Affected Administration and all\nbordering Administrations are Interfering.\nFig.3 depicts the current situation of the Quality of Service\nfor Italian receiving points and the interference produced\nby the Italian networks in the bordering Administrations,\n6 VOLUME 4, 2016FIGURE 3. Italian FM network:Current Scenario.\nFIGURE 4. Italian FM network: Post Power Reduction.\nwhereas Fig. 4 depicts the resulting scenario after the appli-\ncation of the power reduction computed by the LP Model\n(yvariables). The maps are anonymized, but is clearly vis-\nible that the population served at quality Q3 ranges from\n35,655,676 to37,802,995 with more than 2million in-\ncremental users. Incremental users computed using the LP\nmodel and reported in Table 5 are slightly greater than 1.6\nmillion (half a million more!).\nHence we see that, even though a little obfuscated by\nthe constraints of anonymity, there is an interesting effect.\nThe simulated population covered by the optimized network\nis often greater than that computed by the optimization\nalgorithm. This is due to the fact (among others) that the\nsimulator does not fix the server trafor a given network\na∈Aand a given receiving point r∈R, but computes the\nSINR for all the potential servers of the network ainrand\nchooses the maximum. Hence, the power reduction preserves\nthe original servers but also \"promotes\" server transmittersTABLE 5. Service improvement for some (anonymous) Italian networks\n# Pop. Now ∆Pop. MILP ∆Pop. LP\n1 51,131,899 1,151,046 906,729\n2 48,859,575 1,586,859 1,030,842\n3 48,348,080 1,998,243 1,476,221\n4 42,622,905 2,028,532 1,596,385\n5 41,769,484 3,433,057 2,327,596\n6 41,429,485 2,703,513 2,036,093\n7 37,227,052 1,334,123 896,442\n8 37,152,381 2,975,092 2,423,119\n9 36,527,932 4,283,262 3,365,787\n10 36,014,797 2,475,593 1,848,234\n11 35,713,731 2,361,952 1,655,042\n12 35,157,455 2,259,645 1,503,116\n13 34,403,832 1,779,844 1,386,900\n14 33,733,807 2,079,745 1,627,538\n15 29,105,075 2,375,934 1,397,902\n16 27,773,242 2,677,454 2,286,051\n17 26,871,897 2,407,246 1,809,111\n18 26,870,571 2,722,907 2,301,353\n19 24,035,047 1,807,826 1,522,963\n20 13,931,454 643,932 416,682\nTABLE 6. Service improvements for some bordering Administrations\n# Adm Ch Pop. Now ∆Pop. LP ∆Pop. MILP\n1 F 94,1 523,681 116,148 116,148\n2 TUN 93,9 1,612,963 86,373 86,373\n3 ALB 92,8 541,257 72,439 72,439\n4 ALB 100 49,029 72,351 72,351\n5 F 96,8 606,811 71,129 71,444\n6 TUN 92,7 1,010,310 67,131 16,722\n7 HRV 92,6 68,604 62,366 61,885\n8 F 100,7 826,403 60,314 59,960\n9 F 106 91,187 55,155 38,930\n10 HRV 96,6 124,434 49,872 49,707\n11 TUN 89,5 599,781 49,620 49,620\n12 F 95 459,459 46,429 46,429\n13 ALB 90,7 65,628 40,652 40,498\n14 HRV 104,5 228,472 39,436 38,788\n15 SUI 93,6 59,785 39,062 39,062\n16 SUI 101 2,563 37,000 37,000\n17 ALB 101,6 1,073,895 36,239 36,239\n18 F 97,1 188,315 35,987 35,987\n19 F 101,2 187,857 35,230 35,230\n20 HRV 99,3 446,505 34,643 35,112\nthat were impaired by interference in the current scenario.\nThe effect of the optimization is particularly noticeable in\nseveral critical spots; for example, the service in Milan and\nTrieste in Italy is greatly improved and the interference\ncaused in Corsica, Malta, and Albania is strongly reduced.\nNote also the striking effect of Power Reduction in the Italian\nregion of Puglia (South East Adriatic ) and Albania: The\narea served at quality Q4 in Puglia is significantly extended\n(in the main cities of Foggia, Bari and Brindisi) while the\ninterference in Albania is strongly reduced. A \"win-win\"\nscenario.\nVII. CONCLUSIONS\nIn this paper we used a mixed-integer programming model\nto analyze the relation between the transmission power and\npopulation coverage in an FM radio infrastructure. The case\nstudy is based on an extensive dataset involving all Italian FM\nVOLUME 4, 2016 7transmitters and all transmitters interfering with the Italian\nnetworks present in the ITU Master Register. Experimental\nresults show that reducing the transmission power can lead to\nan even better coverage of the population, thanks to the re-\nduction in interference. If implemented by regulatory bodies,\nthese counterintuitive outcomes could bring significant ben-\nefits in terms of the quality of service, energy consumption,\nand electromagnetic pollution.\nACKNOWLEDGEMENTS\nThe authors wish to thank Antonio Provenzano, Marco Ric-\nchiuti, and Massimo Brienza of AGCOM (Italian Authority\nfor Telecommunications) for useful and insightful discus-\nsions and for providing the publicly available data of their\nRegister in a manageable form.",
      "metadata": {
        "filename": "Power Reduction in FM Networks by Mixed-Integer Programming. A Case Study.pdf",
        "hotspot_name": "Aluminium_Die_Casting",
        "title": "Power Reduction in FM Networks by Mixed-Integer Programming. A Case\n  Study",
        "published_date": "2023-10-30T12:32:50Z",
        "pdf_link": "http://arxiv.org/pdf/2310.19492v1",
        "query": "die casting aluminium radiator energy consumption reduction process optimization techniques"
      }
    },
    "Power-Constrained Trajectory Optimization for Wireless UAV Relays with Random Re": {
      "full_text": "arXiv:2002.09617v2  [cs.IT]  28 Feb 2020Power-Constrained Trajectory Optimization for\nWireless UA V Relays with Random Requests\nMatthew Bliss and Nicol` o Michelusi\nAbstract —This paper studies the adaptive trajectory design\nof a rotary-wing UA V serving as a relay between ground nodes\ndispersed in a circular cell and a central base station. Assu ming\nthe ground nodes generate uplink data transmissions random ly\naccording to a Poisson process, we seek to minimize the expec ted\naverage communication delay to service the data transmissi on re-\nquests, subject to an average power constraint on the mobili ty of\nthe UA V . The problem is cast as a semi-Markov decision proces s,\nand it is shown that the policy exhibits a two-scale structur e,\nwhich can be efﬁciently optimized: in the outer decision, up on\nstarting a communication phase, and given its current radiu s,\nthe UA V selects a target end radius position so as to optimall y\nbalance a trade-off between average long-term communication\ndelay and power consumption; in the inner decision, the UA V\nselects its trajectory between the start radius and the sele cted\nend radius, so as to greedily minimize the delay and energy\nconsumption to serve the current request. Numerical evalua tions\nshow that, during waiting phases, the UA V circles at some opt imal\nradius at the most energy efﬁcient speed, until a new request is\nreceived. Lastly, the expected average communication dela y and\npower consumption of the optimal policy is compared to that o f\nstatic and mobile heuristic schemes, demonstrating a reduc tion\nin latency by over 50% and 20%, respectively.\nIndex Terms —Rotary-wing UA Vs, wireless communication net-\nworks, adaptive trajectory optimization, delay minimizat ion\nI. I NTRODCUTION\nMuch recent research has gone into studying UA Vs operat-\ning in wireless networks [1]–[4]. The primary motivation fo r\nthis interest is due to the unique beneﬁts that UA Vs acting as\nﬂying base stations, mobile relays, etc., provide in improv ing\nthe overall network performance over terrestrial infrastr ucture\nin terms of mobility, maneuverability, and enhanced line-o f-\nsight (LoS) link probability [1]–[5].\nAlready, the literature has shown that consideration of UA V\ndeployment strategies, in terms of optimal positioning or\ntrajectory design, can go a long way to increase network\nperformance of many of the useful metrics. In [6], dynamic\nrepositioning led to increase in spectral efﬁciency over heuris-\ntics under both FDMA and TDMA schemes. The works of\n[7], [8] utilized UA Vs in communication and maximized the\ntotal service time , with [7] outperforming static and random\ndeployment methods, and [8] meeting BER requirements.\nAlthough showing potential to improve these performance\nmetrics, the design of UA V deployment strategies is not\nwithout challenges [1]–[3]. Optimal trajectory design mus t\nbe formulated appropriately to incorporate realistic cons traints\nimposed on the UA Vs. Already, works such as [8]–[10]\nhave gone at length to incorporate constraints on the limite d\nonboard energy and mission times inherent to low-altitude\nplatforms (LAPs) [1].\nDespite the enormous interest in the design of UA V-assisted\nwireless communication networks, most of the prior workfocuses on deterministic models, in which the data trafﬁc\ngenerated by ground nodes (GNs) is known beforehand, and\nthere are no uncertainties in the network dynamics. However ,\nthis is impractical in realistic systems, where uncertaint y\ndominates, and random request arrivals must be accounted\nfor. Therefore, the design of UA V-assisted communication\nnetworks with random data trafﬁc is still an open problem .\nTo address this problem, our previous paper [5] considered\nthe optimization of the UA V trajectory and communication\nstrategy under random trafﬁc generated by two GNs. However,\nthat model was limited to two GNs, neglected the power\nconsumption of the UA V , and assumed that the UA V is the\ndestination of the data trafﬁc. In this paper, we extend the\nmodel to densely deployed GNs that need to transmit data\npayloads to a backbone-connected base station; we investig ate\nthe optimal trajectory and communication strategy of the UA V ,\nunder an average power constraint on the UA V mobility.\nTo investigate this added complexity, we consider a scenari o\nin which an UA V acts as a relay between multiple GNs\ndispersed uniformly in a circular cell and a central base sta tion\n(BS), receiving transmission requests from the GNs accordi ng\nto a Poisson process. We formulate the problem as that of\ndesigning an adaptive trajectory, with the goal to minimize\nthe average long-term communication delay incurred to serv e\nthe requests of the GNs, subject to a constraint on the long-\nterm average UA V power consumption to support its mobility.\nWe show that the optimal trajectory in the communication\nphase operates according to a two-scale decision-making pr o-\ncess, which can be efﬁciently optimized: in the outer decision ,\nthe UA V , given its current radius, selects a target end radiu s\nposition, which balances optimally the trade-off between av-\nerage long-term communication delay and power; in the inner\ndecision , given its current radius and the selected end radius,\nthe UA V greedily minimizes the delay and energy trade-off\nto serve the current request. Our numerical results reveal t hat\nduring waiting phases , the UA V tends to circle at some optimal\nradius at an energy-efﬁcient speed determined by the outer\ndecision process, until receiving a new uplink transmission\nrequest. Additionally, we show that the optimal trajectory\ndesign vastly outperforms sensible heuristics in terms of d elay\nminimization: it outperforms a static hovering scheme by\nroughly 50% and a mobile heuristic scheme by up to 20%,\nwhile maintaining the same average power consumption on\nthe UA V .\nThe rest of the paper is organized as follows. In Sec. II, we\nintroduce the system model, state the optimization problem ,\nand cast it as a semi-Markov decision process; in Sec. III, we\npresent the two-scale optimization approach; in Sec. IV, we\nprovide numerical results; lastly, in Sec. V, we conclude th eHBqU(t)\nHU Request\nat time tvU(t)\n(r,θ)\nFig. 1: System model depiction of an uplink transmission req uest at time t.\npaper with some ﬁnal remarks.\nII. S YSTEM MODEL AND PROBLEM FORMULATION\nConsider the scenario depicted in Fig. 1, where multiple\nground nodes (GNs) distributed uniformly over a circular ce ll\nof radiusarandomly generate data packets of Lbits, that need\nto be transmitted to a backbone-connected base station (BS) ,\nlocated in the center of the cell in position qB= (0,0).1The\ndensity of GNs within the circular radius is denoted as λG\n[GNs/m2]. Each GN generates uplink transmission requests\naccording to a Poisson process with rate λP[requests/GN/sec].\nOverall, uplink transmission requests of Lbits arrive in\ntime according to a Poisson process with rate λ=λG·λP\n[requests/sec/m2]. Requests are received uniformly within the\ncircular cell, so that the probability density function (pd f) of\na request received in position (r,ψ)is expressed as\nfR,Ψ(r,ψ) =r\nπa2,∀r≤a,ψ∈[0,2π). (1)\nDirect communication between the GNs and the BS might\nnot be possible due to severe pathloss. An UA V is thus\ndeployed, ﬂying at a ﬁxed height HU, to relay the trafﬁc\nbetween the GNs and the BS. Let qU(t) = (rU(t),ψU(t))∈\nR+×[0,2π)be the projection of its position on the ground\nsurface at time t. Due to constraints on UA V mobility, its\nspeed is subject to a maximum constraint, expressed in polar\ncoordinates as\nvU(t)/defines/radicalBig\n(r′\nU(t))2+(rU(t)·ψ′\nU(t))2≤Vmax, (2)\nwheref′denotes the derivative of fwith respect to time.\nWe model the instantaneous UA V power consumption as\nPU(t) =Pc(t)+Pmob/parenleftbig\nvU(t)/parenrightbig\n, (3)\nwherePc(t)is the total power used onboard for communica-\ntion processes and Pmob/parenleftbig\nvU(t)/parenrightbig\nis the forward ﬂight mobility\npower, a non-convex function of the UA V speed vU(t)[10],\n[11]. We use the model in [10], [11], wherein\nPmob(V)=P0/parenleftBigg\n1+3V2\nU2\ntip/parenrightBigg\n+Pi/radicaltp/radicalvertex/radicalvertex/radicalbt/radicalBigg\n1+V4\n4v4\n0−V2\n2v2\n0+βV3,(4)\nwhereP0andPiare scaling constants, Utipis the rotor blade\ntip speed,v0is the mean rotor induced velocity while hovering,\n1Unless otherwise stated, we use polar coordinates to expres s positions, so\nthatq= (r,θ)denotes the position at distance rfrom the center, with angle\nθwith respect to the x-axis, as shown in Fig. 1.UAV Forward Foight Speed V [m/s]0 10 20 30 40 50Required Power [kW]\n11.21.41.61.82\nFig. 2: UA V power vs. forward ﬂight speed, simulated with the same physical\nparameters found in [10].\nβ/definesd0ρsA/2,d0is the fuselage drag ratio, sis the rotor\nsolidity,ρis the air density and Ais the rotor disc area\n(see [10]). An example using the physical parameters in [10]\nis shown in Fig. 2. Interestingly, the most power efﬁcient\noperation is not achieved while hovering, but while ﬂying\nat a ﬁxed speed Vmin≃20[m/s]; in addition, it is noted that\nthe communication power Pc(t)(order of 1W, as in [10])\nis dwarfed by the amount of power used for UA V ﬂight,\nPmob(V)(order of x 100W). Thus in this paper we will neglect\nPc(t), and approximate PU(t)≈Pmob/parenleftbig\nvU(t)/parenrightbig\n.\nWe assume that communication intervals experience line of\nsight (LoS) links, and that the channel faces no probabilist ic\nelements. In fact, UA Vs in LAPs generally tend to have a\nhighly likely occurrence of LoS links [12]. When an uplink\ntransmission request is received at time tfrom a GN in\nposition(rG,ψG), the communication phase begins, in which\nthe UA V ﬁrst receives the data payload from the GN, and then\nforwards it to the BS using a decode-and-forward, ﬂy-hover-\ncommunicate strategy [10]. Any additional requests received\nduring this communication phase are dropped.2This phase is\nconstituted of four distinct operations:\n1) The UA V ﬂies from its current position qU(t) = (rU,ψU)\nto a new position qGU= (rGU,ψGU), at constant speed\nv1; the duration of this operation is\n∆1=v−1\n1/radicalBig\nr2\nGU+r2\nU−2rGU·rUcos(ψGU−ψU),(5)\nand its energy cost is E1= ∆1Pmob/parenleftbig\nv1/parenrightbig\n;\n2) The UA V hovers in position qGUwhile the GN transmits\nits data payload of Lbits to the UA V; assuming a ﬁxed\ntransmission power PGN, the transmission rate is\nRGU(dGU)/definesBlog2/parenleftBig\n1+γGU\nd2\nGU/parenrightBig\n, (6)\nwhereBis the channel bandwidth, γGUis the SNR of the\nGN→UA V link referenced at 1-meter, and\ndGU=/radicalBig\nH2\nU+r2\nGU+r2\nG−2rGU·rGcos(ψGU−ψG)(7)\nis the UA V-GN distance; the associated duration and energy\ncost are\n∆2=L\nRGU(dGU), E2= ∆2Pmob/parenleftbig\n0/parenrightbig\n; (8)\n2Alternatively, they might be served directly from the BS. Th is possibility\nwill be investigated in our future work.3) The UA V ﬂies from its current position qGUto a new\npositionqUB= (rUB,ψUB), at constant speed v3; the\nduration of this operation is\n∆3=v−1\n3/radicalBig\nr2\nGU+r2\nUB−2rGU·rUBcos(ψGU−ψUB),\nand its energy cost is E3= ∆3Pmob/parenleftbig\nv3/parenrightbig\n;\n4) The UA V hovers in position qUB while it relays the\ndata payload to the BS; assuming a ﬁxed transmission\npowerPUAV and reuse of the same frequency band, the\ntransmission rate is given by\nRUB(dUB)/definesBlog2/parenleftBig\n1+γUB\nd2\nUB/parenrightBig\n, (9)\nwhereγUBis the SNR of the UA V →BS link referenced at\n1-meter,dUBis the UA V-BS distance,\ndUB=/radicalBig\n(HU−HB)2+r2\nUB, (10)\nandHBis the height of the BS antenna; the duration and\nenergy cost of this operation are\n∆4=L\nRUB(dUB), E4= ∆4Pmob/parenleftbig\n0/parenrightbig\n.\nThe positions qGU,qUBand speedsv1,v3are part of the de-\nsign. Overall, the delay and energy cost of the communicatio n\nphase to serve a GN in position (rG,ψG)is given by\nE(c)=∆1Pmob/parenleftbig\nv1/parenrightbig\n+∆3Pmob/parenleftbig\nv3/parenrightbig\n+[∆2+∆4]Pmob/parenleftbig\n0/parenrightbig\n,(11)\n∆(c)= ∆1+∆2+∆3+∆4. (12)\nOnce the communication phase is completed at time t+∆(c),\nthe UA V , now in position qU(t+ ∆(c))=qUB, enters the\nwaiting phase , where it awaits for new requests, and the\nprocess is repeated indeﬁnitely. During this process of com -\nmunication and waiting for new requests, the UA V follows\na trajectory, part of our design, with the goal to minimize\nthe average communication delay, subject to an average UA V\npower constraint (which, given an onboard battery capacity ,\ntranslates into an endurance constraint), as formulated ne xt.\nA. Problem Formulation\nLet∆(c)\nuandE(c)\nube the delay and energy cost incurred to\ncomplete the communication phase of theuth request serviced\nby the UA V , as given by (11) and (12). Let ∆(w)\nuandE(w)\nube\nthe duration and energy cost of the waiting phase preceding i t.\nLetTu=∆(w)\nu+∆(c)\nuandEu=E(w)\nu+E(c)\nube the duration and\nenergy cost of the uth waiting and communication cycle. Let\nMtbe the total number of requests served and completed up to\ntimet. Then, we deﬁne the expected average communication\ndelay and average UA V power consumption under a given\ntrajectory policy µ(deﬁned later), with the UA V starting from\nthe geometric center qU(0) = (0,0)as\n¯Dµ/defineslim\nt→∞Eµ/bracketleftBigg/summationtextMt−1\nu=0∆(c)\nu\nMt/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleqU(0) = (0,0)/bracketrightBigg\n, (13)\n¯Pµ/defineslim\nt→∞Eµ/bracketleftBigg/summationtextMt−1\nu=0Eu/summationtextMt−1\nu=0Tu/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleqU(0) = (0,0)/bracketrightBigg\n. (14)We then seek to solve\n¯D∗\nµ= min\nµ¯Dµs.t.¯Pµ≤Pavg, (15)\nwhose minimizer is denoted by the optimal policy µ∗.\nNote that this problem is non-trivial. Consider, for instan ce,\nthe power unconstrained delay minimization problem in whic h\nthe UA V is the only destination of the packets. Here, the mini -\nmum delay to serve a request is achieved by ﬂying towards the\nGN at maximum speed to improve the link quality. However,\nthis strategy may not be optimal in an average delay sense,\ndue to the potentially longer distance that must be covered\nby the UA V to serve a subsequent request. Thus, it might\nbe preferable for the UA V to operate closer to the geometric\ncenter of the cell, where new requests can more readily be\nserved, as observed in our earlier work in [5]. Intriguingly ,\nunder an average power constraint, more interesting tradeo ffs\nmay emerge. For instance, maximizing the speed to improve\nthe link quality may no longer be a viable option, due to\nhigh power consumption, and hovering might not be the most\nenergy efﬁcient operation (see Fig. 2). Letting\n[¯Eµ,¯Tµ]/defineslim\nt→∞Eµ/bracketleftBigg/summationtextMt−1\nu=0[Eu,Tu]\nMt/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleqU(0) = (0,0)/bracketrightBigg\n,\nbe the average energy and time of a waiting and commu-\nnication cycle, we can use Little’s Theorem (see [13]) to\nexpress the average power as ¯Pµ=¯Eµ/¯Tµ, so that the power\nconstraint can be equivalently expressed as ¯Eµ−Pavg¯Tµ≤0.\nHence, (15) can also be expressed equivalently as\nµ∗= argmin\nµ¯Dµs.t.¯Eµ−Pavg¯Tµ≤0. (16)\nAs we will see, this is a more tractable form to work with,\nbecause it removes the metric ¯Tµfrom the denominator, and\nallows one to express the optimization problem as a semi-\nMarkov decision processes (SMDP).\nTo deal with the inequality constraint in (16), we let\ng(ν) = min\nµ¯Dµ+ν(¯Eµ−Pavg¯Tµ) (17)\n= min\nµlim\nt→∞Eµ/bracketleftBigg/summationtextMt−1\nu=0(∆(c)\nu+νEu−νPavgTu)\nMt/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleqU(0)/bracketrightBigg\n,\nbe the dual function with dual variable ν, which can be\noptimized by solving the related dual maximization problem\nmax\nν≥0g(ν). (18)\nB. SMDP Formulation\nIn this section, we formulate (17) as a SMDP, and charac-\nterize its states, actions, cost metrics, and policy. We the n opti-\nmize this SMDP via discretization and dynamic programming.\nIn general, the state at any given time trequires knowledge\nof the UA V position qU(t)=(rU(t),ψU(t)), whether there is\na request for uplink transmission, and if a request exists, t he\nlocation of the GN that originated it, qG(t)=(rG(t),ψG(t)).\nHowever, during waiting phases, the angular coordinate ψU(t)\nof the UA V is irrelevant to the decision process; only its\nradiusrU(t)is. In fact, the angular coordinate of requestsis uniform in [0,2π), irrespective of ψU(t). During commu-\nnication phases, only the position of the GN relative to that\nof the UA V matters, i.e., their radii rU(t),rG(t)and relative\nangular coordinate θG(t)/definesψG(t)−ψU(t)∈[0,2π), which is\nuniformly distributed in [0,2π). Hence, the state can be more\ncompactly expressed as rU(t)(for the UA V position) and\n(rG(t),θG(t)/definesψG(t)−ψU(t))(for a request). Let\nRUAV/definesR+,QGN/defines[0,a]×[0,2π) (19)\nbe the set of all radii positions of the UA V , rU∈ RUAV, and\nof all polar coordinates of requests by the GNs, relative to t he\nangular coordinate of the UA V , (rG,θG)∈ QGN. With that,\nwe deﬁne the set of waiting andcommunication states as\nSwait=RUAV×{(−1,−1)},Scomm=RUAV×QGN,\nwhere(−1,−1)denotes no active request, so that the overall\nstate space is S=Swait∪Scomm . To deﬁne this SMDP, we\nsample the continuous time interval to deﬁne a sequence of\nstates{sn,n≥0}⊆S with the Markov property, along with\nassociated time, delay and energy costs, as speciﬁed below.\nIf the UA V is in state sn= (rU,−1,−1)∈ Swait at time\nt, i.e., it is in the radial position rUand there are no active\nrequests, then the actions available are to move with veloci ty\nvector(vr,θc), over an arbitrarily small but ﬁxed interval of\nduration∆0≪πa2λ. The terms vrandθcdenote the radial\nand angular velocity components, respectively; since they must\nobey the velocity constraint (2), they take values from the\naction space\nAwait(rU)/defines/braceleftBig\n(vr,θc)∈R2/vextendsingle/vextendsingle/vextendsingle/radicalBig\nv2r+r2\nU·θ2c≤Vmax/bracerightBig\n.(20)\nAfter this interval, the new radial position becomes rU(t+\n∆0) =rU+vr∆0. Moreover, with probability e−πa2λ∆0, no\nnew request has been received in the time interval [t,t+∆0]\nso that the new state sn+1, sampled at time t+∆0, becomes\nsn+1= (rU+vr∆0,−1,−1)∈ Swait. Otherwise, a new re-\nquest is received in position (rG,θG)with the pdf given by (1),\nso that the new state is sn+1=(rU+vr∆0,rG,θG)∈ Scomm .\nOverall, the transition probability from the waiting state\nsn= (rU,−1,−1)under action an=(vr,θc)is expressed as\nP(sn+1=(rU+vr∆0,−1,−1)|sn,an) =e−πa2λ∆0,\nP(sn+1∈(rU+vr∆0,F)|sn,an) =A(F)·(1−e−πa2λ∆0)\nπa2,\n∀F⊆Q GN, whereA(F)is the area of the region F\non thex-yplane. To complete the deﬁnition of the\nSMDP, we need to deﬁne the cost metrics under each\nstate and action. The duration of action an=(vr,θc)in\nstatesn=(rU,−1,−1)isT(sn,an)/defines∆0, its delay cost is\n∆(sn,an)/defines0(the UA V is not communicating), and its energy\ncost isE(sn,an)/defines∆0Pmob/parenleftBig/radicalbig\nv2r+r2\nU·θ2c/parenrightBig\n.\nUpon reaching state sn=(rU,rG,θG)∈Scomm at timet, the\nUA V has received a request to serve the transmission of L\nbits from a GN located at (rG,θG)(relative to its current\nangle coordinate). The actions available to the UA V at this\npoint are all trajectories starting from its current positi on that\nfollow the 4-step communication phase procedure describedearlier. Thus, we denote an action as an= (qGU,v1,qUB,v3),\nwhose duration and communication delay is T(sn,an) =\n∆(sn,an)/defines∆(c)as given by (12), and whose energy cost is\nE(sn,an)/definesE(c)as given by (11). After the communication\nphase is completed at time t+ ∆(c), the new state sn+1\nis sampled. At this point, a new waiting phase begins and\nthe radial position of the UA V is rUB(the radial position\ncorresponding to qUB), so that the transition probability from\nstatesnunder action anis expressed as\nP(sn+1= (rUB,−1,−1)|sn= (rU,rG,θG),an) = 1.(21)\nWith the states and actions deﬁned, we can deﬁne a policy\nµ. Speciﬁcally, for states (rU,−1,−1)∈ Swait,µselects\na velocity vector (vr,θc)∈ Await(rU), as deﬁned in (20).\nLikewise, for states (rU,rG,θG)∈ Scomm , the policy selects\nan action (qGU,v1,qUB,v3)as has been prescribed.\nHaving now deﬁned a stationary policy µ, we can reformu-\nlate the Lagrangian term L(ν)\nµ/defines¯Dµ+ν(¯Eµ−Pavg¯Tµ), which\nwe seek to minimize to ﬁnd the dual function in (17). In the\ncontext of the SMDP, this can be expressed as\nL(ν)\nµ= lim\nK→∞E/bracketleftBigg\n1\nK/summationtextK−1\nn=0ℓν(sn,µ(sn))\n1\nK/summationtextK−1\nn=0χ(sn∈ Scomm)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingles0/bracketrightBigg\n, (22)\nwheres0= (0,−1,−1)(the UA V begins in the center with no\ntransmission requests), χ(C)is the indicator function of the\neventC, and we have deﬁned the overall Lagrangian metric\nin statesunder action aas\nℓν(s,a)/defines∆(s,a)+ν/parenleftbig\nE(s,a)−PavgT(s,a)/parenrightbig\n. (23)\nUsing Little’s Theorem [13], we can rewrite L(ν)\nµin terms of\nthe steady-state pdf of being in state sin the SMDP, Πµ(s),\nL(ν)\nµ=1\nπcommˆ\nSΠµ(s)ℓν(s,µ(s))ds, (24)\nwhereπcomm/defines´\nScommΠµ(s)dsis the steady-state probability\nof being in a communication state in the SMDP, which is\nprovided in closed-form in the next lemma.\nLemma 1. Letπwaitandπcomm be the steady-state probabil-\nities that the UAV is in the waiting and communication phases\nin the SMDP . We have that\nπwait=1\n2−e−πa2λ∆0, πcomm=1−e−πa2λ∆0\n2−e−πa2λ∆0.(25)\nProof. Letpww,pwc,pcw, andpccbe the probabilities of\nremaining in the waiting phase ( ww), moving from a waiting\nstate to a communication state ( wc), from a communication\nto a waiting state ( cw), or remaining in the communication\nphase (cc), in one state transition of the SMDP. Then, pww=\ne−πa2λ∆0(if no request is received, the SMDP remains in the\nwaiting state), pwc= 1−pww,pcw= 1, andpcc= 0 (after\nthe communication phase, the waiting phase begins, see (21) ).\nTherefore,πwait andπcomm satisfyπwait+πcomm= 1 and\nπwait=pwwπwait+pcwπcomm=e−πa2λ∆0πwait+πcomm,\nπcomm=pwcπwait+pccπcomm= (1−e−πa2λ∆0)πwait,\nwhose solution is given in the statement of the lemma. /squaresolidThe minimization problem of (17) can then be expressed as\ng(ν) = min\nµL(ν)\nµ=1\nπcommmin\nµˆ\nSΠµ(s)ℓν(s,µ(s))ds(26)\nwith the subsequent dual maximization problem given in (18) .\nIII. T WO-SCALE SMDP O PTIMIZATION\nTo reduce the complexity of the problem, we exploit a\ndecomposition of the policy µ, such that the total optimization\nproblem of (26) and its dual maximization consist of solving\nsimpler, inner and outer optimization problems separately in\na two-scale decision-making approach.\nNote that the steady-state pdf Πµ(s)depends on the policy µ\nonly through vrforwaiting state actions(vr,θc)and through\nthe radiusrUBofqUB= (rUB,θUB)forcommunication\nstate actions(qGU,v1,qUB,v3). By separating vrfromθc\nin the waiting states andrUBfrom the other action elements\nin the communication states , we have created a decomposition\nwhich allows parts of the optimal cost of a state-action pair ,\nℓ∗\nν(s,µ(s)), to be solved separately from the steady-state\nprobabilities Πµ(s). Next, we formalize this decomposition.\nLetW(s)/definesvr∈[−Vmax,Vmax]deﬁne the radial velocity\npolicy of the waiting states that speciﬁes the radial veloc-\nity component of a waiting action a=(vr,θc)∈Await(rU).\nAdditionally, let U(s)/definesrUB∈RUAV deﬁne the next radius\nposition policy of the communication states that speciﬁes the\nend radius position of the communication action. Under this\ndecomposition, we can express the pdf Πµsolely as a function\nof the policies W,U , i.e.ΠW,U, so that we have that\ng(ν) =1\nπcommmin\nW,Uˆ\nSΠW,U(s)ℓ∗\nν(s,Z(s))ds, (27)\nwhereZ(s) =W(s)fors∈ Swait,Z(s) =U(s)fors∈\nScomm , andℓ∗\nν(s,Z(s))is obtained by greedily minimizing\nℓ∗\nν(s,a)with respect to the components of the actions not\nspeciﬁed by W,U (hence not affecting ΠW,U). Namely, for\nwaiting states s= (rU,−1,−1)andW(s) =vr,\nℓ∗\nν(s,vr)=min\nθcν/bracketleftbigg\nPmob/parenleftbigg/radicalBig\nv2r+r2\nU·θ2c/parenrightbigg\n−Pavg/bracketrightbigg\n∆0,\ns.t./radicalBig\nv2r+r2\nU·θ2c≤Vmax. (28)\nNote that the minimizer θ∗\ncof (28) is the angular velocity that\nminimizes the UA V power consumption for some given radial\nvelocityvrand UA V radius rU, solvable ofﬂine through ex-\nhaustive search (for the case in which the power curve follow s\nthe unimodal function given in Fig. 2, it can be determined in\nclosed form as θc= 0 ifvr≥vmin/definesargmin V>0Pmob(V)\nandθc=/radicalbig\nv2\nmin−v2r/rUifvr<vmin). For communication\nstatess= (rU,rG,θG)andU(s) =rUB,\nℓ∗\nν(s,rUB) = min\nqGU,θUB\nv1,v3≤Vmax(1−νPavg)∆(c)+νE(c)(29)\nwhere∆(c)andE(c)are the delay and energy costs given\nby (11) and (12). Due to low dimensionality, the solutions to\nℓ∗\nν(s,U(s))can also be found through an exhaustive search.\nOnceℓ∗\nν(s,Z(s))has been determined for all states s, the\nproblem of (27) can then be solved for a given value of νthrough discretization and dynamic programming (i.e., val ue\niteration or policy iteration), where the subsequent dual m axi-\nmization resorts to adjusting νiteratively, either by exhaustive\nsearch or subgradient-based methods.\nIV. N UMERICAL RESULTS\nFor the simulation parameters, we use a channel band-\nwidthB=1MHz, 1-meter reference SNRs γGU=γUB=40dB,\nUA V height HU=120 m, BS height HB=60m, maximum\nUA V speed Vmax=55m/s, and a Poisson arrival rate of\nλ=2.693×10−9[requests/sec/m2]. For the power consumption\nmodel, we use the power-speed relationship given in (4) (see\nFig. 2) and the same parameters utilized in [10].\nTo solve an approximation of the problem, we discretize\nthe state and action spaces, solving an inner SMDP for a\ngivenν(k)via value iteration, updating ν(k+1), and repeating\nuntil the maximum is found. In discretizing the state space,\nwe select a cell radius of a=1600 m andN=10 equispaced\ndiscretized radii, with a single GN located in the center, M=3\nequispaced angular positions at the next discretized radiu s\nposition, 2Min the next one, 3Min the next one, and so\non until we reach the Nth radius value, ensuring that the\ndistribution of GNs in the circular area approximates the pd f\nof the GNs described in the system model. We discretize the\nradial velocity actions into K=13 equispaced values such that\nvr∈{−Vmax,...,0,...,V max}. For the next radius position\nactions, we utilize the radii positions indexed by the set\n{1,...,N}. Lastly, the action duration ∆0from waiting states\nis chosen in such a way that it is unlikely to receive more than\none request in an interval [0,∆0]. We choose e−πa2λ∆0≃0.93.\nIn Fig. 3, we depict the waiting phase policy, where the\ntarget average power constraint is ﬁxed at Pavg=1000 Watts,\nand the data payload value is varied for comparison. Note tha t\nfor small data payload values, more consideration is placed on\npower minimization, hence the UA V seeks to move towards\na positive radius value and move circularly at the power-\nminimizing speed until receiving a request; for larger data\npayloads, the minimization focuses more on communication\ndelay, hence, the UA V seeks to reach the geometric center of\nthe GNs quickly in order to position itself best in anticipat ion\nof future transmission requests.\nNext, we look at an example of the communication phase\npolicy for a ﬁxed data payload value of L= 1Mbit, UA V\nposition(rU,θU)=(710,0)m, request radius rG=1600 m, and\nvaried relative request angles, θG, as illustrated in Fig. 4. We\nobserve the pattern of the relative distance between a reque st-\ning GN and the associated position qGU, where the UA V ﬁrst\nreceivesLbits from the GN. Additionally, a common end\nradius ofrUB=178 m is found to be an optimal end point,\neven under variations in the relative request angles θG.\nFinally, in Fig. 5, we ﬁx the data payload value at\nL=1Mbit and show how the optimal expected average delay,\n¯D∗\nµ, changes for various target Pavgvalues in the range\n[875,1850] Watts. As expected, ¯D∗\nµdecreases with increasing\nUA V power consumption. Additionally, the performance is\ncompared to several other heuristics:\n1)Hover at center : The UA V always hovers at the center of\nthe cell. The expected average delay is 90.59[s], noticeablyrU[m]0 200 400 600 800 1000 1200 1400 1600vr[m/s]\n-1000100\nL = 0.01 Mbits\nrU[m]0 200 400 600 800 1000 1200 1400 1600vr[m/s]\n-100-500\nL = 0.05 Mbits\nrU[m]0 200 400 600 800 1000 1200 1400 1600vr[m/s]\n-40-200\nL = 1 Mbits\nFig. 3: Optimal radial velocities vrfor each of the waiting state UA V radii,\nshown across 3different data payloads\nx[m]-1500 -1000 -500 0 500 1000 1500y[m]\n-1500-1000-500050010001500\nFig. 4: Communication phase policy ( U∗(s) =rUB= 178 m andv∗\n1,v∗\n3=\n29.1m/s in all cases) with data payload L= 1 Mbits for UA V position\n(rU,θU) = (710 ,0)m,rG= 1600 m, and varied θG.\nworse than the delay yielded by the optimal policy µ∗\nfor any of the tested Pavgtargets. Alternatively, if the\nGNs always transmit directly to the BS, we found that the\nperformance is roughly the same, since the UA V-BS link\nincurs small delay thanks to the small UA V-BS distance.3\n2)Start-end at center : The UA V hovers at the center, awaiting\nrequests; once a request is received, it moves at speed v\ntowards the GN at a certain radius rGU, receives the L\nbits from the GN, travels back at speed vto the center,\nwhere it hovers to relay the data payload to the BS; rGU\nis optimized to minimize the communication delay of each\nrequest, whereas vis varied so as to obtain different power\nconsumptions. Note that the expected average communica-\ntion delay is worse than the optimal policy µ∗across all\nvalues ofPavg, except for smaller values of Pavg(due to\nthe approximation introduced by discretization).\nOverall, the optimal policy outperforms these heuristic\nschemes by a signiﬁcant margin, hence demonstrating the\nimportance of an adaptive design that optimizes the average\nlong-term performance.\nV. C ONCLUSIONS\nIn this paper, we studied the trajectory optimization prob-\nlem of one UA V acting as a relay in a two-phase, decode-\nand-forward strategy, servicing random uplink transmissi on\n3For this case, we use optimistically the same pathloss model as for the\nGN-UA V-BS links.Pavg[Watts]1000 1200 1400 1600 1800¯D∗\nµ[sec]\n405060708090\nOptimal Policy µ*\nStart From Center Heuristic\nHover at Center\nFig. 5: Expected average communication delay vs. average po wer constraint\nfor the optimal policy µ∗and two heuristic schemes, for data payload value\nL= 1 Mbit.\nrequests by GNs seeking to communicate a data payload\nto a centralized BS. We formulated a continuous state and\naction space, discretized the problem into an SMDP, and\nsolved it through dynamic programming. It was shown that the\nproblem exhibits an interesting two-scale structure. Nume rical\nevaluations demonstrate consistent improvements in the de lay\nperformance over sensible heuristics.",
      "metadata": {
        "filename": "Power-Constrained Trajectory Optimization for Wireless UAV Relays with Random Re.pdf",
        "hotspot_name": "Aluminium_Die_Casting",
        "title": "Power-Constrained Trajectory Optimization for Wireless UAV Relays with\n  Random Requests",
        "published_date": "2020-02-22T04:09:51Z",
        "pdf_link": "http://arxiv.org/pdf/2002.09617v2",
        "query": "die casting aluminium radiator energy consumption reduction process optimization techniques"
      }
    },
    "DRL-Based Injection Molding Process Parameter Optimization for Adaptive and Prof": {
      "full_text": "DRL -Based Injection Molding Process Parameter \nOptimization for Adaptive and Profitable Production  \nJoon -Young Kim 1,2*, Jecheon Yu 1*, Heekyu Kim 1, Seunghwa Ryu 1‡ \n1 Department of Mechanical Engineering, Korea Advanced Institute of Science and Technology \n(KAIST), Daejeon, Republic of Korea  \n2 Industrial Intelligence Research Group, AI/DX Center , Institute for Advanced Engineering (IAE) , \nYongin , Republic of Korea  \n \nAbstract:  Plastic injection molding remains essential to modern manufacturing . However, \noptimizing process parameters to balance product quality and profitability under dynamic \nenvironmental and economic conditions remains a persistent challenge. This study presents a novel \ndeep reinforcement learning (DRL) -based framework for real -time process optimization in \ninjection molding, integrating product quality and profitability into the control objective.  A profit \nfunction was developed to reflect real -world manufacturing costs, incorporating resin, mold wear, \nand electricity prices, incl uding time -of-use variations. Surrogate models were constructed to \npredict product quality and cycle time, enabling efficient offline training of DRL agents using soft \nactor-critic (SAC) and proximal policy optimization (PPO) algorithms. Experimental results \ndemonstrate that the proposed DRL framework can dynamically adapt to seasonal and operational \nvariations, consistently maintaining product quality while maximizing profit.  Compared to \ntraditional optimization methods such as genetic algorithms, the DRL models achieved comparable \neconomic performance with up to 135 × faster inference speeds, making them well -suited for real -\ntime applications. The framework’s scalability and adaptability highlight its potential as a \nfoundation for intelligent, data -driven decision -making in modern manufacturing environments.  \n \nKeywords:  Plastic in jection molding ; Process parameter  optimization ; Profit  optimization ; \nAdaptive production systems ; Deep reinforcement learning  (DRL) ; Proximal policy optimization  \n(PPO) ; Soft actor-critic (SAC)  \n  1    INTRODUCTION  \n1.1    Research Background  \n Plastic injection molding is a cornerstone of modern manufacturing, enabling the mass \nproduction of plastic components with high precision, repeatability, and efficiency [1,2]. Its \napplications span various  industries, including automotive, electronics, medical devices, and \nconsumer goods, where lightweight and durable plastic components have replaced mainly  \ntraditional materials such as  metal and glass. The capacity to produce complex geometries at scale \nwith minimal material waste has further established injection molding as a vital manufacturing \nprocess [3]. \n Despite widespread adoption, the injection molding industry operates under intense cost \npressures due to its high -volume, low -margin nature. Manufacturers must maintain a delicate \nbalance between ensuring product quality and optimizing operational costs [4]. Inefficiencies such \nas excessive material usage, prolonged cycle times, and elevated electricity  consumption can \nsignificantly erode profit margins  [5–8]. This challenge is exacerbated by external factors , \nincluding  fluctuations in raw material prices, electricity costs, and seasonal variations, all of which \ncan impact the overall profitability of injection molding operations [9,10] . \n Traditionally, process parameters in injection molding, such as injection pressure, mold \ntemperature, cooling time, and holding pressure, have been optimized with a primary focus on \nproduct quality.  Numerous studies have investigated the influence of these parameters on defect \nrates, mechanical performance, and surface finish [11]. In recent years, statistical models and \nmachine learning techniques have been increasingly adopted to predict optimal process conditions \nfor consistent product quality  [12–16].  \n However, these methodologies fail to address a pivotal element of dynamic cost fluctuations in \nreal-world manufacturing contexts.  These methodologies  optimize for quality but do not account \nfor the financial impact of changing electricity  prices, equipment operation , and maintenance costs. As a result, manufacturers may achieve high -quality production but fail to maximize profitability, \nleading to potential financial losses despite optimal technical performance. For example, electricity \npricing varies significantly based on time of use, with  peak-hour electricity  costs often being \nconsiderably higher than off -peak rates [17–20]. In regions where electricity pricing follows a \ndynamic pricing  structure, operating injection molding machines during peak hours without \nadjusting process parameters accordingly can lead to unnecessarily high production costs. \nEnvironmental conditions such as ambient temperature and relative humidity can also affect \nmachine performance, influencing cycle times and cooling efficiency [21–26]. When process \nsettings remain static, they cannot respond effectively to these external variations, leading to \nincreased electricity  consumption and reduced cost -effectiveness.  \n To overcome these limitations, developing a decision -making model capable of dynamically \nadjusting process parameters in response to real -time operational and environmental conditions is \nessential . This necessitates transitioning  from conventional static optimization methods to an \nadaptive, real -time decision -making approach.  Reinforcement learning  (RL)  methods present a \npromising solution because they allow systems to learn continuously from operational feedback \nand to improve decision -making strategies based on changing cost and quality considerations  [27–\n29]. \n This study proposes a deep reinforcement learning (DRL) -based framework for optimizing \ninjection molding process parameters with the dual objectives of ensuring product quality and \nmaximizing profitability. Traditional optimization approaches have primarily focused on reducing \ndefects under fixed operating conditions, often overlooking the e conomic impact of dynamic \nvariables such as electricity pricing, resin costs, and mold wear. To address this gap, the proposed \nframework integrates cost -related factors directly into the optimization process, enabling adaptive \ncontrol in variable manufactu ring environments  [30–33].   The framework employs soft actor-critic (SAC)  [34,35]  and proximal policy optimization \n(PPO) [34] algorithms , which are well -suited for continuous control tasks with stability and sample \nefficiency. Surrogate models were developed using production data collected through a Design of \nExperiments (DOE) methodology to predict product quality and cycle time. These models allow \nthe DRL agent to be trained offline in a simulated environment that closely reflects real -world \nprocess dynamics. Once deployed, the agent optimizes process parameters in real time, adjusting \nto fluctuating environmental and cost conditions within sub -second inference time.  \n \nFig. 1. Framework  for real -time process parameter optimization using deep reinforcement \nlearning considering environmental variations in injection molding processes.  \n \n As illustrated in Fig. 1 , the proposed framework achieves profitability levels comparable to \nglobal optimization methods such as genetic algorithms while offering over 100 -fold faster \ndecision -making. This approach not only supports real -time manufacturing control but also \ncontributes to the development of intelligent, cost -aware production systems that are scalable and \napplicable across various industrial contexts. The results demonstrate the feasibility of DRL as a \npractical solution for sustainable, economically optimized manufacturing operations.  \n The structure of this paper is as follows. Section 1.2  presents a comprehensive review of \nprior studies on the application of Deep Learning (DL) and RL for injection molding process \noptimization . Section 2  outlines the problem definition, incorporating the formulation of the \nprofit function and the Markov Decision Process (MDP)  of DRL -based decision -making \nmodel s. It also introduces the surrogate models used for quality classification and cycle time \nprediction, as well as the theoretical foundations of PPO and SAC.  Section 3  describes  the \nexperimental results, including  data acquisition, surrogate model evaluation, and DRL  agent \nfor process parameter optimization. This section also provides a comparative analysis of PPO \nand SAC, benchmarked against a Genetic Algorithm (GA), and presents deployment results of \nthe DRL agents under three different seasonal scenarios i n a virtual production environment.  \nSection 4  discusses the implications, limitations, and future research directions of the proposed \nDRL framework . Finally, Section 5  summarizes key findings and  highlights their implications \nfor the injection molding industry.  \n \n1.2    Related works  \n Injection molding is a complex manufacturing process that requires precise control over \nparameters such as injection pressure, mold temperature, cooling time, and holding pressure to \nensure both product quality and operational efficiency. Traditional optim ization approaches, \nincluding DOE and Response Surface Methodology (RSM), have been extensively employed to identify optimal process settings and improve performance. DOE has been successfully applied in \noptimizing injection molding for various applications such as automotive components, micro -\ninjection processes, and geometrically complex parts [34–36]. These studies have demonstrated \nimprovements in defect reduction, dimensional accuracy, and part stability. DOE has also been \nused to minimize contour distortion in composite components and to enhance structural integrity \nin molded products  [37]. A combined simu lation and experimental approach has validated DOE -\ndriven parameter selection for thin -shell plastic parts, ensuring dimensional consistency and defect \nreduction [38]. RSM, particularly when integrated with advanced optimization techniques, has \nfacilitated fine -tuning of process parameters  [39]. For instance, combining RSM with nonlinear \nprogramming and Artificial Neural Networks (ANNs) has enabled the precise optimization of \nmolding processes for plastic lenses and reinforced polycarbonate composites, resu lting in better \nmechanical properties and reduced variability [40,41]. However, these methods generally require \nextensive experimental trials and may be limited in their ability to capture the nonlinear and \ndynamic nature of modern injection molding systems  [42,43].  \n With the advancement of computational intelligence, machine learning (ML) and deep learning \n(DL) techniques have emerged as powerful tools for modeling complex interactions between \nprocess parameters and product outcomes.  One study applied various DOE methods with artificial \nneural networks (ANN) to assess their impact on modeling efficiency and accuracy  [44]. Another \ndeveloped a regression -based ML system for real -time defect prediction, improving production \nefficiency and reducing waste  [45]. ML has also been used to reveal links between processing \nconditions, structural features, and material properties in polypropylene moldi ng, enabling \nparameter optimization for better mechanical performance [46]. In addition, supervised learning \nhas been employed to predict cooling time, aiding in production planning and cycle time reduction  \n[47]. DL techniques have addressed class imbalance in fault detection, enhancing defect \nclassification and process control [48], while DL -based optimal tracking control methods have been proposed to monitor resin flow in molding machines  [49]. These studies highlight the growing \nrole of deep learning in optimizing injection molding, improving accuracy in defect detection, \nparameter prediction, and process stability, ultimately leading to better product quality and \nmanufacturing efficiency .  \n RL is a method in which an agent interacts with an environment to learn an optimal policy for \nmaximizing rewards without requiring expert knowledge, making it an attractive approach for \nvarious applications. Similarly, DRL has emerged by combining RL with deep neural networks, \nallowing agents to learn effectively in broader and more complex state and action spaces [30,35] . \nAdvances in DRL have gained significant attention by demonstrating impressive performance in \nhighly complex games such as Go [36] and StarCraft II [37]. Recently, DRL has also been actively \napplied in the manufacturing domain. For instance, its applications have been reported in areas \nsuch as system design [38–40], process planning [41,42]  process control and optimization [43–46] \nand quality control [47,48] .  \n In the domain of injection molding, RL has been applied to real -time process parameter \noptimization. One study proposed a decision -making framework that used offline data and a self -\npredictive ANN to dynamically adjust parameters and maintain consistent pr oduct quality [50]. \nHowever, this approach relied on simulation -based surrogate models, which may introduce \nmodeling inaccuracies, and utilized the Deep Deterministic Policy Gradient (DDPG) algorithm, \nwhich is known to suffer from instability and limited exploration in complex, high -dimensional \nspaces. Moreover, most existing studies have primarily focused on quality objectives, such as \ndimensional accuracy or part thickness, without integrating cost or profit metrics into the learning \nprocess.  Other studies have explored various RL strategies for optimizing molding processes. RL \nhas been employed to refine ram velocity and packing pressure profiles  [27], optimize flow \ndistribution in liquid composite molding  [49], and improve trajectory control using Iterative \nLearning Control (ILC) combined with RL  [50]. Approaches such as Deep Q -Networks (DQN) have been applied to achieve intelligent decision -making under complex production conditions  \n[30]. In addition, DRL has been utilized for intelligent temperature control in stretch blow molding \nand for temperature compensation using Actor -Critic framework, surpassing traditional PID and \nGPC controllers in stability and accuracy  [32,51] . Further developments include RL -based self -\nrecovery mechanisms for correcting non -optimal conditions [52], two -dimensional Q -learning for \nunknown system tracking  [53], and fault -tolerant tracking control using off -policy RL for robust \noperation under actuator failures [54].  \n In summary, while deep learning and RL have significantly advanced the optimization of \ninjection molding processes, integrating profitability into these models remains an emerging area. \nIt is imperative to acknowledge the limitations of previous studies and to develop more accurate \nsurrogate m odels, employ robust RL algorithms such as PPO or SAC, and expand the scope of the \nconsidered process parameters. This approach will lead to more adaptive and profitable production \nstrategies.  \n \n2    METHODOLOGY  \n2.1    Data Acquisition  \n A real-world dataset was employed to develop the DRL agent capable of optimizing process \nparameters in injection molding . The data were acquired from a fully instrumented injection \nmolding testbed, which has been described in detail in our prior study  [55]. The experimental setup \nincluded the installation of environmental sensors to monitor factory  and machine -level \ntemperature and relative humidity , integrating a Central Monitoring System (CMS) for capturing \nprocess parameters, and deploying  an automated vision inspection system for product quality \nassessment.  \n The collected  dataset comprises  process parameters such as injection speed, pressure, position, \nand hold time, along with external environmental variables including temperature and relative humidity , both within the factory and at the machine level. A total of 2,794 samples were collected. \nEach data sample comprises  10 controllable process parameters, 4 external environmental \nvariables, and 1 binary quality label (good or defective), which was automatically assigned through \nvisual  inspection. The target product for the experiment was a circular cosmetic container cap made \nfrom acrylonitrile butadiene styrene  (ABS ), a thermoplastic material commonly used in industrial \napplications.  \nTable 1. Statistical analysis results of the dataset.  \n Variable name  Mean  Std \nDev Min 25% 50% 75% Max Unit \nProcess \nparameter  Injection speed 1  29.5 6.5 20.0 25.0 30.0 35.0 40.0 % \nInjection speed 2  30.1 6.5 20.0 25.0 30.0 35.0 40.0 % \nInjection speed 3  20.1 6.5 10.0 15.0 20.0 25.0 30.0 % \nInjection pressure 1  129.9  6.5 120.0  125.0  130.0  135.0  140.0  bar \nInjection pressure 2  130.1  6.5 120.0  125.0  130.0  135.0  140.0  bar \nInjection pressure 3  140.1  6.5 130.0  135.0  140.0  145.0  150.0  bar \nInjection position 1  46.0 1.3 44.0 45.0 46.0 47.0 48.0 mm \nInjection position 2  37.8 3.9 32.0 35.0 38.0 41.0 44.0 mm \nInjection position 3  30.0 1.3 28.0 29.0 30.0 31.0 32.0 mm \nHold time  1.2 0.8 0.0 0.6 1.2 1.8 2.4 sec \nEnvironmental \nvariables  Machine temperature  15.4 4.2 5.5 13.3 14.5 19.8 21.8 ℃ \nMachine humidity  42.1 9.8 23.6 33.0 44.7 51.0 62.4 % \nFactory temperature  15.3 4.4 5.6 12.1 14.3 19.9 22.8 ℃ \nFactory humidity  42.6 10.6 23.0 33.2 47.1 50.8 63.6 % \n \n The environmental conditions were recorded continuously throughout the production process \nand naturally fluctuated over time. Conversely, process parameters were systematically varied \naccording to a DOE scheme, specifically utilizing an L81 orthogonal arra y. This methodological \ndesign ensured broad parameter space coverage  and improved the dataset's representativeness \nconcerning  the complex, nonlinear interactions present in real -world injection molding \nenvironments.  For a detailed description of the experi mental configuration, the testbed architecture , \nand quality labeling methodology, readers are directed to our previous work. A statistical summary \nof the dataset is presented in  Table 1 . 2.2    Problem definition and MDP formulation of DRL -based decision -making model  \n The primary  objective of this research is to develop a DRL -based optimization framework \nthat maximizes profitability in the injection molding process. Conventional  approaches have \nprimarily focused  on optimizing process parameters to enhance product quality, often without \nincorporating critical economic factors such as electricity  consumption, mold costs, and overall \nproduction efficiency. To address this limitation, a profit function is formulated that integrates both \nquality -related and cost -related components. This formulation supports a holistic and financ ially \nsustainable optimization strategy.  \n The profit function, which represents profit per a single production cycle , is formulated as \nfollows:  \n𝑓(𝑥)=𝑝∑𝑦𝑖 𝐶𝑣\n𝑖=1− ∑(𝑐𝑖resin+𝑐𝑖mold(𝑃max)+𝑐𝑖elect(𝑃max))𝐶𝑣\n𝑖=1(1) \nWhere,   \n● 𝐶𝑣 represents  the number of cavities per cycle (in this case, 𝐶𝑣=4). \n● 𝑝 represents the unit price per cavity (in this case, 𝑝 = 0.2$/cavity).  \n● 𝑦𝑖 represents  a binary variable that indicates whether the product from the 𝑖-th cavity is \ngood  (1) or defective (0).  \n𝑦𝑖= { 1,       if  𝑖–th cavity  is good  cavity         \n0,       if  𝑖–th cavity  is defective  cavity(2) \n● 𝑐𝑖resin represents  the resin cost per cavity (in this case, 𝑐𝑖resin = 0.04$/cavity).  \n● 𝑐𝑖mold(𝑃max) represents the mold cost per cavity ($/cavity), which is determined based \non the maximum injection pressure. This cost is derived from a method utilized by the \nfactory in this study for cost analysis, where the estimated mold wears  per cycle and \nremaining usable cycles are calculated based on maximum injection pressure. The cost per cavity is obtained by distributing the total mold replacement cost over 4 cavities per \ncycle.  \n𝑐𝑖mold(𝑃max)= {0.025 ,          if  𝑃max <140  bar \n 0.02775 ,     if  140  bar ≤ 𝑃max(3) \n● 𝑐𝑖elect(𝑃max) represents the electricity cost per cavity ($/cavity), which depends on the \nmaximum injection pressure. The electricity consumption per cycle is 0.8 kWh for  \n𝑃max <135  bar, 1.0 kWh for 135  bar ≤𝑃max <145  bar, and 1.1 kWh for \n145 bar ≤ 𝑃max. Dividing by 4 cavities per cycle gives per -cavity values of 0.2 kWh, \n0.25 kWh, and 0.275 kWh. The electricity cost per cavity is then determined by \napplying seasonal and time -dependent electricity pricing.  Tables 2 and 3 summarize \nthe time -of-use electricity prices and their seasonal classifications. Fig. 2 illustrates the \ncorresponding hourly price variations across seasons.  \n𝑐𝑖elect(𝑃max)= {0.2    ×electricity  price ,if  𝑃max <135 bar                        \n 0.25  ×electricity  price ,if  135  bar ≤𝑃max <145  bar   \n 0.275 ×electricity  price ,if  145  bar ≤ 𝑃max                       (4) \n By maximizing the profit  function, the DRL agent can learn an optimal control policy that \nbalances product quality, operational efficiency, and cost minimization in dynamic production \nenvironments. The proposed DRL -based framework is designed to adjust process parameters \nautonomously in real time . This allow s it to respond effectively to fluctuating external conditions, \nthereby enabling adaptive and data -driven decision -making.  Several key factors are considered in \nthe problem definition . Environmental conditions, including ambient temperature and humidity, \nsignificantly influence resin behavior and affect flow dynamics, necessitating real -time parameter \nadjustments to maintain acceptable product quality.  Another essential  factor  is cycle time, which \nmust be mini mized to enhance productivity while still ensuring that products meet quality standards . \nIn addition , electricity  cost variability plays a role, as electricity prices fluctuate depending on time \nand season, requiring optimized scheduling to reduce costs. The m old cost is also crucial because frequent high -pressure cycles accelerate wear, shortening  mold lifespan. Therefore, the agent must \nbalance production speed with the long -term health of the mold.  \n \nTable 2. Time -of-use electricity prices in South Korea in USD/kWh.  \n(The exchange rate is 1,000 KRW to a USD)  \n Spring/Fall  Summer  Winter  \nOff-peak hours  0.0995  0.0995  0.1065  \nMid-peak hours  0.1220  0.1524  0.1526  \nOn-peak hours  0.1527  0.2345  0.2101  \n \nTable 3. Time -of-use classification by season in South Korea.  \n Spring/Fall  Summer  Winter  \nOff-peak hours  22:00 - 08:00  22:00 - 08:00  22:00 - 08:00  \nMid-peak hours  08:00 - 11:00  \n12:00 - 13:00  \n18:00 - 22:00  08:00 - 11:00  \n12:00 - 13:00  \n18:00 - 22:00  08:00 - 09:00  \n12:00 - 16:00  \n19:00 - 22:00  \nOn-peak hours  11:00 - 12:00  \n13:00 - 18:00  11:00 - 12:00  \n13:00 - 18:00  09:00 - 12:00  \n16:00 - 19:00  \n \nTable 4. Ranges of process parameters including upper -lower bounds, small -large step size.  \nProcess parameter  Lower bound  Upper bound  Small step size  Large step size  \nInjection speed 1  20 40 0.1 5.0 \nInjection speed 2  20 40 0.1 5.0 \nInjection speed 3  10 30 0.1 5.0 \nInjection pressure 1  120 140 1.0 20 \nInjection pressure 2  120 140 1.0 20 \nInjection pressure 3  130 150 1.0 20 \nInjection position 1  44 48 0.1 1.0 \nInjection position 2  32 44 0.1 1.0 \nInjection position 3  28 32 0.1 1.0 \nHold time  0 2.4 0.1 1.0  \nFig. 2. Seasonal time -of-use electricity price profiles by hour for spring/fall, summer, and \nwinter.  \n \nTo enable the DRL agent to make optimal decisions in dynamic production environments, the \ndecision -making problem is formulated as MDP . The MDP framework provides a mathematical \nfoundation for sequential decision -making, aligning well with the challenges of injection molding , \nwhere external conditions and operational parameters continuously evolve.  An MDP is \ncharacterized by a tuple  (𝑆,𝐴,𝑅,𝑇), representing the state space, action space, reward function, \nand transition probability, respectivel y [56,57] . This study defines these components  to reflect the \nspecific requirements of injection molding optimization.  \nFirst, a state space 𝑆 represents the set of all possible states that the environment can assume. \nA state 𝑠𝑡∈𝑆 is defined as a vector that  incorporates  not only the controllable process parameters \nbut also external environmental variables and electricity price variations, all of which critically \naffect product quality, cycle time, and production  costs:  \n𝑠𝑡=[𝑝𝑡,𝑒𝑡,𝑐𝑡]∈𝑆 (5) \nwhere 𝑝𝑡 denotes the vector of controllable process parameters, 𝑒𝑡 represents the \nenvironmental variables, and 𝑐𝑡 corresponds to the electricity pricing information at time 𝑡. \nThe process parameter vector 𝑝𝑡 is defined as:  \n𝑝𝑡=[𝑣𝑡1,𝑣𝑡2,𝑣𝑡3,𝑃𝑡1,𝑃𝑡2,𝑃𝑡3,𝑥𝑡1,𝑥𝑡2,𝑥𝑡3,ℎ𝑡] (6)  \nwhere 𝑣𝑡𝑘, 𝑃𝑡𝑘, and 𝑥𝑡𝑘 represent the injection speed, pressure, and position at three distinct \nstage s (𝑘=1,2,3) of the injection process, and ℎ𝑡 denotes hold time.  The environmental \nvariable vector 𝑒𝑡 is defined as:  \n𝑒𝑡=[𝑇𝑡machine,𝑇𝑡factory,𝐻𝑡machine,𝐻𝑡factory] (7) \nrepresenting the temperature  and relative humidity conditions  at both the machine and factory.  \nThe electricity price 𝑐𝑡 is encoded as a 9 -dimensional one -hot vector, reflecting seasonal and \ntime-dependent price categories outlined in Tables 2 and 3. To stabilize the learning  process, \nthe continuous elements of the state vector are normalized to the range [−1,1], using the upper \nand lower bounds listed in Table 4. \n Second, an action space 𝐴 denotes the set of possible adjustments that the agent can make to \nthe process parameters. An action  𝑎𝑡∈𝐴 is defined  as a continuous adjustment vector, allowing \nthe agent to control the process parameters under varying environmental conditions:  \n𝑎𝑡=[Δ𝑣𝑡1,Δ𝑣𝑡2,Δ𝑣𝑡3,Δ𝑃𝑡1,Δ𝑃𝑡2,Δ𝑃𝑡3,Δ𝑥𝑡1,Δ𝑥𝑡2,Δ𝑥𝑡3,Δℎ𝑡]∈𝐴 (8) \nwhere each  Δ component represents  the adjustment to the corresponding controllable process \nparameter: injection  speed (𝑣𝑡𝑘), pressure (𝑃𝑡𝑘), position (𝑥𝑡𝑘), and hold time (ℎ𝑡) at different \nstages (𝑘=1,2,3). The evolution of process parameters over time follows a cumulative \nadjustment mechanism formulated as:  \n𝑝𝑡=𝑝0+∑𝑎𝑖𝑡−1\n𝑖=0(9)  \nwhere the current process parameters 𝑝𝑡 are determined by adding the initial process parameter \nsetting 𝑝0 and the cumulative sum of the past adjustment actions up to timestep 𝑡−1. To stabilize \nthe learning process and maintain consistency with the normalized state representation, the DRL agent outputs actions normalized within the range [−1,1]. In the environment, these normalized \nactions are rescaled using predefined adjustment step sizes specific to each process  parameter , as \nlisted in  Table 5 .  Two types of step sizes are defined: (i) a small step size, corresponding to one \nfourth of the difference between the upper and lower bounds of each process parameters , and (ii) a \nlarger step size, corresponding to one half of that difference. Furthermore, to ensure operational \nfeasibility, any action  resulting  in a combination of process parameter s exceeding its allowed \nbounds, as specified in  Table 4, is clipped accordingly.  \n \nTable 5  Adjustment step sizes for normalized actions used in the environment.  \nProcess parameter  Small step size  Large step size  \nInjection speed 1  5.0 10.0 \nInjection speed 2  5.0 10.0 \nInjection speed 3  5.0 10.0 \nInjection pressure 1  5.0 10.0 \nInjection pressure 2  5.0 10.0 \nInjection pressure 3  5.0 10.0 \nInjection position 1  1.0 2.0 \nInjection position 2  3.0 6.0 \nInjection position 3  1.0 2.0 \nHold time  0.6 1.2 \n \n Third, the reward function 𝑅 assigns a reward 𝑟𝑡=𝑅(𝑠𝑡,𝑎𝑡) based on the action 𝑎𝑡 taken in \nstate 𝑠𝑡. In the defined  problem, the agent generates newly adjusted process parameters, which are \nthen applied to the injection molding machine. After executing the injection process with these \nparameters, the reward is computed as the profit over a 10 -minute production interval, formulated \nbased on the profit function defined in Eq. 1  and expressed as:  \n𝑟𝑡=600\n𝑇×[   𝑝∑𝑦𝑖𝐶𝑣\n𝑖=1 – ∑(𝑐𝑖resin+𝑐𝑖mold(𝑃max)+𝑐𝑖elect(𝑃max))𝐶𝑣\n𝑖=1   ] (10) \nwhere 𝑇 represents the cycle time  in seconds , 𝐶𝑣 is the number of cavities per cycle, 𝑦𝑖 indicates \nthe quality  of i-th cavity . 𝑐𝑖resin, 𝑐𝑖mold, and 𝑐𝑖elect denote the resin cost, mold cost, and electricity cost per cavity  within a single production cycle , respectively. The choice to formulate the reward \nbased on a 10 -minute interval is made to match the setting where each timestep within an episode \ncorresponds to a 10 -minute production period during training  phase . Both the cycle time ( 𝑇) and \nthe cavity quality labels (𝑦𝑖) are functions of the process parameters  and environmental variables. \nThese quantities are inferred during training using the trained surrogate model s embedded within \nthe environment, enabling rapid reward computation without real -world experiments . The \nsurrogate models are described in detail in  Section 2.3 . \n Fourth, the transition probability 𝑃 represents the likelihood of transitioning to the next state \n𝑠𝑡+1 when an action 𝑎𝑡 is taken in the current state 𝑠𝑡, expressed as 𝑝(𝑠𝑡+1|𝑠𝑡,𝑎𝑡). It defines the \nstate transition dynamics within the MDP  framework . The agent does not explicitly model or \nestimate this transition function in model -free RL . Instead, it learns through trial -and-error \ninteractions with the environment, implicitly capturing the underlying transition dynamics from \nsampled transitions . Despite the absence of an explicit transition model, agents can successfully \nlearn state value functions, state -action value functions, and optimal policies by leveraging \ntemporal difference methods  [56]. \n \n2.3    Surrogate model s for quality classification and cycle time regression  \n A reliable simulation environment was required to enable efficient offline training of the DRL \nagent . This was achieved  by developing two surrogate models . One model was designed to predict \nproduct quality, while the other was developed to estimate cycle time.  These models serve as key \ncomponents of the virtual environment where the DRL agent can simulate the outcomes of different \nprocess parameters without directly interacting  with the physical injection molding machine .  \n The first surrogate model performs quality classification  by predicting whether a product \nmanufactured under a given set of process parameters is acceptable or defective.  This model was \ninitially  developed and validated in our previous study on diffusion -based quality estimation and process parameter inference in real manufacturing environments  [55]. It was trained using \nproduction data labeled with quality outcomes, along with controllable process parameters and \nenvironmental variables. The model replicates quality behavior observed in the real -world testbed \nand outputs a binary label representing the predicted quality classification.  \n In contrast, the surrogate model for cycle time regression was newly developed in this study to \nsupport the novel objective of profitability -oriented process optimization. This regression model \nestimates production cycle time based on given process paramet ers. To identify the most effective  \nregression algorithm , 18 ML models  were tested  using the Py Caret package  [58], including \nRandom Forest, Gradient Boosting, Light Gradient Boosting Machine  (LightGBM ), Decision Tree, \nand others. The dataset was split into training and testing sets in an 80:20 ratio, with 10% of the \ntraining data further allocated to a validation set.  Model evaluation was conducted through 10 -fold \ncross -validation, and the average performance scores across folds were compared.  Although \nRMSE (Root Mean Squared Error) and MAE (Mean Absolute Error) are widely used  metrics  in \nregression evaluation, this study adopted R²  (coefficient of determination) as the performance \nmetric. RMSE can disp roportionately penalize outliers due to the squaring of errors . While  MAE \noffers a straightforward average error, it does not capture the model’s ability to explain the variance \nof the target variable. Both are also scale -dependent and thus less suitable for comparing models \nacross varying process settings. In contrast, R²  offers a normalized, interpretable measure of how \nmuch variance in the dependent variable is accounted for by the model, making it more suitable for \nevaluating predictive performance in c omplex, multivariable environments like injection molding. \nUsing 10 -fold cross -validation, LightGBM was identified as the best -performing model among the \n18 tested, achieving the highest average R²  score. Its excellent predictive accuracy, fast training \ntime, and strong generalization capability made it an ideal choice for modeling the nonlinear and \ndynamic nature of cycle time in real manufacturing scenarios. The results are presented in  Table \n6. Further details  of surrogate models  are provided in  Section 1 of the Supplementary Materials . Table 6. The performance results from 10 -fold cross -validation on 18 ML models for cycle \ntime regression.  \nModel  RMSE  MAE  R² \nLight Gradient Boosting Machine  0.1468  0.0632  0.9743  \nRandom Forest Regressor  0.1537  0.0583  0.9719  \nExtra Trees Regressor  0.1559  0.0451  0.971 0 \nDecision Tree Regressor  0.1674  0.0475  0.9666  \nGradient Boosting Regressor  0.1749  0.0986  0.9645  \nK Neighbors Regressor  0.3776  0.2285  0.8365  \nAdaBoost Regressor  0.3916  0.3123  0.8246  \nRidge Regression  0.7239  0.627 0 0.4011  \nBayesian Ridge  0.7239  0.6271  0.4011  \nLinear Regression  0.7239  0.627 0 0.4011  \nElastic Net  0.9142  0.7613  0.0467  \nOrthogonal Matching Pursuit  0.9333  0.7768  0.0064  \nLasso Regression  0.9388  0.783 0 -0.0052  \nLasso Least Angle Regression  0.9388  0.783 0 -0.0052  \nDummy Regressor  0.9391  0.7832  -0.0058  \nHuber Regressor  1.1059  0.8215  -0.4008  \nLeast Angle Regression  1.1678  0.7724  -0.5819  \nPassive Aggressive Regressor  1.3215  1.0765  -1.0907  \n \n2.4    DRL algorithms  \n Model -free DRL algorithms can be categorized into value -based and policy -based methods. In \nvalue -based methods, the agent first learns a value function and subsequently derives a policy based \non it. However, value -based approaches are generally restricted to discrete ac tion spaces and are \nnot readily applicable to continuous action domains. To overcome  this limitation, policy -based \nmethods learn a policy directly, enabling the handling of continuous action spaces  [59]. \nNevertheless , policy -based approaches often  suffer from high gradient variance, negatively \nimpacting  learning stability and overall  performance. A framework that integrates both approaches \nis the actor -critic method, where in the actor  is responsible for learning the policy, and the critic \nestimates the value function to guide and stabilize policy updates  [60].   Additionally, DRL algorithms can be classified into on -policy and off -policy methods \ndepending on whether the policy used for training matches the policy that determines  the actions. \nIn on -policy methods, the policy being updated is identical to the one that determined the action. \nBy contrast , off-policy methods enable  learning from samples collected  by a different policy. \nConsequently , on-policy methods cannot reuse sample s generated by previous policies and must \nrely on newly collected trajectories  for each training iteration . In contrast , off-policy methods can \nleverage  past experiences by storing them in an experience replay buffer and incorporating them \ninto training. On-policy methods offer  more  stable learning and exhibit better convergence \nproperties. However, they are susceptible to getting trapped in local optima and suffer from high \nsample complexity. Off -policy methods are potentially less stable but achieve greater sample \nefficiency by ext ensively reusing prior experiences  [56,61,62] . \n On-policy DRL algorithms collect data using the current policy and update the policy based on \nthose collected samples. Trusted region policy optimization (TRPO)  [63] ensures stable policy \nimprovement by restricting updates within a trust region. However, TRPO  suffers from high \ncomputational complexity and implementation difficulty. PPO  [34] addresses these limitations by \nintroducing a clipped surrogate objective 𝐿CLIP(𝜃), as shown in  Eq. 1 1, thereby  replacing the \ncomplex trust region constraint in TRPO. The actor network is trained to maximize this  surrogate  \nobjective. To further enhance the stability of policy updates, PPO employs Generalized Advantage \nestimation (GAE) 𝐴̂𝑡GAE, which is  defined in  Eq. 1 2 and Eq. 1 3. The actor network is trained to \nmaximize 𝐿actor defined in  Eq. 1 4, which incorporates an entropy bonus term 𝑆[𝜋𝜃](𝑠𝑡) to \nencourage sufficient exploration. The critic network estimates the state value function 𝑉𝜙(𝑠𝑡), \nwhich is u tilized in  comput ing the GAE and guid ing updates of the actor network . The critic is \ntrained by minimizing the loss function 𝐿critic(𝜙) given in  Eq. 1 5. \n𝐿CLIP(𝜃)=𝐸̂𝑡[𝜋𝜃(𝑠𝑡)\n𝜋𝜃𝑜𝑙𝑑(𝑠𝑡)𝐴̂𝑡GAE,clip (𝜋𝜃(𝑠𝑡)\n𝜋𝜃𝑜𝑙𝑑(𝑠𝑡),1−𝜖,1+𝜖)𝐴̂𝑡GAE ] (11) 𝐴̂𝑡=𝛿𝑡+(𝛾𝜆)𝛿𝑡+1+⋯+⋯+(𝛾𝜆)𝑇−𝑡+1 (12)  \n𝑤ℎ𝑒𝑟𝑒 𝛿𝑡=𝑟𝑡+𝛾𝑉𝜙(𝑠𝑡+1)−𝑉𝜙(𝑠𝑡) (13) \n𝐿actor=𝐸̂𝑡[𝐿𝑡CLIP(𝜃)+𝑐𝑆[𝜋𝜃](𝑠𝑡)] (14) \n𝐿critic(𝜙)=𝐸̂𝑡[(𝑉𝜙(𝑠𝑡)−𝑅̂𝑡)2] (15) \nwhere 𝜃 and 𝜙 denote the parameters of the actor and critic network, respectively . The term \n𝜋𝜃(𝑠𝑡)\n𝜋𝜃𝑜𝑙𝑑(𝑠𝑡) represents the probability ratio between the current policy and the previous policy. The \nhyperparameter 𝜖,𝛾, and 𝜆 represent the clipping parameter, the discount factor, and GAE \nsmoothing parameter, respectively. 𝛿𝑡 denotes the temporal difference error. 𝑆[𝜋𝜃](𝑠𝑡) denotes \nthe entropy of the policy 𝜋𝜃 at state 𝑠𝑡, with 𝑐 as its weighting coefficient. The estimated return 𝑅̂𝑡 \nis computed as the discounted sum of future rewards.  \n Various off -policy algorithms have been developed to achieve high sample efficiency by \nreusing samples from experience replay to address challenges associated with continuous state and \naction spaces . A representative approach is Deep Deterministic Policy Gradient (DDPG)  [35], \nwhich optimizes a deterministic policy.  However, DDPG relies on external noise for exploration \nrather than incorporating stochasticity into the policy itself, making it sensitive to hyperparameter \nsettings and prone to unstable training.  In contrast , SAC  [64,65]  adopts a stochastic policy and \nintroduces entropy regularization by incorporating the expected policy entropy 𝐻(𝜋(𝑠𝑡)) in Eq. \n16. This framework enables the policy to explicitly balance  exploration and exploitation, \nfacilitating broad action sampling  during the early training phase and mitigating premature  \nconvergence to local optima. The policy  objective  𝐽̂𝜋(𝜃) is formulated in  Eq. 1 7 using the \nminimum of two soft Q -value functions, implementing a clipped double Q -learning to reduce \noverestimation bias.  The critic consists of four networks: two Q - networks for learning and two \ncorresponding target networks.  The Q -networks are trained by minimizing the loss defined in  Eq. 18, where the target value is computed according to Eq. 19 . The target networks are updated \nvia a soft update as defined in Eq. 20 . \n𝐽(𝜋)=∑𝐸𝑡[𝑟(𝑠𝑡,𝑎𝑡)+𝛼𝐻(𝜋(𝑠𝑡))]  𝑇\n𝑡=0(16) \n𝐽̂\n𝜋(𝜃)=𝐸̂𝑡[𝛼𝑙𝑜𝑔 𝑙𝑜𝑔  𝜋𝜃(𝑠𝑡) −𝑄𝜙𝑗(𝑠𝑡,𝑎𝑡) ]  (17) \n𝐽̂𝑄(𝜙𝑖)=𝐸𝑡[(𝑦−𝑄𝜙𝑖(𝑠𝑡,𝑎𝑡))2\n] (18)  \n𝑦=𝑟𝑡+1+𝛾(𝑄𝜙targ ,𝑗(𝑠𝑡,𝑎𝑡) −𝛼 𝑙𝑜𝑔  𝜋𝜃(𝑠𝑡+1) ) (19) \n𝜙𝑡𝑎𝑟𝑔 ,𝑖=𝜏𝜙𝑖+(1−𝜏)𝜙targ ,𝑖 (20) \nwhere 𝜃,𝜙, and 𝜙targ are parameters of the actor network, the Q -network, and the corresponding \ntarget network, respectively.  𝐻(𝜋(𝑠𝑡)) denotes the entropy  of the policy, and 𝛼 is the entropy \ntemperature coefficient. 𝑄(𝑠𝑡,𝑎𝑡) represents the soft Q -value function, which estimates the \nexpected return given state 𝑠𝑡 and action 𝑎𝑡. 𝜏∈(0,1) is the Polyak averaging coefficient used for \ntarget network updates.  \n \nTable 7. Summary of the hyperparameters used for the PPO and SAC algorithms  \nHyperparameters  PPO SAC  \nPolicy learning rate  3×10−4  3×10−4  \nValue learning rate  1×10−3  3×10−4  \nTotal timesteps  180,000  180,000  \nBuffer size (Experience replay)  720 25,000  \nDiscount factor 𝛾 0.99 0.99 \nClipping parameter 𝜖 0.2 N/A \nGAE smoothing parameter 𝜆 0.95 N/A \nEntropy coefficient 𝑐 0.005  N/A \nLearning starts (steps)  N/A 2016  \nTarget update rate 𝜏 N/A 0.005  \nTraining frequency (steps)  N/A 72  \nEntropy temperature coefficient 𝛼  N/A 0.1 \nHidden layer sizes  [256, 256]  [256, 256]  \nActivation function  Tanh  ReLU  \nOptimizer  Adam; betas=(0.9, 0.999)  Adam; betas=(0.9, 0.999)  \n The optimal policy was trained using PPO and SAC with hyperparameters summarized in  \nTable 7.  Each episode correspond ed to a 24 -hour period comprising 144 steps with 10 -minute \nintervals. The training process was conducted over 1,250 episodes, resulting in a total of 180,000 \nsteps. All training procedures were executed on an Intel Core i9 -13900K CPU, and the DRL \nframework  was implemented based on the OpenAI Gym library  [66]. \n \n2.5    Training and deployment process in DRL   \n The DRL agent receives a state input that includes the current process parameters and \nenvironmental conditions such as the temperature, relative humidity, and electricity prices.  Based \non this state, the agent selects an action corresponding to adjusting  the process parameters , which \nis applied to the environment. However, conducting real -world experiments  to train the agent using \nprocess parameters derived from virtual environmental scenarios would be prohibitively expensive. \nThe surrogate models introduced in Section 2.3  simulate the outcomes of different process settings \nto address this challenge . These models enable the agent to interact with a virtual environment in \nan offline setting and learn to optimize the reward function defined in Eq. 10 . This reward is \ndesigned to encourage both profitability and product quality under variable operating conditions.  \n Fig. 3 illustrates the framework of the proposed decision -making model for optimizing \ninjection molding process parameters.  The framework consists of an offline training phase and an \nonline deployment phase. In the offline training phase , the DRL  agent is trained under a virtual \nenvironmental scenario rather than using real -time environmental variables from the actual \nmachine. The virtual environmental scenario simulated variations in temperature and relative \nhumidity  within the machine and factory over 144 timesteps, corresponding to a total duration of \n24 hours , and is treated as one training episode.  The magnitude of environmental variations is \nrestricted to 10% of the range between the maximum and minimum environmental values designed \nbased on  the DOE as shown in Table 1. Additionally, electricity price fluctuations are introduced randomly over time  to reflect dynamic cost conditions. The DRL agent is trained across a series of \nrandomly generated virtual environmental scenarios, aiming to develop a policy that maximizes \nprofit under varying conditions  and enhances robustness against environmental variability. During \ntraining , the agent adopts a stochastic policy modeled by a normal distribution, enabling the \nsampling of random actions and facilitating effective exploration.  \n \n \nFig. 3. Training phase and deployment phase of the DRL -based  decision -making  model for \ninjection molding process parameter optimization  \n \n In the online deployment phase, the DRL agent operates within a real -world production \nenvironment, receiving inputs such as sensor -measured temperature and relative humidity  as well \nas electricity prices that vary by season and time.  Leveraging the policy trained during the offline \nphase, the agent optimizes process parameters within approximately 0.5 seconds under the given \nenvironmental conditions.  During deployment, a deterministic policy is adopted by selecting the \nmeans of the normal distribution, ensu ring that identical states consistently yield identical actions. \nThis approach enhances the reliability of the decision -making process. It should be noted that only \nthe actor network is utilized during deployment, whereas both the actor and critic networks  are \nrequired during training.  \n \n3    RESULTS  \n3.1    Performance evaluation of the developed  DRL -based  decision -making models  \n3.1.1    Training and evaluation results of developed  DRL  models  \n In the training phase, each episode corresponds to one of the 1,250 distinct predefined virtual \nscenarios, and the initial process parameters are randomly initialized to enhance the robustness of \nthe learned policy. A higher reward indicates that the trained policy effectively adjusts the process \nparameters to maximi ze profit under given process conditions, environmental variables, and \nelectricity prices.  \n Figs.  4a and 4b illustrate the change in reward over 180,000 training steps, equivalent to 1,250 \nepisodes, for two DRL algorithms. The curve represents the moving average of 50 episodes, and \nthe shaded region denotes one standard deviation around the mean. Since each episode is associated \nwith a distinct scenario with randomly initialized process parameters, the maximum achievable \nreward varies across episodes, which leads to a certain degree of variability in the reward curves in \nthe training phase. Nevertheless, both al gorithms exhibit convergence of the average reward toward \napproximately 6.3. In addition, SAC exhibits slightly faster convergence compared to PPO. This is attributed to its higher sample efficiency as an off -policy algorithm, whereas PPO relies on recently \ncollected data for training, necessitating  a larger number of samples. Figs.  4c and 4d show the \nnumber of defective cavities produced during training, approaching  zero as the learning process . \nHowever, some defects still occur even after the policy has sufficiently converged, particularly \nwhen the given initial process parameters inherently lead to the production of defective cavities.  \nSAC also exhibits a faster reduction in defective cavities compared to PPO.  \n \nFig. 4. Training curves of the average reward for  models trained with  (a) SAC and (b) PPO. \nNumber of defective cavities during training of models using (c) SAC and (d) PPO. All curves are \nsmoothed over 50 episodes using a moving average.  \nFigs.  5a and 5b show the total accumulated profit obtained by testing the trained policy \nunder a specific environmental variation scenario. In this scenario, the temperature and relative \nhumidity variations correspond to the Scenario 1  curves in Figs. 6a  and 6b, respectively, while \nthe seasonal electricity price followed the spring price fluctuation pattern shown in Fig. 2 . The \ntrained policy makes a one -time adjustment to the process parameters in response to external \nenvironmental variations, aiming to maximize profit.  In the test phase, the action was chosen \ndeterministically as the mean of the policy distribution, in contrast to the training phase , where \nthe action was stochastically sampled from the policy. When evaluating the average \nperformance after 60,000 timesteps , where the policy had sufficiently converged, the average \nprofits were 953.76 for SAC and 958.99 for PPO.  The difference can be attributed to the \ninherent characteristics of the two algorithms, which lead to different timesteps required to \nreach the optimal solution with SAC requiring more timesteps than PPO.  \n \nFig. 5. Total profit over a single episode evaluated on a predefined scenario using policies \ntrained with (a) SAC and (b) PPO. The scenario corresponds to \"Scenario 1\" in Fig. 6 , which \nassumes seasonal electricity pricing based on spring/fall rates. Profit is evaluated using a \ndeterministic version of the policy, where the mean action is selected instead of sampling from \nthe stochastic distribution.  \n \nFig. 6. Seasonal variations in environmental conditions for injection molding processes. (a) \nTemperature profiles for spring, summer, and winter. (b) Relative h umidity profiles for spring, \nsummer, and winter.  \n \n3.1.2    Comparison of  the developed  DRL models under specific environmental conditions  \n The performance of the DRL  models trained in Section 3. 1.1 was evaluated to determine \nwhether they c ould adjust process parameters from diverse initial process parameter settings to \nmaximize profit.  To this end, 9 initial process parameter cases  were selected under a fixed \nenvironmental condition of 14℃  temperature and 45% relative humidity for both the machine and \nfactory  with spring off -peak electricity pricing, as listed in Table 8 . Six cases  produced \nnondefective  cavities with varying profit levels, whereas three resulted in defective cavities.  Each \ninitial process parameter case underwent a sequence of 10 consecutive adjustments, with each \nadjustment corresponding to one time -step. A deterministic policy was followed by taking the \nmean s of the policy distribution at every step , ensuring that identical inputs consistently yield \nidentical actions.  Parameters were  updated cumulatively without reinitialization, so that the result \nof each adjustment served as the next state, forming a continuous trajectory of parameter evolution.  \n \nTable 8. Initial process parameter cases and corresponding profit and defective cavity count \nunder spring off -peak conditions ( temperature = 14℃, relative h umidity = 45%)  \nProcess Parameter  Case  \n1 2 3 4 5 6 7 8 9 \nInjection speed 1  32.5 38.8 26.2 36.0 44.4 30.5 37.6 44.3 38.2 \nInjection speed 2  32.5 38.8 26.2 21.7 23.2 43.1 32.2 31.3 34.3 \nInjection speed 3  20 25 15 12.8 12.8 25.6 25.4 12.0 12.4 \nInjection pressure 1  130 135 125 131.5  136.2  120.7  121.14  133.5  124.7  \nInjection pressure 2  130 135 125 133.4  127.1  120.8  132.17  122.5  125.0  \nInjection pressure 3  140 145 135 132.2  144.3  133.0  140.36  134.7  137.8  \nInjection position 1  46 47 45 45.6 45.2 45.4 46.0 47.4 44.4 \nInjection position 2  38 41 35 40.0 34.7 40.7 40.3 39.4 41.1 \nInjection position 3  30 31 29 30.7 29.6 30.5 28.3 31.3 29.9 \nHold time  1.2 1.8 0.6 0.36 2.30 2.23 0.13 0.03 0.10 \nProfit  6.224  6.021  6.383  6.674  6.082  6.548  3.356  0.881  -2.323  \nDefective cavity count  0 0 0 0 0 0 1 2 3 \n \nTable 9. Performance evaluation of SAC -base model : Profit variation by step across 9 initial \nprocess parameter cases under spring off -peak conditions ( temperature = 14℃, relative \nhumidity = 45%)  \nStep Profit by case  \n1 2 3 4 5 6 7 8 9 \n0(initial parameter)  6.224  6.021  6.383  6.674  6.082  6.548  3.356  0.881  -2.323  \n1 6.639  6.101  6.717  6.656  6.154  6.654  6.376  6.730  6.680  \n2 6.737  6.102  6.746  6.712  6.668  6.773  6.762  6.733  6.722  \n3 6.763  6.104  6.776  6.727  6.717  6.799  6.768  6.742  6.730  \n4 6.768  6.295  6.788  6.752  6.750  6.800  6.799  6.753  6.788  \n5 6.774  6.691  6.799  6.788  6.752  6.800  6.800  6.788  6.799  \n6 6.799  6.747  6.800  6.799  6.788  6.800  6.800  6.791  6.799  \n7 6.799  6.774  6.800  6.800  6.799  6.800  6.800  6.800  6.800  \n8 6.800  6.775  6.800  6.800  6.800  6.800  6.800  6.800  6.800  \n9 6.800  6.774  6.800  6.800  6.800  6.800  6.800  6.800  6.800  \n10 6.800  6.799  6.800  6.800  6.800  6.800  6.800  6.800  6.800  \nMaximum Profit  6.800  6.799  6.800  6.800  6.800  6.800  6.800  6.800  6.800  \n \n \n Table 10. Performance evaluation of PP O-based model : Profit variation by step across 9 initial \nprocess parameter cases under spring off -peak conditions ( temperature = 14℃, relative \nhumidity = 45%)  \nStep Profit by case  \n1 2 3 4 5 6 7 8 9 \n0(initial parameter)  6.224  6.021  6.383  6.674  6.082  6.548  3.356  0.881  -2.323  \n1 6.758  6.396  6.698  6.718  6.364  6.587  6.481  6.726  6.728  \n2 6.773  6.777  6.793  6.797  6.772  6.769  6.791  6.796  6.762  \n3 6.799  6.773  6.794  6.793  6.773  6.772  6.795  6.793  6.769  \n4 6.799  6.799  6.795  6.795  6.799  6.799  6.795  6.795  6.795  \n5 6.799  6.799  6.795  6.795  6.799  6.799  6.795  6.799  6.799  \n6 6.799  6.799  6.795  6.799  6.799  6.799  6.799  6.799  6.799  \n7 6.799  6.799  6.799  6.799  6.799  6.799  6.799  6.799  6.799  \n8 6.799  6.799  6.799  6.799  6.799  6.799  6.799  6.799  6.799  \n9 6.799  6.799  6.799  6.799  6.799  6.799  6.799  6.799  6.799  \n10 6.799  6.799  6.799  6.799  6.799  6.799  6.799  6.799  6.799  \nMaximum Profit  6.799  6.799  6.799  6.799  6.799  6.799  6.799  6.799  6.799  \n \n Both SAC - based and PPO -based  models  converged  to profit -maximizing process parameters \nwithin 10 adjustment steps , regardless of the initial process parameter settings, and the respective \nresults are presented in Tables  9 and 10. SAC-based model  reached an average profit of 6.800 in \n10 steps, whereas PPO -based model  achieved 6.799 in 7 steps.  The changes in average profit over  \nadjustment  steps for SAC -based  and PPO -based model  are illustrated in Fig. 7 . In Section 3. 1.1, \nPPO-based model  yielded higher profit than SAC -based model  when evaluated on the virtual \nscenario with a single adjustment from the DRL agent, which can be attributed to the characteristic \nof PPO to converge in fewer steps. Specifically, the mean first -step profit across all  nine initial \nsettings  in Tables 9 and 10 was 6.523 for the SAC -based model and 6.606 for the  PPO-based \nmodel , which is consistent with the results presented in the previous section.   \nFig. 7. Average profit by step for DRL models  trained with SAC and PPO . \n \n The results obtained using the DRL models were compared with those produced by a GA \nmethod utilizing the same surrogate model s. GA is a population -based optimization method \ninspired by natural selection and genetics principles . It is one of the most widely adopted static \napproaches for solving complex global optimization problems  and has been applied to injection \nmolding studies  [13,67] . This study used GA  to search for process parameters that maximize the \nsame profit function defined in Eq. 1 , consistent with the DRL models . The optimization was \nperformed under identical environmental conditions , including 14°C temperature, 45% relative \nhumidity, and the spring off -peak electricity price . Using the DEAP library, we implemented  the \nGA with simulated binary crossover and polynomial mutation, which are widely used in continuous, \nbounded optimization problems  [68].Parent selection was performed using tournament selection , \nand the population size and number of generations were set to 40 and 2 5, respectively. The detailed \nhyperparameter settings for  the GA are provided in Section 2 of the Supplementary Materials . \n Fig. 8 shows the change in average profit for 9 initial populations over 25 generations. The \nresults are compared with the profits obtained from the optimal parameters identified by SAC -\nbased  and PPO -based model . The GA converged to an average profit of 6.799 after 20 generations \n(800 profit evaluations) with profit ranging from 6.784 to 6.805 and a standard deviation of 0.00613. \nFor the DRL models , SAC -based  and PPO -based model  achieved profits of 6.799 and 6.800, \nrespectively. Despite being trained in a stochastic manner, the model produced consistent results \nacross trials in the deployment phase due to the use of a deterministic policy. This highlights the \nhigh robustness of the DRL model under the given environmental conditions, while still achieving \nperformance comparable to  the GA. \n \nFig. 8. Evolution of the average best profit  during genetic algorithm optimization, performed \nwith 9 random initial populations  under spring off -peak conditions (temperature = 14℃, \nrelative humidity = 45%) . The shaded region represents  the range of fitness variation across \npopulations. Results are compared with the average profits achieved  by SAC -based  and PPO -\nbased model  after 10 and 7 adjustment steps, respectively . \n \nTable 11. Comparison of DRL models and GA \nMethod  Average profit  Standard deviation  Computational time  Training time  \nDRL – SAC  6.800  – 0.421s (10 steps)  206 min \nDRL – PPO 6.799  – 0.287s (7 steps)  182 min \nGA 6.799  0.00613  21.20 1s (20 gens)  – \n \nTable 11  presents the optimization and training times as well as the average profit and standard \ndeviation for the two DRL models  and the GA method . As shown in the previous results, SAC -\nbased  and PPO -based model  required 10 and 7 profit evaluations respectively to reach convergence, \nwhile  the GA required 20 generations, corresponding to a total of 800 evaluations. The total training \ntimes were 206 minutes for SAC -based model  and 182 minutes for PPO -based model , while  the \nGA required no prior training. However, it is important to note that the training process for the \nDRL models can be conducted offline, before  deployment, and does not need to occur within the \nproduction environment. As such, the training time is not  a critical constraint in practical \napplications.  In contrast, the time required for online optimization is a key constraint in injection \nmolding processes, where optimization must be completed within the cycle time. In this regard, the \nDRL -based approaches demonstrated a significant advantage by performing optimization tens of \ntimes faster than  the GA, achieving inference times of 0.421 seconds for SAC -based model  and \n0.287 seconds for PPO -based model , compared to 21.201 seconds for  the GA. Furthermore,  the \nGA searches for optimal process parameters under fixed environmental conditions, whereas DRL \nincorporates environmental variables into the state representation, enabling dynamic optimization \nthat can adapt to changing conditions. To properly assess the effect iveness of the trained DRL \nmodels, it is important to verify their ability to perform reliably under fixed environmental \nconditions and  across a diverse range of scenarios, which is evaluated in  Section 3. 1.3. \n \n3.1.3    Evaluation of the developed DRL models under varying  environmental conditions  \n   To assess whether the trained DRL models can track optimal process parameters under \ndynamic conditions, we performed a 24-hour virtual deployment in three seasonally \nrepresentative scenarios  including spring, summer, and winter. In South Korea, spring and \nautumn electricity pricing and environmental characteristics are nearly identical. Accordingly, \nspring, summer, and winter were selected as representative seasons for evaluating the effects \nof seasonal variations.  It is important to note that the scenarios representing seasonal variations \ncan be adapted according to country -specific environmental characteristics and conditions.  \nEach scenario followed  the corresponding temperature –humidity trajectory shown in Fig. 6 , \nunder the simplifying assumption that the machine and factory shared identical seasonal \nconditions at every time step. These profiles were selected to cover various  seasonal conditions while remaining within the experimental bounds listed in Table 1 , ensuring that the trained \npolicy could adapt to practical operating environments.  \nThe cumulative profit defined in Eq. 1 over a 24 -hour operation of a single injection molding \nmachine was compared with DRL  models and GA method  under the three seasonal scenarios. \nThe machine was assumed to operate continuously for 24 hours, with production occurring \nimmediately  after each cycle. Process parameters were optimized for all methods based on the \ntemperature and relative humidity  measured at the beginning of each cycle.  For the DRL \nmodels, t he final performance was consistent across different  initial process parameters  \n(Section 3. 1.2); thus , the initial condition for optimization  was set to the midpoint (50%) of the \nrange listed in Table 1, corresponding to a normalized state 𝒔0=𝟎. The number of steps \nrequired for convergence was determined to be 10 steps for SAC -based  model  and 7 steps for \nPPO-based model , based on the results in Tables 9 and 10, and the process parameters obtained \nat these steps were adopted as the optimized parameters. The same hyperparameter settings \nused in Section 3.1.2  were applied for the GA , and optimization was conducted at each cycle \nbased on the updated environmental conditions.  \nFigs. 9a, 9b,  and 9c show the cumulative profit changes over 24 hours for the DRL models \ntrained with SAC and PPO, and for the GA method, under three seasonal scenarios. For all methods, \nthe cumulative profit consistently followed the order of spring, winter, and summer, corresponding \nto the decreasing electricity prices shown in Fig. 2 . Table 12  summarizes the total profit, the \nnumber of cavities produced over 24 hours, and the computational time measured for the spring \nscenario. SAC -based model  achieved profits of $958.8 8, $915. 63, and $930. 85 in the spring, \nsummer, and winter scenarios, respectively, while PPO -based model  achieved profits of $958.33, \n$914.68, and $929.85 under  the same scenarios.  Meanwhile, SAC -based model  produced 8,644, \n8,744, and 8,824 cavities , whereas PPO -based model  produced 8,640, 8,736, and 8,816 cavities  under  the same scenarios . The G A recorded profits of $959.69, $915.87, and $932.66 and produced \n8,652, 8,748, and 8,844 cavities in the spring, summer, and winter scenarios, respectively.  \nAlthough the GA  achieved the highest profits and production volumes across all scenarios, the \nDRL models exhibited highly comparable performance  in both economic and operational terms, \ndemonstrating their practical competitiveness under varying environmental conditions.  \n \nFig. 9. Cumulative profit over a 24 -hour period under varying environmental conditions is \npresented for 2 DRL  models trained using (a) SAC and (b) PPO, and (c) the GA method . All \nmethods were deployed under dynamic environmental conditions, incorporating the \ntemperature and relative humidity  profiles shown in Fig. 6  and the electricity price variations \nillustrated in Fig. 2 . Across all approaches, the seasonal ranking of cumulative profit remains \nconsistent ( spring > winter > summer ). \n \n While the DRL models and the GA  method  exhibited highly comparable performance levels , \ntheir computational efficiency differed markedly. In this study, computational time refers to \ninference time  for the trained DRL models and full optimization time for the GA method.  Inference \ntime refers to the time required for a trained DRL model to produce process parameter adjusetment \ndecision via multiple forward passes of the policy network.  The SAC -based  and PPO -based  models \nrequired only 15.5 minutes and 10.4 minutes, respectively, primarily due to differences in the \nnumber of steps required to reach convergence. In contrast, the GA method required a substantially \nlonger time of 781.0 minutes. Importantly, once traine d to accommodate dynamic environmental \nvariations, the DRL models can infer optimal process parameters through a single forward pass of \nthe policy network, incurring minimal computational overhead during operation. Conversely, the \nGA method must perform a complete optimization routine whenever environmental conditions \nchange, resulting in significant and persistent computational costs duri ng real -time deployment.  \nTable 1 2 Comparison of total profit and number of cavities produced (in parentheses)  over 24 \nhours by season for DRL model s (trained using SAC and PPO ) and the GA method . \nComputational time  is measured based on the spring scenario . \n SAC  PPO GA \nSpring  $958.8 8 (8,644)  $958.33 (8,640)  $959.69 (8,652)  \nSummer  $915. 63 (8,744)  $914.68 (8,736)  $915.87 (8,748)  \nWinter  $930. 85 (8,824)  $929.85 (8,816)  $932.66 (8,844)  \nComputational time  15.5 min  10.4 min  781.0 min  \n \n3.2    Comparison of the DRL models under different adjustment  step size settings  \n The impact of different adjustment step sizes , as presented in Table 5 , on the performance of \nthe DRL models was systematically analyzed. Section 3. 1.3 evaluated the models using a large \nadjustment step size across 3 seasonal scenarios. This section examines a smaller adjustment step \nsize for comparative analysis.  Two cases were considered under the small adjustment condition: \n(1) maintaining the number of steps identical to the large adjustment case (10 steps for SAC, 7 \nsteps for PPO), and (2) doubling the number of steps (20 steps for SAC, 14 steps for PPO).  \n Table s 13 and 14 provide quantitative comparisons of cumulative profit and the number of \ncavities produced over 24 hours  for SAC -based and PPO -based models under different step size \nand step count conditions. When the smaller adjustment step size was employed with the same \nnumber of steps (10 for SAC, 7 for PPO), the SAC -based model experienced a decrease in both \nprofit and  production, particularly during summer and winter. Similarly, the PPO -based model \nshowed decreased performance in spring and summer scenarios, although  a slight profit increase \noccurred in winter. Increasing the number of steps to 20 for SAC and 14 for PPO improved \nperformance for both models. Specifically, SAC achieved near -equivalent performance to the large adjustment scenario, whereas PPO showed improved but slightly lower outcomes. Computational \ntimes for both models increased proportionally with the number of steps.  Fig.10 shows the total \nprofit the total profit and number of cavities over 24 hours for SAC - and PPO -based models across \nthree seasonal senarios  and varying step conditions , summarizing the results presented in Tables \n12, 13, and 14. \n \nTable 13. Comparison of total profit and number of cavities produced (in parentheses) over 24 \nhours for SAC -based  model operated  with different step sizes and numbers of steps.  \nComputational time  is measured based on the spring scenario . \n SAC  \n(large step size, 10 steps)  SAC  \n(small step size, 10 steps)  SAC  \n(small step size, 20 steps)  \nSpring  $958.88 (8,644)  $958.79(8,644)  $958.79(8,644)  \nSummer  $915.63 (8,744)  $912.91(8,720)  $914.29 (8,732)  \nWinter  $930.85 (8,824)  $928.64 (8,804)  $929.09 (8,808)  \nComputational time  15.5 min  15.3 min  30.5 min  \n \nTable 1 4. Comparison of total profit and number of cavities produced (in parentheses) over 24 \nhours for PPO -based  model operated with different step sizes and numbers of steps.  \nComputational time  is measured based on the spring scenario . \n PPO  \n(large step size, 7 steps)  PPO \n(small step size, 7 steps)  PPO \n(small step size, 14 steps)  \nSpring  $958.33 (8,640)  $952.81 (8,592 ) $956.04(8,620)  \nSummer  $914.68 (8,736)  $909.70 (8,688)  $911.54  (8,704)  \nWinter  $929.85 (8,816)  $930.76  (8,824)  $930.76  (8,824)  \nComputational time  10.4 min  10.3 min  20.5 min  \n \n The observed slight profit increase for the PPO -based model in the winter scenario under the \nsmaller adjustment setting (7 steps) is not considered decisive. Evaluating DRL models must \nemphasize their capability to consistently identify optimal parameters across diverse environmental \nconditions, rather than isolated improvements in specific scenarios. Moreover, while doubling the \nstep count improved performance, this enhancement was accompanied by significantly increased \ncomputational time and operational c osts. Thus, a smaller adjustment step size demands greater computational resources for convergence without guaranteeing a substantial or consistent \nadvantage. Consequently, a larger adjustment step size emerges as the preferable choice, offering \na robust and computationally efficient strategy for real -time process  optimization.  \n \nFig. 10. Comparison of total profit and number of cavities produced over 24 hours for SAC -\nbased and PPO -based mod el operated  with different step sizes and numbers of steps  under three \nseasonal  scenarios  (spring, summer,  and winter)   \n \n4    DISCUSSION  \n4.1    Performance Discussion  of DRL-based  decision -making  models  \n We developed decision -making models based on two DRL algorithms, SAC and PPO, to \nidentify  optimal process parameters  under dynamic environmental conditions reflecting temporal \nand seasonal variation s in temperature , relative humidity , and electricity prices.  Under the large \nadjustment step size condition, comparisons across three virtual seasonal scenarios showed that \nSAC -based model  achieved slightly better cumulative profit and production volume compared to \nPPO-based model . However, due to the characteristics of its maximum entropy framework, SAC \nmaintains higher policy stochasticity to explore a broader  action space, requiring more steps to \nconverge than PPO .  \n Under the small adjustment step size condition, optimizati on required a gr eater number of steps ; \nnevertheless, SAC -based model  was able to maintain performance comparable to that under the \nlarge step size setting , while PPO-based model  showed a stronger tendency to converge to a locally \noptimal policy . The inefficiency and extended convergence time resulting from smaller adjustment \nsteps have also been observed in previous research that applied DDPG for optimizing injection \nmolding processes  [44]. Off-policy algorithms such as SAC and DDPG can reuse previously \ncollected data, enabling broader exploration of the state-action  space  [35,65] . In contrast, the on -\npolicy PPO algorithm relies solely on newly collected trajectories, which limits data diversity and \nnarrows the exploration range  [34]. This algorithmic difference was reflected in our small \nadjustment step size experiments, as SAC could approach near -global optima, whereas PPO more \nfrequently settled into locally optimal solutions . \n The optimization performance of the developed DRL -based  models was evaluated against a \nstatic method, GA, under diverse environmental conditions.  The SAC -based model could achieve \nresults comparable to  the GA, with profit differences of $0.81, $0.24, and $1.81 over 24 hours for \nthe spring, summer, and winter scenarios, respectively . In the deployment phase,  SAC -based model  required 15.5 minutes to optimize a single scenario, while  the GA took 781 minutes, demonstrating \nthat DRL  approach  is much more efficient in terms of optimization cost. The faster inference speed \nof the DRL models was achieved by training them not only with process parameters  but also with \nenvironmental variables and electricity prices included in the state  representation , allowing them \nto adapt  to dynamic environmental changes. In contrast,  the GA optimizes process parameters \nunder fixed conditions and must be rerun  whenever the environment changes.  While this can yield \nmore precise results in static settings, it is inefficient for real -time applications under continuously \nvarying conditions.  \n This difference  becomes even more critical as the number of process param eters increases since \nthe opt imization time of the GA scales with the input dimensionality  [69], whereas the inference \ntime of the DRL -based models remains effectively constant.  In particular, the  previous study \nidentified 10 key process parameters  among 43 candidates based on their practical adjustability by  \nfield experts , and this study builds upon that selection.  If additional process paramters must be \nconsidered , the computational cost of  the GA rises sharply, making real -time deployment even \nmore challenging.  In contrast, although the DRL models  becomes more complicated to train with \na greater number of variables, it can still achieve rapid inference speed once training is completed, \nmaking it more suitable for real -time adaptation under continuously changing conditions.  \n \n4.2    Expectation of DRL -based decision -making models  \n The proposed DRL -based decision -making models , utilizing SAC and PPO, dynamically adapt \nto seasonal and time -dependent fluctuations while optimizing process parameters to enhance \nproduction quality and profitability. Leveraging DRL supports adaptive and profitable production, \nenabling real -time decision -making in evolving manufacturing environments.  \n A key strength of these  models  is their ability to generalize across diverse environmental \nconditions. Once trained, the DRL models  can efficiently determine optimal process parameters that ensure consistent profitability, even when fluctuations in electricity pricing, environmental \nfactors, and production constraints occur . Unlike static optimization methods, DRL -based dynamic \noptimization  enables real -time adaptation, maintaining both cost -efficiency and production \nperformance. Comparative experiments with the GA , a widely used global optimization method, \nconfirm that the DRL -based models achieve comparable profitability while reducing optimization \ntime by up to 135 times. This computat ional efficiency is a technical advantage and  a critical factor \nfor practical deployment in real -world manufacturing environments.  \n Field experts apply a recommended process parameter setting in actual manufacturing \nenvironments, especially in injection molding . These experts need enough time to review the \nsuggested parameters, assess their plausibility, and input them into the molding machine before the \nnext cycle begins. Given that the target product's cycle time in this study is approximately 39 \nseconds, the GA’s optimization time of 21 seconds alone consumes more than half of the available \ncycle time. This severely limits the remai ning time available for expert verification and manual \nsetup, making GA impractical for real -time field deployment.   \n Although  the GA may achieve slightly higher profitability under some seasonal settings, with \ngains ranging from $0.86 to $2.81 per 24 -hour operation compared to SAC -based  or PPO -based \nmodels , realizing such improvements would require significantly higher computational \nspecifications, which in turn leads to increased investment capital. In the injection molding industry, \nwhich typically operates under low -margin, cost -sensitive conditions, inv esting in high -\nperformance computing infrastructure solely to support GA -based  optimization is economically \ninefficient and unrealistic. Factories in this sector  are often reluctant to invest in computational \nupgrades, prioritizing low operational costs over marginal gains in theoretical optimization \nperformance.  In contrast, the DRL  models offer a cost -effective and scalable solution. Their rapid \ninference time aligns well with the actual production cycle, allowing field experts sufficient time \nfor evaluation and implementation without disrupting production flow.   Importantly, the generality of the proposed models is largely driven by their profit function \nformulation. Since the profit function can be customized to reflect the economic goals and \nconstraints of different manufacturing contexts, the DRL -based  dynamic  optimization  framework \nare not limited to plastic injection molding. Instead, they provide a scalable and transferable \nsolution for real -time process optimization in other industries, such as semiconductor fabrication, \nmetal forming, and food processing, w here dynamic operating conditions and cost variables \ninfluence profitability . By combining data -driven insights with reinforcement learning, this study \npresents a flexible decision -making framework capable of adapting to a wide range of production \nsystems that require rapid, cost -effective, and stable operation.  \n \n4.3    Limitation of DRL -based decision -making models  \n One of the principal limitations of this research is the dependence of DRL performance on the \naccuracy of the surrogate model, which serves as the offline training environment for the DRL  \nagent. The success of DRL -based optimization critically hinges on the surrogate model’s ability to \napproximate the real injection molding process faithfully . Suppose the surrogate model lacks \nsufficient precision due to measurement errors, environmental uncertainties, or inherent modeling \nlimitations. In that case,  the DRL  agent may learn suboptimal policies that fail to generalize to \nactual production environments. Significant discrepancies between the surrogate model and the \nreal-world process can result in policies that perform poorly when deployed, leading to unexpected  \ndeclines in product quality or profitability and ultimately compromising the effectiveness of the \nDRL -based optimization.  If the surrogate model lacks sufficient training data, its predictive \naccuracy remains limited, ultimately affecting the optimization  quality of the DRL -based  model . \nThus, securing extensive datasets is crucial for improving overall system performance.  Ensuring \nhigh surrogate model fidelity necessitates using  a sufficiently large and representative dataset for \ntraining. However, in industrial environments, data collection is often constrained by restricted access to production systems, variability in operating conditions, and high acquisition costs. \nTraining the surrogate model on limited or biased datasets can degrade its predictive accuracy, \nthereby diminishing the optimization quality and robustness of th e DRL model . Therefore, securing \nextensive and diverse datasets is critical to enhancing surrogate model fidelity, enabling the DRL \nagent to develop reliable and transferable policies  to real production systems.  \n Another limitation arises as the number of process parameters increases or interdependencies \namong parameters become more complex. Under such conditions, the single -agent DRL approach \nemployed in this study may struggle to efficiently explore the solution space and identify globally \noptimal process parameters. The increased complexity can cause the agent to converge to local \noptima, thus hindering the achievement of the most effective production settings. To overcome this \nchallenge, multi -agent DRL algorit hms should be considered. The m ulti-agent algorithms  would \nallow individual agents to specialize in optimizing subsets of process parameters while \ncollaborating to achieve a globally optimized solution. This approach could enhance adaptability \nand scalability in highly complex manufacturing environments, wh ere single -agent DRL  \nalgorithms  may encounter performance limitations.  \n \n4.4    Future work  \n While the proposed DRL -based decision -making models have demonstrated their effectiveness \nin dynamically adapting to changing manufacturing environments, further advancements are \nnecessary to address certain limitations and enhance practical applicability.  One key area for future \nresearch is improving the surrogate model by expanding the available dataset. Since the accuracy \nof the surrogate model directly impacts the performance of the DRL model,  securing a sufficient \nand diverse dataset is essential. However, operational limitations often constrain industrial data \ncollection , which necessitates  alternative approaches to improve model robustness.  A promising \ndirection is the generation of synthetic data to supplement real -world datasets. This can be achieved by leveraging multi -fidelity simulation techniques, which integrate high - and low -fidelity models \nto create realistic training environments for the DRL agent. Additionally, transfer learning and few -\nshot learning can be explored to enhance the model’s adap tability when only limited real -world \ndata are available. By incorporating these techniques, future studies can alleviate data scarcity \nissues and improve the overall accuracy and reliability of the surrogate model.  \n Furthermore, as the number of process parameters increases or the interdependencies among \nvariables become more complex,  single -agent DRL algorithms may struggle to optimize all \nparameters efficiently. In such scenarios, multi -agent DRL algorithms should be considered to \novercome local optima and improve scalability. A multi -agent framework allows individual agents \nto optimi ze specific process variables while collaborating to achieve a globally optimal solution. \nThis approach can significantly enhance the flexibility and robustness of DRL -based optimization, \nmaking it more suitable for large -scale and complex manufacturing processes.  \n By refining the surrogate model and investigating multi -agent reinforcement learning, future \nresearch can further improve the adaptability, efficiency, and scalability of DRL -based \noptimization for injection molding and broader industrial applications. These advancements will \nsupport the development of more intelligent, data -efficient, and real -time adaptive manufacturing \nsystems, ensuring higher reliability in highly dynamic production environments.  \n \n5    CONCLUSION  \n This study presented a Deep Reinforcement Learning (DRL) -based framework and models for \noptimizing injection molding process parameters with the dual objectives of improving product \nquality and maximizing profitability. The proposed framework addressed a c ritical limitation of \nconventional optimization approaches by formulating a profit -driven objective function that \ncomprehensively integrates material costs, mold maintenance costs, electricity consumption, and \nreal-time electricity pricing. To enable effic ient offline training, surrogate machine learning models were developed for quality classification and cycle time prediction, allowing DRL agents to be \ntrained without direct interaction with physical production systems.  \n Experimental results demonstrated that the developed DRL -based decision -making models, \nutilizing SAC and PPO algorithms, effectively adapted process parameters to fluctuating \nenvironmental conditions. The models achieved profitability comparable to a genetic algorithm \nwhile significantly reducing optimization time, offering a clear computational advantage. The rapid \ninference capability  enables parameter optimization within seconds, making  the framework highly \nsuitable for real -time deployment in manufactu ring environments where timely and adaptive \ndecision -making is essential.  \n The key contributions of this research are summarized as follows. First, a profit -driven \noptimization strategy was proposed that directly links operational decisions to economic outcomes, \naddressing a gap often overlooked in conventional quality -focused a pproaches. Second, a DRL -\nbased real -time decision -making framework was developed, demonstrating stable and efficient \noptimization performance under dynamic environmental and economic conditions. Third, the \nproposed models exhibited robust adaptability acro ss diverse seasonal scenarios, validating their \ngeneralizability in real -world manufacturing environments. Fourth, the framework was designed \nto be scalable and transferable to other industrial domains, highlighting its potential applicability \nbeyond injec tion molding. Finally, by identifying limitations related to surrogate modeling and \noptimization complexity, this study lays the foundation for future research to improve  system \nscalability through enhanced data -driven modeling and multi -agent reinforcemen t learning \nstrategies.  \n Overall, this research underscores the transformative potential of DRL -based optimization in \nsmart manufacturing systems. Future work will focus on expanding datasets to improve surrogate \nmodel fidelity and investigating multi -agent reinforcement learning  approaches to enhance \nscalability, robustness, and adaptability in increasingly complex industrial environments.   \nAcknowledgement s: This work was supported by the Technology Innovation Program (RS -\n2023 -00284506) funded by the Ministry of Trade, Industry and Energy (MOTIE, Korea), the \nTechnology Development Program (S3207585) funded by the Ministry of SMEs and Startups \n(MSS, Korea), the National Research Foundation of Korea (NR F) grant funded by the Ministry \nof Science and ICT (RS -2023 -00222166), and a grant from the Ministry of Food and Drug \nSafety (RS -2023 -00215667).  \n \nData Availability Statement:  The data used  are not publicly available.  \n \nConflicts of Interest:  The authors declare no conflict of interest.  \n \n \n \n  REFERENCE   \n[1] EL Ghadoui M, Mouchtachi A, Majdoul R. Exploring and optimizing deep neural networks for precision \ndefect detection system in injection molding process. J Intell Manuf 2024:1 –18. \nhttps://doi.org/10.1007/S10845 -024-02394 -3/TABLES/9.  \n[2] Selvaraj SK, Raj A, Rishikesh Mahadevan R, Chadha U, Paramasivam V. A Review on Machine Learning \nModels in Injection Molding Machines. Advances in Materials Science and Engineering \n2022;2022:1949061. https://doi.org/10.1155/2022/1949061.  \n[3] Fernandes C, Pontes AJ, Viana JC, Gaspar -Cunha A. Modeling and Optimization of the Injection -\nMolding Process: A Review. Advances in Polymer Technology 2018;37:429 –49. \nhttps://doi.org/10.1002/ADV.21683.  \n[4] Breaking Down the Cost Factors in Plastic Manufacturing n.d. \nhttps://www.goldengatemolders.com/post/breaking -down -the-cost-factors -in-plastic -manufacturing \n(accessed March 3, 2025).  \n[5] Wang HS, Wang YN, Wang YC. Cost estimation of plastic injection molding parts through integration of \nPSO and BP neural network. Expert Syst Appl 2013;40:418 –28. \nhttps://doi.org/10.1016/J.ESWA.2012.01.166.  \n[6] Wang HS. Application of BPN with feature -based models on cost estimation of plastic injection products. \nComput Ind Eng 2007;53:79 –94. https://doi.org/10.1016/J.CIE.2007.04.005.  \n[7] Mcmurtrey A. ENERGY SAVING STRATEGIES FOR PLASTICS INJECTION MOLDING: \nLUBRICATION ADAM MCMURTREY, EXXONMOBIL, SPRING, TX - Google 검색  n.d. \nhttps://www.ilsag.info/wp -content/uploads/Exxon -Mobil -EE-Idea-Attachments_April -2020.pdf \n(accessed March 3, 2025).  \n[8] Optimizing an Injection Mold Process - Turner Group n.d. https://turnergroup.net/optimizing -an-\ninjection -mold -process/ (accessed March 3, 2025).  \n[9] Thiriez A, Gutowski T. An environmental analysis of injection molding. IEEE International Symposium \non Electronics and the Environment 2006;2006:195 –200. https://doi.org/10.1109/ISEE.2006.1650060.  \n[10] Huang J, Li Y, Li X, Ding Y, Hong F, Peng S. Energy Consumption Prediction of Injection Molding \nProcess Based on Rolling Learning Informer Model. Polymers 2024, Vol 16, Page 3097 2024;16:3097. \nhttps://doi.org/10.3390/POLYM16213097.  \n[11] AYadav R, Scholar R, SVJoshi P, NKKamble A. Recent Methods for Optimization of Plastic Injection \nMolding Process -A Literature Review. Int J Sci Eng Res 2012;3.  \n[12] Hernández -Vega JI, Reynoso -Guajardo LA, Gallardo -Morales MC, Macias -Arias ME, Hernández A, de \nla Cruz N, et al. Plastic Injection Molding Process Analysis: Data Integration and Modeling for Improved \nProduction Efficiency. Applied Sciences 2024, Vol 14, Pa ge 10279 2024;14:10279. \nhttps://doi.org/10.3390/APP142210279.  \n[13] EL Ghadoui M, Mouchtachi A, Majdoul R. A hybrid optimization approach for intelligent manufacturing \nin plastic injection molding by using artificial neural network and genetic algorithm. Scientific Reports \n2023 13:1 2023;13:1 –15. https://doi.org/10.1038/s 41598 -023-48679 -0. \n[14] Kariminejad M, Tormey D, Ryan C, O’Hara C, Weinert A, McAfee M. Single and multi -objective real -\ntime optimisation of an industrial injection moulding process via a Bayesian adaptive design of \nexperiment approach. Scientific Reports 2024 14:1 2024;14:1 –19. https://doi.org/10.1038/s41598 -024-\n80405 -2. \n[15] Shi H, Gao Y, Wang X. Optimization of injection molding process parameters using integrated artificial \nneural network model and expected improvement function method. International Journal of Advanced Manufacturing Technology 2010;48:955 –62. https://doi.org/10.1007/S00170 -009-2346 -7/FIGURES/10.  \n[16] Uglov A, Nikolaev S, Belov S, Padalitsa D, Greenkina T, Biagio MS, et al. Surrogate modeling for \ninjection molding processes using deep learning. Structural and Multidisciplinary Optimization \n2022;65:1 –13. https://doi.org/10.1007/S00158 -022-03380 -0/TABLES/10.  \n[17] Rivers N. Leveraging the Smart Grid: The Effect of Real -Time Information on Consumer Decisions. vol. \n127. 2018. https://doi.org/10.1787/6AD4D5E3 -EN. \n[18] KEPCO Electric Rates Table 2023. \nhttps://home.kepco.co.kr/kepco/EN/F/htmlView/ENFBHP00109.do?menuCd=EN060201 (accessed \nMarch 5, 2025).  \n[19] Muniain P, Ziel F. Probabilistic forecasting in day -ahead electricity markets: Simulating peak and off -\npeak prices. Int J Forecast 2020;36:1193 –210. https://doi.org/10.1016/J.IJFORECAST.2019.11.006.  \n[20] Brännlund R, Vesterberg M. Peak and off -peak demand for electricity: Is there a potential for load shifting? \nEnergy Econ 2021;102:105466. https://doi.org/10.1016/J.ENECO.2021.105466.  \n[21] Wu Y, Feng Y, Peng S, Mao Z, Chen B. Generative machine learning -based multi -objective process \nparameter optimization towards energy and quality of injection molding. Environmental Science and \nPollution Research 2023;30:51518 –30. https://doi.org/10.1007/S 11356 -023-26007 -3/FIGURES/15.  \n[22] Lovrec D, Tic V, Tasner T. Dynamic behaviour of different hydraulic drive concepts - comparison and \nlimits. International Journal of Simulation Modelling 2017;16:448 –57. \nhttps://doi.org/10.2507/IJSIMM16(3)7.389.  \n[23] Madan J, Mani M, Lyons KW. Characterizing Energy Consumption of the Injection Molding Process. \nASME 2013 International Manufacturing Science and Engineering Conference Collocated with the 41st \nNorth American Manufacturing Research Conference, MSEC 2013 20 13;2. \nhttps://doi.org/10.1115/MSEC2013 -1222.  \n[24] Liu H, Zhang X, Quan L, Zhang H. Research on energy consumption of injection molding machine driven \nby five different types of electro -hydraulic power units. J Clean Prod 2020;242:118355. \nhttps://doi.org/10.1016/J.JCLEPRO.2019.118355.  \n[25] Meekers I, Refalo P, Rochman A. Analysis of Process Parameters affecting Energy Consumption in \nPlastic Injection Moulding. Procedia CIRP 2018;69:342 –7. \nhttps://doi.org/10.1016/J.PROCIR.2017.11.042.  \n[26] Li W, Kara S, Qureshi F. Characterising energy and eco -efficiency of injection moulding processes. \nInternational Journal of Sustainable Engineering 2015;8:55 –65. \nhttps://doi.org/10.1080/19397038.2014.895067.  \n[27] Wang F, Dong S, Danai K, Kazmer DO. Input Profiling for Injection Molding by Reinforcement Learning. \nASME International Mechanical Engineering Congress and Exposition, Proceedings, vol. 2, American \nSociety of Mechanical Engineers Digital Collection; 2021,  p. 701 –8. \nhttps://doi.org/10.1115/IMECE2001/DSC -24587.  \n[28] Nievas N, Pagès -Bernaus A, Bonada F, Echeverria L, Domingo X. Reinforcement Learning for \nAutonomous Process Control in Industry 4.0: Advantages and Challenges. Applied Artificial Intelligence \n2024;38. https://doi.org/10.1080/08839514.2024.2383101.  \n[29] Párizs RD, Török D. An experimental study on the application of reinforcement learning in injection \nmolding in the spirit of Industry 4.0. Appl Soft Comput 2024;167:112236. \nhttps://doi.org/10.1016/J.ASOC.2024.112236.  \n[30] Mnih V, Kavukcuoglu K, Silver D, Rusu AA, Veness J, Bellemare MG, et al. Human -level control \nthrough deep reinforcement learning. Nature 2015;518:529 –33. https://doi.org/10.1038/NATURE14236.  [31] Lee S, Cho Y, Lee YH. Injection Mold Production Sustainable Scheduling Using Deep Reinforcement \nLearning. Sustainability 2020, Vol 12, Page 8718 2020;12:8718. https://doi.org/10.3390/SU12208718.  \n[32] Hsieh PC. Intelligent Temperature Control of a Stretch Blow Molding Machine Using Deep \nReinforcement Learning. Processes 2023, Vol 11, Page 1872 2023;11:1872. \nhttps://doi.org/10.3390/PR11071872.  \n[33] Ren Z, Tang P, Zheng W, Zhang B. A Deep Reinforcement Learning Approach to Injection Speed Control \nin Injection Molding Machines with Servomotor -Driven Constant Pump Hydraulic System. Actuators \n2024, Vol 13, Page 376 2024;13:376. https://doi.org/10.3390/A CT13090376.  \n[34] Schulman J, Wolski F, Dhariwal P, Radford A, Openai OK. Proximal Policy Optimization Algorithms. \nArXiv Preprint ArXiv:170706347 2017.  \n[35] Lillicrap TP, Hunt JJ, Pritzel A, Heess N, Erez T, Tassa Y, et al. Continuous control with deep \nreinforcement learning. 4th International Conference on Learning Representations, ICLR 2016 - \nConference Track Proceedings 2015.  \n[36] Siilver D, Huang A, Maddison CJ, Guez A, Sifre L, van den Driessche G, et al. Mastering the game of \nGo with deep neural networks and tree search. Nature 2016;529:484 –9. \nhttps://doi.org/10.1038/nature16961.  \n[37] Vinyals O, Babuschkin I, Czarnecki W, Mathieu M, Dudzik A, Chung J, et al. Grandmaster level in \nStarCraft II using multi -agent reinforcement learning. Nature 2019;575:350 –4. \nhttps://doi.org/10.1038/s41586 -019-1724 -z. \n[38] Zhang Y, Qiao J, Zhang G, Tian H, Li L. Artificial Intelligence -Assisted Repair System for Structural and \nElectrical Restoration Using 3D Printing. Advanced Intelligent Systems 2022;4:2200162. \nhttps://doi.org/10.1002/AISY.202200162.  \n[39] Huang J, Su J, Chang Q. Graph neural network and multi -agent reinforcement learning for machine -\nprocess -system integrated control to optimize production yield. J Manuf Syst 2022;64:81 –93. \nhttps://doi.org/10.1016/J.JMSY.2022.05.018.  \n[40] Li C, Chang Q. Hybrid feedback and reinforcement learning -based control of machine cycle time for a \nmulti -stage production system. J Manuf Syst 2022;65:351 –61. \nhttps://doi.org/10.1016/J.JMSY.2022.09.020.  \n[41] Wu W, Huang Z, Zeng J, Fan K. A fast decision -making method for process planning with dynamic \nmachining resources via deep reinforcement learning. J Manuf Syst 2021;58:392 –411. \nhttps://doi.org/10.1016/J.JMSY.2020.12.015.  \n[42] Wu W, Huang Z, Zeng J, Fan K. A decision -making method for assembly sequence planning with \ndynamic resources. Int J Prod Res 2022;60:4797 –816. https://doi.org/10.1080/00207543.2021.1937748.  \n[43] Ogoke F, Farimani AB. Thermal control of laser powder bed fusion using deep reinforcement learning. \nAddit Manuf 2021;46:102033. https://doi.org/10.1016/J.ADDMA.2021.102033.  \n[44] Guo F, Zhou X, Liu J, Zhang Y, Li D, Zhou H. A reinforcement learning decision model for online process \nparameters optimization from offline data in injection molding. Appl Soft Comput 2019;85:105828. \nhttps://doi.org/10.1016/J.ASOC.2019.105828.  \n[45] He Z, Tran KP, Thomassey S, Zeng X, Xu J, Yi C. Multi -objective optimization of the textile \nmanufacturing process using deep -Q-network based multi -agent reinforcement learning. J Manuf Syst \n2022;62:939 –49. https://doi.org/10.1016/J.JMSY.2021.03.017.  \n[46] Mattera G, Caggiano A, Nele L. Optimal data -driven control of manufacturing processes using \nreinforcement learning: an application to wire arc additive manufacturing. J Intell Manuf 2024;36:1291 –\n310. https://doi.org/10.1007/S10845 -023-02307 -W/FIGURES/18.  [47] Cheng CK, Tsai HY. Enhanced detection of diverse defects by developing lighting strategies using \nmultiple light sources based on reinforcement learning. J Intell Manuf 2022;33:2357 –69. \nhttps://doi.org/10.1007/S10845 -021-01800 -4/TABLES/3.  \n[48] Paraschos PD, Koulinas GK, Koulouriotis DE. Reinforcement learning for combined production -\nmaintenance and quality control of a manufacturing system with deterioration failures. J Manuf Syst \n2020;56:470 –83. https://doi.org/10.1016/J.JMSY.2020.07.004.  \n[49] Szarski M, Chauhan S. Instant flow distribution network optimization in liquid composite molding using \ndeep reinforcement learning. J Intell Manuf 2023;34:197 –218. https://doi.org/10.1007/S10845 -022-\n01990 -5/FIGURES/22.  \n[50] Ruan Y, Zhang Y, Mao T, Zhou X, Li D, Zhou H. Trajectory optimization and positioning control for \nbatch process using learning control. Control Eng Pract 2019;85:1 –10. \nhttps://doi.org/10.1016/J.CONENGPRAC.2019.01.004.  \n[51] Ruan Y, Gao H, Li D. Improving the Consistency of Injection Molding Products by Intelligent \nTemperature Compensation Control. Advances in Polymer Technology 2019;2019:1591204. \nhttps://doi.org/10.1155/2019/1591204.  \n[52] Qin Y, Zhao C, Gao F. An intelligent non -optimality self -recovery method based on reinforcement \nlearning with small data in big data era. Chemometrics and Intelligent Laboratory Systems 2018;176:89 –\n100. https://doi.org/10.1016/J.CHEMOLAB.2018.03.010.  \n[53] Wen X, Shi H, Su C, Jiang X, Li P, Yu J. Novel data -driven two -dimensional Q -learning for optimal \ntracking control of batch process with unknown dynamics. ISA Trans 2022;125:10 –21. \nhttps://doi.org/10.1016/J.ISATRA.2021.06.007.  \n[54] Li X, Luo Q, Wang L, Zhang R, Gao F. Off -policy reinforcement learning -based novel model -free \nminmax fault -tolerant tracking control for industrial processes. J Process Control 2022;115:145 –56. \nhttps://doi.org/10.1016/J.JPROCONT.2022.05.006.  \n[55] Kim JY, Kim H, Nam K, Kang D, Ryu S. Development of an injection molding production condition \ninference system based on diffusion model. J Manuf Syst 2025;79:162 –78. \nhttps://doi.org/10.1016/J.JMSY.2025.01.008.  \n[56] Sutton R, Barto A. Reinforcement learning: An introduction. 2018.  \n[57] Bellman R. A Markovian decision process. Journal of Mathematics and Mechanics 1957;6:679 –84. \n[58] Ali M. PyCaret: An open source, low -code machine learning library in Python 2020.  \n[59] Williams RJ. Simple statistical gradient -following algorithms for connectionist reinforcement learning. \nMach Learn 1992;8:229 –56. https://doi.org/10.1007/BF00992696.  \n[60] Shakya AK, Pillai G, Chakrabarty S. Reinforcement learning algorithms: A brief survey. Expert Syst Appl \n2023;231:120495. https://doi.org/10.1016/J.ESWA.2023.120495.  \n[61] Vincent François -Lavet, Peter Henderson, Riashat Islam, Marc G. Bellemare, Joelle Pineau. An \nintroduction to deep reinforcement learning. Foundations and Trends®  in Machine Learning \n2018;11:219 –354. https://doi.org/10.1561/2200000071.  \n[62] Kang H, Jung S, Kim H, Jeoung J, Hong T. Reinforcement learning -based optimal scheduling model of \nbattery energy storage system at the building level. Renewable and Sustainable Energy Reviews \n2024;190:114054. https://doi.org/10.1016/J.RSER.2023.114054.  \n[63] Schulman J, Levine S, Abbeel P, Jordan M, Moritz P. Trust Region Policy Optimization. Proceedings of \nthe 32nd International Conference on Machine Learning, PMLR; 2015, p. 1889 –97. \n[64] Haarnoja T, Zhou A, Hartikainen K, Tucker G, Ha S, Tan J, et al. Soft Actor -Critic Algorithms and \nApplications. ArXiv Preprint ArXiv:181205905 2018.  [65] Haarnoja T, Zhou A, Abbeel P, Levine S. Soft Actor -Critic: Off -Policy Maximum Entropy Deep \nReinforcement Learning with a Stochastic Actor, PMLR; 2018, p. 1861 –70. \n[66] Brockman G, Cheung V, Pettersson L, Schneider J, Schulman J, Tang J, et al. OpenAI Gym. ArXiv \nPreprint ArXiv:160601540 2016.  \n[67] Tsai K -M, Luo H -J. An inverse model for injection molding of optical lens using artificial neural network \ncoupled with genetic algorithm. J Intell Manuf 2017;28:473 –87. https://doi.org/10.1007/S10845 -014-\n0999 -Z. \n[68] Félix -Antoine Fortin, François -Michel De Rainville, Marc -André Gardner Gardner, Marc Parizeau, \nChristian Gagné. DEAP: Evolutionary Algorithms Made Easy. The Journal of Machine Learning \nResearch 2012;13:2171 –5. \n[69] Goldberg D. Genetic Algorithms in Search, Optimization and Machine Learning. 1st ed. New Delhi: \nPearson Education India; 2013.  \n  ",
      "metadata": {
        "filename": "DRL-Based Injection Molding Process Parameter Optimization for Adaptive and Prof.pdf",
        "hotspot_name": "PBT_Injection_Molding",
        "title": "DRL-Based Injection Molding Process Parameter Optimization for Adaptive\n  and Profitable Production",
        "published_date": "2025-05-16T08:35:31Z",
        "pdf_link": "http://arxiv.org/pdf/2505.10988v1",
        "query": "injection molding PBT housing energy efficiency optimization cycle time reduction"
      }
    },
    "Efficient Algorithms for All Port-Based Teleportation Protocols": {
      "full_text": "Efficient Algorithms for All Port-Based Teleportation Protocols\nAdam Wills,1Min-Hsiu Hsieh,1and Sergii Strelchuk2\n1Hon Hai Research Instiute, Taipei\n2DAMTP, Centre for Mathematical Sciences, University of Cambridge, Cambridge CB30WA, UK\nPort-based teleportation (PBT) is a form of quantum teleportation in which no corrective unitary\nis required on the part of the receiver. Two primary regimes exist - deterministic PBT in which\nteleportation is always successful, but is imperfect, and probabilistic PBT, in which teleportation\nsucceeds with probability less than one, but teleportation is perfect upon a success. Two further\nregimes exist within each of these in which the resource state used for the teleportation is fixed to\na maximally entangled state, or free to be optimised.\nRecently, works resolved the long-standing problem of efficiently implementing port-based tele-\nportation, tackling the two deterministic cases for qudits. Here, we provide algorithms in all four\nregimes for qubits. Emphasis is placed on the practicality of these algorithms, where we give poly-\nnomial improvements in the known gate complexity for PBT, as well as an exponential improvement\nin the required number of ancillas (albeit in separate protocols).\nOur approach to the implementation of the square-root measurement in PBT can be directly\ngeneralised to other highly symmetric state ensembles. For certain families of states, such a frame-\nwork yields efficient algorithms in the case that the Petz recovery algorithm for the square-root\nmeasurement runs in exponential time.\nI. INTRODUCTION\nThe 30-year old quantum teleportation protocol [1] is a fundamental operation in quantum information\ntheory in which an unknown quantum state may be reliably transmitted between spatially separated parties\nusing only shared quantum entanglement and the transmission of classical information between them. The\nstandard algorithm requires that the receiver, often referred to as Bob, performs a unitary recovery operation\non the teleported state which depends on the outcome of a measurement performed by the sender, Alice,\ntransmitted to Bob via the classical channel. Several questions immediately arise; among them is whether\nteleportation is possible without any corrective operation on the part of the receiver.\nBuilding on the work of Knill, Laflamme and Milburn [2], Ishizaka and Hiroshima introduced port-based\nteleportation [3, 4] to answer this question. Alice and Bob, once again spatially separated, this time share a\n2N-qubit (or more generally qudit) entangled state, where they each hold Nsuch systems, called ports. Alice\nholds a further qubit (or qudit) which she wishes to teleport, and does so by performing a POVM on her N+ 1\nsystems. From the outcome of this POVM, she sends classical information to Bob telling him in which of his\nports the teleported state may be found.\nOne quite striking, and useful, feature of such a protocol, is its unitary equivariance - Bob may perform\nany operation Eto every one of his ports before teleportation has even taken place, and ultimately obtain the\nteleported state acted on by Eafter teleportation and the choice of the correct port. However, this power of\nthe protocol is also, in some ways, a curse, since the no-programming theorem [5] forbids such behaviour taking\nplace faithfully and deterministically in systems of finite dimensions.\nTherefore, in the physical case of finite N, either unit teleportation fidelity or unit success probability must\nbe sacrificed. Ishizaka and Hiroshima thus define two primary regimes: deterministic port-based teleporta-\ntion (dPBT) and probabilistic port-based teleportation (pPBT), where in the former, teleportation is always\nconsidered to have succeeded, but the fidelity (averaged over uniformly distributed input states) falls below\n1, whereas in the latter, teleportation succeeds with probability (averaged over all uniformly distributed input\nstates) below 1, but is perfect in the case of success. In the case of dPBT, Alice’s POVM has Noutcomes, each\nof which correspond to a port in which Bob may find the teleported state. For pPBT, Alice’s POVM has N+ 1\noutcomes, where the additional outcome now corresponds to the failure of the protocol.\nIn [4], Ishizaka and Hiroshima further introduce two regimes within dPBT and pPBT. First is the case\nof ‘maximally entangled resource state’, where the state shared by Alice and Bob is fixed to |ψ−⟩⊗N, where\n|ψ−⟩=|01⟩−|10⟩√\n2, and the ‘optimised resource state’ case, where this 2N-qubit state is free to take any form.\nWithin these four regimes, Ishizaka and Hiroshima analytically determine the optimal POVMs (and resource\nstates in the case of optimisable resource states) that maximise fidelity in the case of dPBT and success\nprobability in the case of pPBT. They found different POVMs in each case, where for dPBT, the fidelity\ntends to 1 in the limit N→ ∞, and similarly for success probability in the case of pPBT. Interestingly, it was\nlater shown in [6] that the same measurement could be optimal in both dPBT regimes given the appropriate\nchoice of optimal resource state. While this is surprising at first, it is possible owing to the fact that the convex\noptimisation problems relevant to port-based teleportation may have multiple optimising solutions.\nThemeasurementinquestionfordPBTisanexampleofaprettygoodmeasurement(orsquare-rootmeasure-\nment) [7–11]. Quantum algorithms for general pretty good measurements based on the Petz recovery channelarXiv:2311.12012v2  [quant-ph]  12 Feb 20242\nwere presented in [12], but these algorithms are inefficient for the case in question1. To obtain an efficient algo-\nrithm for the square-root measurement of dPBT, and also for pPBT, it is essential to exploit the symmetries\nof the state ensemble at hand.\nSeveral applications of port-based teleportation are known, both practical and more theoretical. While\nthe no-programming theorem forbids the existence of a faithful and deterministic universal programmable\nprocessor in finite dimensions, the unitary equivariance property of PBT allows it to be used as either a\nprobabilistic, or approximate universal programmable processor even with finite N, in the cases of pPBT\nand dPBT respectively. Our new algorithms for pPBT therefore open up this former case for the first time.\nIn addition, port-based teleportation has applications in instantaneous non-local quantum computation and\nposition-based cryptography [13], where the use of PBT allows for an exponential decrease in the amount of\nentanglement required to attack such a protocol. There are further implications for PBT in holography [14, 15]\nand channel discrimination [16].\nA. Our Results\nRecently, works resolved the long-standing problem of the efficient implementation of port-based teleporta-\ntion [17, 18] by showing how the POVM for dPBT could be implemented by leveraging a theory of ‘twisted\nSchur-Weyl duality’ - see [19] for another closely related work. In this work, we give efficient algorithms for\nall four regimes of port-based teleportation - both dPBT and pPBT, with maximally entangled and optimised\nresource states. Because the same POVM is optimal for both dPBT cases, it will suffice to consider only three\ncases. We focus on practicality in this work, going to some lengths to keep gate complexities and ancilla counts\ndown. To this end, we provide a polynomial speedup in the known gate complexity for port-based teleportation,\nalso aiming to minimise the amount of ‘practically difficult’ operations used, such as amplitude amplification.\nFurthermore, although not in the same protocols, we show how each regime of port-based teleportation may be\nexecuted using only O(log(N))qubits - an exponential improvement over all other methods.\nInterestingly, all of our implementations will come from relatively simple manipulations within the spin\ncoupling formalism [20, 21], a manifestation of the usual Schur-Weyl duality, as opposed to the twisted (or\nmixed) Schur-Weyl duality employed in other works implementing PBT [17–19]. On the one hand, not using\ntwisted Schur-Weyl duality restricts our algorithms to qubits, rather than qudits. However, on the other hand,\nthe simplicity of the building blocks of our algorithms likely affords them greater generalisability. Implementing\nother pretty good measurements in the common case that the state ensemble has some high degree of symmetry\nmay be possible to do efficiently using the framework presented herein, which is very useful in cases where the\nalgorithm of [12] runs inefficiently, as is true for dPBT.\nSchur coupling features heavily throughout. There is a choice to be made within each one of our algorithms.\nTo perform the Schur coupling, one may either use the BCH Schur transform of [22] or the spin coupling Schur\ntransform of [21]. The former runs faster, but uses O(Nlog(N))qubits, whereas the latter is slower, but uses\nO(log(N))qubits. The choice of which of these to use will reflect in the gate complexity and ancilla count\nof the final algorithm. The gate complexities and ancilla counts for our deterministic port-based teleportation\nare summarised in Table I, where we note again that the same POVM may be used in dPBT for maximally\nentangled resource state and optimal resource state.\nResource State Maximally Entangled Optimised\nSchur Transform BCH Spin Coupling BCH Spin Coupling\nClifford + T\nGate Complexity˜O(N3/2)polylog (1\nϵ)˜O(N7/2)polylog (1\nϵ)˜O(N3/2)polylog (1\nϵ)˜O(N7/2)polylog (1\nϵ)\nAncilla Count O(Nlog(N)) O(log(N)) O(Nlog(N)) O(log(N))\nAsymptotic (Average)\nTeleportation Fidelityf∼1− O\u00001\nN\u0001\nf∼1− O\u00001\nN2\u0001\nTABLE I. We present only one algorithm for dPBT in Section VA, since there is only one POVM to perform. Depending\non the choice of resource state used, the teleportation fidelity obtained will differ as shown. Also, one has the choice\nover whether to use the BCH Schur transform [22] or the spin coupling transform [21] in the algorithm, which will effect\nthe gate complexities and ancilla counts in the way shown. The symbol ˜Ohides polylogarithmic factors, and ϵmeasures\naccuracy of the Naimark unitary implementing the POVM.\nFor pPBT, we present two main algorithms, one for the POVM of the maximally entangled resource state\n1FortheprettygoodmeasurementrelevanttodPBT,theaverageofthestateensemblehasexponentiallysmallnon-zeroeigenvalue,\nas we will go on to see. This leads to the algorithm of [12] running inefficiently in the dPBT case.3\ncase in Section VB1 and one for the POVM of the optimised resource state case in Section VB3. The results\nfor these algorithms are summarised in Table II.\nResource State Maximally Entangled Optimised\nSchur Transform BCH Spin Coupling BCH Spin Coupling\nClifford + T\nGate Complexity˜O(N)polylog (1\nϵ)˜O(N3)polylog (1\nϵ)˜O(N3/2)polylog (1\nϵ)˜O(N7/2)polylog (1\nϵ)\nAncilla Count O(Nlog(N)) O(log(N)) O(Nlog(N)) O(log(N))\nAsymptotic (Average)\nSuccess Probabilityp∼1− O\u0010\n1√\nN\u0011\np∼1− O\u00001\nN\u0001\nTABLE II. Our two main algorithms for pPBT, with maximally entangled resource state and optimised resource state,\nare presented in Sections VB1 and VB3, respectively.\nOne notes that with optimised resource state, the gate complexities are the same for pPBT as for dPBT.\nHowever, one can also see that there is an improvement by a factor of O(√\nN)from dPBT to pPBT with\nmaximally entangled resource state. This is because, with a careful treatment of the POVM operators, we\nshow that it is possible to execute this algorithm using only a constant (five, to be precise) rounds of oblivious\namplitude amplification, as opposed to the O(√\nN)rounds of amplitude amplification used in the other cases.\nAside from the use of the usual Schur-Weyl duality versus twisted Schur-Weyl duality, our algorithms bear\nsome similarities to the one for dPBT in [17], in which an extensive use of amplitude amplification is employed.\nHowever, the constant number of rounds of amplitude amplification in our algorithm for pPBT (with maximally\nentangled resource state), as well as the lower overall gate complexities, gives this protocol a greater chance of\nbeing practical for near-term devices. Another advantage of our algorithm in this domain is the single qubit\nused for the block-encoded unitary matrices throughout the paper, in contrast to the logarithmic number used\nin [17].\nFinally, the use of a small constant number of rounds of amplitude amplification in the algorithm of Section\nVB1 for pPBT with maximally entangled resource state leads one to the question of how well one can do\nwhen no amplitude amplification is used at all. To this end, we present an alternative algorithm for pPBT\nwith maximally entangled resource state in Section VB2 that is intended to maximise potential practicality for\nnear-term devices. No amplitude amplification is employed at all, and the gate complexities and ancilla counts\nare the same for this algorithm as for our main algorithm for pPBT with maximally entangled resource state\nshown on the left of Table II. However, unfortunately, its overall success probability is1\n4of what it is for normal\npPBT. As a result, the asymptotic success probability is1\n4, rather than the usual 1. Nevertheless, in a practical\nsituation, only a (small) constant number of copies of the state to be teleported would be required to overcome\nthis. This algorithm therefore seems to form the best currently available option for a practical implementation\nof port-based teleportation.\nB. Concurrent Work\nConcurrently and independently of the work presented in this manuscript, Grinko, Burchardt and Ozols\nderived algorithms for all port-based teleportation protocols on qudits with poly (n, d,log 1/ϵ)bounds on the\ngate complexity and ancilla count. Subsequent to discussions with these authors on our algorithms, they\nwere able to adapt these ideas to their setting to derive similar complexity improvements, hence the temporal\nseparation in the publications. By using phase estimation techniques (where we use amplitude amplification),\nthey are able to derive further polynomial improvements in the gate complexity, as may be found in [23].\nII. OUTLINE OF RESULTS\nIn Section III, we give the necessary preliminaries on the Schur transform defined via SU(2)spin coupling\nand a formal definition of port-based teleportation, in particular the POVMs used in each regime. Those for\nwhom the spin coupling definition of the Schur transform is unfamiliar, but the more traditional definition as in\nthe work of Harrow [24] is familiar, are recommended to read Appendix B. This Appendix gives a translation\nbetween these two perspectives on the Schur transform, while also clearing up confusion that arises between the\ntwo definitions.\nIn Section IV, we perform the necessary analysis on the set of POVM operators in each case, in particular\nfinding their eigendecomposition, which will be essential for our later implementations. Then, in Section V,\nwe provide our algorithms for port-based teleportation. This Section is intended to be independent of Section4\nIV - all necessary results proven in Section IV are repeated in Section V. The algorithm for dPBT may be\nfound in Section VA, the main algorithm for pPBT with maximally entangled resource state may be found in\nSection VB1, the alternative algorithm (using no amplitude amplification) for pPBT with maximally entangled\nresource state may be found in Section VB2 and lastly the algorithm for pPBT with optimised resource state\nmay be found in Section VB3. A gate complexity analysis of these algorithms takes place in Appendix A, and\nlastly we provide a discussion in Section VI.\nIII. PRELIMINARIES\nSince the notion will be used throughout this paper, we start by introducing SU(2)spin couplings in Section\nIIIA before defining the Schur transform in IIIB and describing how it may be efficiently implemented via spin\ncoupling. Lastly, in Section IIIC, we formally define port-based teleportation.\nThose who have prior experience with the Schur transform not from the spin coupling perspective, but from\nthe more mathematical perspective such as that used by Harrow [24] (the more common perspective in quantum\ncomputing) are recommended to read Appendix B for the translation between the two perspectives.\nA. Spin Couplings\nA coupling of nspins is described via angular momenta, defined as follows.\nDefinition III.1. Onnqubits, we let (σ(i)\nx, σ(i)\ny, σ(i)\nz)denote the usual Pauli matrices acting on the i-th qubit.\nThe angular momentum operator on the i-th qubit is then defined via\n⃗S(i)=1\n2\nσ(i)\nx\nσ(i)\ny\nσ(i)\nz\n. (1)\nIt will then be important to be able to express so-called Z-angular momentum and total angular momentum\nas operators acting on a subset of the qubits.\nDefinition III.2. Given a subset of nqubits denoted by the subset a⊆[n] ={1, ..., n}, their total angular\nmomentum operator is\nS2\na= X\ni∈a⃗S(i)!\n· X\ni∈a⃗S(i)!\n, (2)\nwhere ·denotes the usual dot product of 3-vectors, and their Z-angular momentum is\nZa=1\n2X\ni∈aσ(i)\nz. (3)\nIf a subset of qubits ais in a state that is an eigenvector of S2\nawith eigenvalue j(j+ 1), the set of qubits is\nsaid to have total spin j(or simply just “spin j”). It can be verified that S2\n{i}=3\n4Ifor an individual qubit i, so\nwe treat individual qubits as having spin 1/2. If a subset of qubits ais in a state that is an eigenvector of Za\nwith eigenvalue m, the set of qubits is said to have Z-spin m.\nThe following fact is important for us and has a simple proof [25]:\nClaim III.1. For non-empty subsets of the qubits aandb, we have the following commutativity between oper-\nators:\na∩b=∅=⇒[S2\na, S2\nb] = 0 (4)\na⊆b=⇒[S2\na, S2\nb] = 0 (5)\na⊆b=⇒[S2\na, Zb] = 0. (6)\nIn particular, we see that [S2\n[n], Z[n]] = 0. We can thus write out a basis for the space of the nqubits where\neach basis state has total spin jandZ-spin m, for some jandm, denoting such a simultaneous eigenstate\nas|j, m⟩.jonly takes values inN0\n2for any set of qubits, and given some fixed j,monly takes values in\n{−j,−j+ 1, ..., j−1, j}. However, |j, m⟩does not identify one particular state of nqubits in general. For\nexample, it can be checked that the dimension of the eigenspace with j= 1/2andm= 1/2on 3 qubits is5\ngreater than 1. In order to fix this degeneracy, and uniquely identify states via their various spins, we need the\nconcept of a spin eigenbasis.\nAspineigenbasisisasimultaneouseigenbasisarisingfromacompletesetof ncommutingtotaland Z-angular\nmomentum operators. The allowed choices of such operators are explained in the following.\nDefinition III.3. Consider non-empty, proper qubit subsets a1, ..., a n−2⊆[n]such that, for each aiandaj,\nthey are either disjoint or one is a subset of the other. A spin eigenbasis is then the simultaneous eigenbasis\ncorresponding to the set of commuting observables (S2\na1, ..., S2\nan−2, S2, Z)where we adopt the short-hand S2:=\nS2\n[n],Z:=Z[n].\nSuch sets of operators are indeed commuting by Claim III.1, and complete [20]. As an example, on three\nqubits, we can identify a basis via the choice of complete and commuting operators (S2\n{1,2}, S2, Z). We have\nthus fixed the previous degeneracy by using a complete set of operators, and states in this basis are uniquely\nidentified as |k, j, m ⟩, where kis now the spin on the first two qubits.\nTo get a better grasp of these bases, we introduce the notion of spin coupling. Spin coupling is defined\nvia Clebsch-Gordan coefficients, where here we deal only with SU(2)Clebsch-Gordan coefficients, denoted\nCJ,M\nj1,m1;j2,m2. Given some set of qubits awith total spin j1, whose Z-spin m1is allowed to vary, and similarly\nsome other set of qubits bwith total spin j2, whose Z-spin m2is allowed to vary, the two sets of qubits may be\nbrought together to form a state on qubits a∪bwhich has definite total spin JandZ-spin M:\n|J, M⟩=X\nm1,m2CJ,M\nj1,m1;j2,m2|j1, m1⟩|j2, m2⟩ (7)\nwhere the sum is taken over the allowed values of m1andm2mentioned earlier, i.e. m1∈ {− j1, ..., j 1−1, j1}\netc. The state in Equation 7 is an eigenstate of S2\na∩b,Za∩b,S2\naandS2\nb, but it is not an eigenstate of either Za\norZb, like|j1, m1⟩and|j2, m2⟩are, respectively. Given some j1andj2both inN0\n2, which total spins Jcan we\nget? Any |J, M⟩can be produced with M∈ {− J, ..., J −1, J}as long as\n|j1−j2| ≤J≤j1+j2and j1+j2+J∈Z (8)\nand so, in particular, nqubits may have any total spin in {1\n2,3\n2, ...,n\n2}ifnis odd, or in {0,1, ...,n\n2}ifnis even.\nIfj1,j2andJdo not satisfy the relations of Equation (8), any corresponding Clebsch-Gordan coefficient will\nreturn the value 0. Another important property of Clebsch-Gordan coefficients is that CJ,M\nj1,m1;j2,m2= 0unless\nm1+m2=M, a property referred to as “conservation of angular momentum”.\nThe Clebsch-Gordan coefficients CJ,M\nj1,m1;j2,m2are numbers that can be calculated classically in\npoly(J, M, j 1, m1, j2, m2)time [26, 27]. From here, we see how we can build up spin eigenbases. As an example,\nwe can find the states given by coupling two qubits:\n|j= 1, m=−1⟩=|11⟩ | j= 1, m= 0⟩=|01⟩+|10⟩√\n2|j= 1, m= 1⟩=|00⟩ (9)\n|j= 0, m= 0⟩=|01⟩ − |10⟩√\n2(10)\nbyidentifyingthecomputationalbasisstatesofaqubit |0⟩and|1⟩withspinstates |j=1\n2, m=1\n2⟩,|j=1\n2, m=−1\n2⟩,\nrespectively. We then further couple these to get the 8 states in the spin eigenbasis on 3 qubits defined by\n(S2\n1,2, S2, Z). This eigenbasis then contains states |k, j, m ⟩as follows:\n\f\f\f\f1,3\n2,−3\n2\u001d \f\f\f\f1,3\n2,−1\n2\u001d \f\f\f\f1,3\n2,1\n2\u001d \f\f\f\f1,3\n2,3\n2\u001d\n(11)\n\f\f\f\f1,1\n2,−1\n2\u001d \f\f\f\f1,1\n2,1\n2\u001d\n(12)\n\f\f\f\f0,1\n2,−1\n2\u001d \f\f\f\f0,1\n2,1\n2\u001d\n(13)\nB. The Schur Transform\nHere we define the Schur transform from the physical point of view, via spin coupling. Those for whom\nthis is a new perspective may find Appendix B useful in which we discuss the two different notions of the\nSchur transform and why it’s important to appreciate both of them. We also provide a translation between the\nnotations in the mathematical and physical perspectives to ease this transition.\nFor us, the Schur basis is a spin eigenbasis of particular importance.6\nDefinition III.4. The Schur basis on nqubits is the spin eigenbasis defined by the operators\u0010\nS2\n{1,2}, S2\n{1,2,3}, ..., S2\n{1,...,n−1}, S2, Z\u0011\n. The Schur transform is the unitary operator on nqubits mapping the\ncomputational basis state to the Schur basis.\nStates in this basis are called Schur states and may then be labelled by n−1total spins and one Z-spin.\nWe write states in this basis as |k1, k2, ..., k n−2, j, m⟩, where k1is the spin on the first two qubits, k2is the spin\non the first three qubits, and so on. kn−2is therefore the spin on the first n−1qubits, and jis the spin on all\nthe qubits. It is readily verified that these may be explicitly written\n|k1, k2, ..., k n−2, j, m⟩=X\nx1,...,x nCk1,x1+x2\n1\n2,x1;1\n2,x2Ck2,x1+x2+x3\nk2,x1+x2;1\n2,x3...\nCkn−2,x1+...+xn−1\nkn−3,x1+...+xn−2;1\n2,xn−1Cj,m\nkn−2,x1+...+xn−1;1\n2,xn|x1...xn⟩(14)\nFor convenience, in this expression, each xiis being summed over the set {±1\n2}and we are writing out\ncomputational basis states |xi⟩with a slightly different notation to usual:\f\f−1\n2\u000b\ninstead of |1⟩and\f\f1\n2\u000b\ninstead\nof|0⟩.\nWe note that the definition given above is a definition from a very physical perspective, inspired by the\nwork on Permutational Quantum Computing from Jordan [20] which dates back to the work of Marzuoli and\nRasetti [28]. Much of the more modern literature approaches the Schur transform from a more mathematical,\nbut essentially equivalent perspective in relation to the beautiful theory of Schur-Weyl duality. See [22] for an\nexcellent introduction. We note that in this more mathematical formalism all spin eigenbases are examples of\nSchur bases, but theSchur basis given above is one that is subgroup-adapted in a particular way.\nAlgorithms exist for efficiently performing the Schur transform on a quantum computer. See [21] for an\nalgorithm that fits into this perspective. Briefly, this algorithm takes place in two stages. First, computational\nbasis states are coherently mapped to “encodings” of Schur states via the “pre-mapping” stage:\n|x1...xn⟩ 7→ | k1⟩|k2⟩...|kn−2⟩|j⟩|m⟩. (15)\nEach ket on the right-hand side is a computational basis state encoding its displayed value. For example, k3\nmay take three values 0, 1 and 2, and so |k3⟩is a computational basis state on two qubits. In total, there are\nO(nlog(n))qubits on the right-hand side. Next, these Schur encodings are mapped to Schur states themselves:\n|k1⟩|k2⟩...|kn−2⟩|j⟩|m⟩ 7→ | k1, k2, ..., k n−2, j, m⟩ (16)\nin what is known as the “coupling” stage. This again takes place coherently over all Schur encodings. Notice\nthat the right-hand side is a state on nqubits now - the ancillas have become unentangled and discarded.\nThroughout, we will differentiate Schur states from Schur encodings by using one ket |k1, k2, ..., k n−2, j, m⟩and\nmultiple kets |k1⟩|k2⟩...|kn−2⟩|j⟩|m⟩, respectively.\nTheSchurtransformhasfoundwide-spreaduseinquantuminformationapplications. Onepointthatisoften\nunder-appreciated but critical in some of these applications is the need to perform a “clean” transform, where\nn-qubit computational basis states |x⟩are mapped to n-qubit Schur states |k1, ..., k n−2, j, m⟩and vice-versa.\nThis is important whenever further unitary n-qubit computation will take place after the Schur transform, such\nas in [20] or [29]. For other applications, often, it is only the inverse of the coupling stage that is used, where n\nqubit Schur states are mapped to their Schur encodings. In fact, this operation itself is defined in many places as\nthe Schur transform, and there are efficient implementations of this [24, 30, 31], but this has led to confusion in\nsome places for algorithm designers who require a clean transform2. In Appendix B, we go into further detail on\nthe discrepancies between different notions of the Schur transform, addressing particular sticking points where\nconfusion can arise.\nFinally, we note that the above algorithm, as outlined in Equations (15) and (16), uses O(nlog(n))ancillas.\nHowever, a “compressed” version is shown in [21] that uses only O(log(n))ancillas. In this version the pre-\nmapping stage is\n|x1...xn⟩ 7→ | y1...yn−2⟩|kn−2⟩|j⟩|m⟩, (17)\nwhere |y1...yn−2⟩is an (n−2)-qubit computational basis state giving an efficient encoding of the values\nk1, ..., k n−2, and then the coupling stage performs\n|y1...yn−2⟩|kn−2⟩|j⟩|m⟩ 7→ | k1, k2, ..., k n−2, j, m⟩. (18)\n2We note that [31] can also perform a clean transform. However, the algorithm of [21] offers the lowest gate complexity in terms\nof the Clifford + T universal set of any clean transform, as well as the greatest simplicity, and the ability to perform the unitary\nto any spin eigenbasis, not just the Schur transform.7\nThe Clifford + T gate complexity of this is O(n3log(n) log(n\nϵ)), where ϵis its accuracy in the trace norm.\nIn this work, all that will be needed will be the inverse of the pre-mapping stage, which for many people\nis just the Schur transform, as mentioned. It will also be important for us that the algorithm of [22] gives an\nalgorithm for this operation with gate complexity npoly(logn,log1\nϵ), but using O(nlog(n))ancillas.\nC. Port-Based Teleportation\nIshizaka and Hiroshima introduced the ‘port-based’ method of quantum teleportation in [3, 4] based on the\nwork in [2]. The scheme aims to offer a method of teleportation for which no corrective unitary operation is\nrequired on the part of the receiver, in contrast to the standard teleportation protocol [1]. It may be described\nas follows, in a similar way to [4].\nThe sender, Alice, and receiver, Bob, assumed to be spatially separated, each have Nqubit systems, re-\nspectively labelled A1, ..., A NandB1, ..., B Nin a joint entangled state. The state Alice wishes to send is in a\nfurther qubit system which we denote AN+1. She performs a POVM on her N+ 1qubit systems and sends her\nresult, a number in {1, ..., N}to Bob as classical information. This number tells Bob in which port he may find\nthe teleported state, and he need do nothing other than select that port.\nOne of the most attractive features of such a scheme is its ‘unitary equivariance’, meaning that Bob can\nperform some unitary Uon every port before the classical information has even arrived telling him which\nport to look in, ultimately obtaining the teleported state having been acted on by U. Unfortunately, the no-\nprogrammingtheorem[5]prohibitsthisprocessfromtakingplaceperfectly(withfidelity1)anddeterministically\n(with probability 1) in a system of finite size. Thus, with finite N, one must sacrifice at least one of the ability\nto perform the teleportation perfectly or deterministically.\nTwoprimaryparadigmsthenemerge,thatofdeterministicport-basedteleportation(dPBT)andprobabilistic\nport-based teleportation (pPBT). In the former, teleportation is always achieved, but with fidelity less than 1\n(strictly, this is fidelity averaged over all uniformly distributed pure input states), and in the latter, teleportation\nis achieved with probability less than 1 (again, strictly, this is averaged probability) but fidelity 1 in the case\nof a success. Note that in this latter case, Alice in fact has N+ 1outcomes from her measurement, the first N\ndenoting a success (and the corresponding port in which to find the state) and the final one, N+ 1, denoting a\nfailure. In both regimes, Ishizaka and Hiroshima determine the POVM that will optimise fidelity and probability\nrespectively. The optimal fidelity and probability tend towards 1 as N→ ∞in each case.\nFurthermore, IshizakaandHiroshimadefinetwomoreregimeswithindPBTandpPBT:onewithamaximally\nentangled resource state, where the shared resource state between the 2Nports is fixed to |ψ−⟩⊗N, where\n|ψ−⟩=|01⟩−|10⟩√\n2, and one for which this resource state is simultaneously optimised with the POVM.\nFor the four regimes defined, Ishizaka and Hiroshima find the following optimal POVMs. For dPBT with\nmaximally entangled resource state, the optimal POVM operators are given by the square-root measurement,\nalso called the pretty good measurement\nΠi=ρ−1/2σ(i)ρ−1/2+ ∆fori= 1, ..., N (19)\nwhere σ(i)=1\n2N−1|ψ−⟩⟨ψ−|AiAN+1⊗I¯Aifor¯Ai=A1...Ai−1Ai+1...ANandρ=PN\ni=1σ(i). The inverse ρ−1is\ndefined only on the support of ρand the term\n∆ =1\nN \nI−NX\ni=1ρ−1/2σ(i)ρ−1/2!\n(20)\nisaddedtoeveryPOVMelementsothatPN\ni=1Πi=I. IshizakaandHiroshimathengiveaseparatemeasurement\nforthecaseofdPBTwithoptimisedresourcestate,howeveritwasshownin[6]thattheprettygoodmeasurement\nis also optimal in this case too - note this may be the case because the relevant optimisation problem may have\nmultiple solutions.\nForpPBTwithmaximallyentangledresourcestate, IshizakaandHiroshimashowthatthesuccessprobability\nis optimised by the POVM operators\nΠi=|ψ−⟩⟨ψ−|AiAN+1⊗˜Θi¯Aifori= 1, ..., Nand ΠN+1=I−NX\ni=1Πi (21)\nwhere ˜Θi¯Aidenotes the operator ˜Θiacting on the qubits ¯Ai=A1...Ai−1Ai+1...AN. This is an N−1qubit\noperator defined as8\nOptimal\nFidelity (dPBT)Optimal\nSuccess Probability (pPBT)\nMaximally Entangled\nResource Statef∼1− O\u00001\nN\u0001\np∼1− O\u0010\n1√\nN\u0011\nOptimised\nResource Statef∼1− O\u00001\nN2\u0001\np∼1− O\u00001\nN\u0001\nTABLE III. Optimal fidelities and success probabilities for the four regimes in the limit N→ ∞. Notice that for both\ndPBT and pPBT we get a quadratic improvement in the ‘deviation from 1’ by moving to the optimised protocol.\n˜Θi=1\n2N−1(N−1)/2X\ns=smin1\nλ+\ns+1/2I(s) (22)\nwhere\nsmin=(\n0ifN−1is even\n1\n2ifN−1is odd, λ+\nj=1\n2N\u0012N\n2+j+ 1\u0013\n(23)\nandI(s)is the projector onto the subspace of the N−1qubits with total spin s. Lastly, for pPBT with optimised\nresource state, Ishizaka and Hiroshima show that optimal POVM operators are given by\nΠi= (O−1\nA⊗IAN+1)\u0010\n|ψ−⟩⟨ψ−|AiAN+1⊗˜Θi¯Ai\u0011\n(O−1\nA⊗IAN+1)fori= 1, ..., Nand ΠN+1=I−NX\ni=1Πi(24)\nwhere A=A1...ANdenotes Alice’s ports and\nO=N/2X\nj=jminp\nν(j)I(j)Afor ν(j) =2Nh(N)(2j+ 1)\ng[N](j)and jmin=(\n0ifNis even\n1\n2ifNis odd(25)\n˜Θi=(N−1)/2X\ns=sminu(s)I(s)for u(s) =2N+1h(N)(2s+ 1)\nNg[N−1](s)(26)\nwhere I(j)is the projector onto the subspace of Nqubits with spin j,h(N) =6\n(N+1)(N+2)(N+3)andg[N](j) =\n(2j+1)N!\n(N\n2−j)!(N\n2+1+j)!.\nIshizaka and Hiroshima show that the optimal fidelities (for dPBT) and success probabilities (for pPBT)\nbehave as in Table III.\nIV. DIAGONALISING THE PBT OPERATORS\nNoting that we can use the same POVM operators for both cases in dPBT, we now study the three sets of\nPOVM operators - those for dPBT (Section IVA), those for pPBT with maximally entangled resource state\n(Section IVB1) and those for pPBT with optimised resource state (Section IVB2).\nOur aim will be to diagonalise these POVM operators. We will find that it is relatively straightforward to\ndo so for all these POVMs using just standard spin coupling, demonstrating the power of the formalism. The\ndiagonalisation is necessary so that we can perform the POVM via Naimark dilation. Indeed, a standard way\nto perform a POVM described by the operators {Πi}M\ni=1on the state |ψ⟩, is to attach an ancillary register |i⟩\nof dimension Mand perform the operation\n|ψ⟩|1⟩ 7→MX\ni=1p\nΠi|ψ⟩|i⟩ (27)\nwhere {|i⟩}M\ni=1forms an orthonormal basis for the ancillary register. Then, by measuring the ancillary register\nprojectively, one recovers the outcome iwith the desired probability ⟨ψ|Πi|ψ⟩. Note that the operation in\nEquation (27) is in general quite non-trivial to implement, but because the operation on the right-hand side has\nnorm 1, we are guaranteed that there is some unitary acting in this way - a so-called “Naimark unitary”.9\nTo diagonalise the POVM operators, we will need a consistent notation for Schur states on N+ 1qubits.\nWe denote a Schur state on N+ 1qubits by |k1, ..., k N−2, j, s, m ⟩, where k1is the spin on the first two qubits,\nk2is the spin on the first three qubits, and so on. kN−2is therefore the spin on the first N−1qubits, which in\nour case are the qubits A1, ..., A N−1, and jis the spin on the first Nqubits, i.e. A=A1, ..., A N, all of Alice’s\nports. sis then the spin on all N+ 1qubits (all of Alice’s ports and the state she intends to send), and mis\ntheZ-spin on all these qubits3.\nOften, this notation will prove too cumbersome, and we in fact find it better to simply write |k, j, s, m ⟩\nwhere kis standing for all k1, ..., k N−2. Sometimes, we will want to address kN−2, but none of the other k’s. In\nthis case, we will write the Schur state as |˜k, kN−2, j, s, m ⟩, where ˜kis standing for k1, ..., k N−3.\nA. Deterministic Port-Based Teleportation\nLet us study the POVM operators used in dPBT, i.e. the square-root measurement. Acting on Alice’s ports\nA=A1...ANand the input state AN+1, we recall that these are Πi=ρ−1/2σ(i)ρ−1/2+ ∆fori= 1, ..., N\nwhere σ(i)=1\n2N−1|ψ−⟩⟨ψ−|AiAN+1⊗I¯Aifor¯Ai=A1...Ai−1Ai+1...AN,ρ=PN\ni=1σ(i)and|ψ−⟩=|01⟩−|10⟩√\n2.\nThe inverse ρ−1is defined only on the support of ρand the term\n∆ =1\nN \nI−NX\ni=1ρ−1/2σ(i)ρ−1/2!\n(28)\nis added to every POVM element so thatPN\ni=1Πi=I.\nIshizaka and Hiroshima show that the operator ρis diagonalised by the Schur basis |k, j, s, m ⟩, and in\nparticular\nρ|k, j, s, m ⟩=λ(j, s)|k, j, s, m ⟩where λ(j, s) =(\nλ−\nj=1\n2N\u0000N\n2−j\u0001\nifs=j+1\n2\nλ+\nj=1\n2N\u0000N\n2+j+ 1\u0001\nifs=j−1\n2.(29)\nFrom here, it is clear to see where ρis and is not supported. We see that ρhas zero eigenvalue exactly when\nj=N\n2ands=j+1\n2which are, in fact, the maximal spin states on the N+ 1qubits - these are the states with\nk1= 1,k2=3\n2,k3= 2and so on, up to j=N\n2ands=N+1\n2. There are, in fact, N+ 2such states, where mis\nthe only spin allowed to vary.\nFrom here, we see that ∆ =1\nN\u0000\nI−ρ−1/2ρρ−1/2\u0001\n, recalling that ρ−1is the inversion of ρonly on its support.\nTherefore,\n∆ =1\nNI\u0012N+ 1\n2\u0013\nAAN+1(30)\nwhere I\u0000N+1\n2\u0001\nAAN+1is the projector onto the maximal spin states (with spinN+1\n2) of all N+ 1qubits. We\ntherefore see that ∆simply acts to attach a factor of1\nNto each maximal spin state, whereas ρ−1/2σ(i)ρ−1/2\nannihilates every maximal spin state. ∆annihilates any Schur state with non-maximal spin, so it remains to\nfind how ρ−1/2σ(i)ρ−1/2acts on Schur states of non-maximal spin. We will in fact just determine the action of\nΠN, and then use Πi=SWAP AiANΠNSWAP AiAN.\nBy using the expression for (N+ 1)-qubit Schur states in Equation (14), as well as the explicit formulae for\nClebsch-Gordan coefficients [26]\nCj±1\n2,M\nj, M−1\n2;1\n2,1\n2=±s\n1\n2\u0012\n1±M\nj+1\n2\u0013\nand Cj±1\n2,M\nj, M+1\n2;−1\n2,1\n2=s\n1\n2\u0012\n1∓M\nj+1\n2\u0013\n, (31)\none finds the partial inner product\n⟨ψ−|ANAN+1|k, j, s, m ⟩=c(kN−2, j, s)|k1, ..., k N−2, m⟩¯AN(32)\n3We note that Appendix B contains a useful translation between this physical (spin) perspective of the Schur states and the more\ncommon mathematical perspective which will likely be of use to those who are previously familiar with the latter perspective\nonly.10\nwhere,tobeexplicit, |ψ−⟩ANAN+1isthesingletstate,|01⟩−|10⟩√\n2,onthelasttwoqubits ANandAN+1,|k, j, s, m ⟩is\nan(N+1)-qubit Schur state. |k1, ..., k N−2, m⟩¯ANis an (N−1)-qubit Schur state on the qubits ¯AN=A1...AN−1,\nwith spin on the first two qubits k1, on the first three qubits k2, up to spin on all N−1qubits of kN−2and\nZ-spin on all N−1qubits of m. The coefficient c(kN−2, j, s)is given in the following:\nc(kN−2, j, s) =\n\nq\ns\n2s+1ifs=j+1\n2andj=kN−2−1\n2\n−q\ns+1\n2s+1ifs=j−1\n2andj=kN−2+1\n2\n0 otherwise. (33)\nIn particular, noting that jmay be greater than or less than kN−2by1\n2andsmay be greater than or less than\njby1\n2, we are seeing that ⟨ψ−|ANAN+1|k, j, s, m ⟩is zero if kN−2differs from si.e. if s=kN−2±1. From this,\none easily finds the coefficients of ρ−1/2σ(N)ρ−1/2in the Schur basis:\n⟨k′, j′, s′, m′|ρ−1/2σ(N)ρ−1/2|k, j, s, m ⟩=(λ(j′, s′)λ(j, s))−1/2\n2N−1c(k′\nN−2, j′, s′)c(kN−2, j, s)δkk′δmm′(34)\n=(λ(j′, s′)λ(j, s))−1/2\n2N−1c(k′\nN−2, j′, s′)c(kN−2, j, s)δkk′δmm′δss′(35)\nwhere, going into the second line, we have simply added in δss′, which we are allowed to do because Equation\n(34) shows that the expression vanishes unless kN−2=k′\nN−2(from δkk′) and kN−2=s(from c(kN−2, j, s)) and\nk′\nN−2=s′(from c(k′\nN−2, j′, s′)). Note also that in this expression we have assumed that neither |k, j, s, m ⟩nor\n|k′, j′, s′, m′⟩are maximally entangled states i.e. s̸=N+1\n2ands′̸=N+1\n2since then (λ(j′, s′)λ(j, s))−1/2would\nbe undefined.\nWe therefore find that ΠN|k, j, s, m ⟩= 0ifkN−2̸=sand, if kN−2=s, we find the action of ΠNseparately\non|k, j, s, m ⟩based on whether j=s−1/2orj=s+ 1/2:\nΠN|˜k, s, s−1\n2, s, m⟩=c(s, s−1\n2, s)2\n2N−1λ(s−1\n2, s)|˜k, s, s−1\n2, s, m⟩+c(s, s−1\n2, s)c(s, s+1\n2, s)\n2N−1q\nλ(s−1\n2, s)λ(s+1\n2, s)|˜k, s, s +1\n2, s, m⟩\n(36)\nΠN|˜k, s, s +1\n2, s, m⟩=c(s, s+1\n2, s)2\n2N−1λ(s+1\n2, s)|˜k, s, s +1\n2, s, m⟩+c(s, s−1\n2, s)c(s, s+1\n2, s)\n2N−1q\nλ(s−1\n2, s)λ(s+1\n2, s)|˜k, s, s−1\n2, s, m⟩\n(37)\nrecalling that ˜kdenotes k1, ..., k N−3. We can see that the states |˜k, s, s±1\n2, s, m⟩span a 2-dimensional invariant\nsubspace under ΠN. In this subspace, ΠNtakes the form\n\u0012\nα(s)2α(s)β(s)\nα(s)β(s)β(s)2\u0013\nforα(s) =c(s, s−1\n2, s)q\n2N−1λ(s−1\n2, s)andβ(s) =c(s, s+1\n2, s)q\n2N−1λ(s+1\n2, s). (38)\nThis is then an easily diagonalised matrix, having eigenvalues 0andα(s)2+β(s)2, and we conclude that ΠN\nhas the following eigenvectors and corresponding eigenvalues.\n|k, j, s, m ⟩with s=N+ 1\n2i.e. maximal spin states have eigenvalue1\nN(39)\n|k, j, s, m ⟩with s <N+ 1\n2andkN−2̸=s have eigenvalue 0 (40)\nβ(s)p\nα(s)2+β(s)2|˜k, s, s−1\n2, s, m⟩ −α(s)p\nα(s)2+β(s)2|˜k, s, s +1\n2, s, m⟩have eigenvalue 0 (41)\nα(s)p\nα(s)2+β(s)2|˜k, s, s−1\n2, s, m⟩+β(s)p\nα(s)2+β(s)2|˜k, s, s +1\n2, s, m⟩have eigenvalue α(s)2+β(s)2.\n(42)\nFinally, because Πi=SWAP AiANΠNSWAP AiAN, as mentioned, the eigenvectors for Πican be found by acting\non those of ΠNwith SWAP AiAN, and they will have the same eigenvalues.11\nB. Probabilistic Port-Based Teleportation\n1. pPBT with Maximally Entangled Resource State\nWe recall that in this case we deal with the POVM operators Πi=|ψ−⟩⟨ψ−|AiAN+1⊗˜Θi¯Aifori= 1, ..., N\nandΠN+1=I−PN\ni=1Πi, where ˜Θi¯Aidenotes the operator ˜Θiacting on the qubits ¯Ai=A1...Ai−1Ai+1...AN.\nThis is an N−1qubit operator defined as\n˜Θi=1\n2N−1(N−1)/2X\ns=smin1\nλ+\ns+1/2I(s) (43)\nwhere smin=(\n0ifN−1is even\n1\n2ifN−1is odd,λ+\nj=1\n2N\u0000N\n2+j+ 1\u0001\nandI(s)is the projector onto the subspace of the\nN−1qubits with total spin s.\nUsing the same notation as in the previous subsection, and using the same strategy of diagonalising ΠNfirst\nand then using Πi=SWAP AiANΠNSWAP AiAN, we find in this case that\n⟨k′, j′, s′, m′|ΠN|k, j, s, m ⟩=c(k′\nN−2, j′, s′)c(kN−2, j, s)\n2N−1λ(s+1\n2, s)δkk′δmm′δss′. (44)\nWe see that ΠNis annihilating Schur states |k, j, s, m ⟩for which kN−2̸=si.e. if kN−2=s±1. We find the\naction of ΠNon states with kN−2=sis as follows:\nΠN|˜k, s, s−1\n2, s, m⟩=1\n2N−1λ(s+1\n2, s)(2s+ 1)\u0014\ns|˜k, s, s−1\n2, s, m⟩ −p\ns(s+ 1)|˜k, s, s +1\n2, s, m⟩\u0015\n(45)\nΠN|˜k, s, s +1\n2, s, m⟩=1\n2N−1λ(s+1\n2, s)(2s+ 1)\u0014\n(s+ 1)|˜k, s, s +1\n2, s, m⟩ −p\ns(s+ 1)|˜k, s, s−1\n2, s, m⟩\u0015\n(46)\nandagainthestates |˜k, s, s±1\n2, s, m⟩forma2Dinvariantsubspaceunder ΠN. Thistime, ΠNtakesthefollowing\nform in this subspace.\n1\n2N−1λ(s+1\n2, s)(2s+ 1)\u0012\ns −p\ns(s+ 1)\n−p\ns(s+ 1) s+ 1\u0013\n(47)\nwhich is again a matrix that is easily diagonalised. We conclude that ΠNhas the following eigenvectors and\ncorresponding eigenvalues.\n|k, j, s, m ⟩with kN−2̸=s have eigenvalue 0 (48)\nr\ns+ 1\n2s+ 1|˜k, s, s−1\n2, s, m⟩+rs\n2s+ 1|˜k, s, s +1\n2, s, m⟩have eigenvalue 0 (49)\nrs\n2s+ 1|˜k, s, s−1\n2, s, m⟩ −r\ns+ 1\n2s+ 1|˜k, s, s +1\n2, s, m⟩have eigenvalue1\n2N−1λ(s+1\n2, s).(50)\nAgain, theeigenvectorsof Πifori= 1, ..., N−1maybefoundbyapplying SWAP AiANtoeachoftheeigenvectors\nofΠN, giving the same corresponding eigenvalue, because we have Πi=SWAP AiANΠNSWAP AiAN.\nWe must, however, make extra steps in this case because ΠN+1=I−PN\ni=1Πirequires diagonalisation also.\nWe compute\nNX\ni=1Πi=N−1\n2X\ns=smin1\nλ+\ns+1\n2NX\ni=1|ψ−⟩⟨ψ−|AiAN+1⊗I(s)¯Ai\n2N−1(51)\n=N−1\n2X\ns=smin1\nλ+\ns+1\n2ρ(s) (52)12\nwhere ρ(s) =I(s)AAN+1ρI(s)AAN+1, and here I(s)AAN+1is the projector onto the subspace of the N+ 1qubits\nwith total spin s4. Knowing the eigenbasis for ρis|k, j, s, m ⟩, we may conclude that ΠN+1has eigenvectors and\ncorresponding eigenvalues as follows.\n|k, j, s, m ⟩with s=j+1\n2have eigenvalue 1−λ(s−1\n2, s)\nλ(s+1\n2, s)(53)\n|k, j, s, m ⟩with s=j−1\n2have eigenvalue 0. (54)\n2. pPBT with Optimised Resource State\nLastly, let us diagonalise the POVM operators used in the case of pPBT with optimised resource state, which\nwe recall are Πi= (O−1\nA⊗IAN+1)\u0010\n|ψ−⟩⟨ψ−|AiAN+1⊗˜Θi¯Ai\u0011\n(O−1\nA⊗IAN+1)fori= 1, ..., Nand ΠN+1=\nI−PN\ni=1Πiwhere A=A1...ANdenotes Alice’s ports and\nO=N/2X\nj=jminp\nν(j)I(j)Afor ν(j) =2Nh(N)(2j+ 1)\ng[N](j)and jmin=(\n0ifNis even\n1\n2ifNis odd(55)\n˜Θi=(N−1)/2X\ns=sminu(s)I(s)for u(s) =2N+1h(N)(2s+ 1)\nNg[N−1](s)(56)\nwhere I(j)is the projector onto the subspace of Nqubits with spin j,h(N) =6\n(N+1)(N+2)(N+3)and\ng[N](j) =(2j+1)N!\n(N\n2−j)!(N\n2+1+j)!. Using the strategy once again of diagonalising ΠNfirst before employing Πi=\nSWAP AiANΠNSWAP AiAN, we find this time that\n⟨k′, j′, s′, m′|ΠN|k, j, s, m ⟩= (ν(j′)ν(j))−1/2u(s)c(k′\nN−2, j′, s′)c(kN−2, j, s)δkk′δmm′δss′,(57)\nfinding again that any |k, j, s, m ⟩is annihilated with kN−2̸=s. The action of ΠNon states |k, j, s, m ⟩with\nkN−2=sis then\nΠN|˜k, s, s−1\n2, s, m⟩=u(s)\nc(s, s−1\n2, s)2\nν(s−1\n2)|˜k, s, s−1\n2, s, m⟩+c(s, s−1\n2, s)c(s, s+1\n2, s)q\nν(s+1\n2)ν(s−1\n2)|˜k, s, s +1\n2, s, m⟩\n\n(58)\nΠN|˜k, s, s +1\n2, s, m⟩=u(s)\nc(s, s+1\n2, s)2\nν(s+1\n2)|˜k, s, s +1\n2, s, m⟩+c(s, s−1\n2, s)c(s, s+1\n2, s)q\nν(s+1\n2)ν(s−1\n2)|˜k, s, s−1\n2, s, m⟩\n.\n(59)\nIn this 2D invariant subspace, ΠNtakes the form\nu(s)\u0012\nγ(s)2γ(s)δ(s)\nγ(s)δ(s)δ(s)2\u0013\nwhere γ(s) =c(s, s−1\n2, s)q\nν(s−1\n2)and δ(s) =c(s, s+1\n2, s)q\nν(s+1\n2). (60)\nWe thus conclude that ΠNhas eigenvectors with corresponding eigenvalues\n|k, j, s, m ⟩with kN−2̸=s have eigenvalue 0 (61)\nδ(s)p\nγ(s)2+δ(s)2|˜k, s, s−1\n2, s, m⟩ −γ(s)p\nγ(s)2+δ(s)2|˜k, s, s +1\n2, s, m⟩have eigenvalue 0 (62)\nγ(s)p\nγ(s)2+δ(s)2|˜k, s, s−1\n2, s, m⟩+δ(s)p\nγ(s)2+δ(s)2|˜k, s, s +1\n2, s, m⟩have eigenvalue u(s)\u0000\nγ(s)2+δ(s)2\u0001\n.\n(63)\n4This latter step may be seen by, for example, showing I(s)\u0010\n|ψ−⟩⟨ψ−|ANAN+1⊗I¯AN\u0011\nI(s) =|ψ−⟩⟨ψ−|ANAN+1⊗I(s)¯ANby\nshowing equality of their matrix elements in the Schur basis, before showing equality of every term in the sum by acting with\nSWAPs.13\nOnce again, the eigenvectors and corresponding eigenvalues of Πiare found by acting with SWAP AiANon the\neigenvectors of ΠN.\nFinally, we must diagonalise ΠN+1=I−PN\ni=1Πi. We compute\nNX\ni=1Πi=\u0000\nO−1\nA⊗IAN+1\u0001NX\ni=1\u0010\n|ψ−⟩|ψ−⟩AiAN+1⊗˜Θi¯Ai\u0011\u0000\nO−1\nA⊗IAN+1\u0001\n(64)\n= 2N−1\u0000\nO−1\nA⊗IAN+1\u0001N−1\n2X\ns=sminu(s)ρ(s)\u0000\nO−1\nA⊗IAN+1\u0001\n(65)\nThis manifestly has an eigenbasis |k, j, s, m ⟩and so we find that ΠN+1has the set of eigenvectors |k, j, s, m ⟩\nwith eigenvalues\n1−2N−1λ(j, s)u(s)\nν(j). (66)\nV. ALGORITHMS FOR IMPLEMENTING PORT-BASED TELEPORTATION\nWenowpresentalgorithmsforthePOVMsofdPBTandpPBT.Again, onlythreecasesrequireconsideration\nto cover the four regimes because the same POVM may be used for both maximally entangled resource state\nand optimised resource state in the case of dPBT [6]. As such, deterministic port-based teleportation is handled\nin one go in Section VA, pPBT with maximally entangled resource state is handled in Section VB1, and pPBT\nwith optimised resource state is handled in Section VB3. Additionally, we provide an alternative protocol for\npPBT with maximally entangled resource state in Section VB2. This algorithm is optimised for practicality for\nnear-term devices, where we use no amplitude amplification at all. This is at the expense of success probability\n- this algorithm unfortunately has an asymptotic success probability of1\n4, but in many cases this is acceptable,\nfor example if one has some (small) constant number of the input state. This section is independent of the\nprevious section - all results proved in the previous section that are required here will be quoted. We do,\nhowever, suggest that the following sections are read sequentially, since there are common themes between each\nalgorithm.\nWe will use in every instance the ( N+ 1)-qubit inverse coupling stage of the Schur transform (which recall\nis what many people simply refer to as the Schur transform)5, where Schur states |k, j, s, m ⟩are mapped to\nencodings of their various spins |k⟩|j⟩|s⟩|m⟩. In each case, for this operation, one could either choose to use\nthe algorithm of [21], which is slower but uses fewer ancillas, or [22], which is faster but uses more ancillas.\nWe mention one largely unimportant difference, which is that in the former case the register |k⟩is an encoding\ncontaining n−2qubits, and in the latter case it contains O(nlog(n))qubits.\nWe will not mention the differences between using these two algorithms for this operation for the remainder\nof this section to avoid confusion. We will simply say that we will perform the (inverse coupling stage for the)\nSchur transform. We will delegate the analysis of the various possibilities to Appendix A, where explicit gate\ncomplexities and ancilla counts will be discussed, and full tables showing these values may be found in the\nintroduction.\nWe will repeatedly use the technique of oblivious amplitude amplification as described in Theorem 28 of\n[32]. We state this now as in that work.\nTheorem V.1 (Theorem 28 of [32] on Robust Oblivious Amplitude Amplification) .Letn∈N+be odd, let\nϵ∈R+, letUbe a unitary, let ˜Π,Πbe orthogonal projectors, and let W:img(Π)→img(˜Π)be an isometry,\nsuch that\f\f\f\f\f\fsin\u0010π\n2n\u0011\nW|Ψ⟩ −˜ΠU|Ψ⟩\f\f\f\f\f\f≤ϵ (67)\nfor all |Ψ⟩ ∈img(Π). Then we can construct a unitary ˜Usuch that for all |Ψ⟩ ∈img(Π),\n\f\f\f\f\f\fW|Ψ⟩ −˜Π˜U|Ψ⟩\f\f\f\f\f\f≤2nϵ (68)\nwhich uses a single ancilla qubit, with nuses of UandU†,nuses of CΠNOTandnuses of C˜ΠNOTgates and\nnsingle qubit gates, where\nCΠNOT =X⊗Π +I⊗(I−Π). (69)\n5We go into the different definitions of the Schur transform, and provide useful translations between them in Appendix B, which\nwill likely be of use to those who come from different backgrounds.14\nWe note that in the case of ϵ= 0,Whas the same action as ˜Π˜Uon img (Π), and so in particular is an\nisometry mapping into img (˜Π). As a result, in the case ϵ= 0,˜Umust map into img (˜Π), since otherwise\nthe non-trivial action of ˜Πmakes ˜Π˜Unot an isometry. For the purposes of explaining the algorithms in this\nsection, we will assume everywhere that ϵ= 0, i.e. all of our gates can be performed perfectly, and will consider\ninaccuracies in the complexity analysis of Appendix A.\nWe also note that amplitude amplification, while an extraordinarily useful and flexible tool for algorithm\ndesign, is quite demanding in practical terms, and so one of our aims will be to keep the number of rounds of\namplitude amplification to a minimum i.e. keeping the integer nlow.\nWith a description of oblivious amplitude amplification given, we can state that the algorithms in each case\nshare the same broad framework. For the square-root measurement of dPBT, as implemented in Section VA,\nthe algorithms for pPBT with maximally entangled resource state in Sections VB1 and VB2 and the algorithm\nfor pPBT with optimised resource state in Section VB3, the steps of each may be loosely summarised as follows.\n1. Rotate to a shared eigenbasis of the POVM elements.\n2. Usinganancillaryqubit |r⟩, attachtheeigenvaluesofthePOVMelementsinthesubspace |r= 0⟩, rotating\neverything we don’t want into some “junk” subspace |r= 1⟩\n3. Amplify the |r= 0⟩subspace with oblivious amplitude amplification.\nThe exact meaning of each of these steps will become clearer in the coming sections, but we list them\nhere with the intention of commenting on their potential for generalisation. In certain cases, the method\nfor implementing the square-root measurement via the Petz recovery [12] runs inefficiently, including dPBT,\ndue to the exponentially small positive eigenvalues on the ensemble average state. To implement such an\noperation efficiently, one is obliged to exploit the shared symmetries of the state ensemble in question, as we\ndo here. These symmetries make both the rotation of Step 1, above, efficiently implementable, and also make\nthe eigenvalues highly degenerate, making the second step feasible. Our approach can therefore be seen as a\npotentially fruitful framework for implementing the square-root measurement for other highly symmetric state\nensembles exponentially more efficiently than the Petz recovery approach.\nWe lastly comment that we will use the same notation as the previous section, where we denote (N+1)-qubit\nSchur states by |k1, k2, ..., k N−2, j, s, m ⟩, where k1is the spin on the first two qubits, k2is the spin on the first\nthree qubits, and so on, until kN−2is the spin on the first N−1qubits (which for us is A1...AN−1) and jis\nthe spin on the first Nqubits (which for us is all of Alice’s ports A=A1...AN).sis then the spin on all N+ 1\nqubits (Alice’s ports and the input state, AAN+1) and mis the Z-spin on all N+ 1qubits. Again, when this\nnotation is too cumbersome, we will abbreviate it to |k, j, s, m ⟩where kstands for k1, ..., k N−2, and in cases\nwhere we want to address kN−2but none of the other k’s, we will write |˜k, kN−2, j, s, m ⟩where ˜kstands for\nk1, ..., k N−3. We emphasise the fact that each consecutive spin may only differ from each other by 1/2and\nmay not fall below zero as a consequence of the rules of addition of angular momentum in Equation (8). In\nparticular, to have s=N+1\n2, we must have k1= 1,k2=3\n2,k3= 2, and so on, up to j=N\n2.\nA. Deterministic Port-Based Teleportation\nStarting with a state |ψ⟩onN+ 1qubits, we attach an ancillary register |i⟩of dimension N(which, in\nparticular, may be made up of ⌈log(N)⌉qubits) with an orthonormal basis {|i⟩}N\ni=1. The goal is to implement\nthe operation\n|ψ⟩|1⟩i7→NX\ni=1p\nΠi|ψ⟩|i⟩ (70)\nwhere we recall here the eigendecomposition of each Πi. We have Πi=SWAP AiANΠNSWAP AiAN, and ΠN\nhas the set of eigenvectors with corresponding eigenvalues\n|k, j, s, m ⟩with s=N+ 1\n2i.e. maximal spin states have eigenvalue1\nN(71)\n|k, j, s, m ⟩with s <N+ 1\n2andkN−2̸=s have eigenvalue 0 (72)\nβ(s)p\nα(s)2+β(s)2|˜k, s, s−1\n2, s, m⟩ −α(s)p\nα(s)2+β(s)2|˜k, s, s +1\n2, s, m⟩have eigenvalue 0 (73)\nα(s)p\nα(s)2+β(s)2|˜k, s, s−1\n2, s, m⟩+β(s)p\nα(s)2+β(s)2|˜k, s, s +1\n2, s, m⟩have eigenvalue α(s)2+β(s)2.\n(74)15\nwhere we have\nα(s) =s\n2s\n(2s+ 1)( N+ 1−2s), β (s) =−s\n4s+ 4\n(2s+ 1)( N+ 3 + 2 s)(75)\nfrom which we compute\nα(s)2+β(s)2=4(N+ 1)\n(N+ 1−2s)(N+ 3 + 2 s). (76)\nThe first step of our algorithm will be to create the superposition\n|ψ⟩|1⟩i7→1√\nNNX\ni=1|ψ⟩|i⟩ (77)\nafter which we apply the operation\nC−SWAP =NX\ni=1SWAP AiAN⊗ |i⟩⟨i| (78)\nto obtain\n1√\nNNX\ni=1|ψi⟩|i⟩ (79)\nwhere |ψi⟩=SWAP AiAN|ψ⟩. Then, the idea is to implement√ΠNon the |ψi⟩, before applying C−SWAP\nagain. In order to implement√ΠN, we will need one further ancillary qubit in the register |r⟩, which we initialise\nto|0⟩6.\nNow that we have1√\nNPN\ni=1|ψi⟩|i⟩|0⟩r, the idea is simple. Rotate to the eigenbasis of ΠN, put the eigen-\nvalues of√ΠNonto the corresponding eigenvectors, rotating everything we don’t want into |r= 1⟩, and lastly\nuse amplitude amplification to amplify the |r= 0⟩space.\nTo do this explicitly, we need a little more notation so that we can capture more of the eigenvectors simul-\ntaneously. With s <N+1\n2, we make the following definitions:\n|˜k,−,−, s, m⟩=|˜k, s−1, s−1\n2, s, m⟩ (80)\n|˜k,+,−, s, m⟩=|˜k, s+ 1, s+1\n2, s, m⟩ (81)\n|˜k,−,+, s, m⟩=β(s)p\nα(s)2+β(s)2|˜k, s, s−1\n2, s, m⟩ −α(s)p\nα(s)2+β(s)2|˜k, s, s +1\n2, s, m⟩ (82)\n|˜k,+,+, s, m⟩=α(s)p\nα(s)2+β(s)2|˜k, s, s−1\n2, s, m⟩+β(s)p\nα(s)2+β(s)2|˜k, s, s +1\n2, s, m⟩ (83)\nwhich recall have respective eigenvalues 0,0,0andα(s)2+β(s)2under ΠN. With s=N+1\n2we simply define\n|˜k,+,+, s, m⟩=|˜k,N−1\n2,N\n2,N+ 1\n2, m⟩ (84)\nwhich have eigenvalues1\nNunder ΠN. Recall these are the maximal spin states and may only differ by their\nvalue of m. Note that in this latter expression, the choice of (+,+)is completely arbitrary, we just make some\nnotational choice so that |ψi⟩may be completely written out in terms of the eigenbasis of ΠN:\n|ψi⟩=X\n˜k,(a,b)∈{±}2,s,mc(i)\n˜k,a,b,s,m|˜k, a, b, s, m ⟩ (85)\nwhere we are only summing over the indices (˜k, a, b, s, m )that are allowed i.e. that produce valid eigenstates.\naandbare symbols each being summed over the set {±}.\n6This register is what allows us to create a block-encoding of√ΠN. Note one advantage of our algorithm is that it uses only one\nancillary qubit for the block-encoding as opposed to the logarithmic number used in [17].16\nWe can see that it won’t be too hard to rotate to the eigenbasis of ΠN. Indeed, we need only un-compute\nthe Schur coupling, and then do one simple rotation to straighten out the coefficients in Equations (82) and\n(83). Indeed, we start by doing the inverse of the coupling stage of the Schur transform, which again is what\nmany people simply refer to as the Schur transform, thus mapping each eigenvector in the following way:\n|˜k,−,−, s, m⟩ 7→ | ˜k⟩|s−1⟩|s−1\n2⟩|s⟩|m⟩ (86)\n|˜k,+,−, s, m⟩ 7→ | ˜k⟩|s+ 1⟩|s+1\n2⟩|s⟩|m⟩ (87)\n|˜k,−,+, s, m⟩ 7→β(s)p\nα(s)2+β(s)2|˜k⟩|s⟩|s−1\n2⟩|s⟩|m⟩ −α(s)p\nα(s)2+β(s)2|˜k⟩|s⟩|s+1\n2⟩|s⟩|m⟩(88)\n|˜k,+,+, s, m⟩ 7→α(s)p\nα(s)2+β(s)2|˜k⟩|s⟩|s−1\n2⟩|s⟩|m⟩+β(s)p\nα(s)2+β(s)2|˜k⟩|s⟩|s+1\n2⟩|s⟩|m⟩(89)\nifs <N+1\n2, whereas if s=N+1\n2, we just get\n|˜k,+,+, s, m⟩ 7→ | ˜k⟩|N−1\n2⟩|N\n2⟩|N+ 1\n2⟩|m⟩. (90)\nWe may then perform a rotation acting on the second, third and fourth registers of these expressions, with the\naim of just straightening out the coefficients in Equations (88) and (89). Explicitly,\n|N−1\n2⟩|N\n2⟩|N+ 1\n2⟩ 7→ |N−1\n2⟩|N\n2⟩|N+ 1\n2⟩ (91)\n|s−1⟩|s−1\n2⟩|s⟩ 7→ | s−1⟩|s−1\n2⟩|s⟩ (92)\n|s+ 1⟩|s+1\n2⟩|s⟩ 7→ | s+ 1⟩|s+1\n2⟩|s⟩ (93)\n|s⟩|s−1\n2⟩|s⟩ 7→β(s)p\nα(s)2+β(s)2|s⟩|s−1\n2⟩|s⟩+α(s)p\nα(s)2+β(s)2|s⟩|s+1\n2⟩|s⟩ (94)\n|s⟩|s+1\n2⟩|s⟩ 7→−α(s)p\nα(s)2+β(s)2|s⟩|s−1\n2⟩|s⟩+β(s)p\nα(s)2+β(s)2|s⟩|s+1\n2⟩|s⟩ (95)\nso that in total we have acted on the eigenstates of ΠNas\n|˜k,−,−, s, m⟩ 7→ | ˜k⟩|s−1⟩|s−1\n2⟩|s⟩|m⟩ (96)\n|˜k,+,−, s, m⟩ 7→ | ˜k⟩|s+ 1⟩|s+1\n2⟩|s⟩|m⟩ (97)\n|˜k,−,+, s, m⟩ 7→ | ˜k⟩|s⟩|s−1\n2⟩|s⟩|m⟩ (98)\n|˜k,+,+, s, m⟩ 7→ | ˜k⟩|s⟩|s+1\n2⟩|s⟩|m⟩. (99)\nfors <N+1\n2, whereas if s=N+1\n2,\n|˜k,+,+, s, m⟩ 7→ | ˜k⟩|N−1\n2⟩|N\n2⟩|N+ 1\n2⟩|m⟩. (100)\nRecalling that adjacent spins may only differ from each other by 1/2, we note that given a fixed s, the value of\nthe third register in Equations (96) - (100) may only be s±1\n2. Similarly, give some value jfor the third register,\nthe second register may only take two values again: j±1\n2. Since the encoding of one of two values may be done\nby a single qubit, we can therefore compress the second and third registers into one qubit each, mapping\n|s−1⟩|s−1\n2⟩|s⟩ 7→ |−⟩|−⟩| s⟩ (101)\n|s+ 1⟩|s+1\n2⟩|s⟩ 7→ | +⟩|−⟩| s⟩ (102)\n|s⟩|s−1\n2⟩|s⟩ 7→ |−⟩| +⟩|s⟩ (103)\n|s⟩|s+1\n2⟩|s⟩ 7→ | +⟩|+⟩|s⟩ (104)17\nfors <N+1\n2and if s=N+1\n2,\n|N−1\n2⟩|N\n2⟩|N+ 1\n2⟩ 7→ | +⟩|+⟩|N+ 1\n2⟩. (105)\nIn total, we have mapped\n|ψi⟩=X\n˜k,a,b,s,mc(i)\n˜k,a,b,s,m|˜k, a, b, s, m ⟩ 7→X\n˜k,a,b,s,mc(i)\n˜k,a,b,s,m|˜k⟩|a⟩|b⟩|s⟩|m⟩. (106)\nIt is now time to attach the eigenvalues of√ΠN. We write the block-encoding ancilla |r= 0⟩back in, which of\ncourse has been there the entire time, and perform the following. If s <N+1\n2, do\n|−⟩|−⟩| s⟩|0⟩r7→ |−⟩|−⟩| s⟩|1⟩r (107)\n|+⟩|−⟩| s⟩|0⟩r7→ |+⟩|−⟩| s⟩|1⟩r(108)\n|−⟩|+⟩|s⟩|0⟩r7→ |−⟩| +⟩|s⟩|1⟩r(109)\n|+⟩|+⟩|s⟩|0⟩r7→p\nα(s)2+β(s)2|+⟩|+⟩|s⟩|0⟩r+p\n1−(α(s)2+β(s)2)|+⟩|+⟩|s⟩|1⟩r(110)\nwhere we can check that α(s)2+β(s)2≤1fors <N+1\n2, actually achieving equality with s=N−1\n2, and lastly\nfors=N+1\n2,\n|+⟩|+⟩|N+ 1\n2⟩|0⟩r7→1√\nN|+⟩|+⟩|N+ 1\n2⟩|0⟩r+r\n1−1\nN|+⟩|+⟩|N+ 1\n2⟩|1⟩r(111)\nwhere we see we have attached the exact eigenvalues we want for√ΠN. Therefore, now undoing the whole\nrotation to the eigenbasis of Equation (106), we have in total achieved\n|ψ⟩|1⟩i|0⟩r7→1√\nNNX\ni=1|ψi⟩|i⟩|0⟩r7→1√\nNNX\ni=1p\nΠN|ψi⟩|i⟩|0⟩r+|∗⟩|1⟩r(112)\nwhere |∗⟩is an unimportant state to us. Lastly, acting with C−SWAPagain gives\n1√\nNNX\ni=1p\nΠi|ψ⟩|i⟩|0⟩r+|∗⟩|1⟩r. (113)\nIt only remains to perform amplitude amplification to amplify the |r= 0⟩eigenspace. Indeed, using the notation\nof Theorem V.1, which in turn uses the notation of [32], we set Π =|i= 1⟩⟨i= 1| ⊗ |r= 0⟩⟨r= 0|and˜Π =\n|r= 0⟩⟨r= 0|, and define the isometry W:img(Π)→img(˜Π)by\n|ψ⟩|i= 1⟩|r= 0⟩ 7→NX\ni=1p\nΠi|ψ⟩|i⟩|r= 0⟩. (114)\nUp until now, we have constructed a unitary Ufor which\n˜ΠU(|ψ⟩|i= 1⟩|r= 0⟩) =1√\nNW(|ψ⟩|i= 1⟩|r= 0⟩). (115)\nNote that in an ideal world we would have1√\nN= sin\u0000π\n2n\u0001\nwith nan odd integer to apply Theorem V.1, and we\nusually will not. This can be dealt with in a straightforward way by just letting the number c∗be the smallest\nnumber greater than or equal to 1 such that there is an odd integer nfor which1\nc∗√\nN= sin\u0000π\n2n\u0001\n, and then\nmodifying the rotations of Equations (110) and (111) so that we actually end up with\n1\nc∗√\nNNX\ni=1p\nΠi|ψ⟩|i⟩|0⟩r+|∗⟩|1⟩r. (116)\nWith this detail, we simply note that we need to perform n=O(√\nN)rounds of amplitude amplification to\nproduce the desired final statePN\ni=1√Πi|ψ⟩|i⟩|r= 0⟩, which we can do via the prescription of Theorem 28 of\n[32], noting that in our case the choices of Πand˜Πmake CΠNOTandC˜ΠNOTeasy to perform.18\nB. Probabilistic Port-Based Teleportation\n1. pPBT with Maximally Entangled Resource State - Algorithm 1\nIn this case, we have a state on N+ 1qubits and, because we have an (N+ 1)-element POVM, we attach\nan ancillary register |i⟩of dimension N+ 1, and the goal is to implement\n|ψ⟩|1⟩i7→N+1X\ni=1p\nΠi|ψ⟩|i⟩. (117)\nAgain, we will use one ancillary qubit in the register |r⟩to do the block-encoding, initialised in the state |0⟩,\nwhich we do not write for now. We will find that, because of the form of the POVM elements in this case, we\nare able to do this particularly efficiently - requiring only a (small) constant number of rounds of amplitude\namplification, in contrast to the previous subsection. We recall that in this case, ΠNhas the eigendecomposition\n|k, j, s, m ⟩with kN−2̸=s have eigenvalue 0 (118)\nr\ns+ 1\n2s+ 1|˜k, s, s−1\n2, s, m⟩+rs\n2s+ 1|˜k, s, s +1\n2, s, m⟩have eigenvalue 0 (119)\nrs\n2s+ 1|˜k, s, s−1\n2, s, m⟩ −r\ns+ 1\n2s+ 1|˜k, s, s +1\n2, s, m⟩have eigenvalue σ(s) (120)\nwhere σ(s) =4\nN+3+2 sand for i= 1, ..., N −1,Πi=SWAP AiANΠNSWAP AiAN. We then also have the\neigendecomposition of ΠN+1, which is\n|k, j, s, m ⟩with s=j+1\n2have eigenvalue τ(s) (121)\n|k, j, s, m ⟩with s=j−1\n2have eigenvalue 0 (122)\nwhere τ(s) =2(2s+1)\nN+3+2 s.\nIn this case, we start by creating the superposition\n|ψ⟩|1⟩i7→1√\n2NNX\ni=1|ψ⟩|i⟩+1√\n2|ψ⟩|N+ 1⟩. (123)\nThis form of the initial superposition is chosen carefully so that we only need to do a constant number of rounds\nof amplitude amplification later on, making our algorithm quite efficient. We use a similar strategy to last time,\nnow applying\nC−SWAP =NX\ni=1SWAP AiAN⊗ |i⟩⟨i|+I⊗ |N+ 1⟩⟨N+ 1| (124)\nto get\n1√\n2NNX\ni=1|ψi⟩|i⟩+1√\n2|ψ⟩|N+ 1⟩, (125)\nwhere again |ψi⟩=SWAP AiAN|ψ⟩. We start by applying√ΠNon the first term before then applyingp\nΠN+1\non the second.\nPursuing a similar treatment to last time, we make definitions for the eigenvectors of ΠN, which are slightly\ncleaner this time because we do not need to differentiate between the cases s=N+1\n2ands <N+1\n2. We define\n|˜k,−,−, s, m⟩=|˜k, s−1, s−1\n2, s, m⟩ (126)\n|˜k,+,−, s, m⟩=|˜k, s+ 1, s+1\n2, s, m⟩ (127)\n|˜k,−,+, s, m⟩=r\ns+ 1\n2s+ 1|˜k, s, s−1\n2, s, m⟩+rs\n2s+ 1|˜k, s, s +1\n2, s, m⟩ (128)\n|˜k,+,+, s, m⟩=rs\n2s+ 1|˜k, s, s−1\n2, s, m⟩ −r\ns+ 1\n2s+ 1|˜k, s, s +1\n2, s, m⟩ (129)19\nwhich have eigenvalues 0,0,0andσ(s)respectively under ΠN. We once again express |ψi⟩in this basis and will\nagain perform the operation\n|ψi⟩=X\n˜k,(a,b)∈{±}2,s,mc(i)\n˜k,a,b,s,m|˜k, a, b, s, m ⟩ 7→X\n˜k,a,b,s,mc(i)\n˜k,a,b,s,m|˜k⟩|a⟩|b⟩|s⟩|m⟩ (130)\n(where the second and third registers on the right-hand side are each on one qubit). We describe this briefly,\nbecause it is so similar to the previous case, in fact simpler because we do not have to differentiate between\ns=N+1\n2ands <N+1\n2.\nTo do the operation described in Equation (130), one starts by performing the inverse of the coupling stage\nof the Schur transform, which again is referred to simply as the Schur transform in other cases. We then perform\na similar rotation to that described in Equations (91) to (95) on the middle three of the five registers. This will\nbe\n|s−1⟩|s−1\n2⟩|s⟩ 7→ | s−1⟩|s−1\n2⟩|s⟩ (131)\n|s+ 1⟩|s+1\n2⟩|s⟩ 7→ | s+ 1⟩|s+1\n2⟩|s⟩ (132)\n|s⟩|s−1\n2⟩|s⟩ 7→r\ns+ 1\n2s+ 1|s⟩|s−1\n2⟩|s⟩+rs\n2s+ 1|s⟩|s+1\n2⟩|s⟩ (133)\n|s⟩|s+1\n2⟩|s⟩ 7→rs\n2s+ 1|s⟩|s−1\n2⟩|s⟩ −r\ns+ 1\n2s+ 1|s⟩|s+1\n2⟩|s⟩. (134)\nFinally, we can just compress the second and third registers into one qubit each in the same way as last time7.\nWe have therefore performed the operation of Equation (130).\nNext, we add on the eigenvalues of√ΠNin a very similar way. Reintroducing the register |r⟩, which we\nrecall is initialised in the state |0⟩, we perform the following operation on the registers |a⟩|b⟩|s⟩|r⟩:\n|−⟩|−⟩| s⟩|0⟩r7→ |−⟩|−⟩| s⟩|1⟩r(135)\n|+⟩|−⟩| s⟩|0⟩r7→ |+⟩|−⟩| s⟩|1⟩r(136)\n|−⟩|+⟩|s⟩|0⟩r7→ |−⟩| +⟩|s⟩|1⟩r (137)\n|+⟩|+⟩|s⟩|0⟩r7→r\nN\n4σ(s)|+⟩|+⟩|s⟩|0⟩r+s\n1−\u0012N\n4σ(s)\u00132\n|+⟩|+⟩|s⟩|1⟩r(138)\nwhere we note thatN\n4σ(s)≤N\nN+3<1. This exact choice of the coefficientq\nN\n4σ(s)is also important for\nmaking the algorithm for efficient, avoiding a large amount of amplitude amplification. Having attached these\neigenvalues, we undo the operation of Equation (130) so that we now have\n1\n2√\n2NX\ni=1p\nΠN|ψi⟩|i⟩|0⟩r+|∗⟩|1⟩r+1√\n2|ψ⟩|N+ 1⟩. (139)\nNotice that this pre-factor on the first term has appeared because of the choice of the coefficientq\nN\n4σ(s)in\nEquation (138). Applying C−SWAPagain gives us\n1\n2√\n2NX\ni=1p\nΠi|ψ⟩|i⟩|0⟩r+|∗⟩|1⟩r+1√\n2|ψ⟩|N+ 1⟩. (140)\nWe now treat the latter term. This is much more straightforward. Writing |ψ⟩in the usual Schur basis, we may\nundo the Schur coupling as follows:\n|ψ⟩=X\nk,j,s,mck,j,s,m |k, j, s, m ⟩ 7→X\nk,j,s,mck,j,s,m |k⟩|j⟩|s⟩|m⟩. (141)\nThen, reintroducing the ancilla |r⟩, which again is initialised to the state |0⟩, we may act on the registers\n|j⟩|s⟩|r⟩as follows (recalling that for a fixed s,jmay only take values in {s±1\n2}):\n7There is one essentially unimportant detail to mention here. In this case, the entire quantum state is made up of |ψ⟩|N+ 1⟩as\nwell asPN\ni=1|ψi⟩|i⟩so we can’t strictly talk about registers actually changing size on just the left term. However, we can talk\nabout qubits being added to the left-hand term when strictly they are being added to the whole expression and staying in a fixed\nstate|0⟩on the right-hand term. Similarly, removing qubits from the left-hand term means they are returned to a fixed state |0⟩\non this term and then remain untouched.20\n|s−1\n2⟩|s⟩|0⟩r7→1\n2p\nτ(s)|s−1\n2⟩|s⟩|0⟩r+r\n1−τ(s)\n4|s−1\n2⟩|s⟩|1⟩r (142)\n|s+1\n2⟩|s⟩|0⟩r7→ |s+1\n2⟩|s⟩|1⟩r(143)\nnoting that τ(s)≤1, in fact achieving equality for s=N+1\n2. The factor of1\n2in Equation (142) may seem\nsuperfluous at first - the reason for this will become clear very soon. Undoing the operation of Equation (141),\nwe see that we have obtained overall\n1\n2√\n2NX\ni=1p\nΠi|ψ⟩|i⟩|0⟩r+1\n2√\n2p\nΠN+1|ψ⟩|N+ 1⟩+|∗⟩|1⟩r=1\n2√\n2N+1X\ni=1p\nΠi|ψ⟩|i⟩|0⟩r+|∗⟩|1⟩r(144)\nwhere now the factor of1\n2in Equation (142) makes sense - it was so that there is a common pre-factor between\nthe first and second terms of the left-hand side of Equation (144).\nIt then remains to do the amplitude amplification in exactly the same way as last time. We again define\nΠ =|i= 1⟩⟨i= 1| ⊗ |r= 0⟩|r= 0⟩and ˜Π =|r= 0⟩⟨r= 0|and the isometry W:img(Π)→img(˜Π)via\n|ψ⟩|1⟩i|0⟩r7→PN+1\ni=1√Πi|ψ⟩|i⟩|0⟩r. Calling the unitary that we have constructed U, we see that we have\n˜ΠU(|ψ⟩|i= 1⟩|r= 0⟩) =1\n2√\n2W(|ψ⟩|i= 1⟩|r= 0⟩) (145)\nandso, becausewehavebeencarefulwithourdefinitions, weonlyneedaconstantnumberofroundsofamplitude\namplification in this case. Because sin\u0000π\n10\u0001\n<1\n2√\n2<sin\u0000π\n6\u0001\n, exactly n= 5rounds of amplitude amplification\nare required in this case. Note that in order to be precise, we must adjust the constants in Equations (138) and\n(142) so that the unitary Uactually gives us\nsin\u0010π\n10\u0011N+1X\ni=1p\nΠi|ψ⟩|i⟩|0⟩r+|∗⟩|1⟩r, (146)\nin Equation (144) but this is not done above to simplify the presentation.\n2. pPBT with Maximally Entangled Resource State - Algorithm 2\nWenowincludeafurtheralgorithmforpPBTwithmaximallyentangledresourcestate, whichweonlyinclude\nfor the sake of practicality. We saw in the previous subsection that pPBT can be executed with maximally\nentangled resource state using only five rounds of amplitude amplification, which is quite practical. We might\nask how well we can do with no amplitude amplification at all, to maximise the possibility of practical PBT for\nnear-term devices.\nIf we turn to Equation (144), we could start by projectively measuring the |r⟩register, obtaining the desired\noutcome of r= 0with probability1\n8. Upon such a success, one may measure the |i⟩register, succeeding in\nteleporting the state with the usual success probability of ⟨ψ|PN\ni=1Πi|ψ⟩(and note that we get the post-\nmeasurement state that we would usually expect upon a success, which is√Πi|ψ⟩, up to normalisation).\nTherefore, in total we can perform pPBT with a success probability of1\n8of the usual success probability, and\nin particular our asymptotic (average) success probability is1\n8. We will show that it is actually possible to do\nbetter than this - succeeding with asymptotic probability1\n4, using no amplitude amplification. We describe this\nquite briefly given that there are several similarities to the previous case.\nWith|ψ⟩an(N+ 1)-qubit state, we attach a 2N-dimensional ancillary register |i⟩with orthonormal basis\n{|i⟩}2N\ni=1. We will prepare a state\n1\n2NX\ni=1p\nΠi|ψ⟩|i⟩+2NX\ni=N+1|∗i⟩|i⟩ (147)\nwhere the exact identities of |∗i⟩are unimportant to us. To do this, we start by creating the superposition\n|ψ⟩|1⟩i7→1√\nNNX\ni=1|ψ⟩|i⟩ (148)\nand apply\nC−SWAP =NX\ni=1SWAP AiAN⊗ |i⟩⟨i|+I⊗2NX\ni=N+1|i⟩⟨i| (149)21\nto obtain\n1√\nNNX\ni=1|ψi⟩|i⟩ (150)\nwhere once again |ψi⟩=SWAP AiAN|ψ⟩. We now perform a very similar operation to before, writing |ψi⟩in\nterms of the eigenbasis of ΠNand rotating to this basis:\n|ψi⟩=X\n˜k,(a,b)∈{±}2,s,mc(i)\n˜k,a,b,s,m|˜k, a, b, s, m ⟩ 7→X\n˜k,a,b,s,mc(i)\n˜k,a,b,s,m|˜k⟩|a⟩|b⟩|s⟩|m⟩. (151)\nIn previous versions, what we’ve now done is attached the eigenvalues that we do want to the |r= 0⟩part\nand rotated everything we don’t want onto the |r= 1⟩part. Here, we have no |r⟩register, and in fact do the\nfollowing, operating on the registers |a⟩|b⟩|s⟩|i⟩:\n|−⟩|−⟩| s⟩|i⟩ 7→ |−⟩|−⟩| s⟩|N+i⟩ (152)\n|+⟩|−⟩| s⟩|i⟩ 7→ | +⟩|−⟩| s⟩|N+i⟩ (153)\n|−⟩|+⟩|s⟩|i⟩ 7→ |−⟩| +⟩|s⟩|N+i⟩ (154)\n|+⟩|+⟩|s⟩|i⟩ 7→r\nN\n4σ(s)|+⟩|+⟩|s⟩|i⟩+s\n1−\u0012N\n4σ(s)\u00132\n|+⟩|+⟩|s⟩|N+i⟩ (155)\nfori= 1, ..., N, so that we are now putting everything we don’t want onto {|i⟩}2N\ni=N+1. Undoing the operation\nof Equation (151) and performing C-SWAP once again leaves us with the desired state of Equation (147).\nFor this state, we may attempt to perform the POVM by measuring the |i⟩register projectively, obtaining\nan outcome in {1, ..., N}with probability1\n4⟨ψ|PN\ni=1Πi|ψ⟩,1\n4of the usual success probability. Note also that\nupon such a successful measurement, one also obtains the post-measurement state normally expected from PBT:√Πi|ψ⟩, uptonormalisation. Itwouldbeinterestingtoknowifonecandobetterthanthisasymptotic(average)\nsuccess probability of1\n4using similar means without any amplitude amplification or similar operations. The\nfactor of1\n4originates from the need to haveN\n4σ(s)≤1in Equation (155). Is there some deeper reason why the\neigenvalue σ(s)must be of this order and no smaller? We discuss this and similar ideas a little more in Section\nVI.\n3. pPBT with Optimised Resource State\nLastly, we show how to efficiently implement the POVM associated with pPBT with optimised resource\nstate. This will bear significant resemblance to pPBT with maximally entangled resource state. Recall that in\nthis case, our POVM is made up of N+ 1elements {Πi}N+1\ni=1, where ΠNhas the following eigendecomposition:\n|k, j, s, m ⟩with kN−2̸=s have eigenvalue 0 (156)\nδ(s)p\nγ(s)2+δ(s)2|˜k, s, s−1\n2, s, m⟩ −γ(s)p\nγ(s)2+δ(s)2|˜k, s, s +1\n2, s, m⟩have eigenvalue 0 (157)\nγ(s)p\nγ(s)2+δ(s)2|˜k, s, s−1\n2, s, m⟩+δ(s)p\nγ(s)2+δ(s)2|˜k, s, s +1\n2, s, m⟩have eigenvalue u(s)\u0000\nγ(s)2+δ(s)2\u0001\n(158)\nwhere\nγ(s) =rs\n(2s+ 1)ν(s−1\n2)δ(s) =−s\ns+ 1\n(2s+ 1)ν(s+1\n2)u(s) =2N+1h(N)(2s+ 1)\nNg[N−1](s)(159)\nand\nν(j) =2Nh(N)(2j+ 1)\ng[N](j)g[N](j) =(2j+ 1)N!\u0000N\n2−j\u0001\n!\u0000N\n2+ 1 + j\u0001\n!h(N) =6\n(N+ 1)( N+ 2)( N+ 3)(160)\nand once again we have that Πi=SWAP AiANΠNSWAP AiANfori= 1, ..., N −1. The eigendecomposition for\nΠN+1may then be easily stated. The operator has an eigenbasis |k, j, s, m ⟩with corresponding eigenvalues\n1−2N−1λ(j, s)u(s)\nν(j)where λ(j, s) =(\n1\n2N\u0000N\n2−j\u0001\nifs=j+1\n2\n1\n2N\u0000N\n2+j+ 1\u0001\nifs=j−1\n2. (161)22\nWe use a very similar strategy to pPBT with maximally entangled resource state, except this time there is\nno benefit to be gained from the superposition of Equation (125) because too many of the eigenvalues are of\nconstant magnitude in this case. We therefore simply start by creating the superposition\n|ψ⟩|1⟩i7→1√\nN+ 1N+1X\ni=1|ψ⟩|i⟩ (162)\nwhere |i⟩is an ancillary register of dimension N+ 1and|ψ⟩is an (N+ 1)-qubit state. We act with C−SWAP\nas in Equation (124) to get\n1√\nN+ 1NX\ni=1|ψi⟩|i⟩+1√\nN+ 1|ψ⟩|N+ 1⟩ (163)\nwhere, as before, |ψi⟩=SWAP AiAN|ψ⟩. We then make the expected definitions for the eigenbasis of ΠN:\n|˜k,−,−, s, m⟩=|˜k, s−1, s−1\n2, s, m⟩ (164)\n|˜k,+,−, s, m⟩=|˜k, s+ 1, s+1\n2, s, m⟩ (165)\n|˜k,−,+, s, m⟩=δ(s)p\nγ(s)2+δ(s)2|˜k, s, s−1\n2, s, m⟩ −γ(s)p\nγ(s)2+δ(s)2|˜k, s, s +1\n2, s, m⟩ (166)\n|˜k,+,+, s, m⟩=γ(s)p\nγ(s)2+δ(s)2|˜k, s, s−1\n2, s, m⟩+δ(s)p\nγ(s)2+δ(s)2|˜k, s, s +1\n2, s, m⟩ (167)\nand, writing |ψi⟩in terms of this eigenbasis, we rotate to it using the usual prescription of inverting the Schur\ncoupling and performing a rotation and compression on the |a⟩|b⟩|s⟩registers:\n|ψi⟩=X\n˜k,(a,b)∈{±}2,s,mc(i)\n˜k,a,b,s,m|˜k, a, b, s, m ⟩ 7→X\n˜k,(a,b)∈{±}2,s,mc(i)\n˜k,a,b,s,m|˜k⟩|a⟩|b⟩|s⟩|m⟩(168)\nWe use, as always, one qubit in an ancillary register |r⟩to perform the block-encoding. We act on the registers\n|a⟩|b⟩|s⟩|r⟩in the expected fashion:\n|−⟩|−⟩| s⟩|0⟩r7→ |−⟩|−⟩| s⟩|1⟩r (169)\n|+⟩|−⟩| s⟩|0⟩r7→ |+⟩|−⟩| s⟩|1⟩r(170)\n|−⟩|+⟩|s⟩|0⟩r7→ |−⟩| +⟩|s⟩|1⟩r(171)\n|+⟩|+⟩|s⟩|0⟩r7→p\nu(s) (γ(s)2+δ(s)2)|+⟩|+⟩|s⟩|0⟩r+q\n1−u(s)2(γ(s)2+δ(s)2)2|+⟩|+⟩|s⟩|1⟩r(172)\nwhere we are guaranteed that u(s)\u0000\nγ(s)2+δ(s)2\u0001\n≤1by virtue of the fact that {Πi}N+1\ni=1is a valid POVM and\nsoΠN≤I. Undoing the operation of Equation (168) before applying C−SWAPagain gives us\n1√\nN+ 1NX\ni=1p\nΠi|ψ⟩|i⟩|0⟩r+|∗⟩|1⟩r+1√\nN+ 1|ψ⟩|N+ 1⟩ (173)\nwhere the identity of |∗⟩is unimportant to us. We then treatp\nΠN+1in a similar way to in the case of pPBT\nwith maximally entangled resource state. We may write |ψ⟩in terms of the usual Schur basis and undo the\nSchur coupling:\n|ψ⟩=X\nk,j,s,mck,j,s,m |k, j, s, m ⟩ 7→X\nk,j,s,mck,j,s,m |k⟩|j⟩|s⟩|m⟩ (174)\nbefore acting on |j⟩|s⟩|r⟩to attach the eigenvalues as usual:\n|j⟩|s⟩|0⟩r7→s\n1−2N−1λ(j, s)u(s)\nν(j)|j⟩|s⟩|0⟩r+s\n1−\u0012\n1−2N−1λ(j, s)u(s)\nν(j)\u00132\n|j⟩|s⟩|1⟩r.(175)\nwhere we are again guaranteed that 0≤1−2N−1λ(j,s)u(s)\nν(j)≤1given that 0≤ΠN+1≤1since{Πi}N+1\ni=1is a\nvalid POVM. By then undoing the operation of Equation (174), we get\n1√\nN+ 1N+1X\ni=1p\nΠi|ψ⟩|i⟩|0⟩r+|∗⟩|1⟩r(176)\nwhich we may then amplify. With the exact same treatment as in the case of dPBT, we may amplify the r= 0\neigenspace to create the desired statePN+1\ni=1√Πi|ψ⟩|i⟩|0⟩rusing n=O(√\nN)rounds of oblivious amplitude\namplification.23\nVI. DISCUSSION\nIn this work, we presented efficient quantum algorithms for the optimal POVMs for the four regimes of port-\nbased teleportation defined in Ishizaka and Hiroshima’s work [4]. We present explicit gate complexities and\nancilla counts for each protocol. One very interesting question surrounds the given gate complexities, where we\nhave seen that these bounds vary between the paradigms due to the amount of amplitude amplification required.\nWe saw that pPBT with maximally entangled resource state was implemented more efficiently than the other\ntwo cases arising from the lower need for amplitude amplification. This came from the use of the superposition\nof Equation (125) and the rotation of Equation (138) and this, in turn, was possible because of the magnitude\nof the eigenvalues of the POVM operators in this case being of the order of1\nN(for all but ΠN+1), as opposed\nto the other regimes where some of the eigenvalues of all the POVM operators are of constant order. It seems,\ntherefore, that greater efficiency is possible with these methods in these cases where the POVM operators have\nsmall eigenvalues.\nIt is known that the optimisation problems relevant to PBT can have multiple solutions - indeed dPBT\nwith optimised resource state does have multiple optimising POVMs and resource states, as discussed. Morally,\nthe optimisation problem does not care about gate complexity or efficiency generally. Therefore, it might\nbe interesting to view the relevant optimisation problems from a complexity theoretic standpoint and see if\nalternative optimising solutions exist that allow for more efficient algorithms.\nEven further, one could consider non-optimal solutions that make up for their lack of optimality with greater\nefficiency - this is in the spirit of our alternative algorithm for pPBT with maximally entangled resource state\n(Section VB2) which required no amplitude amplification at all, but had asymptotic success probability only\n1\n4.\nFinally, we note that one problem that remains open is the efficient implementation of multi-port-based\nteleportation [33–35], which allows for the teleportation of composite systems with greater efficiency than\nrepeated, or ‘packaged’ ordinary PBT. Whether this can be described via the usual Schur-Weyl duality relied\non in this work, or requires ‘twisted’ Schur-Weyl duality [17–19], is unknown.\nAcknowledgements : The authors wish to thank Dmitry Grinko and M¯ aris Ozols for informative and lengthy\ndiscussions both on their PBT algorithms and the differing notions of Schur transforms that exist. Thanks is\nalso extended to Felix Leditzky for discussions on the optimality of the pretty good measurement for dPBT.\nSS acknowledges support from the Royal Society University Research Fellowship and “Quantum Simulation\nAlgorithms for Quantum Chromodynamics” grant (ST/W006251/1).\n[1] C. H. Bennett, G. Brassard, C. Crépeau, R. Jozsa, A. Peres, and W. K. Wootters, Teleporting an unknown quantum\nstate via dual classical and einstein-podolsky-rosen channels, Physical review letters 70, 1895 (1993).\n[2] E. Knill, R. Laflamme, and G. J. Milburn, A scheme for efficient quantum computation with linear optics, nature\n409, 46 (2001).\n[3] S. Ishizaka and T. Hiroshima, Asymptotic teleportation scheme as a universal programmable quantum processor,\nPhysical review letters 101, 240501 (2008).\n[4] S. Ishizaka and T. Hiroshima, Quantum teleportation scheme by selecting one of multiple output ports, Physical\nReview A 79, 042306 (2009).\n[5] M. A. Nielsen and I. L. Chuang, Programmable quantum gate arrays, Physical Review Letters 79, 321 (1997).\n[6] F. Leditzky, Optimality of the pretty good measurement for port-based teleportation, Letters in Mathematical\nPhysics112, 98 (2022).\n[7] Y. C. Eldar and G. D. Forney, On quantum detection and the square-root measurement, IEEE Transactions on\nInformation Theory 47, 858 (2001).\n[8] N. Dalla Pozza and G. Pierobon, Optimality of square-root measurements in quantum state discrimination, Physical\nReview A 91, 042334 (2015).\n[9] P. Hausladen and W. K. Wootters, A ‘pretty good’measurement for distinguishing quantum states, Journal of\nModern Optics 41, 2385 (1994).\n[10] A. S. Holevo, On asymptotically optimal hypotheses testing in quantum statistics, Teoriya Veroyatnostei i ee Prime-\nneniya23, 429 (1978).\n[11] A. W. Harrow and A. Winter, How many copies are needed for state discrimination?, IEEE transactions on infor-\nmation theory 58, 1 (2012).\n[12] A. Gilyén, S. Lloyd, I. Marvian, Y. Quek, and M. M. Wilde, Quantum algorithm for petz recovery channels and\npretty good measurements, Physical Review Letters 128, 220502 (2022).\n[13] S. Beigi and R. König, Simplified instantaneous non-local quantum computation with applications to position-based\ncryptography, New Journal of Physics 13, 093036 (2011).\n[14] A. May, Quantum tasks in holography, Journal of High Energy Physics 2019, 1 (2019).\n[15] A. May, Complexity and entanglement in non-local computation and holography, Quantum 6, 864 (2022).24\n[16] S. Pirandola, R. Laurenza, C. Lupo, and J. L. Pereira, Fundamental limits to quantum channel discrimination, npj\nQuantum Information 5, 50 (2019).\n[17] J. Fei, S. Timmerman, and P. Hayden, Efficient quantum algorithm for port-based teleportation, arXiv preprint\narXiv:2310.01637 (2023).\n[18] D.Grinko, A.Burchardt,andM.Ozols,Gelfand-tsetlinbasisforpartiallytransposedpermutations, withapplications\nto quantum information, arXiv preprint arXiv:2310.02252 (2023).\n[19] Q. T. Nguyen, The mixed schur transform: efficient quantum circuit and applications, arXiv preprint\narXiv:2310.01613 (2023).\n[20] S. P. Jordan, Permutational quantum computing, arXiv preprint arXiv:0906.2508 (2009).\n[21] A. Wills and S. Strelchuk, Generalised coupling and an elementary algorithm for the quantum schur transform,\narXiv preprint arXiv:2305.04069 (2023).\n[22] D. Bacon, I. L. Chuang, and A. W. Harrow, The quantum schur transform: I. efficient qudit circuits, arXiv preprint\nquant-ph/0601001 (2005).\n[23] D. Grinko, A. Burchardt, and M. Ozols, Efficient quantum circuits for port-based teleportation, arXiv preprint\narXiv:2312.03188 (2023).\n[24] A. W. Harrow, Applications of coherent classical communication and the schur transform to quantum information\ntheory, arXiv preprint quant-ph/0512255 (2005).\n[25] V. Havlíček, S. Strelchuk, and K. Temme, Classical algorithm for quantum su (2) schur sampling, Physical Review\nA99, 062336 (2019).\n[26] Wikipedia contributors, Clebsch–gordan coefficients — Wikipedia, the free encyclopedia (2023), [Online; accessed\n7-November-2023].\n[27] Weisstein, Eric W., Wigner 3j-symbol (2022).\n[28] A. Marzuoli and M. Rasetti, Computing spin networks, Annals of Physics 318, 345 (2005).\n[29] H. Zheng, Z. Li, J. Liu, S. Strelchuk, and R. Kondor, On the super-exponential quantum speedup of equivariant\nquantum machine learning algorithms with su ( d) symmetry, arXiv preprint arXiv:2207.07250 (2022).\n[30] W. M. Kirby and F. W. Strauch, A practical quantum algorithm for the schur transform, arXiv preprint\narXiv:1709.07119 (2017).\n[31] H. Krovi, An efficient high dimensional quantum schur transform, Quantum 3, 122 (2019).\n[32] A. Gilyén, Y. Su, G. H. Low, and N. Wiebe, Quantum singular value transformation and beyond: exponential\nimprovements for quantum matrix arithmetics, in Proceedings of the 51st Annual ACM SIGACT Symposium on\nTheory of Computing (2019) pp. 193–204.\n[33] P. Kopszak, M. Mozrzymas, M. Studziński, and M. Horodecki, Multiport based teleportation–transmission of a large\namount of quantum information, Quantum 5, 576 (2021).\n[34] M. Mozrzymas, M. Studziński, and P. Kopszak, Optimal multi-port-based teleportation schemes, Quantum 5, 477\n(2021).\n[35] M. Studziński, M. Mozrzymas, P. Kopszak, and M. Horodecki, Efficient multi port-based teleportation schemes,\nIEEE Transactions on Information Theory 68, 7892 (2022).\n[36] A. Alex, M. Kalus, A. Huckleberry, and J. von Delft, A numerical algorithm for the explicit calculation of su (n)\nand sl (n, c) clebsch–gordan coefficients, Journal of Mathematical Physics 52, 023507 (2011).\n[37] V. Havlíček and S. Strelchuk, Quantum schur sampling circuits can be strongly simulated, Physical review letters\n121, 060505 (2018).\n[38] R. Penrose, Angular momentum: an approach to combinatorial space-time, Quantum theory and beyond , 151\n(1971).\n[39] C. Krattenthaler, Growth diagrams, and increasing and decreasing chains in fillings of ferrers shapes, Advances in\nApplied Mathematics 37, 404 (2006).\nAppendix A: Complexity Analysis for the PBT Algorithms\nWe will now provide the complexity analysis for the algorithms. We recall that we provide four algorithms\nin total, for each of which there are two possible versions. These four algorithms are for dPBT in Section VA,\nfor pPBT with maximally entangled resource state - 2 different algorithms are presented in Sections VB1 and\nVB2 - and lastly for pPBT with optimised resource state in Section VB3. The two versions of each algorithm\narise because one has an option over whether to use the Schur transform as in [22] (the BCH Schur transform) or\n[21] (the spin coupling Schur transform). The former runs faster - with gate complexity npoly(log(n),log(1 /ϵ))\nfor accuracy ϵ, but uses more ancillas - O(nlog(n)). In contrast, the algorithm of [21] runs with gate complexity\nO(n3log(n) log( n/ϵ))but uses O(log(n))ancillas.\nTo obtain explicit gate complexities for these algorithms in terms of the Clifford + T universal set, we\nuse the same technique as employed in [30]. As in [30], we use the fact that an arbitrary unitary Umay be\ndecomposed into a sequence of two-level rotations whose length equals the number of non-zero elements on or\nbelow the diagonal of U, not counting 1’s on the diagonal8. Subsequently, a two-level rotation on nqubits may\nbe decomposed to an accuracy of δinO(nlog(1 /δ))Clifford + T gates. This latter fact allows us to translate\nbetween a ‘two-level rotation complexity’ i.e. the number of two-level rotations in a given sequence of two-level\nrotations, and ‘Clifford + T complexity’.\n8It is typical to upper bound this by the number of non-zero elements in U, not counting 1’s on the diagonal.25\nWenotethatinoneroundofamplitudeamplification, thecomplexityofimplementing CΠNOTandC˜ΠNOT\n(for our particular Πand˜Π) and single qubit gates (which are special cases of two-level rotations) is sub-leading\ncompared to that of the unitary we implement in each block, U. It can also be noted that every two-level\nrotation we employ throughout each algorithm acts only on O(log(N))qubits, since every operation throughout\nthe algorithm acts only on O(log(N))qubits. As such, suppose that the two-level rotation gate complexity\nforU, without the complexity of the Schur transform, is p. Each of these ptwo-level rotations should then be\ndecomposed to accuracy O\u0010\nϵ\npn\u0011\nin the Clifford + T gate set, where nis the number of rounds of amplitude\namplification used, which can be done in O(log(N) log( pn/ϵ))such gates. The whole unitary Uin one round\nof amplitude amplification then has a Clifford + T gate complexity TSch+plog(N) log\u0000pn\nϵ\u0001\n, where TSchis\nthe Clifford + T gate complexity of whichever Schur coupling we are using, the BCH Schur transform [22], or\nthe spin coupling transform [21], implemented to accuracy O\u0000ϵ\nn\u0001\n. We therefore obtain a Clifford + T gate\ncomplexity for the entire algorithm of O\u0002\u0000\nTSch+plog(N) log\u0000pn\nϵ\u0001\u0001\nn\u0003\nwith an overall accuracy of ϵfor the\nNaimark unitary.\nWe have the following values of pandnfor each algorithm.\ndPBTpPBT\nMaximally Entangled Resource State\nAlgorithm 1pPBT\nMaximally Entangled Resource State\nAlgorithm 2pPBT\nOptimised Resource State\npO(N) O(N) O(N) O(N)\nnO(√\nN) Θ(1) Θ(1) O(√\nN)\nTABLE IV. The values of p(the two-level rotation gate complexity for each round of amplitude amplification, without\nthe Schur coupling) and n(the number of rounds of amplitude amplification).\nFurther, we have the values for TSch:\nTSch=(\nNpoly(log(N),log(1 /ϵ))BCH Schur Transform [22]\nO(N3log(N) log( N/ϵ))Spin Coupling Schur Transform [21](A1)\nwhere we recall that we differentiate the two transforms because the former uses more ancillary qubits\n(O(Nlog(N))than the latter (O(log(N)).\nIt would be somewhat tedious to go through and prove each value of pindividually. Instead, we prove the\nvalue for dPBT in some detail and then provide two points that are important to mention for why we get the\nsame value of pfor all the pPBT algorithms - otherwise the arguments are essentially exactly the same for each\nvalue of pfor each algorithm.\nFor dPBT, we analyse the relevant operations as follows within the unitary Uthat do not include the Schur\ncoupling:\n•The initial superposition |1⟩i7→1√\nNPN\ni=1|i⟩, may be implemented via, for example, the quantum Fourier\ntransform, QFT N, which has gate complexity polylog (N).\n•The operation C−SWAP =PN\ni=1SWAP AiAN⊗ |i⟩⟨i|may be re-written as\nΠN\ni=1\u0010\nSWAP AiAN⊗ |i⟩⟨i|+I⊗P\nj̸=i|j⟩⟨j|\u0011\ni.e. a product of Noperators where each operator swaps\nAiandANcontrolled on the ancilla being in the state |i⟩. Each of these Noperators has exactly two\nnon-zero elements (other than 1’s on the diagonal): |0⟩Ai|1⟩AN|i⟩ ↔ | 1⟩Ai|0⟩AN|i⟩.\n•After the Schur coupling, we perform the rotation given by Equations (91) to (95). Note that this is\nalready defined as a unitary on the whole space and it has O(N)non-zero entries. The reason for this\nis that smay take O(N)values, and so all three registers may take O(N)values, due to the fact that\nadjacent spins may differ from each other only by ±1\n2. We see that each resulting superposition is over\ntwo computational basis elements, so each column in the operation’s matrix has weight at most two.\n•In the same way, the compression step of Equations (101) to (105) has two-level rotation gate complexity\nO(N).\n•Then, when the eigenvalues are attached in Equations (107) to (110), this operation can easily be extended26\nto a unitary on the whole space:\n|−⟩|−⟩| s⟩|0⟩r↔ |−⟩|−⟩| s⟩|1⟩r(A2)\n|+⟩|−⟩| s⟩|0⟩r↔ |+⟩|−⟩| s⟩|1⟩r(A3)\n|−⟩|+⟩|s⟩|0⟩r↔ |−⟩| +⟩|s⟩|1⟩r(A4)\n|+⟩|+⟩|s⟩|0⟩r7→p\nα(s)2+β(s)2|+⟩|+⟩|s⟩|0⟩r+p\n1−(α(s)2+β(s)2)|+⟩|+⟩|s⟩|1⟩r(A5)\n|+⟩|+⟩|s⟩|1⟩r7→p\n1−(α(s)2+β(s)2)|+⟩|+⟩|s⟩|0⟩r−p\nα(s)2+β(s)2|+⟩|+⟩|s⟩|1⟩r(A6)\nAgain, this has two-level rotation gate complexity O(N)because |s⟩takesO(N)values and each resulting\nsuperposition is over only two elements.\nWe may therefore conclude p=O(N)for dPBT. We now explain why for the second algorithm of pPBT with\nmaximally entangled resource state we may obtain p=O(N)as well. In particular, the operation described by\nEquations (152) to (155), repeated below, looks less efficient than this:\n|−⟩|−⟩| s⟩|i⟩ 7→ |−⟩|−⟩| s⟩|N+i⟩ (A7)\n|+⟩|−⟩| s⟩|i⟩ 7→ | +⟩|−⟩| s⟩|N+i⟩ (A8)\n|−⟩|+⟩|s⟩|i⟩ 7→ |−⟩| +⟩|s⟩|N+i⟩ (A9)\n|+⟩|+⟩|s⟩|i⟩ 7→r\nN\n4σ(s)|+⟩|+⟩|s⟩|i⟩+s\n1−\u0012N\n4σ(s)\u00132\n|+⟩|+⟩|s⟩|N+i⟩. (A10)\nSince the registers |s⟩and|i⟩can both take O(N)values, it appears initially that this operation has two-level\nrotation gate complexity of O(N2). However, to make this more efficient, one may just flip a single qubit\nin the ancillary register |i⟩to map |i⟩ 7→ | N+i⟩. More explicitly, the 2N-dimensional register |i⟩may be\nencoded into |ι⟩|y⟩, where |ι⟩is an N-dimensional register, and |y⟩is a single qubit. We encode |i= 1, ..., N⟩\ninto|ι= 1, ..., N⟩|y= 0⟩and|i=N+ 1, ...,2N⟩into|ι= 1, ..., N⟩|y= 1⟩. Then the operation described by\nEquations (A7) to (A10) does not, in fact, act on |ι⟩, only acting on |±⟩|±⟩| s⟩|y⟩, and so we get a desirable\nO(N)two-level rotation gate complexity here.\nWe make one more point as to why we may get p=O(N)for the pPBT algorithms, after which the values\nofpfollow in the exact same way as for dPBT. In particular, this point relates to algorithm 1 of pPBT with\nmaximally entangled resource state as well as pPBT with optimised resource state. In both of these cases the\nancillary register |i⟩had dimension N+ 1and there are several operations that we wish to control on this\nregister. In particular, in both algorithms, the unitary Uwithin each amplitude amplification round can be\nsummarised by the two steps\n1.PN\ni=1|ψ⟩|i⟩|0⟩r7→PN\ni=1√Πi|ψ⟩|i⟩|0⟩r+|∗⟩|1⟩r\n2.|ψ⟩|N+ 1⟩|0⟩r7→p\nΠN+1|ψ⟩|N+ 1⟩|0⟩r+|∗⟩|1⟩r\nso that we are acting on |i= 1, ..., N⟩in the first step and |i=N+ 1⟩in the second step. Again, at first this\nmakes it appear that we lose efficiency because we are seemingly acting on the whole |i⟩register in addition to\nour other steps. However, in a very similar way to the point just made - we can encode the register |i⟩into some\n|ι⟩|y⟩, where |ι⟩isN-dimensionaland |y⟩isasinglequbit. |i= 1, ..., N⟩isthenencodedinto |ι= 1, ..., N⟩|y= 0⟩\nand|i=N+ 1⟩may be encoded into |ι= 1⟩|y= 1⟩, so that we may simply control our operations on the one\nqubit|y⟩, thus not losing any efficiency.\nAppendix B: Different Notions of Schur Transforms and the Necessity for a Pre-Mapping Stage in some\nApplications\nWe now discuss the two primary notions of the quantum Schur transform that exist, aiming to clear up\nsome confusion that arises between the two of them. In particular, we will show how the traditional view on\nthe Schur transform is insufficient for certain applications in quantum information, and that a “pre-mapping\nstage” as in [21] (or something similar) is essential in these cases. We present these definitions on qubits only -\nthe extension to qudits is natural - see [36] for more details on the representation theory of SU(d).\nIn the work on Permutational Quantum Computing (PQC) in [20], the Schur basis is considered to be\na simultaneous spin eigenbasis of ncommuting, complete observables. With CJ,M\nj1,m1;j2,m2the usual SU(2)\nClebsch-Gordan coefficients, a Schur state may be written as\n|j1, j2, ..., j n−2, J, M⟩=X\nx∈{±1\n2}nCj1,m1\n1\n2,x1;1\n2,x2Cj2,m2\nj1,m1;1\n2,x3...Cjn−2,mn−2\njn−3,mn−3;1\n2,xn−1CJ,M\njn−2,mn−2;1\n2,xn|x1...xn⟩(B1)27\nwhere miis used as shorthand for x1+...+xi+1. Note that here we are writing computational basis states\nas|1\n2⟩and|−1\n2⟩as opposed to |0⟩and|1⟩, just for the sake of convenience. It must be emphasised that these\nstates are simply n-qubit states.\nThese states are labelled by a sequence of spin eigenvalues j1, ..., j n−2, JandMwhere ji∈N0\n2,J∈N0\n2and\nM∈ {− J,−J+ 1, ..., J−1, J}. Furthermore, each consecutive element of j1, ..., j n−2, Jmust differ from each\nother by either +1\n2or−1\n2, and we note that these values may not fall below zero. Because the Schur basis forms\na basis for (C2)⊗n, there must necessarily be 2nvalid such sequences j1, ..., j n−2, J, M.\nIn the work on PQC [20], the Schur transform is defined as the unitaryoperation that maps a computational\nbasis state |x1...xn⟩to a Schur state |j1, ..., j n−2, J, M⟩coherently over all computational basis states. To which\nSchur state each |x1...xn⟩gets mapped is left as an arbitrary choice. In PQC, one starts with some fixed n-\nqubit computational basis state |x⟩, performs the Schur transform to produce some fixed Schur state, performs\na permutation of the qubits, before inverting the Schur transform and measuring in the computational basis9.\nPQC itself was later shown to be classically simulable [25, 37], however ‘upgraded’ versions, dubbed PQC+,\nmay be considered for which the qubit permutation is replaced by some more complicated gate, see for example\n[29] (believed to be non-classical and to have practical applications).\nWe now consider an alternative definition of the Schur transform as in [24, 30]. Here, the Schur transform\nis taken to be an operation that performs the mapping\n|j1, j2, ..., j n−2, J, M⟩ 7→ | j1⟩|j2⟩...|jn−2⟩|J⟩|M⟩ (B2)\ncoherently over all Schur states |j1, j2, ..., j n−2, J, M⟩, which are exactly as defined in Equation (B1). The right-\nhand side of Equation (B2) is a computational basis state on more than nqubits. Each register |ji⟩,|J⟩and|M⟩\nis formed of exactly as many qubits as is needed to encode the given eigenvalue. For example, one can check that\nj3may take values in the set {0,1,2}, and so two qubits make up this register. Because each eigenvalue takes\nat most linearly many values, each register may be formed of O(log(n))qubits, meaning that the right-hand\nside of Equation (B2) is a computational basis state on O(nlog(n))qubits. In this case, the Schur transform\nhas therefore been defined as an isometry rather than a unitary operation, since we have performed a mapping\ninto a larger Hilbert space.\nWe call the first notion of the Schur transform, defined by a unitary mapping a computational basis state to\na Schur state the ‘physical’ notion of the Schur transform, and the notion of the Schur transform mapping each\nSchur basis state to its encoding on computational basis states as in Equation B2 the ‘mathematical’ notion of\nthe Schur transform. We justify this characterisation with the fact that the origins of permutational quantum\ncomputing may be traced back to spin networks [28, 38], whereas the latter notion has a closer relation to\nSchur-Weyl duality.\nThere are two primary differences between the notions of the quantum Schur transform that may each lead\nto confusion. First, they are in a non-precise sense inverses of each other. They are not actually inverses of\neach other - one is a unitary and one is an isometry. However, the physical Schur transform is mapping n-qubit\ncomputational basis states to n-qubit Schur states, whereas the mathematical Schur transform is mapping n-\nqubit Schur states to computational basis states (on O(nlog(n))qubits). One may not need to necessarily see\nthis as a change in definition, one may choose to instead see this as a difference in transformation convention,\nwhere the physical Schur transform gives an ‘active’ transformation convention, and the mathematical Schur\ntransform uses a ‘passive’ transformation convention.\nThe second crucial difference between the two is that the first is a unitary operation, whereas the second\nis an isometry, mapping into a larger space with entangled ancillary qubits, as explained. Some efficient\nimplementations of the Schur transform are only implementing the mathematical transform [24, 30], whereas\nothers can perform both [21, 31]. These latter algorithms that can perform the unitary operation are said to be\nperforming a ‘clean’ Schur transform.\nA smaller difference between the physical and mathematical perspective is notational, which should be easily\nsurmountable. Indeed, in the context of Schur-Weyl duality, Schur states |j1, ..., j n−2, J, M⟩may be denoted as,\nfor example, |λ, qλ, pλ⟩, whereas their encodings in a computational basis state |j1⟩|j2⟩...|jn−2⟩|J⟩|M⟩may be\ndenoted as |λ⟩|qλ⟩|pλ⟩, as in [22], for example. Here, the total angular momentum/spin of all the qubits, J,\ncorresponds to λ, the partition of n, whereas qλ, which indexes within irreps of the unitary group, corresponds\ntoM, and pλ, the symmetric group irrep index, corresponds to all of (j1, j2, ..., j n−2).\nWe will now discuss the algorithm of [21] briefly, which we might refer to as the spin coupling transform, as\nwe feel that it is the most simple of the available algorithms for a clean, unitary quantum Schur transform.\nThe algorithm takes place in two stages. First is the so-called ‘pre-mapping’ stage, in which one maps\n|x1...xn⟩ 7→ | j1⟩|j2⟩...|jn−2⟩|J⟩|M⟩ (B3)\n9In the original paper [20], more general spin eigenbases were considered than the Schur basis (or sequentially coupled basis) alone.\nThese are, however, nothing more than bases instantiating the decomposition of Schur-Weyl duality with different subgroup\nadaptations.28\ncoherently over all n-qubit computational basis states x. Ancillary qubits have therefore been introduced in\nthis operation10. As mentioned, the choice of to which (j1, ..., j n−2, J, M )each xis mapped is arbitrary - the\nuser may have a particular choice, such as that given by the RSK correspondence [39], but we leave this choice\nas arbitrary here. It is worth appreciating the fact that the mapping of Equation (B3), despite being a map\nbetween computational basis states, is non-trivial to implement efficiently. Only particular computational basis\nstates on the right-hand side define valid eigenvalue sequences |j1⟩|j2⟩...|jn−2⟩|J⟩|M⟩and one must view all\nthe qubits in such a computational basis state to make sure that it corresponds to such a valid eigenvalue\nsequence. Thus how to perform this pre-mapping efficiently is not immediately obvious.\nThe second stage of the algorithm of [21] is the ‘coupling stage’, performing\n|j1⟩|j2⟩...|jn−2⟩|J⟩|M⟩ 7→ | j1, j2, ..., j n−2, J, M⟩. (B4)\nAs such, the mathematical Schur transform is exactly the inverse of the coupling stage of the spin coupling\ntransform. It is worth noting that in the present paper, for the PBT algorithms, only the inverse of the coupling\nstage (which many people know as simply the Schur transform) is ever needed. Indeed, for many applications,\nthis operation is sufficient. It is, however, important for some applications that the full, clean, unitary can\nbe performed, and our primary intention with this appendix is to clean up this point for algorithm designers,\nhoping to make clear the necessity of a pre-mapping stage (or just a clean Schur transform more generally) in\ncertain applications.\nIndeed, consider a situation where one wishes to perform the inverse Schur transform (as a unitary), perform\nsome arbitrary n-qubit unitary U, and then perform the forwards Schur transform. Recalling that there are\n2nvalid encodings |j1⟩|j2⟩...|jn−2⟩|J⟩|M⟩, just as there are 2ncomputational basis states |x⟩, it is tempting\nto attempt to perform Udirectly on the |j1⟩|j2⟩...|jn−2⟩|J⟩|M⟩, by only acting on the 2nlevels that form\nvalid encodings. It is not obvious, however, how to do this efficiently for a general n-qubit unitary operation U.\nOne has to check every qubit of |j1⟩|j2⟩...|jn−2⟩|J⟩|M⟩in order to determine if it is a valid encoding of the\neigenvalues. Thus, Ucannot be implemented efficiently (without further information) on these registers, more\nspecifically because the 2nvalid encodings do not have a tensor product structure.\nAs such, these types of algorithms, or similarly an algorithm like PQC+ [29], where the forwards unitary\ntransform is used, followed by some unitary, followed by the inverse unitary transform, do necessitate a ‘clean’\ntransform like [21] or [31].\n10Note there is a version of this algorithm for which only O(log(n))qubits are required here, rather than O(nlog(n)), but we do\nnot go into this for the sake of brevity.",
      "metadata": {
        "filename": "Efficient Algorithms for All Port-Based Teleportation Protocols.pdf",
        "hotspot_name": "PBT_Injection_Molding",
        "title": "Efficient Algorithms for All Port-Based Teleportation Protocols",
        "published_date": "2023-11-20T18:49:16Z",
        "pdf_link": "http://arxiv.org/pdf/2311.12012v2",
        "query": "injection molding PBT housing energy efficiency optimization cycle time reduction"
      }
    },
    "Efficient quantum circuits for port-based teleportation": {
      "full_text": "arXiv:2312.03188v2  [quant-ph]  21 May 2024EFFICIENT QUANTUM CIRCUITS FOR PORT-BASED TELEPORTATION\nDMITRY GRINKO1, ADAM BURCHARDT1, AND MARIS OZOLS1,2\nAbstract. Port-based teleportation (PBT) is a variant of quantum tele portation that, unlike the canonical\nprotocol by Bennett et al., does not require a correction ope ration on the teleported state. Since its introduction\nby Ishizaka and Hiroshima in 2008, no eﬃcient implementatio n of PBT was known. We close this long-standing\ngap by building on our recent results on representations of p artially transposed permutation matrix algebras and\nmixed quantum Schur transform. We construct eﬃcient quantu m algorithms for probabilistic and deterministic\nPBT protocols on nports of arbitrary local dimension, both for EPR and optimiz ed resource states. We describe\ntwo constructions based on diﬀerent encodings of the Gelfan d–Tsetlin basis for nqudits: a standard encoding\nthat achieves ˜O(n)time and O(nlog(n))space complexity, and a Yamanouchi encoding that achieves ˜O(n2)\ntime and O(log(n))space complexity, both for constant local dimension and tar get error. We also describe\neﬃcient circuits for preparing the optimal resource states .\nContents\n1. Introduction 2\n1.1. Background 2\n1.2. Types of PBT protocols 2\n1.3. Summary of our results 3\n1.4. Related work 3\n2. Preliminaries 3\n2.1. Notation 3\n2.2. Representation theory of the partially transposed per mutation algebra 4\n2.3. Mixed quantum Schur transform 5\n2.4. PBT measurement and ﬁgures of merit 7\n2.5. The standard PGM 7\n2.6. POVMs for deterministic and probabilistic PBT 8\n2.7. Naimark dilations and implementation of measurements 9\n3. Eﬃcient quantum algorithms for PBT in standard encoding 11\n3.1. Naimark’s dilation of the standard PGM 11\n3.2. Quantum circuit for standard PGM 13\n3.3. Quantum circuit for deterministic PBT measurement 17\n3.4. Quantum circuit for probabilistic PBT measurement wit h EPR resource state 17\n3.5. Eﬃcient quantum algorithms for generic PBT measuremen ts 18\n3.6. Exponentially improved lower bound for non-local quan tum computation 19\n4. Logarithmic space PBT via Yamanouchi encoding 20\n5. Quantum circuits for optimized resource states 22\nAcknowledgements 25",
      "metadata": {
        "filename": "Efficient quantum circuits for port-based teleportation.pdf",
        "hotspot_name": "PBT_Injection_Molding",
        "title": "Efficient quantum circuits for port-based teleportation",
        "published_date": "2023-12-05T23:39:04Z",
        "pdf_link": "http://arxiv.org/pdf/2312.03188v2",
        "query": "injection molding PBT housing energy efficiency optimization cycle time reduction"
      }
    },
    "Flexible and Comprehensive Patient-Specific Mitral Valve Silicone Models with Ch": {
      "full_text": "Noname manuscript No.\n(will be inserted by the editor)\nFlexible and Comprehensive Patient-Speci\fc\nMitral Valve Silicone Models with Chordae Tendinae\nMade From 3D-Printable Molds\nSandy Engelhardt, Simon Sauerzapf,\nBernhard Preim, Matthias Karck,\nIvo Wolf, Ra\u000baele De Simone\nReceived: date / Accepted: date\nAbstract Purpose Given the multitude of challenges surgeons face during\nmitral valve repair surgery, they should have a high con\fdence in handling\nof instruments and in the application of surgical techniques before they en-\nter the operating room. Unfortunately, opportunities for surgical training of\nminimally-invasive repair are very limited, leading to a situation where most\nsurgeons undergo a steep learning curve while operating the \frst patients.\nMethods In order to provide a realistic tool for surgical training, a commer-\ncial simulator was augmented by \rexible patient-speci\fc mitral valve replica.\nIn an elaborated production pipeline, \fnalized after many optimization cy-\ncles, models were segmented from 3D ultrasound and then 3D-printable molds\nwere computed automatically and printed in rigid material, the lower part\nbeing water-soluble. After silicone injection, the silicone model was dissolved\nfrom the mold and anchored in the simulator.\nResults To our knowledge, our models are the \frst to comprise the full mi-\ntral valve apparatus, i.e. the annulus, lea\rets, chordae tendineae and papillary\nmuscles. Nine di\u000berent valve molds were automatically created according to\nthe proposed work\row (seven prolapsed valves and two valves with functional\nmitral insu\u000eciency). From these mold geometries, 16 replica were manufac-\ntured. A material test revealed that Eco\rexTM00-30 is the most suitable ma-\nterial for lea\ret-mimicking tissue out of seven mixtures. Production time was\naround 36h per valve.Twelve surgeons performed various surgical techniques,\nThe research was supported by the German Research Foundation DFG project 398787259,\nDE 2131/2-1 and EN 1197/2-1.\nSandy Engelhardt \u0001Simon Sauerzapf \u0001Ivo Wolf\nFaculty of Computer Science, University of Applied Sciences Mannheim, Germany\nE-mail: s.engelhardt@hs-mannheim.de\nMatthias Karck \u0001Ra\u000baele De Simone\nDepartment of Cardiac Surgery, University Hospital Heidelberg, Germany\nSandy Engelhardt \u0001Bernhard Preim\nFaculty of Computer Science, Magdeburg University, GermanyarXiv:1904.03704v1  [physics.med-ph]  7 Apr 20192 Engelhardt et al.\ne.g. annuloplasty, neo-chordae implantation, triangular lea\ret resection and\nassessed the realism of the valves very positively.\nConclusion The standardized production process guarantees a high anatom-\nical recapitulation of the silicone valves to the segmented models and the ultra-\nsound data. Models are of unprecedented quality and maintain a high realism\nduring haptic interaction with instruments and suture material.\nKeywords 3D printing \u0001\rexible mitral valve \u0001surgical training simulator\n1 Introduction\nReconstructive mitral valve surgery is a demanding surgical sub-speciality due\nto variation in valve pathologies, required dexterity and tedious acquirement\nof surgical techniques. Mastering the technical skills to successfully perform-\ning needs years of training. At the same time, opportunities to practice are\nlimited and most of the training is conducted by assisting and performing real\nsurgeries, following the surgical principle of \\see one, do one, teach one\" [10].\nEspecially minimally-invasive procedures are considered as very compli-\ncated. It comprises a 5-6 cm incision in the intercostal space, which is re-\nferred to as a right lateral thoracotomy. The space constraints and special\nlong-shafted instruments require surgeons to operate di\u000berently than in open\nsurgery, e.g. the needle must be \fxed in other angles to the needle holder as\nusually, which makes the stitching process more di\u000ecult. Additionally, knot\ntying is performed with a knot pusher, which can easily result in an air knot\nif done inappropriately. According to Holzhey et al. [7], who analyzed a total\nof 3895 operations by 17 surgeons performing their \frst minimally invasive\nsurgery of the mitral valve, the typical number of operations to reach a high\nquality level was between 75 and 125. In addition, more than one of such an\noperation per week was necessary to maintain good results. They identi\fed a\nclear need for good mentoring and training utilities in the learning phase.\nBesides `wetlabs' or virtual simulators, physical phantoms provide options\nfor surgical training. However, only the latter has the unique ability to bring\nthe patient's anatomy into physical reality as a model. Additionally, it provides\nexcellent opportunity for dexterity training with authentic instruments and\nsuture material [9], while o\u000bering materials that are pliable enough to be cut\nand sewn. Moreover, such models have the advantage that they could be used\nbefore the actual surgery takes place to rehearse the procedure and to discuss\nwith colleagues without time constraints.\nMitral valve repair is a highly experienced-based surgery and techniques\nare chosen according to the surgeon's preference. Various surgical repair tech-\nniques exist (e.g. annuloplasty, lea\ret plasty, chordae plasty), which may ad-\ndress di\u000berent parts of the valve [16]. A patient-speci\fc mitral valve phantom\nconsisting of the annulus, lea\rets and the subvalvular apparatus would al-\nlow the performance of the full spectrum of repair techniques and may be a\nvaluable tool for comparison of current `competing' techniques like triangular\nlea\ret resection and neo-chordae implantation. The measurement of chordaeFlexible Silicone Mitral Valve Models 3\nlength as well as implantation and knot tying of neo-chordae at their deter-\nmined length are procedures considered most crucial in mitral valve repair\ntoday. Performing this procedure with a high con\fdence after training on a\nphantom can avoid a sub-optimal surgical outcome such as re-occurrence of\nvalve leakage or restricted lea\ret motion.\nIn a previous work [3], we demonstrated the feasibility for producing elas-\ntic 3D-replica consisting of mitral annulus and the two mitral lea\rets. Using\nour customized software for valve modelling [2] allowed us to extract models\nwith high \fdelity from 3D echocardiography, which is one elementary key to\nthe work at hand. The production pipeline consisted of several comprehensive\nsteps, e.g. image acquisition, 3D-segmentation of the mitral valve, computa-\ntion of negative molds and material casting. In this work, we extended the\nmodels to incorporate the complete mitral valve apparatus, which includes\nthe complex branching structure of the chordae tendineae and the papillary\nmuscles. For this task, we had to completely revise the molding and casting\nconcept and optimized it in several development cycles, as it is not straightfor-\nward to produce a negative mold that can be easily disassembled after silicone\ncasting due to the complex nature of the valvular shape. In this paper, we\npresent the revised production process, novel generic computer-aided design\nparts, a material study and a larger user study conducted with trainee and\nexpert surgeons. The goal of this work was to develop unique valve models\nof unprecedented quality that combine highly realistic anatomy and material\nproperties suitable for surgical training in a cost-e\u000bective production process\nto motivate dissemination in the medical community.\n2 Related Work\nA handful of companies provide physical mitral valve simulators. The Mitral\nvalve holder (LifeLike BioTissue Inc., Ontario, Canada) or the MA-TRAC\nHigh Fidelity Minimally invasive Mitral Valve Repair simulator (Maastricht\nTrading Company, Inc., Maastricht, The Netherlands) are equipped with mi-\ntral valves made of silicone. Others companies provide 2D valves models made\nof felt, such as the MICS MVR Simulator (Fehling Instruments GmbH &\nCo. KG, Karlstein, Germany). However, such valves are generic, lack patient-\nspeci\fc properties and are therefore not suitable to address strategies for par-\nticular valvular conditions.\nExisting works already dealt with direct 3D-printing of the mitral valve\nusing acrylonitrile butadiene styrene (ABS) plastic material [18]. Such sti\u000b\nmaterial showed bene\fts for surgery planning and surgical teaching, but it\nis certainly inadequate for dexterity training. Vukicevic et al. [17] also used\ndirect printing, but employed multi-material elastomeric TangoPlus materials\n(Stratasys, Eden Prairie, Minnesota, USA) that were compared to freshly har-\nvested porcine lea\ret tissue. All TangoPlus varieties were less sti\u000b than the\nmaximum tensile elastic modulus of porcine mitral valve tissue. Furthermore,4 Engelhardt et al.\nthe mesh creation required a lot of manual input and chordae tendineae were\nnot included in the \fnal rapid prototyping models.\nScanlan et al. [8,14] proposed a method for creating 3D-printable molds,\nwhich can be \flled with an elastic material such as silicone to create \rexible\nand tear-resistant pediatric lea\ret models. Beyond that, they compared direct\nprinting with \rexible material. Molding required more time and labor than\ndirectly printed models, but permits materials that better simulate real tissue\nand that are more economical at scale. Furthermore, direct printing of \rex-\nible material usually requires expensive 3D printers and materials. Surgeons\nreported good tissue properties for the cast valves (realistic \rexibility, cuts\nand holds sutures well without tearing) from their personal experiences and\nconsidered the silicone models to be useful for surgical planning and training.\nHowever, their valve models did not include the subvalvular apparatus.\nIn a very recent medical review, Ginty et al. [6] describe a general guidance\nfor creation of dynamic valve models usable in a \row simulator. The authors\nprovide an overview of the production pipeline (imaging, segmentation, 3D\nprinting, material casting) and discuss viable options for each step. Moreover,\nthey present an approach, for which two interesting points shall be highlighted:\nThey just printed the lower impression of the valve and not a full mold. Sili-\ncone was painted on the printed surface in a thin layer, which then cured to\nthe shape of the valve. Furthermore, they incorporated braided nylon \fshing\nline strings into the lea\rets to mimic some strings of the chordae tendineae.\nWith the mentioned steps, it seems to be di\u000ecult to obtain results of the\nsame quality when multiple valves are produced from the same mold (e.g. two\nmodels could have di\u000berent thicknesses or varying chordae attachments). The\nphantoms do not seem to include papillary muscles.\nTo sum up, none of the current approaches allow for production of compre-\nhensive and \rexible models consisting of all anatomical substructures of the\nvalve within the scope of a standardized production process that maintains\nthe quality over di\u000berent fabrications.\n3 Materials and Methods\nOur developments are based on the commercially available MICS MVR Sim-\nulator , which resembles intraoperative space constraints. The \rat valve dum-\nmies, that are delivered with the simulator, are concealed inside this simulator,\nanchored on a bar and can be reached via a small circular access of 50 mm\ndiameter. Note that the geometry of the valve is neither patient-speci\fc nor\nthree-dimensional. Given the generic nature of the dummy, the surgeon can\nperform some reconstruction techniques, but the \\created\" dummy geometry\nis meaningless from an anatomical and functional point of view, not fostering\nindividualized therapy.\nTo o\u000ber advanced means for training and preparing of surgery, we extended\nthe existing simulator by \rexible patient-speci\fc mitral valves generated from\nmedical images. We identi\fed the following requirements (R2 extends our pre-Flexible Silicone Mitral Valve Models 5\nFig. 1 Main steps for creation of mold parts and holders.\nvious requirements [3]; R1-R3 refers to position and shape; R4-R7 to material,\ncosts and reproducibility):\n{R1: The model must be \fxable into the MICS MVR simulator to provide\na realistic port-to-valve relation.\n{R2: The model should consist of all parts of the valve: annulus, anterior\nand posterior lea\rets, papillary muscles and chordae tendineae to allow for\ntraining of the full spectrum of repair techniques.\n{R3: The lea\rets should re\rect end-systolic shape of patient-speci\fc valve\nto incorporate the morphology of potential prolapsing lea\ret segments at\nthe moment of valve closure.\n{R4: Stitching properties of the material should be realistic such that needles\ncan be inserted and sutures pass through with similar tissue resistance as\nin real surgery.\n{R5: Cutting properties of the materials should be realistic in terms of\nresistance.\n{R6: Common materials for low-cost 3D-printing should be used to facilitate\nreproducibility of the approach by other groups.\n{R7: Total production time should be less than 2 days and the process as\nautomated as possible to facilitate integration into the routine clinical use.\nIn the following, we elaborate on the automatic mold creation process. In short,\nfrom given 3D virtual models, a negative shape is computed that is connected\nto generic CAD-parts to build a sealed block for material insertion.\n3.1 Medical Imaging and Mitral Valve Modeling\n3D polygonal surface models of the mitral valve are obtained from trans-\nesophageal echocardiography (TEE) using customized software [2]. The soft-\nware was extended such that the position of papillary muscle heads can be\nlocalized. The mitral valve's annulus, mitral lea\rets and the position of the6 Engelhardt et al.\nFig. 2 Patient-individual valve surfaces are incorporated into the generic frames.\npapillary muscle heads are segmented from the end systolic time frame (R3).\nThis information is used to create an `enhanced model' (Fig. 1): The \fne\nbranching structures of the chordae tendineae are not completely visible on\ncurrent clinical image modalities [6], therefore, a chordal distribution is au-\ntomatically created. It consists of primary and secondary chordae tendineae,\ntaken the lea\ret geometry and the papillary muscle tip as reference. We em-\nployed anatomical descriptions as guide for creating the distribution. For the\nprimary chordae, points at the free edge of the lea\ret surface are connected to\nthe respective papillary muscle tip. The number of main chordal strings is an\nadjustable parameter in the software and the insertion points are automatically\nadopted accordingly. The chordal strings are equidistantly distributed along\nthe free edge of each lea\ret and connected to the anterolateral or posterome-\ndial papillary muscle. Each main chordal string gives rise to an additional left\nand right branch randomly attached at the mid of the string or higher.\n3.2 CAD-Modelling of Generic Parts\nTwo papillary muscle models were created in the freely available software\nBlender v2.79 and were aligned with the individual papillary muscle tips. The\nanterolateral head (diameter of 8.7 mm) has a single head, whereas the pos-\nteromedial papillary muscle has three heads (diameter of 11 mm). They both\nextend towards the lower end to anchor them in a speci\fcally designed papil-\nlary muscle holder (Fig. 2). Other parts are modeled once in the CAD software\nAutodesk Fusion 360 (v.2.0). The parts either serve as a common frame for\nthe mold parts (referred to as upper and lower shells ormold parts ) or as a\n\fxture for securing of the valve into the simulator (referred to as lea\ret holder\nand papillary holder (R1)). The \frst row in Fig. 2 provides an overview ofFlexible Silicone Mitral Valve Models 7\nFig. 3 Injection\nmolding.\nthe generic parts. The lea\ret holder incorporates a circular hole with a ring\nanchor, which will be enclosed by silicone when placed in between the two\nshells. This concept ensures that the holder remains permanently connected\nto the valve model. The lea\ret holder and the shells have undergone several\nmodi\fcations in an iterative process during the scope of this project in com-\nparison to our previous work [3]: the frames have several 4 mm ori\fces that\nallow for injection of silicone by a Luer lock syringe (previously, the material\nhas been cast and not injected); barricades were added to the frames to secure\nthem when they are stuck together; the size of the holder with ring anchor\nwas increased to accommodate bigger valves; 90\u000eoverhangs were removed to\ncut down support structures.\n3.3 Generation of Upper and Lower Casting Mold Part\nIn a next step, the mesh of the generic frames are extended with the patient-\nspeci\fc valve surfaces (Fig. 1). The software toolkit MeVisLab 3.0.2 and the\nC++ software library Visualization Toolkit (VTK) 8.0.1 were used for this\ntask. First, the valve model's position and orientation is normalized. Then,\nthe gaps between generic frame and valve model are closed by triangle strips.\nAfter obtaining this \\enhanced surface model\" (Fig. 1), lea\rets of a certain\nthickness had to be created from the provided surfaces. The previously used\nmethod VTK implicit function [3] delivered a lot of artifacts due to compli-\ncated triangular topology and multiple surfaces. To circumvent this issue, the\nMeVisLab module VoxelizeInventorScene has been employed, which enables\nthe computation of a voxel representation of the scene, i.e. a 3D bounding\nbox volume that masks out the valve. As voxel size, we chose (0.01 mm)3.\nA distance in mm to the lea\ret needs to be set, which adds a thickness to\nthis otherwise \rat surface representation. We set this parameter to 0.8, which\nextends the surface to both sides. The resulting image mask content was sep-\narated into a lower and an upper region representing the patient-speci\fc part\nof the shells. Each part was connected to the respective generic frame yielding\nthe \fnal printable molds.\n3.4 3D-Printing and Silicone Injection\nThe lower and upper shell and the valve holders are sliced in the freely avail-\nable Ultimaker CURA software and 3D-printed. For all parts except of the8 Engelhardt et al.\nlower shell a common rigid \flament can be used, e.g. polylactic acid (PLA) or\nABS. However, one of the key conceptual points is that we use water-soluble\npolyvinyl alcohol (PVA) for printing of the lower shell. Recent low-cost 3D\nprinter with dual extrusion, such as the Ultimaker3 (Ultimaker B.V., Gelder-\nmalsen, The Netherlands) employ this material for support structures (R6).\nWe use it as main material for printing of the lower shell, which is dissolved\nin water after silicone injection. We decided to use silicone early on, as other\ngroups have made good experiences with it [6,14] and it is relatively easy to\nhandle. Di\u000berent silicone mixtures exist (e.g. with di\u000berent shore indices) and\nthe suitable mixture for mitral valve repair was assessed experimentally.\nAs a side note, in the previous work [3], silicone was cast. This is suitable\nfor creation of lea\ret models. However, to \fll the thin tubes of the chordae\ntendinae, more pressure is employable with a syringe. Several speci\fc issues\nare to mention about the developed injection process. If not done in the way\ndescribed in the following, large air bubbles remain in the mold or the mold\nis not \flled completely with material. The silicone material has to be stirred\nand drawn up in a syringe with luer lock. The syringe is closed airtight and a\nvacuum is created by further drawing up the syringe's piston. This procedure\nis conducted for approx. 30s to completely degas the silicone. Subsequently,\nthe connected mold parts are \fxated with a C-clamp, placed upside down and\nhalf of the silicone is injected through the ori\fce at the papillary muscles. This\nwill \fll up the subvalvular apparatus. Then, the lower ori\fces are sealed; the\nmold is turned around and placed in a 45\u000eangle. The ori\fce opposite to the\nreservoir (Fig. 2) is chosen for injection and as much silicone is inserted until\nsilicone oozes out the ori\fce at the opposite side (Fig. 3). Finally, the ori\fce\nis sealed and the mold is put to rest in the same 45\u000eposition.\n4 Experiments\nExperiments were conducted to evaluate the material, the chordal thickness,\nthe feasibility and accuracy and surgeons assessed the quality of the models.\nSilicone mixture: For this experiment, a special reusable material test mold\nwas developed and printed in PLA. This mold only consists of the lea\ret\ngeometry, but is otherwise similar to the mold we presented in Fig. 2. The\nsubvalvular apparatus was assessed in a separate experiment. In total, seven\ndi\u000berent material mixtures were tested (Tab. 5): Erosil 10 (Silikonfabrik.de,\nAhrensburg, Germany), Dragon SkinTM10 Fast and Eco\rexTM00-10, 00-30\nand 00-50 (Smooth-On, Inc., Macungie, Pennsylvania, USA). The goal of this\nexperiment was to \fnd appropriate material for injecting into the gap of the\nmitral valve lea\rets such that 1) the whole gap is \flled and 2) no air bubbles\nremain. Furthermore, the chosen material's viscosity must enable injection\nmolding in general. Viscosity, amount of air bubbles and \flling degree was\naccessed on a 5-point Likert scale. Key surgical factors such as lea\ret-likeFlexible Silicone Mitral Valve Models 9\nFig. 4 Test object printed in PVA for evaluation of minimal possible chordae thickness.\ntissue feeling when stitching with authentic suture material and when cutting\nwith scissors were assessed by an expert mitral valve surgeon.\nChordae thickness: The second experiment was conducted to determine the\nminimum possible thickness of the individual chordae threads for the chosen\nmaterial of the previous experiment. Therefore, a speci\fc test mold object was\ncreated with the CAD-program that incorporates a casting hole in a lit (Fig.\n4). This mold furthermore consisted of di\u000berent tubes with multiple diameters\n(0.5, 1.0, 1.5, 2.0, 2.5 mm) each having tiny air ducts to release spare gas. The\nmold was printed from water-soluble PVA. Silicone was injected, the mold\ndissolved in warm water and it was visually assessed whether the respective\ntube was \flled completely or not.\nFeasibility and accuracy in producing di\u000berent valve geometries from TEE:\nNine di\u000berent virtual valve mold geometries were created from TEE acquired\nwith a Philips IE33 X7-2T matrix array transducer (Philips Healthcare, An-\ndover, MA, USA). The data sets contain di\u000berent pathologies (seven prolapsed\nvalves and two functional mitral valve insu\u000eciency) and degrees of mitral\ninsu\u000eciency (I-III). Valves were segmented and 16 molds were printed us-\ning Ultimaker2+ and Ultimaker3 (R6) to produce 16 replica. The production\nquality, production issues, approximate production time and costs were as-\nsessed. Furthermore, we evaluate the phantom-to-virtual model and the TEE\nimage-to-virtual model accuracy of each valve similarly as in [14], by measur-\ning di\u000berent distances: the anterolateral diameter, the posteromedial diameter\nand the chordae length between lea\ret and each papillary muscle tip, using a\ncaliper and virtual measurement tools on a) the phantoms, b) the correspond-\ning virtual models and c) the corresponding TEEs.\nSurgical Assessment: A group of 12 cardiac surgeons from Heidelberg Univer-\nsity Hospital, ranging from surgical residents to mitral valve experts, assessed\nthe realism of the simulator on a 5-point Likert scale in a questionnaire. Each\nof them performed a mitral valve reconstruction on the produced silicone mod-\nels. The choice of the respective repair techniques was left to the decisions of\nthe surgeons. The group was divided into \fve expert mitral valve surgeons and\nseven non-experts. A surgeon was considered to be an expert if his volume of10 Engelhardt et al.\nBase Material\nA:B:Slacker\nViscosity\nAir bubbles\nFilling degree\nSuturing\nCutting\nErosil 10 1:1:0 2 2 2 too solid ok\nDragon SkinTM10 Fast 1:1:0 5 2 4 good good\nDragon SkinTM10 Fast 1:1:1 4 2 3 too soft too soft, sticky\nDragon SkinTM10 Fast 1:1:2 4 3 2 too sticky too sticky\nEco\rexTM00-10 1:1:0 2 3 2 too soft ok\nEco\rexTM00-30 1:1:0 1 3 3 very good very good\nEco\rexTM00-50 1:1:0 1 3 3 very good very good\nTable 1 Technical criteria for suitability of injection molding for various silicone mixtures\nwere assessed on a 5-point Likert scale (1 - best mark, 5 - worst mork). Surgical properties\nwere evaluated by a mitral valve surgeon.\nmitral valve reconstructions exceeded 50 open and minimally-invasive opera-\ntions. The following categories were assessed: position in simulator, annulus\nshape, lea\ret size, lea\ret shape, lea\ret thickness, chordae thickness, chordae\ndistribution, chordae length, papillary muscle size, tear resistance at lea\ret,\ncutting lea\ret, stitching annulus/lea\ret, tear resistance at chordae, tear re-\nsistance at papillary muscle, suturing papillary muscle, valve color and valve\ntexture.\n5 Results\nSilicone mixture: Tab. 5 summarizes the results of the material tests. Both,\nEco\rexTM00-30 and Eco\rexTM00-50 (without Slacker) were assessed equally\nwell as lea\ret tissue mimicking material. They furthermore got highest ranking\nin their suitability for injection molding. We use the silicone with a lower Shore\nindex, Eco\rexTM00-30, for the rest of our experiments.\nChordae thickness: After dissolving the test object in water, it could be seen\nthat small ducts of less than 1.5 mm were not reliably \flled by silicone (Fig.\n4). The smallest tubes of diameter 0.5 mm are not represented in the created\nmodel at all. This could be due to printing inaccuracies or viscosity of the\nsilicone material. For the rest of the experiments, we set the chordae thickness\nto 1.5 mm. This does not fully resemble the \fne properties of the chordae\nstrings, however, we want to make sure that the strings are reliably represented\nin the \fnal valve model.\nFeasibility and accuracy in producing di\u000berent valve geometries from TEE: 16\nreplica from nine di\u000berent valve models, incorporating the full valvular appa-\nratus of speci\fc patients (R2), were successfully produced. Tab. 5 summarizes\nthe production times and costs. They depend on the size of the valve, in par-\nticular on the distance of the annulus plane to the papillary muscle tips, as theFlexible Silicone Mitral Valve Models 11\nProduction times\nSegmentation 10 min\nUpper Shell* Lower Shell Lea\ret Holder**\nNegative mold 15 min 25 min -\nSlicing 5 min 5 min 2 min\n3D printing 320 min 540 min 120 min\nSilicone injection 15 min\nSilicone curing 240 min\nWater quench 720-840 min\ntotal 2137 min \u001936 h\nProduction costs\nPrinting material PLA (50 g) \u00193,35e PVA (70 g) \u00199,35e PLA (15 g) \u00191,00e\nSilicone approx. 35 g \u00191,70e\ntotal \u001916e\nTable 2 Approximate production times and costs for a single valve. *can be reused, **can\nbe produced on stock independent of the valve geometry\nlower mold's height increases with greater distances. Production took approx.\n36h (R7). It could be signi\fcantly accelerated by approx. 7 hours by using two\n3D printers at the same time and by printing the lea\ret holder on stock. The\nactual producer workload time was around 80 min. Material costs were about\n16efor a single valve, depending on the size of the valve.\nOverall production quality was very good and could be maintained over\nseveral valves (the upper shell can be re-used, the lea\ret holder and lower shell\nneed to be printed for each valve). Visual agreement to the virtual model (R3)\nis high in all cases. Mean phantom-to-virtual model and the TEE image-to-\nvirtual model distances are 0 :87\u00060:92 mm and 1 :27\u00060:92 mm for the antero-\nlateral diameter, 0 :44\u00060:34 mm and 1 :36\u00060:95 mm for the posteromedial\ndiameter, 0 :70\u00060:45 mm and 2 :20\u00062:25 mm for the distance between lea\ret\nand anterolateral papillary muscle as well as 0 :56\u00060:49 mm and 1 :91\u00061:22 mm\nfor the same distance on the posteromedial side. For a few valves, we could\nidentify the following shortcomings: It happened once that the printed mold\nhad a hole and silicone was erroneously released into a supporting structure\ncompartment. After mold dissolving, this could be \fxed by simply cutting\naway unwanted parts. One model had an air bubble at the position of a thin\nchordae string, leading to a rupture. Minor air bubbles at papillary muscle\nprolongations and upper portion of the ring \fxture appeared, however, they\nnever e\u000bected the valve itself.\nSurgical Assessment: Fig. 5 and the supplemental video show examples of\nsilicone replica and di\u000berent procedures conducted on these models, e.g. an-\nnuloplasty, neo-chordae implantation, chordae-loop implantation. The results\nof the questionnaire are presented in Fig. 6. Models maintain a high realism\nduring haptic interaction with instruments and suture material. The realism\nof lea\ret and the annulus (shape, size, thickness) was assessed more positive\nthan the subvalvular apparatus. This was expected, since, due to manufactur-\ning constraints, these structures can not be replicated to reach full satisfaction.12 Engelhardt et al.\nFig. 5 Upper row: Top and\nside view of two valves be-\nfore surgical training. Lower\nrow: Surgeons performing an-\nnuloplasty, neo-chordae and\nchordae-loop implantation on\nthe valves \fxed in the simula-\ntor.\nThe worst vote relates to the realism of the color (median 4) and the texture\n(median 3) of the material.\n6 Discussion\nThe production of individualized high \fdelity and \rexible models from TEE,\nwhich, for the \frst time, also include the chordae tendineae attached to the\npapillary muscles, is feasible with our approach. The included chordae main-\ntain a realistic lea\ret-to-papillary muscle distance and constrain the moving\nspace of the surgical instruments. The standardized production process guar-\nantees a high anatomical recapitulation of the silicone valves to the segmented\nmodels. From a medical point of view, the silicone models are extremely useful\nfor training and patient-individualized rehearsal before surgery, as thoroughly\ndiscussed in our related medical publication [4]. Apart from its bene\fts for\nsurgical training, the presented method is employable for benchmarking novel\ntechnology in development [5] or potentially, after some adjustments, for in-\nvestigating catheter-based techniques. Since the most valid type of evaluation\nof techniques for valvular interventions is the assessment in realistic hemody-\nnamic environments, we plan the integration of the valves into a \row simulator,\nexploiting the echogenicity of the silicone material [4]. This will require a com-\nprehensive re-assessment of the currently used soft materials with regard to\ntheir suitability for mimicking realistic valve functions. However, it is worth\nmentioning that Ginty et al. [6] used the same soft material in their dynamic\nvalve simulator. In-vitro cardiac tissue models are becoming increasingly im-\nportant [12], but are still out of reach to be employed for surgical training. As\na surrogate, porcine hearts are often used (e.g. [13]).\nThe current resolution of typical image modalities is not high enough to\nacquire the thin chordal tree. Researchers lately captured the exercised ovine\nmitral valve with Micro-CT [11] or an exercised porcine mitral valve with 7T\nMRT [15] to gain information on its structure. Having improved imaging data\nwould be a valuable asset for rendering patient-speci\fc valves. A limitation is\nthat the chordae strength is not perfectly resembled in our models owing to\nthe relative soft silicone. In principle, our proposed concept would easily allow\nusage of two di\u000berent silicone materials due to the multiple injection holes: A\nharder silicone type could be injected in the lower shell to \fll the subvalvu-Flexible Silicone Mitral Valve Models 13\n020406080100\n12345 12345 12345 12345 12345 12345 12345 12345Chordae\nLengthChordae\nDistributionChordae\nThicknessLeaflet\nThicknessAnnulus\nShapeLeaflet\nShapeLeaflet\nSizePosition in \nSimulatorPapillary M.\nSize\n12345\n020406080100\n12345Tear Resistance \nLeaflet\n12345Cutting\nLeaflet\n12345Stitching\nAnnulus /Leaflet\n12345Tear Resistance\nChordae\n12345Tear Resistance\nPapillary M.\n12345Suturing\nPapillary M.\n12345Valve Color\n12345Valve Texture\nFig. 6 Five expert surgeons (blue) and seven non-experts (orange) assessed the realism on\na 5-point Likert scale (1-strongly agree, 2-agree, 3-neutral, 4-disagree, 5-strongly disagree).\nlar apparatus \frst, whereas a softer silicone could be used subsequently for\nthe lea\rets. Another limiting factor is the current phantom color and texture .\nIn a related work [1], we were able to show how the realism of the silicone\nmodel appearance could be signi\fcantly increased using a generative adver-\nsarial network called tempCycleGAN, which was trained on video sequences of\nsimulated and real mitral valve repairs. The predicted frames replace arti\fcial\nlooking parts of the simulator by more realistic appearances. Our vision is that\nthe surgeon will perceive these `hyperrealistic' frames on the endoscopic mon-\nitor while operating on the phantom, increasing photo-realism of the surgical\ntarget.\nCompliance with Ethical Standards\nCon\rict of interest The authors declare that they have no con\rict of interest.\nEthical standard All procedures performed in studies involving human par-\nticipants were in accordance with the ethical standards of the institutional\nand/or national research committee and with the 1964 Helsinki Declaration\nand its later amendments or comparable ethical standards.\nInformed Consent Informed consent was obtained from all individual partici-\npants included in the study.",
      "metadata": {
        "filename": "Flexible and Comprehensive Patient-Specific Mitral Valve Silicone Models with Ch.pdf",
        "hotspot_name": "PBT_Injection_Molding",
        "title": "Flexible and Comprehensive Patient-Specific Mitral Valve Silicone Models\n  with Chordae Tendinae Made From 3D-Printable Molds",
        "published_date": "2019-04-07T18:19:54Z",
        "pdf_link": "http://arxiv.org/pdf/1904.03704v1",
        "query": "injection molding PBT housing energy efficiency optimization cycle time reduction"
      }
    },
    "Generalized Population-Based Training for Hyperparameter Optimization in Reinfor": {
      "full_text": "IEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE, 2024 1\nGeneralized Population-Based Training for\nHyperparameter Optimization in Reinforcement\nLearning\nHui Bai and Ran Cheng, Senior Member, IEEE\nAbstract —Hyperparameter optimization plays a key role in\nthe machine learning domain. Its significance is especially pro-\nnounced in reinforcement learning (RL), where agents continu-\nously interact with and adapt to their environments, requiring\ndynamic adjustments in their learning trajectories. To cater\nto this dynamicity, the Population-Based Training (PBT) was\nintroduced, leveraging the collective intelligence of a population\nof agents learning simultaneously. However, PBT tends to favor\nhigh-performing agents, potentially neglecting the explorative\npotential of agents on the brink of significant advancements.\nTo mitigate the limitations of PBT, we present the Generalized\nPopulation-Based Training (GPBT), a refined framework de-\nsigned for enhanced granularity and flexibility in hyperparameter\nadaptation. Complementing GPBT, we further introduce Pairwise\nLearning (PL). Instead of merely focusing on elite agents, PL em-\nploys a comprehensive pairwise strategy to identify performance\ndifferentials and provide holistic guidance to underperforming\nagents. By integrating the capabilities of GPBT and PL, our\napproach significantly improves upon traditional PBT in terms\nof adaptability and computational efficiency. Rigorous empirical\nevaluations across a range of RL benchmarks confirm that our\napproach consistently outperforms not only the conventional\nPBT but also its Bayesian-optimized variant. Source codes are\navailable at https://github.com/EMI-Group/gpbt-pl.\nIndex Terms —Evolutionary Reinforcement Learning,\nPopulation-Based Training, Hyperparameter Optimization.\nI. I NTRODUCTION\nDeep neural networks (DNNs) have established themselves\nas the de facto function approximators in the realm of re-\ninforcement learning (RL). Their powerful representational\ncapacities have been instrumental in enabling RL to make\nsignificant inroads into a wide array of challenges. This\nincludes solving deterministic systems like board games, as\nevidenced by AlphaGo [1], to mastering the stochastic dynam-\nics of arcade games [2]. Furthermore, DNNs have been pivotal\nin addressing more complex, real-world challenges, such as\nrobot control, where the interaction with the environment is\nmultifaceted and nuanced [3].\nAt the heart of these impressive RL accomplishments lies\nthe intricate task of hyperparameter tuning. Both the RL\nalgorithms, which dictate agent learning, and the DNN archi-\ntectures, which define the model’s complexity and capacity, are\ngoverned by a myriad of hyperparameters [4]. These hyper-\nparameters, when optimally configured, have the potential to\nThe authors are with the Department of Computer Science and Engineering,\nSouthern University of Science and Technology, Shenzhen 518055, China. E-\nmail: huibaimonky@163.com, ranchengcn@gmail.com. (Corresponding au-\nthor: Ran Cheng)unlock the full capabilities of DNNs, as has been observed\nacross various deep learning applications [5]. However, a\nunique challenge in RL is its non-stationary nature [6]. Unlike\ntraditional supervised learning, where data distribution remains\nstatic, RL involves agents that learn from a dynamically\nchanging environment. This constant evolution, influenced\nby the agent’s interactions and its learning trajectory, means\nthat a static set of hyperparameters may not remain optimal\nthroughout the training process.\nRecognizing the need for a dynamic and automated ap-\nproach to hyperparameter tuning, the machine learning com-\nmunity has turned to hyperparameter optimization (HPO).\nWithin the burgeoning field of automated machine learning\n(AutoML) [7], HPO has become indispensable. The allure of\nHPO lies in its promise to reduce the manual, often tedious,\ntrial-and-error based approach to model tuning. By automating\nthis process, HPO not only enhances the efficiency of learning\nalgorithms but also contributes to scientific rigor, ensuring\nexperiments are reproducible and unbiased [7]. Predominantly,\nHPO techniques can be stratified into two camps: sequential\noptimization methods, where each evaluation informs the next\n(e.g., Bayesian optimization [8]), and parallel search strategies,\nwhere multiple evaluations occur independently (e.g., random\nsearch and grid search [9]).\nAmidst the plethora of HPO techniques, the Population-\nBased Training (PBT) [10] has emerged as a front-runner,\nespecially given its proven empirical successes across an array\nof computational domains [11], [12]. What sets PBT apart\nis its ability to optimize DNN weights and hyperparameters\nin tandem, by extracting and aggregating insights from a\npopulation of agents during a single training run. PBT’s asyn-\nchronous nature means that agents can periodically refine their\nhyperparameters by emulating better-performing counterparts,\nthereby charting an effective hyperparameter trajectory. Yet,\nPBT is not without its limitations. Its propensity to focus\nprimarily on top-performing agents can sometimes stymie\nbroader exploration. This bias towards winners can lead to\npremature convergence, neglecting agents that might have\nexhibited superior performance given more time or slightly\ndifferent conditions – the so-called late bloomers .\nTo address the inherent limitations of PBT, we introduce\nthe Generalized Population-Based Training (GPBT), a flex-\nible HPO framework that builds upon PBT’s asynchronous\nparallelism. In GPBT’s HPO phase, agents are randomly\npaired, offering the opportunity to adjust hyperparameters\nthrough user-defined strategies. Coupled with GPBT, we havearXiv:2404.08233v2  [cs.LG]  23 Apr 2024IEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE, 2024 2\ntailored the Pairwise Learning (PL) method, which leverages\na pseudo-gradient approach reminiscent of Stochastic Gra-\ndient Descent with Momentum (SGDM) [13]. PL computes\na pseudo-gradient for underperforming agents based on the\nperformance difference with their paired counterparts. These\nagents subsequently refine their behavior using this derived\ngradient and their previous updates. By continually resampling\nthroughout training and harnessing the aggregate knowledge of\nthe population, agents iteratively refine their update trajectories\ntowards optimal directions. Crucially, our approach retains a\nbroad spectrum of agents, both high-performing and those\nlagging, ensuring a diverse population. This diversity fosters\nexploration, enabling agents to navigate beyond local optima,\nstriving for superior performance outcomes. In summary, our\nmain contributions are:\n•We present GPBT, a versatile HPO framework that builds\nupon the foundational principles of PBT. Distinctively,\nGPBT is architected to be inherently adaptable, ac-\ncommodating a broad range of optimization strategies.\nThis adaptability ensures that GPBT remains pertinent\nacross diverse hyperparameter tuning contexts, offering\nresearchers and practitioners a flexible tool that can be\ntailored to specific challenges and scenarios.\n•We have conceptualized and developed PL as an opti-\nmization method tailored for HPO. At its core, PL lever-\nages pseudo-gradients, which serve as heuristics to guide\nthe update trajectories of agents, particularly in complex\nblack-box HPO landscapes. This method ensures that\nagents can make informed adjustments to their behaviors,\neven when the optimization landscape is intricate and\nlacks explicit gradients.\n•We have empirically assessed the efficacy of the in-\ntegrated GPBT-PL approach (GPBT with PL) in the\nrealm of HPO in RL. Through rigorous experiments\nbenchmarked against a comprehensive suite of OpenAI\nGym environments, we provide conclusive evidence of\nGPBT-PL’s robust performance. Notably, our findings\nunderscore its superior performance relative to the tradi-\ntional PBT approach and its Bayesian-optimized variant.\nThis superiority is maintained even when computational\nresources are stringent, attesting to the efficiency and\neffectiveness of our proposed methodology.\nThe paper is structured as follows: Section II reviews\nrelevant literature, Section III presents the research motivation,\nSection IV describes our approach, Section V discusses exper-\nimental results, and Section VI provides concluding remarks.\nII. R ELATED WORK\nHPO is indispensable for optimizing machine learning\nmodel performance. As models grow in complexity, the\ndemand for sophisticated HPO techniques intensifies. This\nsection provides an overview of the evolution of HPO, from\ngeneral-purpose methods to those specifically crafted for RL,\nwith a particular focus on population-based HPO methods.\nA. General HPO Methods\nAt its core, HPO is designed to address black-box optimiza-\ntion problems where the objective function is typically non-differentiable. This necessitates iterative sampling and evalua-\ntion of multiple hyperparameter configurations to identify the\nmost effective setup.\nTypically, any black-box optimization method can be uti-\nlized for HPO. The most fundamental HPO method is grid\nsearch, and however this method suffers from the curse of\ndimensionality. A better alternative method is random search,\nwhich provides a valuable baseline as it is anticipated to con-\nverge toward optimal performance if given enough resources.\nThough straightforward, these two methods often lack compu-\ntational efficiency, evaluating unpromising models extensively\n[9]. When introducing guidance, population-based methods\nsuch as evolutionary algorithms usually perform better than\nrandom search by applying local perturbations (mutations)\nand combinations of different members (crossover) to generate\na new generation of improved configurations. Additionally,\nBayesian optimization emerged as a pivotal method, leveraging\nprobabilistic models to predict promising hyperparameters\n[14]. Though this method can efficiently explore the search\nspace by updating a surrogate probabilistic model of the\nobjective function and using an acquisition function to guide\nthe search toward promising regions, it is computationally\nintensive, especially for high-dimensional parameter spaces\nor expensive-to-evaluate objective functions. Finally, multi-\nfidelity optimization entails a balance between optimization\nefficacy and runtime efficiency by evaluating a configura-\ntion on a small data subset or with limited resources. De-\nspite these advancements, HPO still grapples with challenges\nlike extended optimization durations and striking the right\nexploration-exploitation balance.\nB. HPO in RL\nIn RL, hyperparameters play a pivotal role in shaping agent-\nenvironment dynamics, thereby directly influencing learning\ntrajectories and decision outcomes. A unique challenge in RL\nis the non-stationarity of the environment. As the agent itera-\ntively updates its policy, its interactions with the environment\nevolve, making the hyperparameter tuning process intricate. At\ntimes, minor hyperparameter adjustments can yield profound\nimpacts on agent performance.\nSeveral methods have been utilized, albeit without specific\ncustomization for HPO in RL, such as the multi-fidelity\noptimization methods (e.g., HyperBand [15] and ASHA [16]),\nwhich incorporate dynamic resource allocation and early ter-\nmination for unpromising configurations. Nonetheless, these\nmethods might prematurely discard promising configurations\nand encounter challenges in striking a balance between per-\nformance and computational budgets.\nIn contrast, several methods have been tailored for HPO\nin RL. In [17], the learning rate αand temperature τin\nSarsa( λ) have been optimized by genetic algorithms to balance\nexploration and exploitation for food capture tasks, which\nintegrates learning and evolution to effectively enhance the\nperformance of RL algorithms and obtain sim-to-real robust\npolicies. Another method involves executing parallel RL in-\nstances with distinct initial hyperparameters, augmented with\nGaussian noise [18]. Notably, an off-policy HPO methodIEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE, 2024 3\nfor policy gradient RL algorithms has emerged [19], which\nestimates the performance of candidate configurations by an\noff-policy method and updates the current policy greedily.\nRecently, the population-based HPO methods have risen\nto prominence, offering dynamic hyperparameter adjustments\nthroughout training, free from usage restrictions [10], [11],\n[20].\nC. Population-Based HPO Methods\nIn contrast to traditional HPO strategies that sequentially op-\ntimize individual hyperparameter sets, population-based meth-\nods optimize multiple configurations in parallel [21]. These\nmethods maintain a diverse set of hyperparameters and employ\nevolutionary algorithms to navigate the hyperparameter space,\nensuring a harmonious blend of exploration and exploitation.\nKey population-based methods include Bayesian Optimiza-\ntion and HyperBand (BOHB) [22], Genetic HPO [23], and\nPBT. For instance, BOHB marries Bayesian optimization’s\nprobabilistic function modeling with HyperBand’s resource-\nefficient early termination. Genetic HPO employs evolution-\nary techniques like crossover and mutation to generate new\nhyperparameter sets. However, these methods may encounter\nchallenges in scenarios that demand extensive evaluations or\nhave highly variable evaluation durations, such as in RL.\nD. Population-Based Training\nPopulation-based Training (PBT) [10] stands out as a rep-\nresentative approach in the realm of population-based HPO\nmethods. Central to PBT is the notion of Lamarckian evo-\nlution, wherein agents not only inherit but also evolve their\nattributes. This ensures that hyperparameters are dynamically\nattuned to the ongoing learning phase, thereby optimizing\nlearning outcomes.\nIn PBT, each agent, armed with specific weights and hyper-\nparameters, periodically assesses its performance based on a\npredefined step count, signifying its ready state. Once in this\nstate, all agents are ranked by performance. Agents who do\nnot measure up to their peers adopt attributes from superior\nagents through a two-pronged strategy:\n•exploit : The lagging agent inherits both weights and\nhyperparameters from a superior agent.\n•explore : Hyperparameters are subjected to random pertur-\nbation, either amplified by a factor of 1.2, diminished by\n0.8, or resampled according to their original distribution.\nThis strategy empowers each agent to traverse multiple hyper-\nparameter landscapes during its training journey, an invaluable\ntrait considering the fluidity inherent to deep RL agent train-\ning.\nA key strength of PBT lies in its amalgamation of both\nsequential and parallel optimization techniques. Its asyn-\nchronous architecture ensures continuous training for some\nagents even as others update, resulting in superior performance\nacross varied applications [19], [24], [25]. This asynchronous\nparadigm augments training efficiency, replacing stagnating\nconfigurations with emerging ones and introducing slight\nrandom perturbations, all while other configurations proceedundisturbed. PBT’s proficiency in sculpting hyperparameter\nschedules tailored for complex RL tasks has been well-\ndocumented and validated across numerous benchmarks [11],\n[12], [26].\nExcept for the PBT paradigm, several other population-\nguided methodologies have been proposed for RL [27]–[31].\nHowever, they have distinctive functional differences in the\npopulation, which sample diverse experiences through the\nevolution of populations to address the exploration challenges\nof gradient-based RL algorithms.\nIII. M OTIVATION\nPBT’s asynchronous parallel paradigm has indeed proven\nefficient in a multitude of scenarios. Nonetheless, its pre-\ndominant focus on elite agents inadvertently stymies broader\nexploration capabilities due to its inherent predilection for\nimmediate gains. Specifically, the limitation manifests in two\nprimary ways:\n•PBT’s direct replacement strategy might prematurely dis-\ncard promising regions of the search space. These areas,\nalbeit seemingly suboptimal in the short term, might\nharbor superior solutions in a longer timeframe.\n•The overemphasis on exploiting top-tier solutions raises\nthe specter of converging to local optima. While such\na strategy may deliver satisfactory solutions in the short\nrun, it contravenes the quintessential ethos of population-\nbased HPO, which is to strike a judicious balance be-\ntween exploration and exploitation.\nMaintaining diversity within the population is of paramount\nimportance, especially in the context of RL tasks. A diverse\npopulation fosters the emergence of varied behaviors, culmi-\nnating in more resilient and holistic performance solutions,\nas underscored by the principles of novelty search techniques\n[32], [33]. PBT’s structural design, which typically yields a\nsingle solution per generation, accentuates the importance of\ncrafting diverse and efficient successors.\nIn light of these challenges, the avant-garde Population-\nBased Bandits (PB2) technique leverages Bayesian optimiza-\ntion to invigorate underperforming agents, outclassing PBT in\nterms of efficiency, albeit at an elevated computational expense\n[20]. However, Bayesian optimization, with its computational\nintricacies, often imposes overheads that sometimes eclipse\nthe actual costs associated with hyperparameter evaluations\n[34]. As a result, neither PBT nor PB2 truly achieve their\nfull efficiency potential. More recently, while advanced PBT\nvariants like FIRE PBT and BG-PBT [35], [36] have emerged,\nthey often grapple with challenges tied to their inherent greedy\nbehavior or the time-intensive process of generating new\nhyperparameters.\nTo redress these shortcomings, we introduce the Generalized\nPopulation-Based Training (GPBT) framework. While it builds\non PBT’s asynchronous underpinnings, GPBT distinguishes\nitself by replacing PBT’s direct substitution method with a nu-\nanced dual-agent learning mechanism. When viewed through\nthe lens of evolutionary computation (EC), GPBT aligns with\nthe steady-state EC paradigm, where each iteration introduces\na single new agent. This characteristic makes GPBT especiallyIEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE, 2024 4\nadept at navigating dynamic environments, echoing proficient\nsteady-state EC methodologies [37]. Interestingly, both PBT\nand PB2 can be seamlessly encapsulated within the GPBT\nframework, with the key differentiation being their respective\nstrategies for engendering new agents. Essentially, PBT can\nbe considered a variant within the GPBT framework. In this\ncontext, PBT differentiates itself through its specific method\nfor hyperparameter update, which is conducted via random\nperturbation.\nTo further enhance GPBT and expedite hyperparameter re-\nfinement, we propose the Pairwise Learning (PL) paradigm. In\nthis method, a lagging agent refines its parameters by leverag-\ning insights from a superior counterpart, thus illuminating po-\ntentially overlooked regions of the search space. The diversity\nintroduced by myriad pairings throughout training ensures a\ncomprehensive exploration. Continual steering towards propi-\ntious directions allows the solution sets to progressively gravi-\ntate towards optimal configurations. Drawing inspiration from\nthe Stochastic Gradient Descent with Momentum (SGDM)\nmethodology [13], which is prevalent in deep learning, PL\nemploys a pseudo-gradient as an approximation to the elusive\ngenuine gradient of the hyperparameter objective function. To\nensure stable and coherent updates, PL incorporates historical\ndata of the lagging solution as a momentum component.\nIV. P ROPOSED APPROACH\nWe begin by delineating the problem statement for HPO and\ndiscussing the nuances and constraints of PBT in Section IV-A.\nWe then introduce the GPBT framework and discuss its\nefficacy in Section IV-B. Finally, we detail the implementation\nof PL for hyperparameter updates in Section IV-C.\nA. Problem Statement\nHPO is tasked with identifying an optimal hyperparameter\nvector xwithin the search domain D ∈Rd, where ddenotes\nthe number of hyperparameters. In RL, this is tantamount\nto maximizing the cumulative reward across domain D. The\ncumulative reward, represented as total discounted rewardP\nt≥0γtrtwith γas the discount factor, is derived from\na trajectory τ= (s0, a0, r0, s1, ...)depicting the interplay\nbetween an RL agent and its environment. Here, the agent’s\npolicy governs its actions abased on the current state s,\nand the environment provides a corresponding reward r. The\npolicy, πθ, maps the environment’s state to the agent’s actions,\nand is implemented using a neural network with weights θ.\nThus, the HPO challenge can be articulated as a bi-level\noptimization problem:\nmax\nxf(x, θ∗) s.t. θ∗∈argmax\nθJ(θ;x)\nmax\nθJ(θ;x) where J(θ;x) =Eτ∼πθ[P\nt≥0γtrt],(1)\nwhere the outer loop optimization problem is max xf(x, θ∗),\nand the inner loop optimization problem is max θJ(θ;x).\nSpecially, in population-based HPO, we consider nagents\n(x1,x2, ...,xn), with each agent evolving its hyperparame-\nters over time (xt\n1,xt\n2, ...,xt\nn)t=1,...,T, where trepresents the\nelapsed time steps, epochs, or iterations of an agent’s training.B. Generalized Population-Based Training (GPBT)\nFig. 1 illustrates the GPBT framework. A population of\nagents are initialized with random weights and hyperparame-\nters, and then trained and evaluated in parallel. Upon reaching\ndesignated hyperparameter update intervals (i.e., perturbation\ninterval), ready agents undergo asynchronous random pairing\nto form parent pairs. If a ready agent underperforms, it\nadopts the weights of its superior counterpart and adjusts its\nhyperparameters using specialized learning techniques (e.g.,\nrandom perturbation or pairwise learning). After fulfilling the\nstopping criteria, top-performing agents are identified.\nFrom an evolutionary computation (EC) standpoint, GPBT\naligns with the steady-state EC methodology, which introduces\none new agent per iteration. Steady-state EC is recognized for\nits aptitude in addressing non-stationary challenges character-\nized by gradual, low-frequency alterations [37], [38]. Such\ndynamics closely mirror HPO scenarios, where consecutive\ntraining sessions often involve subtle hyperparameter mod-\nifications. A salient feature of steady-state EC is its quick\nadaptability, made possible as the newly introduced agent\nimmediately joins the mating pool. This swift integration\nfacilitates an early progression towards the optimal solution\nduring the optimization phase [37], [39]. Considering system\nstability, the steady-state EC introduces minimal diversity to\nthe population after an environmental modification, like a hy-\nperparameter revision. Consequently, GPBT not only functions\nas an asynchronous HPO framework but also excels in stably\nnavigating changes in non-static scenarios.\nIn population-based strategies, a pivotal challenge arises\nwhen deciding which agent should be replaced by new entrants\n[40]. Traditional techniques in steady-state EC typically advo-\ncate for replacing either the oldest agent or the least perform-\ning one. However, these strategies are not directly applicable\nto the GPBT paradigm. A primary reason is the asynchronous\ndesign: if the system engages in continual evaluations to\nidentify the least effective agent, the benefits of asynchrony\nare undermined. Intriguingly, within GPBT, replacements are\nexclusively between paired agents, often resulting in the substi-\ntution of the ready (and comparatively older) underperforming\nagent. This strategy presents a nuanced balance between age-\ndriven and performance-driven replacements. Such a strategy\nnot only maintains the asynchrony but also fosters diversity\nwithin the population.\nThis emphasis on diversity is further underscored by var-\nious studies that delve into the evolutionary trajectory of\nEC. They suggest an incremental improvement in population\nperformance over iterations [41]. At a high level, when two\nhigh-performing agents are paired, the likelihood of producing\nan equally or more competent offspring is elevated compared\nto pairings between lower-performing agents. However, this\ndoes not insinuate that elite pairings always produce top-\ntier offspring, nor is there an assurance that all initial agents\nwill exhibit high performance. Consequently, the design of\noffspring generation algorithms is critical, with an emphasis\non consistently producing high-quality agents. To address\nthese challenges, especially in the context of HPO in RL, we\nintroduce the Pairwise Learning (PL) method.IEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE, 2024 5\nPerformance\nHyperparameters\nWeightsHParams \nLearningAsynchronous \nRandom Paring\nPopulation \nInitializationParallel \nTrainingHyperparameter \nUpdateStop?Optimal \nAgents\nFig. 1. Framework of Generalized Population-Based Training (GPBT). A population of agents are initialized with random weights and hyperparameters\nand then trained in parallel. Upon reaching designated hyperparameter update intervals, ready agents undergo asynchronous random pairing for updates. If\nthe ready agent underperforms, it adopts the weights of its superior counterpart and updates its hyperparameters using specialized learning techniques. After\nfulfilling the stopping criteria, top-performing agents are identified.\nC. Pairwise Learning (PL)\nFor the preservation of promising candidates and the formu-\nlation of optimal hyperparameters, we introduce the Pairwise\nLearning (PL) approach tailored for hyperparameter refine-\nment. In PL, beyond the hyperparameters xand weights θ,\nagents possess a d-dimensional velocity vector vdesignated\nfor hyperparameter adjustments, with ddenoting hyperparam-\neter count. This vector is initialized to zero. Each generation\nwitnesses the random pairing of two agents, followed by per-\nformance comparisons. The superior performing agent, termed\nthe ’fast learner’, is directly incorporated into the population.\nConversely, the lesser-performing ‘slow learner’ adopts the\nweights of its counterpart and amends its hyperparameters and\nvelocity through a learning mechanism derived from the fast\nlearner.\nConsider xg\nf,xg\ns,vg\nf, and vg\nsas the hyperparameters and\nvelocities of the fast and slow learners at generation grespec-\ntively. The slow learner’s updates are guided by:\nxg+1\ns=xg\ns+vg+1\ns, (2)\nvg+1\ns=r1vg\ns+r2(Gg\nf(xf;uf)−Gg\ns(xs;us)), (3)\nwhere r1andr2are uniformly distributed random vectors\nwithin [0,1]d. The terms Gg\nf(xf;uf)andGg\ns(xs;us)sym-\nbolize distributions of the fast and slow learners respec-\ntively. The difference between these distributions reflects in\nGg\nf(xf;uf)−Gg\ns(xs;us). Therefore, both learners can be\nconceptualized as samples from their respective distributions.\nDistinguishing between learners with varying performances\nis achieved through their distributions. Sorting based on per-\nformance, top-tier agents epitomize fast learner distributions,\nwhile their lower-tier counterparts represent slow learners.\nThis stratified approach enhances the learning efficiency of\nslow learners. Continuous sampling ensures gradient correc-\ntions between distributions, gravitating towards optimal gradi-\nents.\nInspired by Stochastic Gradient Descent with Momentum\n(SGDM), a prevalent optimization strategy in deep learning,PL operates as a pseudo-gradient-driven approach. SGDM, an\nenhancement of conventional SGD [42], integrates momentum\nto expedite optimization convergence. SGDM’s update equa-\ntions for a maximization problem involving parameter θare:\nθt+1=θt+vt+1, (4)\nvt+1=βvt+η×gradient, (5)\nwhere βdictates momentum contribution and ηmodulates\nthe gradient’s learning step size. The term βvtacts as a\nvelocity component, aiding SGD in oscillation mitigation and\nconvergence acceleration by capturing parameter movement\ntrends across iterations.\nContrary to SGDM, which computes true gradients of a\nloss function via a training data subset, PL estimates surrogate\ngradients concerning hyperparameter values using merely two\nagents. In essence, PL embodies an SGDM variant with a\nbatch size of one, optimized for PBT’s asynchronous nature.\nGiven that PL’s gradient estimates are based on dual samples,\nthe learning direction occasionally deviates from the optimal,\nintroducing noise. Introducing the momentum term r1vg\nsin\nPL averages out this noise, offering a refined estimate closer\nto the original function’s precise derivation. Consequently,\nPL’s frequent hyperparameter updates yield rapid convergence.\nApart from its ease of implementation, convergence guaran-\ntees, and scalability, PL also mirrors traits of the competitive\nswarm optimizer [43], wherein losers are made to learn from\nthe winners via randomly paired competitions. In essence,\nPL exemplifies hyperparameter update strategies, and further\noptimization techniques from gradient-agnostic methods can\nbe woven into GPBT to enhance hyperparameter update\nmethodologies.\nD. GPBT-PL\nBy marrying the flexibility and asynchronous features of\nGPBT with the adaptive learning mechanisms of PL, we\nprovide the integrated approach – GPBT-PL. Algorithm 1\nprovides a detailed procedure for this integrated approach.IEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE, 2024 6\nAlgorithm 1: Generalized Population-Based Training\nwith Pairwise Learning (GPBT-PL)\nInput: Population size n, Hyperparameter ranges R, Perturbation\ninterval δ, Training cost T.\nOutput: The optimal agent.\n1t←0;\n2P←initialize population with random HParams and weights;\n3foragents∈Pdo\n4 while t < T do\n5 P←parallel training of population P;\n6 iftmodδ== 0 then\n7 a(θ,x)←get the ready agent;\n8 P←rank population Paccording to performance;\n9 a′(θ′,x′)←select a better-performing agent;\n10 a(θ) =a′(θ′);\n11 a(x)←PairwiseLearning (a(x), a′(x′))using\n(2)&(3);\nCommencing with the initialization of agents with random\nhyperparameters and weights, the algorithm then delves into\nthe parallel training of the entire population. Periodic checks,\nas determined by the perturbation interval δ, identify agents\nready for updates. These agents then leverage the PL mech-\nanism to adapt their hyperparameters, ensuring a continuous\npush toward enhanced performance.\nUnlike PBT, which follows a predetermined update strategy,\nGPBT offers a more adaptable platform for hyperparameter\nrefinement. This adaptability is further enhanced with the\nintegration of PL, ensuring that hyperparameters are fine-\ntuned through a comparative process between two agents. This\npromotes knowledge sharing and quick adaptation, capitalizing\non the strengths of both the fast learner and the slow learner\nagents.\nAnother crucial aspect of GPBT-PL lies in its asynchronous\nnature, where agents can undergo updates without waiting for\nthe entire population to be ready. This ensures that the system\nremains dynamic and responsive to changes via continuously\nevolving and adapting. Furthermore, the modularity of GPBT-\nPL allows for extensibility. While the current implementation\nis grounded on PL for hyperparameter updates, the framework\ncan easily accommodate other learning or optimization meth-\nods, making it a versatile tool in the domain of HPO.\nV. E XPERIMENTS\nIn our experimental design, we categorize our focus into\ntwo main areas: on-policy RL and off-policy RL. First, we\nconduct experiments to assess the general performance of\nthe proposed GPBT-PL in several tasks in each area. Then,\nacknowledging the sensitivities of PBT-based HPO algo-\nrithms to perturbation intervals, hyperparameter boundaries,\nand scalability with larger populations, we design experiments\nemphasizing robustness and scalability. Specifically, for on-\npolicy RL, we assess the stability against perturbation interval\nvariations and population size changes; for off-policy RL, we\nexamine the HPO algorithms’ robustness to alterations in the\nhyperparameter range.TABLE I\nPARAMETER SETTINGS\nRL Algorithms Hyperparameter Value\nPPOBatch size [1000, 60000]\nGAE λ [0.9, 1.0)\nPPO Clip ϵ 0.99, [0.95, 1.0)\nLearning Rate η [10−5, 10−3)\nDiscount γ 0.99, [0.95, 1.0)\nSGD Minibatch Size 128, [16, 256]\nSGD Iterations 10, [5, 15]\nPolicy Architecture {32, 32}\nFilter MeanStdFilter\nPopulation Size {4, 8, 16}\nPerturbation Interval {1 ×104, 5×104}\nIMPALAEpsilon [0.01, 0.5)\nLearning Rate η [10−5, 10−3), [10−5, 10−2)\nEntropy Coefficient [0.001, 0.1)\nBatch Size 500\nDiscount γ 0.99\nSGD Minibatch Size 500\nSGD Iterations 1\nPolicy Architecture {256, 256}\nPopulation Size 4\nPerturbation Interval 5 ×104\nCommon Hyperparameters\nNumber of Workers 5\nNumber of GPUs 0\nOptimizer Adam\nNonlinearity Tanh\nA. Experimental Settings\nOur experiments predominantly target the RL domain, em-\nphasizing its notorious susceptibility to hyperparameters [44].\nWe embarked on an extensive study across a plethora of tasks\nfrom OpenAI Gym, version five [45]. Fig. 2 showcases our\nchosen eight task scenarios. The experimentation bifurcates\ninto:\n1) Optimizing four hyperparameters for the on-policy RL\nalgorithm, Proximal Policy Optimization (PPO) [46],\nacross the initial six continuous control challenges.\n2) Refining three hyperparameters for the off-policy RL\nalgorithm, Importance Weighted Actor-Learner Archi-\ntecture (IMPALA) [24], for the concluding two discrete\ncontrol tasks.\nEach experimental run is iterated with seven distinct seeds. Ta-\nbles present the apex mean rewards across all seeds, which are\ndefined as the average of the final 10 episodic rewards during\ntraining. Correspondingly, figures illustrate both the mean and\nstandard deviation of these pinnacle rewards. Our objective\nis to spotlight the zenith of mean rewards within tables, a\ncrucial metric in practical applications. All experiments were\nfacilitated by the Ray Tune library [47] and Ray RLlib [48].\n1) Hyperparameter Settings: Table I catalogues the opti-\nmized hyperparameters, their respective boundaries, and cer-\ntain constant hyperparameters. For both experimental cate-\ngories, we scrutinized population sizes n∈ {4,8}and des-\nignated five workers for every agent, ensuring the algorithm’s\nlocal executability on contemporary computational platforms.\nFor the on-policy RL experiments, we delved into robustness\nagainst perturbation interval adjustments (precisely, curtailingIEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE, 2024 7\n(a) BipedalWalker\n (b) Ant\n (c) HalfCheetah\n (d) InvertedDoublePendulum\n(e) Swimmer\n (f) Walker2D\n (g) Breakout\n (h) SpaceInvaders\nFig. 2. Eight RL tasks selected from OpenAI Gym.\nit from 5×104to1×104), and scalability concerning\naugmented populations ( n= 16 ) and increased number of\nhyperparameters (from 4 to 7). In the realm of off-policy\nRL, we assessed resilience against hyperparameter boundary\nmodifications, notably expanding the learning rate domain\nfrom [10−5,10−3]to[10−5,10−2].\n2) HPO Baselines: Random search (RS) serves as our\nfoundational benchmark, courtesy of its assumption-agnostic\napproach, often culminating in asymptotically near-optimal\nperformance [9]. To augment the challenge, RS’s initializa-\ntion process was refined by sampling hyperparameters from\npartitioned grid intervals. PBT remains our principal com-\nparative standard, maintaining configurations congruent with\nGPBT-PL. Specifically, in both PBT and GPBT-PL, agents\npositioned in the bottom quartile (representing slower learners)\nare supplanted by their counterparts from the top quartile\n(exemplifying rapid learners), with a resample probability\npegged at 0.25. Additionally, our outcomes were juxtaposed\nwith PB2 [20], an enhancement of PBT that substitutes the\nrandom heuristic with Bayesian optimization.\nB. On-Policy Reinforcement Learning\nIn experiments for HPO in on-policy RL, we aimed to opti-\nmize four hyperparameters: batch size, GAE λ, PPO clip ϵ, and\nlearning rate ηfor the PPO algorithm. The chosen continuous\ncontrol tasks were BipedalWalker, Ant, HalfCheetah, Inverted-\nDoublePendulum, Swimmer, and Walker2D. We employed\npopulation sizes of 4 and 8 with a perturbation interval of\n5×104. Experiments concluded when all algorithms achieved\nstable convergence, ensuring a fair comparison, contrasting\nwith the approach in the PB2 paper where a fixed number of\ntimesteps defined the stopping criterion. TABLE II tabulates\nthe best mean rewards. Fig. 3 depicts their mean and standardTABLE II\nBEST MEAN REWARDS ACROSS 7SEEDS . THE BEST -PERFORMING\nALGORITHMS ARE BOLDED . THE LAST COLUMN PRESENTS THE\nPERCENTAGE OF PERFORMANCE DIFFERENCE BETWEEN GPBT AND PBT,\nWHERE DIFFERENCES LESS THAN 1% ARE REPRESENTED WITH ≈.\nBenchmarks n RS PB2 PBT GPBT-PL vs. PBT\nBipedalWalker 4 292 303 282 293 +4%\nAnt 4 4283 5000 5347 5497 +3%\nHalfCheetah 4 4834 4938 4911 5262 +7%\nInvertedDP 4 8531 9356 9354 9274 ≈\nSwimmer 4 133 153 134 166 +24%\nWalker2D 4 2267 1851 1800 2372 +32%\nBipedalWalker 8 284 297 298 303 +2%\nAnt 8 4508 5150 5705 6116 +7%\nHalfCheetah 8 4842 5152 5292 5463 +3%\nInvertedDP 8 9185 9340 9340 9356 ≈\nSwimmer 8 155 135 157 168 +7%\nWalker2D 8 2776 2528 2855 3047 +7%\ndeviation across all seeds, with population sizes annotated\nwithin brackets, and Fig. 4 further shows the population\nevolution process along the training timesteps and time (in\nhours) for the top-performing seed.\nPredominantly, GPBT-PL showcased superior performance\nover PBT. In 75% of the scenarios, GPBT-PL surpassed\nother algorithms in realizing the performance ceiling. For the\nremainder, GPBT-PL’s performance closely mirrored the top-\nperforming algorithms.\nFor smaller populations ( n= 4 ), GPBT-PL registered\nmarked enhancements of 24% and 32% over PBT for the\nSwimmer and Walker2D tasks, respectively. The efficacy of\nGPBT-PL was notably higher with smaller populations com-\npared to larger ones ( n= 8 ). Conversely, PBT’s perfor-\nmance lagged behind RS for BipedalWalker and Walker2DIEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE, 2024 8\n0 1 2 3 4 5 6 7\nTimesteps 1e6100\n0100200300Reward\nGPBT-PL\nPBT\nPB2\n(a) BipedalWalker (4)\n0.0 0.2 0.4 0.6 0.8 1.0\nTimesteps 1e70100020003000400050006000RewardGPBT-PL\nPBT\nPB2 (b) Ant (4)\n0.0 0.2 0.4 0.6 0.8 1.0\nTimesteps 1e7010002000300040005000Reward\nGPBT-PL\nPBT\nPB2 (c) HalfCheetah (4)\n0 1 2 3 4 5 6\nTimesteps 1e60200040006000800010000Reward\nGPBT-PL\nPBT\nPB2 (d) InvertedDoublePendulum (4)\n0 1 2 3 4 5 6 7\nTimesteps 1e60255075100125150Reward\nGPBT-PL\nPBT\nPB2\n(e) Swimmer (4)\n0 1 2 3 4 5 6 7\nTimesteps 1e60500100015002000RewardGPBT-PL\nPBT\nPB2 (f) Walker2D (4)\n0 1 2 3 4 5 6 7\nTimesteps 1e6100\n0100200300Reward\nGPBT-PL\nPBT\nPB2 (g) BipedalWalker (8)\n0.0 0.2 0.4 0.6 0.8 1.0\nTimesteps 1e70100020003000400050006000RewardGPBT-PL\nPBT\nPB2 (h) Ant (8)\n0.0 0.2 0.4 0.6 0.8 1.0\nTimesteps 1e7010002000300040005000Reward\nGPBT-PL\nPBT\nPB2\n(i) HalfCheetah (8)\n0 1 2 3 4 5 6\nTimesteps 1e60200040006000800010000Reward\nGPBT-PL\nPBT\nPB2 (j) InvertedDoublePendulum (8)\n0 1 2 3 4 5 6 7\nTimesteps 1e60255075100125150Reward\nGPBT-PL\nPBT\nPB2 (k) Swimmer (8)\n0 1 2 3 4 5 6 7\nTimesteps 1e6050010001500200025003000RewardGPBT-PL\nPBT\nPB2 (l) Walker2D (8)\nFig. 3. Training curves for six OpenAI Gym benchmarks using populations of 4 and 8 agents with GPBT-PL, PBT, and PB2. Thick lines represent the\naverage of the best mean rewards over 7 seeds, with shaded regions denoting the standard deviation. Brackets specify the population size, and the perturbation\ninterval is set to 5×104.\nbut improved upon increasing the population size. This trend\nunderscores PBT’s dependency on extensive computational\nresources, a sentiment echoed in the original PBT publica-\ntion [10]. PBT’s aggressive strategy, which results in the\nearly dismissal of promising candidates, may underpin its\nperformance limitations. Such premature decisions become\nespecially detrimental with smaller populations.\nAll four HPO strategies exhibited commendable perfor-\nmance on the InvertedDoublePendulum task, potentially at-\ntributable to the task’s inherent simplicity, as noted in the PB2\nstudy [20]. While PB2’s performance was subpar to RS for\nWalker2D (4) and Swimmer (8), it outshone the other three\nmethods on BipedalWalker (4) and InvertedDoublePendulum.\nThis observation suggests PB2’s aptitude for less complex\ntasks. However, PB2’s reliance on Bayesian optimization for\nhyperparameter generation is computationally intensive, par-\nticularly with increasing population sizes, as shown in (e)-(h)\nof Fig. 4. Yet, PBT and GPBT-PL exhibited comparable time\nefficiencies, with neither approach showing an increase in time\nconsumption as the population size expanded. Additionally,\nPB2 grapples with the exploration-exploitation dilemma dur-\ning hyperparameter generation, amplifying the intricacies ofthe tuning challenge.\nIn summary, GPBT-PL consistently delivered promising\noutcomes across both small and large populations, record-\ning impressive rewards on challenging tasks like Ant and\nWalker2D. Based on the visualization of population evolution,\nthe performance of GPBT-PL exhibits a gradual increase in\nthe initial stages, followed by a rapid ascent in the middle\nand later stages. This is attributed to GPBT-PL’s ability to\npreserve late bloomers, thereby maintaining superior global\nsearch capability.\nTABLE III\nBEST MEAN REWARDS ACROSS 7SEEDS . THE STANDOUT ALGORITHMS\nARE HIGHLIGHTED IN BOLD . THE FINAL COLUMN DETAILS THE\nPERFORMANCE DIFFERENCE PERCENTAGE BETWEEN GPBT AND PBT.\nBenchmarks n RS PB2 PBT GPBT-PL vs. PBT\nBreakout 4 131 130 141 185 31%\nSpaceInvaders 4 611 485 634 725 14%IEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE, 2024 9\n0.0 0.2 0.4 0.6 0.8 1.0\nTimesteps 1e7010002000300040005000RewardGPBT-PL\nPBT\nPB2\n(a) Ant (4)\n0.0 0.2 0.4 0.6 0.8 1.0\nTimesteps 1e70100020003000400050006000RewardGPBT-PL\nPBT\nPB2 (b) Ant (8)\n0.0 0.2 0.4 0.6 0.8 1.0\nTimesteps 1e7010002000300040005000Reward\nGPBT-PL\nPBT\nPB2 (c) HalfCheetah (4)\n0.0 0.2 0.4 0.6 0.8 1.0\nTimesteps 1e7010002000300040005000Reward\nGPBT-PL\nPBT\nPB2 (d) HalfCheetah (8)\n0 1 2 3\nTime(h)010002000300040005000Reward\nGPBT-PL\nPBT\nPB2\n(e) Ant (4)\n0 1 2 3 4\nTime(h)0100020003000400050006000Reward\nGPBT-PL\nPBT\nPB2 (f) Ant (8)\n0 1 2 3\nTime(h)010002000300040005000Reward\nGPBT-PL\nPBT\nPB2 (g) HalfCheetah (4)\n0 1 2 3 4\nTime(h)010002000300040005000Reward\nGPBT-PL\nPBT\nPB2 (h) HalfCheetah (8)\nFig. 4. Training curves for Ant and HalfCheetah using populations of 4 and 8 agents with GPBT-PL, PBT, and PB2. (a)-(d) take timesteps as the x-axis and\n(c)-(h) take time (in hours) as the x-axis. Thick lines are the best-performing members of the population of each HPO method, with faint lines representing\neach member. Brackets specify the population size, and the perturbation interval is set to 5×104.\n0.0 0.2 0.4 0.6 0.8 1.0\nTimesteps 1e70255075100125150RewardGPBT-PL\nPBT\nPB2\n(a) Breakout\n0.0 0.2 0.4 0.6 0.8 1.0\nTimesteps 1e7100200300400500600700RewardGPBT-PL\nPBT\nPB2 (b) Space Invaders\nFig. 5. Training curves for 4-agent populations using GPBT-PL, PBT, and PB2\non two OpenAI Gym benchmarks. Thick lines denote average best rewards\nover 7 seeds, and shaded regions indicate standard deviation. The learning\nrate is set between [10−5,10−3].\nC. Off-Policy Reinforcement Learning\nIn experiments for HPO in off-policy RL, we optimized\nthe hyperparameters for the IMPALA algorithm on two\ngames from the Arcade Learning Environment: Breakout and\nSpaceInvaders [49]. We used the same three hyperparameters\nas the original IMPALA paper (epsilon, learning rate η, and\nentropy coefficient) and conducted our experiments with a\npopulation size of 4. Training was performed over 10 million\ntimesteps, equivalent to 40 million frames, with a perturba-\ntion interval of 5×104timesteps. Results are presented in\nTABLE III and visualized in Fig. 5.\nGPBT-PL consistently outperformed PBT, with significant\nimprovements of 31% in Breakout and 14% in SpaceInvaders.\nThis lead is more evident when compared against PB2 and\nRS. It is noteworthy that agents with more workers tend\nto perform better during training [24]. Impressively, GPBT-PL, with each agent trained using only 5 workers, matched\nthe performance of a hand-tuned IMPALA with 32 workers\nin SpaceInvaders, as observed in RLlib1. In Breakout, while\nnot reaching this benchmark, GPBT-PL still matched the\nperformance of A3C with 16 workers, as referenced from Fig.\n3 in [50]. This underscores GPBT-PL’s ability to achieve high-\nlevel performance in RL, even with constrained computational\nresources.\nTABLE IV\nBEST MEAN REWARDS ACROSS 7SEEDS . THE BEST -PERFORMING\nALGORITHMS ARE BOLDED . THE LAST COLUMN PRESENTS THE\nPERCENTAGE OF PERFORMANCE DIFFERENCE BETWEEN GPBT AND PBT,\nWHERE DIFFERENCES LESS THAN 1% ARE REPRESENTED WITH ≈.\nBenchmarks n RS PB2 PBT GPBT-PL vs. PBT\nBipedalWalker 4 292 295 300 308 +2%\nAnt 4 4283 3010 4051 5722 +41%\nHalfCheetah 4 4834 5163 5161 6264 +21%\nInvertedDP 4 8531 9343 9358 9355 ≈\nSwimmer 4 133 140 157 158 ≈\nWalker2D 4 2267 1829 2074 2200 +6%\nBipedalWalker 8 284 294 303 297 -2%\nAnt 8 4508 4876 5207 5724 +10%\nHalfCheetah 8 4842 4570 4705 5853 +24%\nInvertedDP 8 9185 9344 9357 9353 ≈\nSwimmer 8 155 129 132 135 +2%\nWalker2D 8 2776 1339 3017 3054 +1%\nD. Robustness to Perturbation Interval\nThe perturbation interval plays a pivotal role in PBT-class\nHPO algorithms. With a fixed training budget, a larger inter-\n1RLlib IMPALA 32-workers experimentsIEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE, 2024 10\n0 1 2 3 4 5 6 7\nTimesteps 1e6100\n0100200300Reward\nGPBT-PL\nPBT\nPB2\n(a) BipedalWalker (4)\n0.0 0.2 0.4 0.6 0.8 1.0\nTimesteps 1e70100020003000400050006000RewardGPBT-PL\nPBT\nPB2 (b) Ant (4)\n0.0 0.2 0.4 0.6 0.8 1.0\nTimesteps 1e70100020003000400050006000Reward\nGPBT-PL\nPBT\nPB2 (c) HalfCheetah (4)\n0 1 2 3 4 5 6\nTimesteps 1e60200040006000800010000Reward\nGPBT-PL\nPBT\nPB2 (d) InvertedDoublePendulum (4)\n0 1 2 3 4 5 6 7\nTimesteps 1e60255075100125150Reward\nGPBT-PL\nPBT\nPB2\n(e) Swimmer (4)\n0 1 2 3 4 5 6 7\nTimesteps 1e60500100015002000RewardGPBT-PL\nPBT\nPB2 (f) Walker2D (4)\n0 1 2 3 4 5 6 7\nTimesteps 1e6100\n0100200300Reward\nGPBT-PL\nPBT\nPB2 (g) BipedalWalker (8)\n0.0 0.2 0.4 0.6 0.8 1.0\nTimesteps 1e70100020003000400050006000RewardGPBT-PL\nPBT\nPB2 (h) Ant (8)\n0.0 0.2 0.4 0.6 0.8 1.0\nTimesteps 1e70100020003000400050006000Reward\nGPBT-PL\nPBT\nPB2\n(i) HalfCheetah (8)\n0 1 2 3 4 5 6\nTimesteps 1e60200040006000800010000Reward\nGPBT-PL\nPBT\nPB2 (j) InvertedDoublePendulum (8)\n0 2 4 6\nTimesteps 1e60255075100125150Reward\nGPBT-PL\nPBT\nPB2 (k) Swimmer (8)\n0 1 2 3 4 5 6 7\nTimesteps 1e605001000150020002500RewardGPBT-PL\nPBT\nPB2 (l) Walker2D (8)\nFig. 6. Training curves for six OpenAI Gym benchmarks using populations of 4 and 8 agents with GPBT-PL, PBT, and PB2. Thick lines represent the\naverage of the best mean rewards over 7 seeds, with shaded regions denoting the standard deviation. Brackets specify the population size, and the perturbation\ninterval is set to 1×104.\nval allows agents more training before each hyperparameter\nchange, ensuring accurate performance evaluations. This, how-\never, may result in fewer hyperparameter adjustments and re-\nduced search space exploration. Conversely, a shorter interval\ncan lead to frequent yet potentially unstable hyperparameter\nupdates due to less precise agent evaluations. Given the task-\nspecific nature of an optimal interval, it is crucial that PBT\nalgorithms remain robust against its variations.\nTo assess this robustness, we reduced the perturbation\ninterval from 5×104to1×104timesteps, testing on our\nearlier tasks with populations of 4 and 8. The outcomes are\ndetailed in TABLE IV and Fig. 6, using RS rewards from\nTABLE II as a baseline.\nGPBT-PL consistently outperformed PBT and PB2. When\nexamining Fig. 3 versus Fig. 6, a shared trend emerged: train-\ning curves became erratic and reward variances widened with\na shorter interval. Though simpler tasks like BipedalWalker\nand InvertedDoublePendulum remained relatively stable, more\ncomplex tasks saw notable performance shifts, especially in\nPBT and PB2.\nInterestingly, GPBT-PL showcased heightened performanceupper bounds in several instances, hinting at its resilience to\ninterval changes. This could be due to its adaptive nature,\nwhich corrects early performance misjudgments as evaluations\nrefine over time. In contrast, PBT’s propensity to discard\npotential solutions prematurely might stunt its long-term per-\nformance, and PB2, reliant on a Bayesian optimization model\nbuilt on suboptimal solutions, might fail to produce superior\nhyperparameters.\nE. Scalability to Larger Populations\nWith the availability of more computational resources, un-\nderstanding the scalability of algorithms becomes vital. To\nthis end, we conducted tests on Ant and HalfCheetah using\na population size of 16 and a perturbation interval of 5×104\ntimesteps.\nUpon comparing Fig. 3 with Fig. 7, it becomes evident\nthat for Ant, merely increasing the population from 8 to 16\ndoes not amplify PBT’s upper-performance limits. In contrast,\nGPBT-PL showcases its ability to surpass local optima in later\nstages, accessing regions of higher rewards. For both GPBT-IEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE, 2024 11\n0.0 0.2 0.4 0.6 0.8 1.0\nTimesteps 1e70100020003000400050006000RewardGPBT-PL\nPBT\n(a) Ant\n0.0 0.2 0.4 0.6 0.8 1.0\nTimesteps 1e7010002000300040005000RewardGPBT-PL\nPBT (b) HalfCheetah\nFig. 7. Training curves for 16-agent populations using GPBT-PL and PBT.\nThick lines represent average best rewards over 7 seeds, with shaded regions\nindicating standard deviation.\nPL and PBT, larger populations appear to enhance efficiency\nin pinpointing favorable solutions.\nIn the HalfCheetah test, even though the average of the best\nmean rewards remains relatively stable for both algorithms,\nthere is a marked uplift in the performance upper bounds.\nIn essence, while GPBT-PL and PBT exhibit variable scal-\nability across tasks, both consistently benefit from increased\nefficiency with more expansive populations.\nF . Scalability to Many Hyperparameters\nAs the number of hyperparameters increases, the complexity\nof HPO problems escalates, typically resulting in a decrease\nin the effectiveness of general HPO methods. Although ex-\nisting PBT-like HPO algorithms have not demonstrated their\nbehavior when the number of hyperparameters exceeds four,\nit remains crucial to explore their efficacy under such cir-\ncumstances. Consequently, we conducted experiments on Ant\nand HalfCheetah using population sizes of 4, 8, and 16,\nwith a perturbation interval of 5×104. Notably, the number\nof optimized hyperparameters was increased from 4 to 7,\nencompassing batch size, GAE λ, PPO clip ϵ, discount γ,\nSGD minibatch size, and SGD iterations. The training curves\nare illustrated in Fig. 8.\nGPBT-PL consistently outperforms PBT on Ant and\nHalfCheetah across different population sizes. However, upon\ncomparing Fig. 8 with Fig. 3 and Fig. 7, it is evident that\nboth GPBT-PL and PBT exhibit inferior performance with an\nincreased number of hyperparameters. Although augmenting\nthe population size proves beneficial, it also substantially\nescalates the demand for computational resources, rendering\nHPO algorithms less generalizable. Consequently, when se-\nlecting hyperparameters for simultaneous optimization, it is\nimperative to consult relevant literature and conduct prelim-\ninary experiments to determine the types and search ranges\nof hyperparameters. For instance, pertinent hyperparameters\nshould be optimized together, and those that vary according\nto specific problems must be included in the optimization\nprocess.\nG. Robustness to Hyperparameter Ranges\nPBT-like HPO algorithms often struggle when the hyperpa-\nrameter range is either not optimally defined or unknown. ThisTABLE V\nBEST MEAN REWARDS FOR 7SEEDS . TOP-PERFORMING ALGORITHMS ARE\nHIGHLIGHTED . THE FINAL COLUMN SHOWS THE PERFORMANCE\nDIFFERENCE PERCENTAGE BETWEEN GPBT AND PBT.\nBenchmarks n RS PB2 PBT GPBT-PL vs. PBT\nBreakout 4 131 110 83 151 82%\nSpaceInvaders 4 611 339 551 685 24%\nissue is exacerbated by their reliance on population size for ef-\nfective hyperparameter space exploration. GPBT-PL addresses\nthis challenge by learning potential update directions within\nthe given range, thus negating the need for random sampling\nor incremental adjustments to pinpoint optimal regions.\nTo assess this robustness, we performed experiments on\nBreakout and SpaceInvaders, extending the learning rate range\nto [10−5, 10−2]. This broader range can lead to inefficient\nagent initialization, especially when an agent uses a learning\nrate of 10−2, causing policy learning to become unstable and\ndivergent. Results in TABLE V and Fig. 9 show that while\nboth GPBT-PL and PBT’s performance declined compared to\nresults in TABLE III and Fig. 5, GPBT-PL consistently outper-\nformed PBT. Specifically, GPBT-PL showcased an impressive\n82% improvement in Breakout.\nVI. C ONCLUSION\nIn this work, we have endeavored to advance the state-of-\nthe-art in hyperparameter optimization by refining the princi-\nples of Population-Based Training (PBT). We began by intro-\nducing the Generalized Population-Based Training (GPBT), an\nextension of PBT that offers increased versatility. By allowing\nusers to tailor hyperparameter learning methods during the\nperturbation phase, GPBT paves the way for a more adaptable\noptimization framework. Its asynchronous parallel structure is\ndesigned to cater to diverse optimization challenges efficiently.\nRecognizing the inherent limitations of PBT, particularly\nits propensity for excessive greed and reliance on random\nheuristics, we proposed the Pairwise Learning (PL) method.\nBy drawing on insights from the top-performing agents in\nthe population, PL provides nuanced guidance for under-\nperforming agents. This strategy not only facilitates quicker\nconvergence but also ensures a comprehensive exploration\nof the hyperparameter space, thereby mitigating the risk of\nlocal optima. The culmination of these efforts is the GPBT-PL\nframework, an amalgamation of the strengths of both GPBT\nand PL.\nThrough rigorous experimentation in the RL domain, we\ndemonstrated the superiority of GPBT-PL over traditional PBT\nand its Bayesian-optimized counterpart. Even in resource-\nconstrained scenarios, GPBT-PL delivered consistently su-\nperior results. Nevertheless, the performance superiority of\nGPBT-PL is not readily apparent in simpler problems; rather,\nits strengths are more pronounced in handling complex tasks.\nAs the number of hyperparameters increases along with their\nexpansive ranges, effectively navigating the search space be-\ncomes increasingly challenging, a common hurdle faced byIEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE, 2024 12\n0.0 0.2 0.4 0.6 0.8 1.0\nTimesteps 1e7010002000300040005000RewardGPBT-PL\nPBT\n(a) Ant (4)\n0.0 0.2 0.4 0.6 0.8 1.0\nTimesteps 1e7010002000300040005000RewardGPBT-PL\nPBT (b) Ant (8)\n0.0 0.2 0.4 0.6 0.8 1.0\nTimesteps 1e70100020003000400050006000RewardGPBT-PL\nPBT (c) Ant (16)\n0.0 0.2 0.4 0.6 0.8 1.0\nTimesteps 1e70100020003000RewardGPBT-PL\nPBT\n(d) HalfCheetah (4)\n0.0 0.2 0.4 0.6 0.8 1.0\nTimesteps 1e701000200030004000RewardGPBT-PL\nPBT (e) HalfCheetah (8)\n0.0 0.2 0.4 0.6 0.8 1.0\nTimesteps 1e701000200030004000RewardGPBT-PL\nPBT (f) HalfCheetah (16)\nFig. 8. Training curves for Ant and HalfCheetah using populations of 4, 8, and 16 agents with GPBT and PBT. Thick lines represent the average of the best\nmean rewards over 7 seeds, with shaded regions denoting the standard deviation. Brackets specify the population size, and the perturbation interval is set to\n5×104. The number of optimized hyperparameters is increased to 7.\n0.0 0.2 0.4 0.6 0.8 1.0\nTimesteps 1e7020406080100120RewardGPBT-PL\nPBT\nPB2\n(a) Breakout\n0.0 0.2 0.4 0.6 0.8 1.0\nTimesteps 1e7100200300400500600RewardGPBT-PL\nPBT\nPB2 (b) Space Invaders\nFig. 9. Training curves for 4-agent populations using GPBT-PL, PBT, and\nPB2 on two OpenAI Gym games. The bold lines represent the average of the\nbest rewards across 7 seeds, with shaded regions indicating standard deviation.\nThe learning rate spans [10−5,10−2].\nconventional HPO methods. Notably, when an underperform-\ning agent learns from a superior one, its hyperparameter\nupdates may deviate in the wrong direction, potentially exac-\nerbating its performance, particularly in scenarios with a large\nnumber of hyperparameters and broad ranges. Hence, there is\na pressing need to devise a strategy capable of discerning the\naccurate update direction for hyperparameters. Additionally,\nselecting hyperparameters from an extensive array of options\npresents a combinatorial optimization challenge, introducing\nnovel hurdles for EC methods. Future investigations should\ndelve into EC-based HPO methods capable of effectively\nnavigating high-dimensional and combinatorial search spaces.",
      "metadata": {
        "filename": "Generalized Population-Based Training for Hyperparameter Optimization in Reinfor.pdf",
        "hotspot_name": "PBT_Injection_Molding",
        "title": "Generalized Population-Based Training for Hyperparameter Optimization in\n  Reinforcement Learning",
        "published_date": "2024-04-12T04:23:20Z",
        "pdf_link": "http://arxiv.org/pdf/2404.08233v2",
        "query": "injection molding PBT housing energy efficiency optimization cycle time reduction"
      }
    },
    "Multi-Objective Population Based Training": {
      "full_text": "Multi-Objective Population Based Training\nArkadiy Dushatskiy1Alexander Chebykin1Tanja Alderliesten2Peter A.N. Bosman1 3\nAbstract\nPopulation Based Training (PBT) is an effi-\ncient hyperparameter optimization algorithm.\nPBT is a single-objective algorithm, but many\nreal-world hyperparameter optimization prob-\nlems involve two or more conflicting objectives.\nIn this work, we therefore introduce a multi-\nobjective version of PBT, MO-PBT . Our exper-\niments on diverse multi-objective hyperparame-\nter optimization problems (Precision/Recall, Ac-\ncuracy/Fairness, Accuracy/Adversarial Robust-\nness) show that MO-PBT outperforms random\nsearch, single-objective PBT, and the state-of-the-\nart multi-objective hyperparameter optimization\nalgorithm MO-ASHA.\n1. Introduction\nThe computational complexity of machine learning tasks has\ndrastically increased in recent years. This has been caused\nby larger models (especially, deep neural networks (Doso-\nvitskiy et al., 2020; Kaplan et al., 2020)) and larger available\ndatasets (Thomee et al., 2016; Byeon et al., 2022). At the\nsame time, the problem of tuning model hyperparameters\nremains crucial for achieving maximal performance (Kadra\net al., 2021; Zhang et al., 2021; Liu et al., 2022). Thus, there\nis a growing demand for efficient algorithms to do hyper-\nparameter tuning. Moreover, in real-world problems, there\nmight be more than one objective that a user is interested in.\nAn example of such a scenario which recently received a lot\nof attention from the machine learning community is finding\na trade-off between the predictive accuracy of a classifier\nand its fairness (Schmucker et al., 2020; Chuang & Mroueh,\n2021). When different objectives are conflicting and the\ntarget trade-off is not known a priori, usually no single best\nmodel (or hyperparameter setting) exists. Thus, many mod-\n1Centrum Wiskunde & Informatica, Amsterdam, the\nNetherlands2Leiden University Medical Center, Leiden, the\nNetherlands3Delft University of Technology, Delft, the\nNetherlands. Correspondence to: Arkadiy Dushatskiy\n<arkadiy.dushatskiy@cwi.nl >.\nProceedings of the 40thInternational Conference on Machine\nLearning , Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright\n2023 by the author(s).els with different trade-offs between the objectives should\nbe presented to the user. Finding hyperparameters that re-\nsult in models with the best trade-offs is a multi-objective\noptimization problem.\nOne of the most efficient approaches to single-objective\nHyperparameter Optimization (HPO) is Population Based\nTraining (PBT) (Jaderberg et al., 2017). PBT has two fea-\ntures which ensure its efficiency. Firstly, it is a highly par-\nallelizable, asynchronous algorithm, which means that the\navailable hardware can be effectively utilized. Secondly, in\ncontrast to standard optimization techniques which usually\ntrain models from scratch in order to estimate the perfor-\nmance of a particular hyperparameter setting, PBT opti-\nmizes hyperparameters during model training. In this work,\nwe propose to expand Population Based Training (Jader-\nberg et al., 2017) to Multi-Objective HPO (MO-HPO). The\npopulation of models used in PBT should be especially\nwell suited for solving Multi-Objective (MO) problems, as\nmaintaining a population is naturally helpful for finding\na good trade-off front of solutions, which is known from\nthe Evolutionary Algorithms (EAs) literature (Deb, 2001;\nMorales-Hern ´andez et al., 2022). EAs such as NSGA-II\n(Deb et al., 2002) have been used for efficiently solving\nMO optimization problems, including Neural Architecture\nSearch (Lu et al., 2019).\nThe main contributions of our work are the following:\n1.We expand Population Based Training to MO-HPO sce-\nnarios. The overview of the proposed Multi-Objective\nPBT ( MO-PBT ) algorithm is demonstrated in Figure 1.\n2.We demonstrate that using single-objective PBT for\nMO-HPO by transforming it into a single-objective\nproblem (via a scalarization technique or simply opti-\nmizing one of the objectives) is an inferior approach to\nMO-PBT which uses an MO technique of domination-\nbased selection (Deb et al., 2002).\n3.On a set of diverse MO-HPO problems, we demon-\nstrate that MO-PBT outperforms the state-of-the-art\nefficient, parallelizable hyperparameter optimization\nalgorithm, Multi-Objective Asynchronous Successive\nHalving (MO-ASHA) (Schmucker et al., 2021).\n1arXiv:2306.01436v1  [cs.LG]  2 Jun 2023Multi-Objective Population Based Training\nf2\nDomination-\nbased\nranking Exploit-\nand-\nexplore\nContinue training the networksPopulation of networks:\nhyperparameters (     )\nand \n weights (     )(maximize)\nf1\n(maximize)f2\n(maximize)\nf1\n(maximize)\nFigure 1. The scheme of the proposed MO-PBT applied to a bi-objective maximization task. After the networks (weights and hyperparam-\neters) in a population have been trained for several epochs, they are ranked using a domination-based procedure. Here, each solution\ninside the smaller, dark-orange oval is dominated by at least one solution inside the larger, green oval (and therefore is considered to be\nworse). The inferior solutions are replaced with copies of the superior ones (copying is depicted with dotted lines), with hyperparameters\nperturbed (depicted by adjusted colors). Then the networks are trained for several more epochs, and the loop continues.\nExperiments are performed on three different types of\nMO-HPO problems: precision/recall of a model, the pre-\ndictive performance of a classifier/its fairness, and accu-\nracy/adversarial robustness.\n2. Related work\n2.1. Multi-objective hyperparameter optimization\napplications\nThere are different scenarios in which a user might be in-\nterested in having the option to choose among models with\ndifferent trade-offs between two (or even more) objectives.\nThe classical example in machine learning is choosing a\ntrade-off between precision and recall of a classifier. The\nimportance of each metric might change depending on the\napplication requirements. Another example is choosing a\ntrade-off between the predictive quality of a classifier and\nits fairness. It was shown in (Chuang & Mroueh, 2021)\nthat these objectives are conflicting. In (Zhang et al., 2019),\nit was demonstrated that the classifier accuracy and its ro-\nbustness to adversarial attacks are conflicting, and therefore,\nfinding a trade-off between them is another interesting MO-\nHPO application.\n2.2. Efficient multi-objective optimization algorithms\nA popular class of algorithms for MO-HPO is the Bayesian\nOptimization (BO) algorithms. Some of them work by re-\nducing an MO optimization problem into a single-objective\none by using scalarization techniques (Knowles, 2006;\nZhang & Golovin, 2020; Paria et al., 2020; Zhang et al.,\n2009). An alternative approach to MO optimization withBO is based on expected hypervolume improvement calcu-\nlation (Emmerich et al., 2011). While BO algorithms are\nsequential by nature, recently they were extended to the\nbatch-wise calculation of an objective function for solving\nMO problems (Daulton et al., 2020; 2021). However, the\nconsidered batch sizes were moderate (up to 32 solutions),\nso parallelization capabilities remain limited.\nOne of the drawbacks of typical BO algorithms is the equal\nallocation of resources to all evaluated solutions. In contrast\nto this, it was proposed to greedily stop underperforming\nmodel evaluations to save computational resources in Suc-\ncessive Halving (Jamieson & Talwalkar, 2016) and its ex-\ntension Hyperband (that proposes a more complex resource\nallocation scheme) (Li et al., 2017). Then, an asynchronous\nversion of Hyperband called Asynchronous Successive Halv-\ning Algorithm (ASHA) was proposed, which was shown\nto achieve a substantial wall-clock time speed-up (Li et al.,\n2020). Hyperband was extended to MO problems by using\nrandom scalarizations in (Schmucker et al., 2020; Guerrero-\nViu et al., 2021). Finally, ASHA was extended to MO\nproblems in (Schmucker et al., 2021). Different approaches\nto adapting ASHA to MO optimization were compared in\n(Schmucker et al., 2021) and it was concluded that the tech-\nniques utilizing the geometry of the Pareto front, in other\nwords, domination-based selection such as in NSGA-II (Deb\net al., 2002), outperform scalarization-based techniques.\nIt was proposed to integrate BO algorithms into Hyper-\nband: (Falkner et al., 2018) replaces random sampling of\nnew candidate solutions by using a Bayesian sampler (TPE\n(Bergstra et al., 2011)) for more efficient search space ex-\nploration. MO-BOHB extends this idea to a multi-objective\nTPE sampler (MOTPE) (Ozaki et al., 2020).\n2Multi-Objective Population Based Training\n2.3. Population Based Training\nA general formulation of PBT was proposed in (Jaderberg\net al., 2017). It was shown to be an efficient way to jointly\noptimize hyperparameters and model weights of agents in\nreinforcement learning tasks, Generative Adversarial Net-\nworks, and Transformer networks applied to the machine\ntranslation task. Later it was shown that it can be also used to\nefficiently optimize data augmentation parameters for stan-\ndard image classification datasets such as CIFAR-10/100\n(Ho et al., 2019) and 3D object detection (Cheng et al.,\n2020). In (Liang et al., 2021) it was proposed to incorporate\nan exploration component in the evaluation procedure of\nthe solutions and add a crossover operator to recombine\nhyperparameter vectors. In (Dalibard & Jaderberg, 2021) a\nmore complex training scheme with multiple populations\nwas proposed in order to improve the original PBT on prob-\nlems where the greedy nature of the algorithm might lead\nto suboptimal results. Other modifications of PBT aim\nat improving its efficiency by integrating BO techniques\n(Parker-Holder et al., 2020; Wan et al., 2022). However,\nwe would like to emphasize that all existing PBT modifica-\ntions are single-objective and are not well-suited to solve\nmulti-objective problems.\nIn this work, we follow the original design of PBT, which is\nsimpler than the later proposed alternatives and was shown\nto work well on a diverse set of problems (Jaderberg et al.,\n2017; Ho et al., 2019; Cheng et al., 2020). However, our\napproach is general and can potentially be used with any\nPBT modification.\n3. Preliminaries\n3.1. Multi-objective optimization\nMO optimization problems are characterized by the pres-\nence of multiple conflicting objectives. Thus, solving the\noptimization problem entails finding the best possible trade-\noffs between the objectives. An MO optimization problem\n(without loss of generality, we consider maximization) with\nKobjectives can be formulated as follows:\nmax\nx∈Xf(x) = max\nx∈X(f1(x), f2(x), . . . , f K(x)),\nwhere X⊆ S is a search space of solutions considered\nfeasible ( Sis a search space of all solutions). It is said that\na solution x′dominates a solution x(x′≻x) if∀i fi(x′)≥\nfi(x)and∃i s.t. f i(x′)> fi(x).\nThe Pareto set Psoffis a set of all non-dominated so-\nlutions, i.e. Ps={x∈X|∄x′:x′≻x}while the\nPareto front Pfis a set of objective values of solutions in\nPs:Pf={(f1(x), f2(x), . . . , f K(x))|x∈ Ps}. While the\nPareto front is often not known, the considered tangible\ngoal of MO optimization algorithms is to obtain a good\napproximation of it. A popular measure of approximationquality is the dominated hypervolume (Zitzler, 1999). The\nhypervolume of a finite set of solutions Sis calculated as\nfollows: HVr(S) =λK(z∈RK:∃y∈S, r≺z≺f(y)),\nwhere r∈RKis a chosen reference point and λKis a\nLebesgue measure. Intuitively, the hypervolume represents\nthe volume (the area in the bi-objective case) between the\nreference point and the non-dominated trade-off front of\nsolutions (see Appendix F, Figure 17 for visualization).\n3.2. Scalarization techniques\nScalarization is a commonly used technique for MO opti-\nmization, which transforms a multi-objective problem into\na single-objective one: max x∈XV(f(x), w), where Vis a\nscalarization function and wis a scalarizing weight vector.\nFollowing MO optimization literature (Karl et al., 2022),\nwe use ParEGO scalarization function (Knowles, 2006)\n(also called augmented Chebyshev scalarization (Steuer &\nChoo, 1983)): VParEGO =ρVWS+VChebyshev , where\nVWSis Weighted Sum scalarization: VWS(f(x), w)) =P\niwifi(x)andVChebyshev is the Chebyshev scalarization:\nVChebyshev (f(x), w)) = min i(wifi(x)),ρis set to 0.05\nin the original ParEGO implementation. Following MO-\nASHA (Schmucker et al., 2021), we also use Golovin scalar-\nization (Zhang & Golovin, 2020): VGolovin (f(x), w)) =\nmini(max(0 , fi(x)/wi))K(Kis the number of objectives).\n4. Multi-Objective Population Based Training\nWe start with a short summary of PBT and then describe\nour extension of it to the MO setting.\nThe goal of PBT is to optimize an objective function f. PBT\nhas a population of Nsolutions P={pi}N\ni=1, where each\nindividual picomprises a tuple of model weights and hyper-\nparameters: (θi,Hi). The main working principle of PBT is\nto optimize weights and hyperparameters in an interleaved\nfashion, which is achieved via two key operators: exploit\nandexplore . The exploit operator replaces a bad solution\nwith a copy of a good one (both weights and hyperparam-\neters are copied). The solution quality is determined by a\nranking procedure. The explore operator creates a new solu-\ntion by, e.g., perturbing the hyperparameters of the existing\none. Between exploit-and-explore steps, the weights of the\nmodels are trained as usual, e.g., using gradient descent.\nHow solutions are ranked needs to be changed when go-\ning from single- to multi-objective optimization. In the\nsingle-objective scenario, the population members can be\nranked according to the optimization objective value, but\nwith multiple objectives, ranking becomes less trivial.\nThe first approach we consider is using a scalarization tech-\nnique, i.e., mapping an objective vector into a scalar. It\nis then used for ranking solutions, just as in the single-\nobjective case. Secondly, we consider domination-based\n3Multi-Objective Population Based Training\nranking, as used for example in NSGA-II (Deb et al.,\n2002). The main component of such an approach is the\nnon-dominated sort of solutions. The idea of the non-\ndominated sort is to partition a population of solutions P\ninto non-dominated fronts of solutions, i.e., P=F1∪\nF2, . . . ,∪FR;Fi∩Fj=∅ ∀i, jsuch that:\n1.All solutions in each front are non-dominated by each\nother:∀k:∀v1, v2∈Fkv1⊁v2andv2⊁v1\n2.In the kthfront ( k >1), all solutions are dominated by\na solution from a front with a smaller index: ∀k,2≤\nk≤R:∀v1∈Fk∃v2∈Fm, m < k :v2≻v1\nIn the sorting procedure, all solutions from F1are ranked\nhigher than the solutions from F2, the ones from F2are\nranked higher than the solutions from F3, etc. Within\neach front, the solutions are ranked according to an ad-\nditional ranking criterion. In the original NSGA-II algo-\nrithm, the crowding distance criterion was used. However,\nin (Schmucker et al., 2021) it was shown that the greedy scat-\ntered subset selection (Bosman & Thierens, 2003) (called\nϵ−network in (Schmucker et al., 2021; Salinas et al., 2021))\nranking performs better when integrated into the MO-ASHA\nalgorithm (compared to MO-ASHA with the crowding dis-\ntance). We also experimentally found that MO-PBT with\nthe greedy scattered subset selection performs slightly bet-\nter than MO-PBT with the crowding distance, as shown in\nAppendix A.3.\nThe main idea behind the greedy scattered subset selection\nis to rank higher the solutions that are further away from the\nothers. Specifically, the next solution is iteratively chosen in\na greedy way such that it has the largest Euclidean distance\n(in the objective space ) to the closest already ranked solu-\ntion. The visualization of this ranking procedure is shown\nin Figure 2, its pseudocode is listed in Appendix G, Algo-\nrithm 1.\nTheexploit andexplore operators of MO-PBT are described\nin Section 6.1.\n5. Multi-objective hyperparameter\noptimization tasks\n5.1.Precision/Recall in classification\nBalancing between precision and recall of a model is a\nclassical trade-off problem in machine learning (Karl et al.,\n2022; L ´evesque et al., 2012). In this work, we use mod-\nern FT-Transformer (Feature Tokenizer Transformer) neural\nnetworks (Gorishniy et al., 2021) and three diverse binary\nclassification datasets: Adult (Dua & Graff, 2017), Higgs\n(Baldi et al., 2014), and Click prediction (Vanschoren et al.,\n2013). We optimize the regularization parameters: weight\ndecay and dropout, and additionally the class weights in the\ncross-entropy-loss function (which is a natural way to bal-\n(maximize)f 2\nf 1\n(maximize)2\n13\n4\n56\n7F1\nF2\nF3Figure 2. The ranking procedure of solutions (shown with cir-\ncles) in MO-PBT. Numbers inside circles show the assigned rank\n(smaller is better). First, solutions are sorted using non-dominated\nsort. Here, it partitions the solutions into the non-dominated front\nF1(green), the second front F2(light orange), and the third front\nF3(dark orange). The solutions in the first front are considered\nfirst. The solution with the largest f1value is ranked first. Then\nother solutions from F1are ranked one-by-one such that the so-\nlution which is the furthest away from the already ranked ones\nis picked next. This distance-based ranking is continued in the\nsecond (third, etc.) fronts.\nance between class-wise performances, and therefore preci-\nsion and recall). The training procedure for FT-Transformer\nis adopted from (Gorishniy et al., 2021) (but without early\nstopping). The early stopping is not included because it is\nnot well-suited for a standard PBT setup (also used here),\nwhere a predetermined number of exploit-and-explore steps\n(and therefore training epochs) is performed.\n5.2. Model Accuracy/Fairness\nThe fairness of a model is understood as its ability to pre-\ndict the target attribute, e.g., income, without a bias on the\nsensitive attribute, e.g., gender or race. In our experiments,\nwe consider the standard setup of model fairness in binary\nclassification, where labels Y∈ {0,1}, sensitive attributes\nA∈ {0,1}, and model predictions are ˆY. Different fairness\nmetrics have been proposed (Garg et al., 2020). Two of the\nmost popular ones are Statistical Parity (SP) and Equalized\nOdds (EO). SP requires the independence of predictions ˆY\non the sensitive attribute A: P(ˆY|A= 0) = P(ˆY|A= 1) .\nEO requires conditional independence of ˆYandAwith re-\nspect to Y:P(ˆY|A= 1, Y=y) =P(ˆY|A= 0, Y=y)\nfory∈ {0,1}.\nFollowing (Madras et al., 2018; Schmucker et al., 2021;\nChuang & Mroueh, 2021), we optimize the relaxed versions\nof SP and EO called Difference in Statistical Parity (DSP)\nand Difference in Equalized Odds (DEO).\nDSP (f) =|Ex∼P0f(x)−Ex∼P1f(x)|\nDEO (f) =X\ny∈{0,1}|Ex∼Py\n0f(x)−Ex∼Py\n1f(x)|,\nwhere Pa=P(·|A=a)andPy\na=P(·|A=a, Y =y).\n4Multi-Objective Population Based Training\nFollowing (Chuang & Mroueh, 2021), the loss during train-\ning can be composed of standard Cross-Entropy (CE) and\nweighted DSP (gap regularization):\nLfairness (f(x), y) =CE(f(x), y) +λDSP (f),\nwhere xis a training sample, yis the target, and λis a\ntrade-off parameter.\nWe consider the Adult dataset with gender as the sensitive\nattribute and income as the target. We use the same setup as\nin 5.1 (FT-Transformer neural networks, optimizing regular-\nization), but instead of a class weighting parameter in the\ncross-entropy loss, we use the Lfairness loss and optimize\ntheλparameter. Also, we use the CelebA dataset (Liu et al.,\n2015) with gender as the sensitive attribute and Attractive-\nness as the binary classification target. The training setup is\nthe same as in Section 5.3, but with the Lfairness loss.\n5.3.Accuracy/Adversarial robustness\nIt has been shown that standard model accuracy and its\nadversarial robustness (accuracy on samples generated by\nan adversarial attack) are conflicting objectives (Zhang et al.,\n2019). In this task, we use the TRADES loss (Zhang et al.,\n2019):\nLTRADES (f(x), y) =CE(f(x), y)\n+ max\nx′∈B(x,ϵ)λCE (f(x), f(x′)),\nwhere xis a training sample, x′is a generated adversarial\nsample in the ϵ−neighborhood of x, andyis the target. The\nparameter λaffects the trade-off between accuracy and ad-\nversarial robustness. We use the same adversarial attack and\nTRADES loss parameters as in (Zhang et al., 2019). We\nsearch for data augmentation parameters: parameters of the\nRandAugment augmentation strategy (Cubuk et al., 2020)\nand Cutout (DeVries & Taylor, 2017) (probability and size).\nExperiments are performed for CIFAR-10/100 datasets us-\ning the WideResNet-28-2 (Zagoruyko & Komodakis, 2016)\nand the training setup from (Zhang et al., 2019).\n5.4. Search spaces\nIn this work, we perform search in discretized search spaces.\nSuch an approach was successfully used, for instance, for\naugmentations search (Ho et al., 2019; Cubuk et al., 2020).\nFor all described optimization tasks, search spaces of hyper-\nparameters are specified in Appendix H.\n6. Experimental setup\n6.1. PBT operators\nHere we describe the operators of MO-PBT following the\nnotation from (Jaderberg et al., 2017).Exploit We use the simple truncation selection operator\nused in the original PBT (Jaderberg et al., 2017) algorithm.\nAfter the population is sorted according to some criterion\n(non-dominated sort followed by the greedy scattered subset\nselection in the case of MO-PBT), each of the bottom τ%\nof solutions in the population is randomly replaced by a\nsolution from the top τ% (we use the default value of τis\n25). The pseudocode of the used exploit operator is listed in\nAppendix G, Algorithm 2.\nExplore We use the explore operator previously used in Pop-\nulation Based Augmentations (Ho et al., 2019). It assumes\nthat the encoding of hyperparameter values in a search space\nis ordinal. The key idea of the operator is locality: the new\nvalue of a hyperparameter is chosen from the vicinity of the\ncurrent value. The pseudocode of the used explore operator\nis listed in Appendix G, Algorithm 3.\nReady In all considered tasks, we perform the exploit-and-\nexplore procedure every 2 epochs of training.\nWe use a population of size 32 in our main experiments\nand in Section 7.4 study how the performance scales with\nincreasing population size. Note that we do not specifi-\ncally tune exploit andexplore operators of MO-PBT, but in\nAppendix A we analyze how their design impacts the per-\nformance and conclude that the considered design options\nperform similarly.\n6.2. Hypervolume as the performance metric\nWe use the hypervolume, a commonly used metric in MO\noptimization (Riquelme et al., 2015) (see Section 3.1). We\ncalculate the reference point r= (r1, . . . , r K)with the fol-\nlowing approach, which is used, for instance, in (Knowles,\n2006; Ishibuchi et al., 2011). First, all non-dominated fronts\nare collected from all evaluation points of all algorithms\nand all performed runs and stored in a set F. Then, the\nreference point ris calculated as ri= min x∈Ff(xi)−\nρ(max x∈Ff(xi)−minx∈Ff(xi)),fori= 1 , . . . , K ,\nwhere ρis typically set to a small value, here we use ρ= 0.1.\nThis strategy selects the reference point that is guaranteed\nto be worse than all points on all fronts. This ensures that\nall non-dominated points are considered in the hypervol-\nume calculation and are not discarded. Furthermore, the\nreference point is shifted with respect to the range of values\nof each objective in order to prevent its positioning too far\naway from the fronts.\nIn the experimental evaluation, we use the following com-\nmon metric (used, e.g., in (Schmucker et al., 2021; Daulton\net al., 2021). First, we obtain an approximation of the Pareto\nfront by collecting all evaluated solutions from all runs of\nall algorithms and selecting the non-dominated subset P∗\nof them. The approximation of the optimal hypervolume\nis then calculated as HV∗=Hypervolume (P∗). The\n5Multi-Objective Population Based Training\nreported performance metric of an algorithm run rat times-\ntamp tis the logarithmic difference of the hypervolume of a\nnon-dominated set of solutions obtained by this timestamp\nand the ideal hypervolume: log10(HV∗−HVt\nr). Finally,\nthis metric is averaged over multiple runs.\nDatasets are split into train/validation/test subsets before\nexperiments. In our main results, we report the above-\ndescribed hypervolume metric on the validation subset to\nevaluate the search performance of the algorithms. Addition-\nally, we provide results on the test subsets in Appendix C.\n6.3. Baselines\n6.3.1. R ANDOM SEARCH\nFirst, we consider a trivial search baseline — random search:\nfor each hyperparameter, a random value is sampled at the\nbeginning of model training.\n6.3.2. S INGLE -OBJECTIVE PBT\nWe use modifications of PBT that convert an MO problem\ninto a SO one. First, we use one of the objectives as the\nfitness function of PBT. Comparing against this baseline can\nshow that the considered MO problems are challenging, and\noptimizing just one objective is inferior to using MO tech-\nniques. Secondly, we implement different scalarization func-\ntions in PBT. The first technique we use is random scalar-\nization as, for instance, in the ParEGO algorithm (Knowles,\n2006): at each invocation of the evaluation procedure, the\nscalarization vector is sampled randomly. Here we use the\nParEGO scalarization function (as defined in Section 3.2) as\nit was originally proposed to use for random scalarizations in\n(Knowles, 2006). Secondly, we use the maximum scalariza-\ntion technique proposed in (Schmucker et al., 2020): the ob-\njective value is calculated as max w∈W,||w||=1V(f(x), w),\nwhere W is a set of randomly sampled unit vectors and V\nis a scalarization function. Following (Schmucker et al.,\n2021), we use |W|= 100 and the Golovin scalarization,\nwhich was demonstrated to outperform other scalarization\nfunctions.\n6.3.3. MO-ASHA VARIANTS\nWe consider MO-ASHA with greedy scattered subset selec-\ntion ranking ( ϵ-network), which was shown to perform better\nthan alternative MO-ASHA variants in (Schmucker et al.,\n2021). Secondly, to compare MO-PBT against a strong BO\nbaseline that is well parallelizable we adapt the MO-BOHB\n(Guerrero-Viu et al., 2021) approach to MO-ASHA. We\nrefer to this MO-ASHA modification as BO-MO-ASHA.6.4. Evaluation setup\nThe main design principle of our evaluation of algorithms\nis to compare the achieved performance with respect to\nelapsed wall-clock time instead of the performed number\nof training epochs. We choose this approach because in\npractice we are more interested in the achieved performance\nby a specific time point rather than a specific epoch. We do\nnot set a time limit for all PBT variants and random search\nbut rather allow them to fully finish the training cycle of all\nsolutions in the population. For MO-ASHA, we allocate\nthe time budget equal to the run time of the slowest PBT\nrun. We ran each algorithm 10 times on tabular datasets\n(Adult, Higgs, Click prediction), and 5 times on image ones\n(CIFAR-10/100, CelebA). When plotting performance over\ntime, we plot mean performance, with the area between the\nworst and the best runs shaded.\nFurther experimental setup details are provided in Ap-\npendix, B. The code is available at https://github.\ncom/ArkadiyD/MO-PBT .\n7. Results\n7.1. Overall performance\nResults of hypervolume-based performance evaluation (as\ndescribed in 6.2) are shown in Figures 3,4,5. On every con-\nsidered task, MO-PBT outperforms baselines (the standard\ndeviations of the hypervolume are provided in Appendix I,\nTable 5). Noteworthy, on the three-objective problems MO-\nPBT is also the best-performing algorithm. The consistently\ngood performance of MO-PBT on the considered diverse\ntasks empirically demonstrates its generality.\nOn Accuracy/Fairness tasks, optimizing the fairness ob-\njective with SO PBT leads to obtaining mostly inaccurate\nmodels, and, therefore, poor hypervolume values. Similarly,\non the Accuracy/Robustness task, if only accuracy is op-\ntimized, the results achieved for the robustness objective\nare poor. PBT with scalarization techniques performs, in\ngeneral, better than single-objective PBT.\nComparing MO-ASHA variants, we cannot conclude that\nBO-MO-ASHA performs better than MO-ASHA. We note\nthat on CIFAR-10/100 Accuracy/Robustness tasks (Fig-\nure 5), MO-ASHA and BO-MO-ASHA perform better than\nMO-PBT in the beginning of the search as they train net-\nworks in a different order compared to MO-PBT: some\nselected networks are fully trained earlier in time than in\nMO-PBT, where all the networks are trained simultaneously.\nHowever, as soon as the population is trained for more\nepochs, MO-PBT catches up and at the end of the search\nsubstantially outperforms MO-ASHA. We note that this be-\nhavior occurs only because the number of parallel workers\nin our experiments is smaller than the population size.\n6Multi-Objective Population Based Training\n0 5 10\nTime (minutes)−2.0−1.5−1.0−0.5Log10 Hypervolume differencePrecision/Recall, Adult\n0 10 20 30\nTime (minutes)−2.0−1.5−1.0−0.5Log10 Hypervolume differencePrecision/Recall, Higgs\n0 25 50 75 100\nTime (minutes)−2.0−1.5−1.0Log10 Hypervolume differencePrecision/Recall, Click prediction\n−0.04 −0.02 0.00 0.02 0.04−0.04−0.020.000.020.04\nrandom search\nPBT: precisionPBT: recall\nPBT: random scalarizationPBT: max. scalarization\nMO-ASHABO-MO-ASHA\nMO-PBT\nFigure 3. Optimization results on the Precision/Recall task.\n0 500 1000\nTime (minutes)−2.5−2.0−1.5−1.0Log10 Hypervolume differenceAccuracy/DSP, CelebA\n0 5 10 15\nTime (minutes)−3.0−2.5−2.0−1.5Log10 Hypervolume differenceAccuracy/DSP, Adult\n0 500 1000\nTime (minutes)−2.5−2.0−1.5−1.0Log10 Hypervolume differenceAccuracy/DSP/DEO, CelebA\n0 5 10 15\nTime (minutes)−3.0−2.5−2.0Log10 Hypervolume differenceAccuracy/DSP/DEO, Adult\n−0.04 −0.02 0.00 0.02 0.04−0.04−0.020.000.020.04\nrandom search\nPBT: accuracy\nPBT: DSPPBT: DEO\nPBT: random scalarizationPBT: max. scalarization\nMO-ASHABO-MO-ASHA\nMO-PBT\nFigure 4. Optimization results on the Accuracy/Fairness task.\n7.2. Analysis of the obtained trade-off fronts\nThe comparison of non-dominated fronts of solutions ob-\ntained by different algorithms is shown in Figure 6. Quan-\ntitative results of the front diversity evaluation (using the\ncoverage metric introduced in (Scriven et al., 2009) and\ndescribed in Appendix F) are shown in Appendix I, Table 7.\nWe observe that on the Precision/Recall and the Accu-\n0 500 1000 1500\nTime (minutes)−2.0−1.5−1.0−0.5Log10 Hypervolume differenceAccuracy/Robustness, CIFAR-10\n0 500 1000 1500\nTime (minutes)−2.0−1.5−1.0Log10 Hypervolume differenceAccuracy/Robustness, CIFAR-100\n−0.04 −0.02 0.00 0.02 0.04−0.04−0.020.000.020.04\nrandom search\nPBT: accuracyPBT: robustness\nPBT: random scalarizationPBT: max. scalarization\nMO-ASHABO-MO-ASHA\nMO-PBTFigure 5. Optimization results on the Accuracy/Robustness task.\nracy/Robustness tasks, MO-PBT achieves substantially bet-\nter coverage of the trade-off front compared to other algo-\nrithms. On Accuracy/DSP tasks, MO-ASHA on average\nhas slightly better coverage than MO-PBT (though the vari-\nance of the results is large and in some runs MO-PBT has\nbetter coverage). Nevertheless, it should be noted that the\nquality of the most points on the trade-off fronts obtained\nby MO-ASHA is worse in terms of domination (can be seen\non Figure 6).\nThese results demonstrate that MO-PBT can not only find so-\nlutions closer to the reference front than MO-ASHA (which\nis reflected in the better hypervolume performance), but\nalso produce more diverse fronts along the entire trade-off\ncurve. For practical usage, this means that more options for\ntrade-offs between objectives are available for the user to\nchoose from.\n7.3. Where do different algorithms focus their search?\nWe analyze how algorithms differ in their search behavior\nby plotting all solutions collected during the search in the\nobjective space and highlighting areas where more solutions\nare concentrated. These plots are shown in Figure 7. They\nshow a clear difference between approaches which turn an\nMO problem into an SO one (scalarization and optimizing\none objective) and MO-PBT. MO-PBT obtains solutions\nscattered more uniformly along the entire trade-off trajectory\nbetween two objectives, in contrast to concentrating on one\narea of it.\n7.4. Scalability\n7.4.1. P OPULATION SIZE\nWe investigate whether MO-PBT benefits from increasing\nthe population size. The scaling experimental results are\nshown in Figure 8. We find that for the population sizes\nwe considered (16, 32, 64), performance keeps improving.\nFor reference, we also ran MO-ASHA with correspondingly\nincreased time budgets and found that it scales similarly. We\nnote that the performance gains when the population size\nis increased (from 16 to 32 and from 32 to 64) are stronger\n7Multi-Objective Population Based Training\n−0.04 −0.02 0.00 0.02 0.04−0.04−0.020.000.020.04\nMO-ASHA MO-PBT\nFigure 6. Comparison of the non-dominated fronts obtained by\ndifferent algorithms. For each algorithm, the run with the median\nhypervolume value is shown. For each front, the values of the\nhypervolume and coverage metrics (multiplied by 100) are reported\nin the corresponding color.\non the task with 3 objectives. This is expected behavior,\nas the algorithm needs more solutions to scatter along 3D\napproximation fronts compared to 2D in the bi-objective\ncase.\n7.4.2. S EARCH SPACE SIZE\nIn our main experiments on image datasets, we search for\nthe two parameters (number of augmentations and their mag-\nnitude) of the RandAugment augmentation policy, which\nwas shown to be effective (Cubuk et al., 2020). The whole\nsearch space has in that case 5 variables. To additionally\nstudy whether MO-PBT is capable of performing search ef-\nficiently in larger search spaces, we construct a substantially\nlarger search space (comprising 31 variables) by replacing\nthe RandAugment policy with an augmentation policy simi-\nlar to the one used in (Ho et al., 2019): the magnitude and\nprobability of each augmentation can be adjusted separately;\nadditionally, the number of applied augmentations is search-\nable too. The results are shown in Figure 9. We observe\nthat in a larger search space, MO-PBT does not lose its\nefficiency and has even a slightly better performance.\n7.5. Further experiments to demonstrate the\neffectiveness of MO-PBT\nIn addition to our main experiments, we compare MO-\nPBT to the algorithms which are not (fully) parallel. InAppendix D we demonstrate that MO-PBT outperforms\nstate-of-the-art BO algorithm for MO optimization: Paral-\nlel Noisy Expected Hypervolume Improvement (qNEHVI)\n(Daulton et al., 2021). Furthermore, we conduct experi-\nments to ensure that MO-PBT is an efficient optimization\nalgorithm even in the scenario when it is executed sequen-\ntially. In Appendix E, MO-PBT is shown to outperform\ncommon MO baselines NSGA-II (Deb et al., 2002) and\nParEGO (Knowles, 2006) in the sequential setup.\n8. Discussion and limitations\nWe have proposed MO-PBT and compared it with various\nbaselines, including a prominent parallelizable algorithm for\nMO-HPO, MO-ASHA, reaching the conclusion that MO-\nPBT performs better. We note however that an advantage\nof MO-ASHA (and similar algorithms such as Hyperband)\nis its ability to perform not only HPO, but also architecture\nsearch, and, furthermore, joint optimization of the archi-\ntecture and hyperparameters. In PBT and MO-PBT all\narchitectures are assumed to be identical in the population,\ntherefore architecture search cannot be performed (without\nadditional modifications).\nWe note that quantifying the results of MO algorithms is,\nin, general, challenging. Many metrics have been proposed\n(Audet et al., 2021) and each has its own pros and cons.\nWhile hypervolume remains, arguably, the most commonly\nused metric, its downside is the dependence on a user-\nselected reference point. Thus, while we ensured that the\nproposed MO-PBT outperformed alternative algorithms in\nterms of hypervolume, we also visually analyzed the ob-\ntained non-dominated fronts of solutions and quantified the\nresults using a coverage metric (Scriven et al., 2009). This\nanalysis also demonstrated good performance of MO-PBT\nin terms of solutions diversity and density (they are well\nspread across different areas of the objective space).\nIn principle, MO-PBT (as well as the original PBT) can\noperate with any type of search space as long as an explore\noperator is defined (moreover, the search spaces can be de-\nfined separately for each hyperparameter). One of the bene-\nfits of the discretized search space used in this work is (in\ncontrast to a real-valued search space), its ability to explic-\nitly set some values: e.g., zero value of λin theLfairness\nturns this loss into the standard cross-entropy. Thus, more\ninterpretable hyperparameter search results can be obtained.\nHowever, a real-valued search space can, potentially, enable\nperforming a more fine-grained search which in some cases\nmight be more important than the interpretability of results.\nFor the main experiments of this work, we used MO-PBT\nwith population size of 32. We additionally observed that its\nperformance scales with increasing population size. How-\never, population size remains a hyperparameter of MO-PBT\n8Multi-Objective Population Based Training\n0.00.20.40.60.81.0\nprecision0.00.20.40.60.81.0recallSO: precision\n0.00.20.40.60.81.0\nprecisionSO: recall\n0.00.20.40.60.81.0\nprecisionmax. scalarization\n0.00.20.40.60.81.0\nprecisionrandom scalarization\n0.00.20.40.60.81.0\nprecisionMO-PBTPrecision/Recall, Higgs\n0.2 0.4 0.6 0.8\naccuracy0.00.20.4robustnessSO: accuracy\n0.2 0.4 0.6 0.8\naccuracySO: robustness\n0.2 0.4 0.6 0.8\naccuracymax. scalarization\n0.2 0.4 0.6 0.8\naccuracyrandom scalarization\n0.2 0.4 0.6 0.8\naccuracyMO-PBTAccuracy/Robustness, CIFAR-10\nFigure 7. 2D histograms of solutions (in the objective space) collected during one run of each algorithm. Darker color denotes more\nsolutions in the corresponding bin (for visualization purposes, bins with more solutions than the 95th percentile of the bin counts have the\ndarkest color on the plot). For each algorithm, solutions obtained during the run with the median hypervolume value are plotted. SO\ndenotes PBT applied to optimizing one of the objectives.\n16 32 64\nPopulation size0.90.920.940.960.981.01.021.04Relative hypervolumeAccuracy/DSP, Adult\nMO-PBT\nMO-ASHA\n16 32 64\nPopulation size0.90.920.940.960.981.01.021.04Accuracy/DSP/DEO, Adult\nFigure 8. Comparison of MO-PBT with increasing population size\nand MO-ASHA. The time budget for MO-ASHA was adjusted\naccordingly. The hypervolume is normalized by the average hyper-\nvolume performance of MO-PBT with population 32.\n5 31\nSearch space size (#variables)0.330.340.350.36HypervolumeAccuracy/Robustness, CIFAR-10\nMO-PBT\nMO-ASHA\nFigure 9. Comparison of MO-PBT and MO-ASHA applied to the\nsearch spaces of different sizes.that needs to be set by the user. Adjusting it automatically\n(for example, as done in EA literature (Harik et al., 1999))\ncould be an interesting direction for future work.\n9. Conclusion\nWe introduced a multi-objective version of Population Based\nTraining: MO-PBT. We considered diverse multi-objective\nhyperparameter optimization tasks and found that a multi-\nobjective approach to ranking solutions, non-dominated\nsort, outperforms more simple ones such as scalarization\ntechniques. This was demonstrated by not only better hy-\npervolume performance, but also a better tradeoff front cov-\nerage by MO-PBT. MO-PBT was shown to outperform\nMO-ASHA variants (standard and Bayesian optimization\nbased), single-objective PBT, and random search.\nAcknowledgements\nThe work in this paper is supported by: the\nDutch Research Council (NWO) through project\nOCENW.GROOT.2019.015 ”Optimization for and with\nMachine Learning (OPTIMAL)”; and project DAEDALUS\nfunded via the Open Technology Programme of the NWO,\nproject number 18373; part of the funding is provided by\nElekta and ORTEC LogiqCare.\n9Multi-Objective Population Based Training",
      "metadata": {
        "filename": "Multi-Objective Population Based Training.pdf",
        "hotspot_name": "PBT_Injection_Molding",
        "title": "Multi-Objective Population Based Training",
        "published_date": "2023-06-02T10:54:24Z",
        "pdf_link": "http://arxiv.org/pdf/2306.01436v1",
        "query": "injection molding PBT housing energy efficiency optimization cycle time reduction"
      }
    },
    "Shrink-Perturb Improves Architecture Mixing during Population Based Training for": {
      "full_text": "Shrink-Perturb Improves Architecture Mixing during\nPopulation Based Training for Neural Architecture Search\nAlexander Chebykina;*, Arkadiy Dushatskiya, Tanja Alderliestenband Peter Bosmana, c\naCentrum Wiskunde & Informatica\nbLeiden University Medical Center, Department of Radiation Oncology\ncDelft University of Technology\nORCiD ID: Alexander Chebykin https://orcid.org/0000-0002-3549-3533,\nArkadiy Dushatskiy https://orcid.org/0000-0003-0945-0262,\nTanja Alderliesten https://orcid.org/0000-0003-4261-7511, Peter Bosman https://orcid.org/0000-0002-4186-6666\nAbstract.\nIn this work, we show that simultaneously training and mixing\nneural networks is a promising way to conduct Neural Architecture\nSearch (NAS). For hyperparameter optimization, reusing the par-\ntially trained weights allows for efficient search, as was previously\ndemonstrated by the Population Based Training (PBT) algorithm. We\npropose PBT-NAS, an adaptation of PBT to NAS where architec-\ntures are improved during training by replacing poorly-performing\nnetworks in a population with the result of mixing well-performing\nones and inheriting the weights using the shrink-perturb technique.\nAfter PBT-NAS terminates, the created networks can be directly used\nwithout retraining. PBT-NAS is highly parallelizable and effective: on\nchallenging tasks (image generation and reinforcement learning) PBT-\nNAS achieves superior performance compared to baselines (random\nsearch and mutation-based PBT).\n1 Introduction\nNeural Architecture Search (NAS) is the process of automatically find-\ning a neural network architecture that performs well on a target task\n(such as image classification [ 28], natural language processing [ 24],\nimage generation [ 15]). One of the key questions for NAS is the ques-\ntion of efficiency, since evaluating every promising architecture by\nfully training it would require an extremely large amount of computa-\ntional resources.\nMany approaches have been proposed for increasing the search\nefficiency: low-fidelity evaluation [ 53,37], using weight sharing via a\nsupernetwork [ 34,6], estimating architecture quality via training-free\nmetrics [ 31,1]. Typically, each approach has two stages: first, finding\nan architecture efficiently, then, training it (or its scaled-up version)\nfrom scratch. This final training usually requires a manual intervention\n(e.g., if an architecture of a cell is searched, determining how many\nof these cells should be used), which diminishes the benefit of an\nautomatic approach (potentially, this could also be automated, but we\nare not aware of such studies in the literature). Ideally, an architecture\nitself (not its proxy version) should be searched on the target problem,\nwith the search result being immediately usable after the search (such\nsingle-stage NAS approaches exist but are limited: e.g., they restrict\npotential search spaces [20] or require costly pretraining [6, 43]).\n∗Corresponding Author. Email: a.chebykin@cwi.nl.For the task of hyperparameter optimization (which is closely re-\nlated to NAS), effective and efficient single-stage algorithms exist\nin the form of Population Based Training (PBT) [ 22] and its exten-\nsions [ 27,10]. The key idea of PBT is to train many networks with\ndifferent hyperparameters (a population) in parallel: as the training\nprogresses, worse networks are replaced by copies of better ones\n(including the weights), with hyperparameter values explored via ran-\ndom perturbation. PBT is highly efficient due to the weight reuse, and\ndue to its parallel nature: given a sufficient amount of computational\nresources, running PBT takes approximately the same wall-clock time\nas training just one network.\nDetermining the best way to adapt PBT to NAS is an open re-\nsearch question [ 10]: if a network architecture has been perturbed,\nthe partly-trained weights cannot be reused (because, e.g., weights\nof a convolutional layer cannot be used in a linear one). The naive\napproach of initializing them randomly does not work well (see Sec-\ntion 5.3), and existing algorithms extending PBT to NAS [ 14,42]\nsidestep the issue at the cost of parallelizability or performance (see\nSection 2.2).\nWe propose to adapt PBT to NAS by modifying the search to rely\nnot on random perturbations but on mixing layers of the networks in\nthe population. An example of this principle is combining an encoder\nand a decoder from two different autoencoder networks, ultimately\nobtaining a better-performing network. In this setting, the source of\nthe weights for the changed layers is natural: they can be copied\nfrom the parent networks. Furthermore, we explore if additionally\nadapting the copied weights with the shrink-perturb technique [ 3]\n(reducing weight magnitude and adding noise) is helpful for achieving\na successful transfer of a layer from one network to another.\nFor many standard tasks (such as image classification), single-\nobjective NAS algorithms are matched by (or show only a small\nimprovement over) the simple baseline of random search [ 26,51]. In\norder to make the potential benefit of PBT-NAS clear, experiments\nin this paper are conducted in two challenging settings: Generative\nAdversarial Network (GAN) training, and Reinforcement Learning\n(RL) for visual continuous control. We further advocate for harder\ntasks and search spaces in Section 6.\nWhile our approach could potentially be extended to include hyper-\nparameter optimization, this paper is focused on architecture search.\nThe contributions of this work are threefold:arXiv:2307.15621v1  [cs.LG]  28 Jul 2023Architectures at epoch e\n...Best\nWorst Quality\nContinue\ntraining\n...Replace worst  \nwith a mix of best\n...Weights and type\nof each layer are\ncopied from the\ncorresponding\narchitectureArchitectures at epoch e + e_step\n...\n......\n...Figure 1 . In each iteration of PBT-NAS, architectures in the population continue training for several epochs and then are sorted by performance. Every\narchitecture from the bottom percentile is replaced with a mix of two architectures from the top percentile. During mixing, each layer is copied from one of these\ntwo architectures (weights from the architecture with worse performance are shrink-perturbed). Different shapes represent different types of layers.\n1.We propose to conduct NAS by training a population of different\narchitectures and mixing them on-the-fly to create better ones\n(while inheriting the weights).\n2.We investigate if applying shrink-perturb [ 3] to the weights is a\nsuperior technique for weight inheritance compared to copying or\nrandom reinitialization.\n3.Integrating these ideas, we introduce PBT-NAS, an efficient and\ngeneral NAS algorithm, and evaluate it on challenging NAS search\nspaces and tasks.\n2 Related work\n2.1 Neural Architecture Search\nNAS is the automatic process of finding a well-performing neural net-\nwork architecture for a specific task. Already in early NAS work [ 53],\nefficiency concerns played a role: candidate architectures were trained\nfor only a few epochs. Similar low-fidelity search methods save com-\npute by using fewer layers [ 30], or only a subset of the data [ 37].\nAnother way to save compute is by utilizing a training-free metric to\nperform NAS without any training [31, 1].\nENAS [ 34] introduced the idea of weight sharing: all candidate ar-\nchitectures are viewed as subsets of a supernetwork, with the weights\nof the common parts reused across the architectures. The final archi-\ntecture is scaled up and trained from scratch. This approach greatly de-\ncreased cost of the search to just several GPU-days. DARTS [ 28] fur-\nther increased efficiency by continuously relaxing the problem. Many\napproaches build upon DARTS by e.g., reducing memory usage [ 48]\nor improving performance [ 8]. AdversarialNAS [ 15] extends the ap-\nproach to GAN training, outperforming previous algorithms [16].\nIn OnceForAll [ 6], a supernetwork is pretrained such that subnet-\nworks would perform well without retraining, in AttentiveNAS [ 43]\nand AlphaNet [ 36] performance is further improved. These ap-\nproaches are a good fit for multi-objective NAS (where in contrast to\nsingle-objective NAS, multiple architectures with different trade-offs\nbetween objectives such as performance and latency are searched).\nHowever, the costs for the proposed pretraining reach thousands of\nGPU-hours. Additionally, in any supernetwork approach, the diversity\nand size of the architectures are restricted by the supernetwork.\nOur approach of exchanging layers and weights between different\nnetworks is distinct from the supernetwork-based weight sharing. The\nweights in the supernetwork are constrained to perform well in a\nvariety of subnetworks, while in our approach, after the weights havebeen copied to the network with a novel architecture, they can be\nfreely adapted to it, independently of what happens to their original\nversion in the parent network.\nThe general idea of creating new architectures by modifying ex-\nisting ones and reusing the weights has been explored in NAS ap-\nproaches [ 12,23] relying on network morphisms[ 7,44]. Network\nmorphisms are operators that change the architecture of a neural\nnetwork without influencing its functionality. Although [ 12,23] suc-\ncessfully used morphisms, the idea was later challenged [ 45] with\nexperiments demonstrating that random initialization of new layers is\nsuperior to morphisms. Morphisms are different from our work: while\nthey create a new architecture by modifying one existing architecture,\nwe seek to mix two distinct architectures and reuse their weights.\n2.2 Population Based Training\nIn hyperparameter optimization, hyperparameters of neural net-\nwork training, such as learning rate or weight decay, are optimized.\nBayesian optimization algorithms [ 21,13] are commonly used for se-\nquentially evaluating promising hyperparameter configurations. Other\napproaches include Evolutionary Algorithms [ 29,27], and random\nsearch [4], a simple but reasonably good baseline.\nIn contrast to the approaches that train weights for each hyperpa-\nrameter configuration from scratch, PBT [ 22] reuses partly-trained\nweights when exploring hyperparameters (see Section 1 for short\ndescription and [22] for details).\nTo the best of our knowledge, two algorithms were proposed for in-\ncluding architecture search into PBT: SEARL [ 14] and BG-PBT [ 42].\nIn SEARL, the architecture is modified by mutation, which can add\na linear layer, add neurons to an existing layer, change an activation\nfunction, or add noise to the weights. We use a SEARL-like mutation\nas a baseline. In BG-PBT, there are multiple generations; in each gen-\neration, network architectures are sampled, initialized with random\nweights, and their training is sped up via distillation from the best net-\nwork of the previous generation. This approach adds complexity in the\nform of multiple generations (the number of which must be manually\ndetermined) and using distillation (that would require adaptation to\neach setting, e.g., GAN training). In addition, sequential generations\ndecrease parallelizability. Both SEARL and BG-PBT were proposed\nexclusively for RL tasks, while we construct PBT-NAS to be a general\nNAS algorithm.\n22.3 Combining several neural networks into one\nNeural networks can be combined in various ways. In evolutionary\nNAS [ 30] where weights are trained from scratch for each considered\narchitecture, crossover is performed between encodings of architec-\ntures. Alternatively, there exist methods combining only the weights\nof networks that have the same architecture [ 41,2]. Naively averaging\nthe weights leads to a large loss in performance [ 2], which moti-\nvated these approaches to align neurons so that they would represent\nsimilar features. Averaging weights without alignment is possible if\nthe weights of the networks are closely related. The idea of model\nsoups [ 47] is to start with a pretrained model, fine-tune it with different\nsets of hyperparameters, and greedily search which of the fine-tuned\nmodels to average.\nIn our approach, we mix different architectures together with the\nweights during training, in contrast to evolutionary NAS algorithms\ncombining only architecture encodings, and training the weights from\nscratch. We also avoid the additional complexity of aligning neurons,\ninstead we continue to train the created network, and allow the gra-\ndient descent procedure to adapt the neurons to each other (which is\nfacilitated by shrink-perturb [3], see Section 3.3).\n3 Method\n3.1 Problem setting\nThe goal of single-objective NAS is to find a network architecture α∗\nfrom a search space Ωthat maximizes an objective function fafter\ntraining the weights θ:\nα∗= arg max\nα∈Ωf(α;θ) (1)\nΩtypically includes network architecture properties such as the num-\nber of layers, types of each layer, and its hyperparameters (e.g., con-\nvolution size). We will describe an architecture αbyMcategorical\nvariables {xi}i=0..M−1, each taking lipossible values. Note that in\nthe case where more than one architecture is searched for (e.g., gener-\nator and discriminator of a GAN), we consider, for simplicity, αto\ninclude architecture parameters of all architectures.\n3.2 Algorithm overview\nIn our algorithm, PBT-NAS, we follow the general structure of PBT,\nwhere Nnetworks are trained in parallel1. In each iteration of the\nalgorithm, every network is trained for e_step epochs. Then, each of\nthe worst τ%of the networks is replaced by a mix of two networks\nfrom the best τ%(according to the objective function f). Over time,\nbetter architectures are created. Mixing architectures during training\nis the key component of PBT-NAS. In Section 3.3, we motivate the\nchoice to do NAS by mixing networks. Further details of how we mix\narchitectures are given in Section 3.4.\nA visual representation of one iteration of PBT-NAS is shown in\nFigure 1, and the pseudocode is listed in Algorithm 1.\n3.3 Key question when modifying architecture during\ntraining: where to get the weights from?\nPBT relies on random perturbations of hyperparameters for exploring\nthe search space while the network weights are being continuously\n1Note that since each network has a different architecture, it has a different\ntraining speed, so to avoid biasing the search towards models that require\nless training time, we use the synchronous variant of PBT.Algorithm 1 PBT-NAS\nInput: search space Ω, number of variables M, population size N,\nnumber of epochs e_total , step size e_step , selection parameter τ,\nprobability pof replacing a layer, parameters λ,γof shrink-perturb\n1:pop←{Nrandom architectures from Ω}\n2:e←0\n3:while e < e _total do\n4: fori←0toN−1do// in parallel\n5: trainpopifore_step epochs\n6: popi.fitness ←evaluate (popi)\n7: end for\n8: sortpopbyfitness\n9:best_nets←the best τ%nets\n10: worst _indices ←indices of the worst τ%nets\n11: forjinworst _indices do\n12: popj←create_architecture (best_nets, p, M, λ, γ )\n// the result of mixing, see Algorithm 2\n13: end for\n14: e←e+e_step\n15:end while\ntrained. This works well when searching for hyperparameters that\ncan be replaced independently of the weights: e.g., after changing the\nlearning rate, the training can continue with the same weights.\nHowever, searching for an architecture means introducing changes\nthat impact the weights, e.g., changing the type of a layer from linear\nto convolutional. After such a change, the training process is disturbed:\nthe weights of one type of layer cannot be used in another one.\nTo follow the paradigm of PBT and continue training the network\nafter an architectural change, the source of the weights needs to be\ndetermined. We consider three potential approaches (Figure 2).\nWeight\ntensor\n(a) Copy (b) Shrink-perturb (c) Reinitialize randomly\nParent\nParent\nFigure 2 . Three potential operations to perform on the weight tensor when\ncopying the corresponding layer from the parent.\nOne approach is initializing the new weights randomly. Intuitively,\nthis could be problematic, as replacing weights of a whole layer\nwith random ones can substantially disrupt the learned connections\nbetween neurons across the whole network.\nInstead of being initialized randomly, the weights of the modified\npart can come from another network in the population. If the new\nvalue of the type of the layer is not generated randomly but copied\nfrom another solution, the corresponding layer weights can be copied\nfrom it too. Straightforward weight copying may be better than ran-\ndom initialization but it faces the following issue: even though the\nlayers at the same depth of different networks should perform simi-\nlar transformations (when trained on the same task), the actual data\n3Algorithm 2 create_architecture\nInput: set of networks to potentially mix nets , probability pof replac-\ning a layer, number of variables M, parameters λ,γof shrink-perturb\n1:net1, net 2←randomly sample from nets\n2:ifnet1.fitness < net 2.fitness then\n3:net1, net 2←net2, net 1// sort by fitness\n4:end if\n5:netnew←copy(net1)\n6:fori= 0toM−1do// iterate over architecture variables\n7: ifrandom_uniform ()< pthen\n8: netnew.αi←net2.αi// copy the value of the variable\n9: if∃net2.Withen\n10: // if the variable is a layer, copy and modify its weights\n11: Wnew←copy(net2.Wi)\n12: shrink_perturb (Wnew, λ, γ)\n13: netnew.Wi←Wnew\n14: end if\n15: end if\n16:end for\n17:return netnew\nrepresentations in each network are likely to be different. The copied\nweights would need to be adapted to a different representation space,\nbut it might be difficult for gradient descent to adapt them quickly.\nShrink-perturb [ 3] is potentially helpful in this scenario. It was\nmotivated by the observation that in online learning, continuing train-\ning from already trained weights when new data comes in can be\nworse than retraining from scratch using all the available data. Shrink-\nperturb consists of modifying the weights of a neural network by\nshrinking (multiplying by a constant λ) and perturbing them (adding\nnoise multiplied by a constant γ; a new initialization of the network\narchitecture is used as the source of noise).\nApplying shrink-perturb to the copied weights is the middle ground\nbetween copying the weights as-is, and initializing them randomly.\nThis preserves some useful information in the weights, while also\npotentially making their adaptation to the new architecture easier.\n3.4 Mixing networks\nAlgorithm 2 shows our procedure for creating a new network. Firstly,\ntwo parent networks are randomly sampled from the top τpercentile\nof the population. An offspring solution is created by copying the\nbetter parent, and replacing with probability peach layer with the\nlayer from the worse parent (including the weights, which are shrink-\nperturbed). Our mixing is a version of uniform crossover [ 39] where\nonly one offspring solution is produced. Note that our mixing requires\nthat layers in the same position can be substituted for each other\n(i.e., the output can be used as the input of the next layer), with\nthe architecture remaining valid after a layer is replaced. We further\ndiscuss this limitation in Section 6.\nUnlike existing approaches to combining neural networks (see\nSection 2.3), we do not expect (or need) the new network to perform\nwell right away. Instead, it will be trained for several epochs in the\nnext iteration of PBT-NAS, the same as the other networks in the\npopulation.\n4 Experiment setup\n4.1 General\nWe evaluate PBT-NAS on two tasks known to require careful tuning of\nnetwork architecture and hyperparameters: GAN training and RL forvisual control. In these settings, architecture can strongly influence\nperformance [ 17,38]. We consider non-trivial architecture search\nspaces, see Sections 4.2 and 4.3. We would like to emphasize that\nachieving a state-of-the-art result on the chosen tasks is not our goal,\ninstead we aim to demonstrate the feasibility of architecture search\nvia simultaneous training and architecture mixing on tasks where\nperformance strongly depends on architecture.\nHyperparameters of PBT-NAS are population size N, step size\ne_step, selection parameter τ(we use the default value from PBT,\n25% , in all experiments), probability pof replacing a layer (which is\nalso set to 25% ). We aim to avoid unnecessary hyperparameter tuning\nto see if our approach is robust enough to perform well without it and\nto save computational resources.\nThe experiments were run in a distributed way, the details on used\nhardware and on GPU-hour costs of experiments are given in Ap-\npendix F. The algorithms used the amount of compute equivalent\nto training Nnetworks. Every experiment was run three times, we\nreport the mean and standard deviation of the performance of the\nbest solution from each run. We use the Wilcoxon signed-rank test\nwith Bonferroni correction for statistical testing (target p-value 0.05,\n4 tests, corrected p0.0125, mentions of statistical significance in the\ntext imply smaller p, allp-values are reported in Appendix C). Our\ncode is available at https://github.com/AwesomeLemon/PBT-NAS, it\nincludes configuration files for all experiments.\n4.2 GANs\nIn AdversarialNAS [ 15], the authors describe searching for a GAN\narchitecture (for unconditional generation) in a search space where\nrandom search achieved poor results — this motivated us to adopt\nthis search space, which we refer to as Gan. In AdversarialNAS, both\ngenerator and discriminator architectures are searched for but we\nnoticed that in the official implementation, the searched discriminator\nis discarded, and an architecture from the literature [ 16] is used instead.\nThis prompted us to create an extended version of the search space\n(which we call GanHard ) that includes discriminator architectures\nresembling the one manually selected by the AdversarialNAS authors.\nAdversarialNAS cannot be used to search in GanHard because some\nof the options cannot be searched for via continuous relaxation (one\nexample is searching whether a layer should downsample: since output\ntensors with and without downsampling have different dimensions, a\nweighted combination cannot be created).\nSpecifics of search spaces are not critical for our research, so we\ngive condensed descriptions here, see Appendix E and our code for\nmore details.\nInGan, operations for three DARTS-like [ 28] cells are searched\n(each cell is a Directed Acyclic Graph (DAG) with operations on the\nedges; in contrast to DARTS, each cell may have a different architec-\nture). Additionally, inspecting the code of AdversarialNAS showed\nthat the output of some cells is pointwise summed with a projection of\na part of a latent vector. Each such projection is a single linear layer\nmapping a part of a latent vector to a tensor of the same dimensionality\nas the cell output: (#channels, width, height ). These projections\ncontain many parameters and are therefore an important part of the\narchitecture. In the code of AdversarialNAS, these projections are\nadjusted for each dataset. As to the discriminator, the architecture\nfrom [16] is used.\nNext, we describe GanHard . InGanHard , the parameters of the pro-\njections in the generator can be searched for. We additionally treat the\nlayer mapping latent vector to the input of the generator as a projec-\ntion, since it is conceptually similar. The projections (one per cell) can\n4be enabled or disabled, except for the first one (connected to generator\ninput) which is always enabled. A projection can take as input either\nthe whole latent vector or the corresponding one-third of it (the first\nthird of the vector for the first projection, etc.). There are three options\nfor the spatial dimensions of the output of a projection: target (equal\nto the output dimensions of the corresponding cell), smallest (equal\nto the input dimensions of the first cell), and previous (equal to the\noutput dimensions of the previous cell). Since the projection output is\nsummed with the cell output pointwise, the dimensions need to match,\nwhich is not the case for the last two options. To upsample the tensor\nto the target dimensions, either bilinear ornearest _neighbour in-\nterpolation is used, which is also a part of the search space. Finally,\ngiven that a projection is a large linear layer with potentially millions\nof parameters (which makes overfitting plausible), we introduce an\noption for a dropout layer in the projection, with possible parameters\n0.0,0.1,0.2.\nThe discriminator search space in GanHard is based on the one in\nAdversarialNAS, the discriminator has 4 cells (each being a DAG with\ntwo branches), each cell has a downsampling operation in the end.\nHowever, we noticed that the fixed architecture from [ 16] that is used\nfor final training of AdversarialNAS downsamples only in the first two\ncells. Additionally, in the first cell, the input is downsampled rather\nthan the output. We amend the discriminator search space to contain\na similar architecture. Firstly, we search whether each cell should\ndownsample or not. Secondly, we add options for downsampling\noperations that are performed at the start of each branch rather than at\nthe end of them. To enrich the search space further, we add two more\nnodes to each cell.\nThe number of variables in Ganis 21 and the size of the search\nspace is ≈3.4·1019. InGanHard , there are 72 variables (32 for the\ngenerator, 40 for the discriminator), and the size of the search space\nis≈2.9·1053.\nFollowing AdversarialNAS, we run the experiments on CIFAR-\n10 [25] and STL-10 [ 9], using both GanandGanHard . In Adversar-\nialNAS, the networks were trained for 600 epochs. We reduce that\nnumber to 300 epochs to save computation time (preliminary experi-\nments showed diminishing returns to longer training), for the other\nhyperparameters, the same values as in AdversarialNAS are used.\nThe Frechet Inception Distance (FID) [ 19] is a commonly used\nmetric for measuring GAN quality. We use its negation as the objec-\ntive function, computing it on 5,000 images during the search. For\nreporting the final result, the FID for the best network is computed on\n50,000 images. We additionally report the Inception Score (IS) [ 35],\nanother common metric of GAN quality. The idea behind both FID\nand IS is to compare representations of real and generated images.\nBased on preliminary experiments, the population size Nis set to\n24, and e_step is set to 10.\n4.3 RL\nWe build upon DrQ-v2 [ 50], a model-free RL algorithm for visual\ncontinuous control. DrQ-v2 achieves great results on the Deep Mind\nControl benchmark [ 40], solving many tasks. Searching for architec-\ntures for solved tasks is not necessary, therefore for our experiments\nwe chose tasks where DrQ-v2 did not achieve the maximum possible\nperformance: Quadruped Run, Walker Run, Humanoid Run.\nDrQ-v2 is an actor-critic algorithm with three components: 1)an\nencoder that creates a representation of the pixel-based environment\nobservation, 2)an actor that, given the representation, outputs prob-\nabilities of actions, and 3)a critic that, given the representation, es-\ntimates the Q-value of the state-action pair (the critic contains twonetworks because double Q-learning is used).\nWe design the search space to include the architectures of all the\ncomponents of DrQ-v2. Each network has three searchable layers. For\nthe encoder, the options are Identity, Convolution {3x3, 5x5, 7x7},\nResNet [ 18] block {3x3, 5x5, 7x7}, Separable convolution {3x3, 5x5,\n7x7}. For the actor and both networks of the critic, the available\nlayers are Identity, Linear, and Residual [ 5] with multiplier 0.5or2.0.\nAdditionally, we search whether to use Spectral Normalization [ 32]\n(for each network separately) and which activation function to use in\neach layer (options: Identity, Tanh, ReLU, Swish). We also search the\ndimensionality of representation: in DrQ-v2 it was set to either 50 or\n100 depending on the task, we have 25,50,100,and150as options.\nThere are 36 variables in total, the search space size is ≈4.6·1021.\nThe hyperparameters of DrQ-v2 are used without additional tuning.\nFor the Walker and Humanoid tasks, DrQ-v2 uses a replay buffer of\nsize106. Our servers do not have enough RAM to allow for such\na buffer size when many agents are training in parallel, therefore\nfor these tasks, we use a shared replay buffer (proposed in [ 14]):\ndifferent agents can learn from the experiences of each other. To run\nin a distributed scenario with no shared storage, the buffers are only\nshared by the networks on the same machine. For fairness, all the\nbaselines also use a shared buffer per machine.\nBased on preliminary experiments, the population size Nis set to\n12. For the Quadruped and the Walker tasks, the DrQ-v2 agent used\n3·106frames. We use the same number of frames per agent, which\nmeans that Ntimes more total frames are used. For the Humanoid\ntask,3·107frames were used in DrQ-v2, we use only 1.5·107per\nagent to save computation time. For uniformness of notation with\nGANs, we also use \"epoch\" in the context of RL, one epoch is defined\nas104frames. This means that 300 epochs are used for the Quadruped\nand Walker tasks, the same as for GANs, and e_step is also set to 10.\nSimilar to BG-PBT [ 42], our preliminary experiments showed that\nhaving longer periods without selection at the start of the training\nis beneficial, therefore during the first half of the training, e_step is\ndoubled from 10 to 20 epochs for the Quadruped and Walker tasks.\nSince for Humanoid only half the training is performed (in terms of\nframes per agent), the step size is fixed at 100 epochs (scaled up from\n10 proportionally to the increase in the number of frames).\n4.4 Baselines\nWe consider two general baselines that parallelize well and that can\nsearch in the proposed challenging search spaces.\n1.Random search. Narchitectures are randomly sampled and\ntrained.\n2.SEARL-like mutation . In order to fairly evaluate the performance\nof a mutation-based architecture search approach like SEARL [ 14],\nwe replace the mixing operator of PBT-NAS with the mutation\noperator from SEARL, adapting it to be applicable to both GAN\nand RL settings: with equal probability, either (a)one variable in\nthe architecture encoding is resampled, (b)weights are mutated\nusing the procedure from SEARL, or (c)no change is performed.\nAdversarialNAS is a specialized baseline only capable of searching\ninGan. In [ 15], the performance for only one seed was reported. We\nrun the official implementation with 5 seeds and report the mean and\nstandard deviation of performance.\n5Table 1 . Results for GAN training (mean ±st. dev.). The best value in each column is in bold.\nFID↓ IS↑ FID↓ IS↑ FID↓ IS↑\nAdversarialNAS [15] 12.29±0.80 8.47±0.14 — — — —\nRandom search 13.39±0.28 8.22±0.15 16.79±0.97 7.80±0.14 28.58±1.77 9.33±0.18\nSEARL-like mutation [14] 13.78±1.02 8.38±0.05 15.72±2.22 8.26±0.21 26.94±0.93 9.66±0.27\nPBT-NAS 12 .21±0.16 8.63±0.17 13 .25±1.64 8.25±0.27 25 .11±0.94 9.71±0.09AlgorithmCIFAR-10 STL-10\nGan GanHard GanHard\n5 Results\n5.1 PBT-NAS vs. the baselines\nAs can be seen in Table 1, PBT-NAS achieves the best performance\namong all tested approaches in all GAN settings2. The improvements\nin FID over both random search and SEARL-based mutation are sta-\ntistically significant. Despite the claim of [ 15] that random search\nperforms poorly in Gan, we find that the gap between it and Adver-\nsarialNAS [ 15] on CIFAR-10 is small, and the difference between all\nalgorithms is overall not large. The decreased performance of random\nsearch in GanHard shows that GanHard is indeed a more challenging\nsearch space. The results of searching in this space for CIFAR-10 and\nSTL-10 show a clear improvement of PBT-NAS over the baselines in\nterms of FID. IS is better in the majority of settings.\nPBT-NAS is also the best among alternatives on RL tasks, achieving\nbetter anytime performance, as shown in Figure 3 (the improvements\nin score over both random search and SEARL-based mutation are\nstatistically significant). For Walker Run, there is no meaningful\ndifference between algorithms, as the task is solved by all tested\napproaches, demonstrating that for differences between performance\nof the algorithms to be clear, both the RL task and the search space\nneed to be of significant complexity.\n0 1 2 3\nTotal frames 1e70200400600800Score\nRandom search\nSEARL-like mutation\nPBT-NAS\n(a) Quardruped Run\n0 1 2 3\nTotal frames 1e70200400600800Score\nRandom search\nSEARL-like mutation\nPBT-NAS (b) Walker Run\n0.0 0.5 1.0 1.5\nTotal frames 1e8050100150200250300350ScoreRandom search\nSEARL-like mutation\nPBT-NAS\n(c) Humanoid Run\nFigure 3 . Results for RL tasks, mean ±st. dev. (shaded area).\n5.2 Mixing networks is better than cloning good\nnetworks\nIn order to show that creating new architectures makes a difference, we\nrun a \"No mixing\" ablation: every component of PBT-NAS is kept the\n2When searching in Ganfor STL-10, we faced reproducibility issues (de-\nspite using the official implementation), see Appendix D for results and\ndiscussion.same, except that a new model is created by mixing a well-performing\nmodel with itself (rather than with another well-performing model).\nThis way, no new architecture is produced, but the other benefits of\nPBT-NAS remain (e.g., replacing poorly-performing networks with\nwell-performing ones). As seen in Table 2, this degrades the perfor-\nmance, clearly showing the impact that creating a better architecture\ncan have.\nTable 2 . Results of ablation studies (mean ±st. dev.). The best value in each\ncolumn is in bold.\nFID↓ Score↑\n(CIFAR-10, (Quadruped Run)\nGanHard )\nPBT-NAS (default) 13 .25±1.64 801±70\nNo mixing 14.90±1.29 672±53\nShrink-perturb coefficients:\n[1, 0] — copy exactly 14.94±0.56 699±4\n[0, 1] — reinitialize randomly 15.06±2.59 532±62Algorithm\n5.3 Shrink-perturb is the superior way of weight\ninheritance\nTable 2 shows that copying weights from the donor without change\n(shrink-perturb parameters [1,0]), or replacing them with random\nweights (shrink perturb [0,1]) leads to worse results in comparison to\nthe usage of shrink-perturb. Thus, in our settings, using shrink-perturb\nis the best method to inherit the weights. The default parameters\nof shrink-perturb from [ 52] ([0.4,0.1]) worked well in PBT-NAS\nwithout any tuning.\nIn [52], shrink-perturb was found to benefit performance, thus\nraising the question if using it gives PBT-NAS an unfair advantage that\nis not related to NAS. In order to test this, we added shrink-perturb\nto random search. As shown in Table 3, performance deteriorates,\nindicating that using shrink-perturb with default parameters in our\nsetting is not helpful outside the context of NAS.\nTable 3 . The effect of using shrink-perturb in random search\nUse shrink-perturb FID↓ Score↑\nin random search (CIFAR-10, GanHard )(Quadruped Run)\nNo (default) 16.79±0.97 616±53\nYes 22.39±2.64 498±30\n5.4 Increasing population size improves performance\nWe design our algorithm to be highly parallel and scalable. Figure 4\ndemonstrates that as the population size increases, the performance\nstrictly improves (although diminishing returns can be observed).\nGiven enough GPUs, the increased population size will not meaning-\nfully increase wall-clock time, since every population member can be\nevaluated in parallel.\n612 24 36\nPopulation size152025FID (lower is better)\nRandom search\nPBT-NAS(a) CIFAR-10, GanHard (FID↓)\n6 12 18\nPopulation size500600700800Score (higher is better)\nRandom search\nPBT-NAS (b) Quadruped Run (Score ↑)\nFigure 4 . Impact of scaling population size, mean ±st. dev.\n5.5 Model soups\nAs mentioned in Section 2.3, the idea of a model soup [ 47] is to\nimprove performance by averaging weights of closely-related neural\nnetworks. As such, it seems like an especially good fit for the PBT\nsetting: although the networks in the population start from different\nweights (and different architectures in the case of PBT-NAS), as worse\nnetworks are replaced by offspring of better networks, the population\ngradually converges. Since creating a model soup is done after training\nand requires a negligible amount of computation (evaluating at most\nNmodels), its inclusion into PBT-like algorithms could give an almost\nfree performance improvement. Therefore, we create model soups\nfollowing the greedy algorithm from [47].\nTable 4 shows that soups improve GAN FID by approximately\n0.4 points. For RL, however, there is no improvement when both\nthe encoder and the actor are averaged (Table 5). We hypothesize\nthat different actors may have dissimilar internal representations im-\nplementing different behaviour logic, unlike the encoders that only\nconvert pixel inputs into representations. Therefore, we tried to sepa-\nrately average encoders, or actors. The results with averaged encoders\nare the best overall but they still do not lead to improved performance.\nFor Walker Run, the task where performance is saturated, there is no\ndifference between settings.\nTable 4 . The difference in metrics between a model soup and the best\nindividual model (GAN), mean ±st. dev.\n∆FID↓ ∆IS↑\nCIFAR-10 −0.48±0.34 0.07±0.05\nSTL-10 −0.35±0.26 0.04±0.25DatasetGanHard\nTable 5 . The difference in score between a model soup and the best individual\nmodel (RL), mean ±st. dev.\nQuadruped Walker Humanoid\nEncoder −6±10 −1±4−23±19\nActor −196±275 −4±7−244±32\nBoth −142±193 0±6−223±20What to average∆Score↑\nPreviously, soups were only demonstrated for classification tasks,\nso it is interesting to see that they could also be beneficial in GANs.\nWhile no improvement was seen for RL, the fact that only the vision-\nrelated network, the encoder, could be averaged without large perfor-\nmance degradation hints at the limitations of the technique.\n6 Discussion\nWe have introduced PBT-NAS, a NAS algorithm that creates new\narchitectures by simultaneously training and mixing a population ofneural networks. PBT-NAS brings the efficiency of PBT (designed\nfor hyperparameter optimization) to NAS, providing a novel way\nto search for architectures. As computation power grows, especially\nin the form of multiple affordable GPUs, having parallelizable and\nscalable algorithms such as PBT-NAS becomes more important. At\nthe same time, this computation power is not limitless, and reusing the\npartly-trained weights during architecture search is important from\nthe perspective of search efficiency.\nCurrently, a large amount of effort in single-objective NAS research\nis directed at searching classifier architectures in cell-based search\nspaces, which are quite restrictive, and where random search achieves\ncompetitive results [ 26,49]. We think that pivoting to more chal-\nlenging search spaces and tasks could lead to NAS having a larger\nimpact (e.g., in constructing state-of-the-art architectures, which is\nstill mostly done by hand), and to comparisons between NAS algo-\nrithms leading to clearer differences. In Section 5.1, we showed that\nPBT-NAS could search in the challenging GanHard space, where an\nexisting efficient algorithm, AdversarialNAS, could not be applied.\nOne limitation of exchanging layers during training is the require-\nment that different layer options (in the same position) need to be\ninteroperable: the activation tensors they produce should be possible\nfor the next layer to take as input (so that after replacing a layer, the\narchitecture remains valid). This means that the number of neurons\ncan be searched only when it does not influence the output shape. This\ncould be addressed by e.g., duplicating neurons if there are too few\nof them and removing excessive ones if there are too many. Another\nlimitation arises due to the greedy nature of PBT-NAS: architectures\nare selected based on their intermediate performance, and, therefore,\nsuboptimal architectures can be selected when early performance of\nan architecture is not representative of the final one.\nAchieving good performance in different tasks with minimal hyper-\nparameter tuning is a desirable property for a NAS algorithm. We used\nhyperparameters from the literature without tuning both in GAN train-\ning and in RL, as well as relying on default selection strategy from\nPBT. PBT-NAS outperformed baselines despite using these default\nvalues, tuning them could potentially further improve the results.\n7 Conclusion\nIn this paper we designed and evaluated PBT-NAS, a novel way to\nsearch for an architecture by mixing different architectures while\nthey are being trained. We find that adapting the weights with the\nshrink-perturb technique during mixing is advantageous compared to\ncopying or randomly reinitializing them.\nPBT-NAS is shown to be effective on challenging tasks (GAN\ntraining, RL), where it outperformed considered baselines. At the\nsame time, it is efficient, requiring training of only tens of networks\nto explore large search spaces. The algorithm is straightforward, par-\nallelizes and scales well, and has few hyperparameters.\nWhile in this work only NAS was considered, in the future, PBT-\nNAS could be adapted to simultaneously search for hyperparameters\nof neural network training, and of the algorithm itself, both of which\nwould be necessary in order to fully automate the process of neural\nnetwork training.\nAcknowledgements\nThis work is part of the research projects DAEDALUS (funded via\nthe Open Technology Programme of the Dutch Research Council\n(NWO), project number 18373; part of the funding is provided by\nElekta and ORTEC LogiqCare) and OPTIMAL (funded by NWO,\nproject OCENW.GROOT.2019.015).\n7",
      "metadata": {
        "filename": "Shrink-Perturb Improves Architecture Mixing during Population Based Training for.pdf",
        "hotspot_name": "PBT_Injection_Molding",
        "title": "Shrink-Perturb Improves Architecture Mixing during Population Based\n  Training for Neural Architecture Search",
        "published_date": "2023-07-28T15:29:52Z",
        "pdf_link": "http://arxiv.org/pdf/2307.15621v1",
        "query": "injection molding PBT housing energy efficiency optimization cycle time reduction"
      }
    },
    "Simultaneous Training of First- and Second-Order Optimizers in Population-Based": {
      "full_text": "Simultaneous Training of First- and Second-Order Optimizers in\nPopulation-Based Reinforcement Learning\nFelix Pfeiffer1Shahram Eivazi1, 2\n1University of T ¨ubingen\n2Festo SE & Co. KG\nfelix.pfeiffer@protonmail.com\nAbstract\nThe tuning of hyperparameters in reinforcement learning\n(RL) is critical, as these parameters significantly impact\nan agent’s performance and learning efficiency. Dynamic\nadjustment of hyperparameters during the training process\ncan significantly enhance both the performance and stabil-\nity of learning. Population-based training (PBT) provides a\nmethod to achieve this by continuously tuning hyperparam-\neters throughout the training. This ongoing adjustment en-\nables models to adapt to different learning stages, resulting\nin faster convergence and overall improved performance. In\nthis paper, we propose an enhancement to PBT by simultane-\nously utilizing both first- and second-order optimizers within\na single population. We conducted a series of experiments us-\ning the TD3 algorithm across various MuJoCo environments.\nOur results, for the first time, empirically demonstrate the po-\ntential of incorporating second-order optimizers within PBT-\nbased RL. Specifically, the combination of the K-FAC opti-\nmizer with Adam led to up to a 10% improvement in overall\nperformance compared to PBT using only Adam. Addition-\nally, in environments where Adam occasionally fails, such as\nthe Swimmer environment, the mixed population with K-FAC\nexhibited more reliable learning outcomes, offering a signif-\nicant advantage in training stability without a substantial in-\ncrease in computational time.\nIntroduction\nThe majority of RL algorithms are sensitive to hyperparame-\nter settings, initialization, and the stochastic nature of the en-\nvironment. Regularization, robust training procedures, and\nstable optimization methods are necessary to enhance the\noverall performance of an RL agent (Zhang et al. 2021). In\nresponse to the time needed for training of RL, there has\nbeen a growing interest in the ability to simultaneously ex-\nplore multiple regions of the hyperparameter space using\nPBT (Jaderberg et al. 2017; Paul et al. 2019; Wu et al. 2020;\nZhao et al. 2023). In this paper, we also focus on PBT and\nits ability to simultaneously explore multiple regions of the\nhyperparameter space for training of RL agents. However,\nwe do not include techniques used in automated machine\nlearning (Auto ML) community (Feurer et al. 2019).\nUnlike traditional training methods that focus on optimiz-\ning a single model, PBT maintains a population of evolving\nmodels that are updated either sequentially (computation-\nally efficient but time-consuming) or in parallel across thepopulations as fast as a model training (Flajolet et al. 2022).\nAlthough these techniques themselves benefit from a large\nnumber of samples, PBT algorithms suffer from their re-\nliance on heuristics for hyperparameter tuning, which can\nlead to underperformance without extensive computational\nresources and often result in suboptimal outcomes. As such,\nmany studies reported in PBT literature focus on the efficient\ntraining of populations. For example, Parker-Holder et al.\n(2020) introduced Population-Based Bandits (PB2), a novel\nalgorithm that maintains the PBT framework but incorpo-\nrates a probabilistic model to guide hyperparameter search\nefficiently. In a recent study, Grinsztajn et al. (2023) intro-\nduced Poppy, a population-based RL method that trains a set\nof complementary policies to maximize performance across\na distribution of problem instances. Rather than explicitly\nenforcing diversity, Poppy uses a novel training objective\nthat encourages agents to specialize in different subsets of\nthe problem distribution.\nMore related to our work Cui et al. (2018) introduced the\nEvolutionary Stochastic Gradient Descent (ESGD) frame-\nwork, which integrates traditional Stochastic Gradient De-\nscent (SGD) with gradient-free evolutionary algorithms to\nenhance the optimization of deep neural networks. By al-\nternating between SGD and evolutionary steps, ESGD im-\nproves the fitness of a population of candidate solutions, en-\nsuring that the best-performing solution is preserved. This\napproach leverages the strengths of both optimization tech-\nniques, resulting in more effective training across a range\nof applications, including speech and image recognition, as\nwell as language modeling.\nOn the other hand, while the use of different optimizers\nfor efficient neural network training is well-studied (Choi\net al. 2019), there appears to be a significant gap in the PBT\nliterature, where the exploration of various optimizers re-\nmains largely unexplored. Although there are approaches of\nmixing different optimization strategies, such as in Landro\net al. (2020), which merges two first-order optimizers di-\nrectly. In this paper, we aim to investigate the impact of\nutilizing different optimizers within the PBT framework.\nDrawing inspiration from recent findings by Tatzel et al.\n(2022), which emphasize the advantages of second-order\nmethods in the later stages of neural network training, our\nstudy seeks to leverage the benefits of diverse optimization\nstrategies throughout the entire training process.arXiv:2408.15421v2  [cs.LG]  4 Sep 2024Second-order methods in the RL domain have been val-\nidated in various settings (Kamanchi et al. 2021; Gebotys\net al. 2022; Salehkaleybar et al. 2022) and algorithms like\nNatural Policy Gradient (NPG) or Trust Region Policy Opti-\nmization (TRPO). Although these methods are computation-\nally more intensive, techniques (Martens et al. 2015) and li-\nbraries (Dangel et al. 2019; Gao 2024) have been developed\nto extract second-order information from a backward pass\nmore easily. We develop and evaluate a PBT-based frame-\nwork that allows agents with different optimizers to be used\nin one population. This approach not only capitalizes on the\nefficiency of first-order methods in early training stages but\nalso utilizes the precision of second-order optimizer as the\npopulation converges. By allowing these diverse agents to\ncoexist and compete within the population, we hypothesize\nthat our method will achieve more robust and efficient opti-\nmization.\nPreliminaries\nOptimization in neural networks involves adjusting the net-\nwork’s parameters (weights and biases) to minimize the\nloss function, which measures the discrepancy between\nthe predicted and true values. The training data is rep-\nresented as {x(i),y(i)∈RI×RC}i∈D, where D=\n{1, ..., N},(x(i),y(i))iid∼Pdataare i.i.d. samples from the\ndata distribution. The model f:RD×RI→RCmakes pre-\ndiction ˆy=f(θ,x)given parameters θ∈RDand input\nx∈RI. The loss function ℓ:RC×RC→Rcompares the\nprediction ˆyto the true label y. The expected risk is defined\nasLPdata(θ) =E(x,y∼Pdata)[ℓ(f(θ,x),y)]. The goal is to find\nθ∗that minimized the empirical risk:\nθ∗= arg min\nθ∈RDLD(θ)\nwith\nLD(θ) =1\n|D|X\ni∈Dℓ(f(θ,x(i)),y(i))(1)\nFor our study, we used the Adam optimization algorithm,\nan adaptive gradient method introduced by Kingma (2014).\nAdam combines the advantages of AdaGrad and RMSprop,\nadjusting the learning rate for each parameter based on the\nfirst and second moments of the gradients.\nSecond Order Optimization\nThe Newton step is a central concept in second-order\noptimization methods. The Newton method incorporates\nsecond-order information through the Hessian matrix, which\ncaptures the curvature of the loss landscape. The Hessian,\ndenoted as H=∇2LPdata(θ), is a square matrix of second-\norder partial derivatives of the loss function. To derive the\nNewton step, we start by considering a quadratic Taylor ap-\nproximation of the loss function around θt:LPdata(θ)≈q(θ) :=LPdata(θt)\n+ (θ−θt)⊤g\n+1\n2(θ−θt)⊤H(θ−θt)(2)\nWhere g:=∇LPdata(θt)andH:=∇2LPdata(θt)≻0.\nThe Hessian is assumed to be positive definite. To get the\nnext iterate θt+1, we set ∇q=g+H(θ−θt)!= 0. We\nobtain the Newton step:\nθt+1=θt−H−1g (3)\nThe Newton method can lead to faster convergence near\nthe optimum due to its potential for quadratic convergence,\noffering a significant speed advantage over first-order meth-\nods in certain scenarios. However, computing and inverting\nthe Hessian matrix is computationally expensive and can be\nimpractical for large neural networks, with a complexity of\nO(n2)for storage, O(n3)for inversion, and O(n2)for com-\nputing the Hessian, where nis the number of parameters of\nthe neural network.\nDealing with Non-Convex Objective Functions:\nThe Generalized Gauss-Newton Matrix\nWhen optimizing non-convex objective functions, the Hes-\nsian matrix may have negative eigenvalues, making it indef-\ninite and potentially leading to non-minimizing steps. This\nis a common challenge in deep learning due to the highly\nnon-convex loss landscape. The Generalized Gauss-Newton\n(GGN) matrix offers a solution by providing a positive semi-\ndefinite approximation of the Hessian (Schraudolph 2002;\nMartens 2020). The GGN matrix is derived by decompos-\ning the Hessian based on the model’s output, specifically\nthrough the mapping θ7→f(θ,x)7→ℓ(f(θ,x),y). This\nallows for expressing the gradient of the loss function with\nrespect to the parameters θ.\n∇θℓ(f(θ,x),y) =J⊤∇fℓ(f(θ,x),y) (4)\nHere,J∈RC×Dis the Jacobian matrix of the model’s\noutput f(θ,x)with respect to the parameters θ.\nThe Hessian matrix Hf∈RC×Ccan then be decom-\nposed into two terms. The first term involves the Jacobian\nand the Hessian of the loss with respect to the model’s out-\nput. The second term in the Hessian decomposition arises\nfrom the chain rule for second derivatives. This term ac-\ncounts for the curvature introduced by the model parameters\nthemselves, where ∇2\nθ[f(x,θ)]cis the second derivative of\nthec-th output of the model with respect to the parameters\nθ.∇fℓ(f(θ,x),y)]cis the gradient of the loss with respect\nto the c-th output of the model:\n∇2\nθℓ(f(θ,x),y) =\nJ⊤HfJ+CX\nc=1∇2\nθ[f(x,θ)]c·[∇fℓ(f(θ,x),y)]c(5)The GGN Gsimplifies this by neglecting the second term\nand focusing only on the first term:\nG=E(x,y)∼Pdata(x,y)[J⊤HfJ] (6)\nThis expectation over the data distribution ensures that\nGcaptures the average curvature of the loss function with\nrespect to the parameters. Since J⊤HfJrepresents a\nquadratic from, the matrix is positive semi-definite.\nDamping and the Trust-Region Problem\nWhen the objective function is convex, the quadratic model\napproximating the objective function can be arbitrarily bad.\nTo address this issue, the concept of a trust region is intro-\nduced (Mor ´e et al. 1983). The idea is to limit the param-\neter updates to a region where the quadratic model qis a\ngood approximation of the actual objective function. This is\nachieved by restricting the parameter updates to lie within a\ncertain radius raround the current parameter θt. Mathemat-\nically, the trust-region problem can be formulated as:\nθt+1= arg min\nθq(θ)such that ∥θt+1−θt∥ ≤r(7)\nCombining the trust-region problem with the Newton step\ngives us a modified version of Newton’s method given by:\nθt+1=θt−(H+δI)−1gwith δ≥0 (8)\nHere, δis the damping parameter. The damping term δI,\nwhere Iis the identity matrix, is added to the Hessian to\ncontrol the step size. Damping can be seen as interpolation\nbetween a first-order and a second-order optimization step.\nDiagonal Gauss-Newton Second Order Optimizer\nThe Diagonal Generalized Gauss-Newton (Diag. GGN) op-\ntimizer is an approach designed to simplify the computation\nand inversion of the Gauss-Newton matrix by focusing only\non its diagonal elements. The full GGN captures the cur-\nvature information comprehensively. However, considering\nonly the diagonal elements, the Diag. GGN optimizer ap-\nproximates the curvature more coarsely. This approximation\nassumes that the off-diagonal elements are less significant,\nwhich may not always be true. The update formula for the\nparameters is similar to the update formula of the Newton\nstep (8). It is given by:\nθt+1=θt−α(G(θt) +δI)−1g(θt) (9)\nWhere αis the step size, Gis the diagonal of the GGN,\nandgis the gradient.\nKronecker-Factored Approximate Curvature\nOptimizer\nThe Kronecker-Factored Approximate Curvature (K-FAC)\noptimizer, introduced by Martens et al. (2015), is a second-\norder method that efficiently approximates the Fisher In-\nformation Matrix (FIM). The FIM measures the amountof information an observable variable carries about an un-\nknown parameter and is closely related to the curvature of\nthe likelihood function with respect to model parameters.\nK-FAC simplifies the inversion of the FIM by approximat-\ning it with a block-diagonal matrix, where each block corre-\nsponds to parameters of a specific layer or group of layers,\nsignificantly reducing computational complexity. For a lin-\near layer, each block of the FIM is structured as follows:\nˆFi=E[aia⊤\ni⊗bib⊤\ni]≈E[aia⊤\ni]⊗E[bib⊤\ni] =:Ai⊗Bi\n(10)\nHere, airepresents the input activations, and birepresents\nthe gradients with respect to the pre-activations. By approx-\nimating the FIM as a Kronecker product of two smaller ma-\ntricesAiandBi, we can efficiently compute the inverse of\neach block:\nˆF−1\ni= (Ai⊗Bi)−1=A−1\ni⊗B−1\ni (11)\nMathematically, the parameter update rule of K-FAC is\ngiven by:\nθt+1=θt−α(A−1\ni⊗B−1\ni)∇θL(θt) (12)\nExperiments\nIn this paper, all experiments were performed using the Twin\nDelayed Deep Deterministic Policy Gradient (TD3) algo-\nrithm based on the widely used CleanRL library (Huang\net al. 2022). TD3 is the successor of the Deep Deterministic\nPolicy Gradient (DDPG) algorithm (Lillicrap et al. 2015),\nwhich combines the ideas from Deep Q-Network (DQN)\n(Mnih et al. 2015) and Deterministic Policy Gradient (DPG)\n(Silver et al. 2014).\nSimulation Tasks\nWe used the MuJoCo environments V4 from the Gymna-\nsium library, including HalfCheetah, Hopper, Humanoid,\nSwimmer, and Walker2d. These scenarios test RL algo-\nrithms in continuous control tasks, aiming to maximize the\nagent’s traversal distance, with each episode limited to 1000\nsteps.\nWe ensured consistency of our experiments by using the\nsame hyperparameters as those used in the RL Baselines3\nZoo, as well as performance comparison. Each environment\nunderwent ten independent training runs for single-agent ex-\nperiments. For the PBT experiments, we trained five popu-\nlations. Consistent with the RL Baselines3 Zoo bwnchmark,\nwe trained the agents for one million steps. After the training\nphase, we performed 150,000 evaluation steps to measure\nthe performance.\nSecond-Order Optimizer\nThe Diag. GGN optimizer was implemented using the Back-\nPACK library (Dangel et al. 2019). For the K-FAC optimizer\nwe used the PyTorch implementation by Gao (2024).Due to the absence of established hyperparameters for\nsecond-order optimizers in the context of TD3, we con-\nducted a thorough grid search. This process focused on\nlearning rate and damping parameters. For each combina-\ntion, we trained ten models over 500,000 steps and evalu-\nated them after training for 150,000 steps to obtain the final\nperformance. This systematic approach allowed us to iden-\ntify effective hyperparameter configurations for the Diag.\nGGN and K-FAC optimizers across various MuJoCo envi-\nronments. Figure 1 presents the grid search for the Diag.\nGGN optimizer.\n0.1\n0.5\n1.0\n1.5\n2.0\n2.5\nDamping0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85Learning RateAnt-v4\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nDamping0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9Learning RateHalfCheetah-v40.1\n0.5\n1.0\n1.5\n2.0\n2.5\nDamping0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85Learning RateHopper-v40.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nDamping0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85Learning RateSwimmer-v4\n0.1\n0.5\n1.0\n1.5\n2.0\n2.5\nDamping0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85Learning RateWalker2d-v4\n123×103\n5.05.56.06.57.0×103\n1.01.52.02.5×103\n020406080\n0.81.01.21.41.6×103\nFigure 1: Grid search for Diag. GGN across multiple envi-\nronments, displaying mean rewards for various hyperparam-\neter settings.\nSimilarly, Figure 2 shows the grid search for the K-FAC\noptimizer. During our experiments, we encountered consis-\ntent failures with the K-FAC optimizer when the damping\nparameter was set below 1.0. This issue arose because the\nCholesky factorization could not be computed, as the inputs\nwere not positive-definite.\nPopulation Based Training\nOur implementation and hyperparameters selection are\nbased on the PBT approach by Jaderberg et al. (2017), where\nthe bottom 20% of agents copy hyperparameters from the\ntop 20%, with random perturbations by factors of 0.8 or 1.2.\nWhen replacing an agent with another using the same op-\ntimizer, we transfer network weights, replay buffer, and hy-\nperparameters, then perturb the hyperparameters (including\nbatch size, learning rates, and, for the second-order opti-\nmizer, additionally, the damping parameter). For agents with\ndifferent optimizers, only network weights and the replay\nbuffer is transferred, while the receiving agent’s hyperpa-\nrameters are perturbed.\n1.0\n1.5\n2.0\nDamping0.45\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75Learning RateAnt-v4\n1.0\n1.5\n2.0\nDamping0.45\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75Learning RateHalfCheetah-v4\n1.0\n1.5\n2.0\nDamping0.45\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75Learning RateHopper-v41.0\n1.5\n2.0\nDamping0.45\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75Learning RateSwimmer-v4\n1.0\n1.5\n2.0\nDamping0.45\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75Learning RateWalker2d-v4\n23×103\n7.58.08.5×103\n1.01.52.02.5×103\n1.52.02.5×102\n3.54.04.5×103Figure 2: Grid search for K-FAC across multiple environ-\nments, displaying mean rewards for various hyperparameter\nsettings.\nPopulation Size\nIn this study, we built on findings from previous research\nhighlighting the importance of population size in PBT. No-\ntably, Bai et al. (2024) demonstrated that significant perfor-\nmance gains could be achieved with as few as four agents,\nwith diminishing returns beyond eight to sixteen agents.\nSimilarly, Shahid et al. (2024) found that a population size\nof eight optimally balances exploration and computational\neffort.\nBased on these insights, we conducted experiments with\npopulations of four, eight, and sixteen agents, using a per-\nturbation interval of 10,000 steps. Results over one mil-\nlion training steps, evaluated across 150,000 steps with five\ndifferent random seeds, showed that increasing population\nsize generally improved rewards, especially from four to\neight agents. Especially in the Ant and HalfCheetah envi-\nronments (See 1). During our study, we observed that agents\nin the Swimmer environment tend to exhibit a binary out-\ncome: they either fail completely, receive minimal rewards,\nor succeed in achieving a consistent reward of approxi-\nmately 360. This dichotomy in performance is the reason\nfor the high standard deviation observed in the Swimmer re-\nsults throughout all our results. Despite improved rewards\nwith larger populations, the benefits of expanding to sixteen\nagents were marginal, leading us to select a population size\nof eight.\nEnvironmentPopulation Size\n4 8 16\nAnt-v4 3669±1173 4703 ±939 5420 ±1070\nHalfCheetah-v4 9755±826 10655 ±948 11158 ±459\nHopper-v4 3521±107 3452 ±155 3461 ±191\nSwimmer-v4 236±153 299 ±159 363 ±1\nWalker2d-v4 4629±579 4833 ±548 5092 ±360\nTable 1: Influence of the population sizes on the perfor-\nmance.Results\nIn the initial experiments, we independently compared each\noptimizer. Table 2 displays the rewards at the end of training.\nThe rewards during training are shown in Figure 3. Adam\nconsistently outperformed the second-order methods in all\nenvironments.\nEnvironment Adam Diag. GGN K-FAC\nAnt-v4 5145±719 3802 ±956 1548 ±2645\nHalfCheetah-v4 9495±1204 7119 ±1575 8696 ±1081\nHopper-v4 3447±141 2737 ±609 2302 ±1441\nSwimmer-v4 213±155 71 ±114 204 ±129\nWalker2d-v4 4499±382 2043 ±1174 3582 ±1630\nTable 2: Performance comparison of different optimizer\nacross various environments\n0.0 0.5 1.0\nSteps×10612345Reward×103Ant-v4\n0.0 0.5 1.0\nSteps×1060.00.20.40.60.81.0×104HalfCheetah-v4\n0.0 0.5 1.0\nSteps×1060123×103Hopper-v4\n0.0 0.5 1.0\nSteps×1060123Reward×102Swimmer-v4\n0.0 0.5 1.0\nSteps×106012345×103Walker2d-v4\nAdam\nDiag. GGN\nK-FAC\nFigure 3: Mean reward during training for single agents us-\ning different optimizer.\nFirst and Second-Order Optimizers in one\nPopulation\nTable 3 shows the reward at the end of training when using\nAdam and Diag. GGN optimizers in one population. Fig-\nure 4 shows the results that were achieved during training.\nIn general, adding Diag. GGN optimizer to the population\nimproved the performance in more than half of the environ-\nments. Specially we observed that in two Diag. GGN set-\ntings, the Swimmer environment no longer fails, and there-\nfore, we can see significant improvement compared to PBT\ntraining using only Adam optimizers (See Table 5).\nTable 4 presents the reward at the end of training for us-\ning Adam and K-FAC optimizer in one population. Figure 5\nillustrates the results obtained during the training process. In\nall cases, using K-FAC improved the performance. We also\nobserved that the reward increases faster in all environments\n0.0 0.5 1.0\nSteps×106123456Reward×103Ant-v4\n0.0 0.5 1.0\nSteps×1060.00.20.40.60.81.01.2×104HalfCheetah-v4\n0.0 0.5 1.0\nSteps×1060123×103Hopper-v4\n0.0 0.5 1.0\nSteps×10601234Reward×102Swimmer-v4\n0.0 0.5 1.0\nSteps×106012345×103Walker2d-v4\n8 Adam – 0 GGN\n6 Adam – 2 GGN\n4 Adam – 4 GGN\n2 Adam – 6 GGN\n0 Adam – 8 GGNFigure 4: Training of Adam and Diag. GGN in one popula-\ntion.\n0.0 0.5 1.0\nSteps×106123456Reward×103Ant-v4\n0.0 0.5 1.0\nSteps×1060.00.20.40.60.81.01.2×104HalfCheetah-v4\n0.0 0.5 1.0\nSteps×1060123×103Hopper-v4\n0.0 0.5 1.0\nSteps×10601234Reward×102Swimmer-v4\n0.0 0.5 1.0\nSteps×1060123456×103Walker2d-v4\n8 Adam – 0 K-FAC\n6 Adam – 2 K-FAC\n4 Adam – 4 K-FAC\n2 Adam – 6 K-FAC\n0 Adam – 8 K-FAC\nFigure 5: Training of Adam and K-FAC in one population.\nbesides Swimmer. Similar to adding the Diag. GGN opti-\nmizer setting, the K-FAC optimizer also helps the agents in\nthe Swimmer environment to no longer fail (See 6). Based\non our experiments, the PBT of the Ant environment seemed\nchallenging (especially with a small population) By adding\nK-FAC, we were able to achieve the same rewards of single-\nagent training. Additionally, earlier, we observed that the in-\ncrease in population size did not improve the performance in\nthe Hopper environment. Adding K-FAC to the population\nfor the first time increased the performance in this environ-\nment significantly.EnvironmentNum. Adam Agents – Num. Diag. GGN Agents\n8 – 0 6 – 2 4 – 4 2 – 6 0 – 8\nAnt-v4 4703±939 4304±1029 3892 ±855 3670 ±555 3513 ±652\nHalfCheetah-v4 10655 ±948 10881±563 9472±1016 9597 ±1518 9197 ±605\nHopper-v4 3452±155 3328 ±180 3478 ±112 3493±128 3261±76\nSwimmer-v4 229±159 361±1 295±130 234 ±153 154 ±153\nWalker2d-v4 4833±548 4109±182 4381 ±535 4385 ±450 2882 ±1240\nTable 3: Summary of reward statistics: Mean and standard deviation for mixed poulations of Adam and Diag. GGN agents\nEnvironmentNum. Adam Agents – Num. K-FAC Agents\n8 – 0 6 – 2 4 – 4 2 – 6 0 – 8\nAnt-v4 4703±939 4327 ±674 4650 ±886 5129±630 4364±791\nHalfCheetah-v4 10655 ±948 11106±784 10598 ±582 11014 ±534 9071 ±561\nHopper-v4 3452±155 3724±38 3695±62 3569 ±80 3617 ±61\nSwimmer-v4 229±159 361±2 361 ±1 359±2 350 ±13\nWalker2d-v4 4833±548 5192 ±529 4588 ±680 5245 ±374 5265±333\nTable 4: Summary of reward statistics: Mean and standard deviation for mixed poulations of Adam and K-FAC agents\nEnvironmentImprovement in %∆ over\nSingle Agent PBT Adam\nAnt-v4 −16 −9\nHalfCheetah-v4 +15 +2\nHopper-v4 +1 +1\nSwimmer-v4 +69 +58\nWalker2d-v4 −3 −9\nTable 5: Improvement in percent of mixed population with\nAdam and Diag. GGN agents over single agents and PBT\nusing only Adam.\nEnvironmentImprovement in %∆ over\nSingle Agent PBT Adam\nAnt-v4 ±0 +10\nHalfCheetah-v4 +16 +8\nHopper-v4 +8 +8\nSwimmer-v4 +69 +58\nWalker2d-v4 +17 +9\nTable 6: Improvement in percent of mixed population with\nAdam and K-FAC agents over single agents and PBT using\nonly Adam.\nBalancing Wall-Clock Runtime Across\nOptimizers\nWe conducted a measurement of the wall-clock runtime for\nthree optimizers (Adam, diag. GGN, and K-FAC) on our\ncompute cluster, using an Intel XEON CPU E5-2650 v4 and\nNvidia GeForce GTX 1080 Ti. Table 9 shows the wall-clock\nruntime in secons for one PBT interval, consisting of 10,000gradient steps, with the parallel execution of four workers.\nEnvironment Adam [s] Diag. GGN [s] K-FAC [s]\nAnt-v4 102±11 182 ±5 365 ±43\nHalfCheetah-v4 96±12 180 ±4 382 ±45\nHopper-v4 105±8 180 ±3 415 ±42\nSwimmer-v4 91±8 177 ±5 377 ±53\nWalker2d-v4 97±8 185 ±6 363 ±63\nTable 9: Adam, Diag. GGN and K-FAC wall-clock runtime\nin seconds for 10,000 gradient steps.\nWe further evaluate the effectiveness of our mixed pop-\nulation approach by normalizing runtime across first and\nsecond-order optimizers through adjusted step counts (i.e.,\noptimizer gradient steps). Table 7 shows performance differ-\nences in populations with Adam and Diag. GNN. In this sce-\nnario, Adam executed 10,000 gradient steps, whereas Diag.\nGNN completed 5,000 steps. Similarly, Table 8 compares\nthe populations using Adam and K-FAC, where Adam again\nperformed 10,000 gradient steps, but K-FAC executed only\n3,000 steps. As anticipated, the overall performance of our\napproach experienced a decline. However, certain popula-\ntion settings demonstrated marginally better results in the\nHopper task when utilizing Diag. GNN and Adam. More-\nover, the Swimmer task was reliably learned. When employ-\ning Adam and K-FAC, the performance remained 5%higher\nin the Hopper and 3%higher in the Walker2d environment.\nAdditionally, the Swimmer task continued to be learned re-\nliably again.\nDiscussion\nIn this study, with the lessons learned from the second order\noptimization research (Kamanchi et al. 2021; Tatzel et al.EnvironmentNum. Adam Agents – Num. Diag. GGN Agents\n8 – 0 6 – 2 4 – 4 2 – 6 0 – 8\nAnt-v4 4703±939 3236±691 3309 ±373 3263 ±304 2688 ±427\nHalfCheetah-v4 10655±948 10402 ±1884 10451 ±875 8947 ±1050 6329 ±951\nHopper-v4 3452±155 3358 ±156 3459±152 3359±138 2511 ±859\nSwimmer-v4 229±159 362±2 227±158 170 ±150 141 ±125\nWalker2d-v4 4833±548 4525±515 3964 ±473 4426 ±603 1956 ±838\nTable 7: Summary of reward statistics: Mean and standard deviation for mixed populations of Adam and Diag. GGN agents.\nStep count adjusted.\nEnvironmentNum. Adam Agents – Num. K-FAC Agents\n8 – 0 6 – 2 4 – 4 2 – 6 0 – 8\nAnt-v4 4703±939 3739±997 3375 ±235 3592 ±173 3335 ±198\nHalfCheetah-v4 10655±948 10439 ±1250 10243 ±594 10035 ±675 3921 ±1857\nHopper-v4 3452±155 3640 ±64 3617±18 3611±88 3523 ±122\nSwimmer-v4 229±159 360±2 358±4 359 ±3 171 ±148\nWalker2d-v4 4833±548 4870 ±552 4163 ±430 5000±331 3155±1192\nTable 8: Summary of reward statistics: Mean and standard deviation for mixed populations of Adam and K-FAC agents. Step\ncount adjusted.\n2022; Salehkaleybar et al. 2022; Gebotys et al. 2022) for\nthe first time, to our knowledge, here we propose to use\nboth first and second order optimizer simultaneously for ef-\nficient training of PBT-based RL. While second-order meth-\nods are less popular in deep learning settings due to com-\nputation cost as well as implementation complexity, inves-\ntigating second-order optimization techniques in PBT-based\nRL is interesting because of their potential to enhance the\nefficiency and effectiveness of learning in complex environ-\nments.\nIn this paper, we first show that a well-tuned Adam opti-\nmizer consistently outperformed the second-order methods\nin all environments when trained as a single agent. Second,\noverall training second-order methods with Adam in one\npopulation not only improve the performance but also help\nagent to avoid failing in environments like Swimmer. We\nprovide empirical evidence that using Adam and K-FAC op-\ntimizer in one population demonstrates significant improve-\nment.\nWe further demonstrate that even under the same runtime\nconstraints, the use of both first-order and second-order op-\ntimizer simultaneously often results in better performance\nthan using only the Adam optimizer in the population. This\nsuggests that the diversity of optimization strategies within\na population can lead to more robust learning outcomes. For\nfurther work we propose including additional second-order\nor different first-order methods into one population. Expand-\ning this diversity to include different RL algorithms could\nalso enhance performance and stability.",
      "metadata": {
        "filename": "Simultaneous Training of First- and Second-Order Optimizers in Population-Based.pdf",
        "hotspot_name": "PBT_Injection_Molding",
        "title": "Simultaneous Training of First- and Second-Order Optimizers in\n  Population-Based Reinforcement Learning",
        "published_date": "2024-08-27T21:54:26Z",
        "pdf_link": "http://arxiv.org/pdf/2408.15421v2",
        "query": "injection molding PBT housing energy efficiency optimization cycle time reduction"
      }
    },
    "Surrogate Modelling for Injection Molding Processes using Machine Learning": {
      "full_text": "Surrogate Modelling for Injection Molding Processes using Machine\nLearning\nArsenii Uglova,, Sergei Nikolaeva,, Sergei Belova, Daniil Padalitsaa, Tatiana Greenkinaa,\nMarco San Biagio, Fabio Cacciatorib\naSkolkovo Institute of Science and Technology, Bolshoy Boulevard 30, bld. 1, Moscow, 121205, Russia\nbIllogic s.r.l, Corso stati Uniti 57,10128, Turin, Italy\nAbstract\nInjection molding is one of the most popular manufacturing methods for the modeling of\ncomplex plastic objects. Faster numerical simulation of the technological process would allow\nfor faster and cheaper design cycles of new products. In this work, we propose a baseline\nfor a data processing pipeline that includes the extraction of data from Mold\row simulation\nprojects and the prediction of the \fll time and de\rection distributions over 3-dimensional\nsurfaces using machine learning models. We propose algorithms for engineering of features,\nincluding information of injector gates parameters that will mostly a\u000bect the time for plastic\nto reach the particular point of the form for \fll time prediction, and geometrical features for\nde\rection prediction. We propose and evaluate baseline machine learning models for \fll time\nand de\rection distribution prediction and provide baseline values of MSE and RMSE metrics.\nFinally, we measure the execution time of our solution and show that it signi\fcantly exceeds\nthe time of simulation with Mold\row software: approximately 17 times and 14 times faster\nfor mean and median total times respectively, comparing the times of all analysis stages for\nde\rection prediction. Our solution has been implemented in a prototype web application\nthat was approved by the management board of Fiat Chrysler Automobiles and Illogic SRL.\nAs one of the promising applications of this surrogate modelling approach, we envision the\nuse of trained models as a fast objective function in the task of optimization of technological\nparameters of the injection molding process (meaning optimal placement of gates), which\ncould signi\fcantly aid engineers in this task, or even automate it.\nKeywords: injection molding, surrogate modeling, machine learning, baseline, deep\nlearning, Autodesk Mold\row, 3d machine learning, 3d data, mesh, point cloud, \ruid\ndynamics simulation\n1. Introduction\nInjection molding is a widespread manufacturing process for producing objects of given\nshapes from various materials. It is the most common modern method of manufacturing\nof plastic parts, and it is ideal for producing high volumes of copies of the same object.\nEmail addresses: urrusmsng@gmail.com (Arsenii Uglov), s.nikolaev@skoltech.ru (Sergei Nikolaev),\nmarco.sanbiagio@gmail.com (Marco San Biagio), cacciatori@illogic.xyz (Fabio Cacciatori)\n1arXiv:2107.14574v1  [cs.LG]  30 Jul 2021Numerical analysis is crucial for designing products with given mechanical properties and a\nminimal number of defects. The industry standard is to use extensive \ruid-dynamics-based\nsimulations with specialized software such as Autodesk Mold\row. This kind of simulation\nis computationally quite burdensome and time-consuming. As well, the \fnal result is highly\ndependent on the experience and skills of the engineers, introducing variance in quality. Some\nmanufacturers are interested in shorter and cheaper design cycles with less human interaction.\nOne way to accelerate a design cycle and expand searchable design space is to implement\nsurrogate-based optimization [1]. Data-driven models such as Gaussian processes, support\nvector machines (SVM) and arti\fcial neural networks (ANN) are common choices for surro-\ngate models.\nIn recent years, we have observed a steady rise in interest in the application of deep\nlearning (DL) models to model physical or engineering processes. For example, generative\nadversarial networks (GANs) are used in high energy-physics for fast approximate simulation\nof detector responses [2, 3], and supervised DL architectures are used in geology for real-time\nsimulation of seismic waves [4] and for semiconductor manufacturing [5].\nIn this work we consider a deep learning approach to creating a surrogate model, based\non Mold\row simulation data, which predicts the distribution of \fll time and de\rection values\nover the 3d surface as targets of the simulation.\nOur research into related works on this topic has, as of 2020, found no description of a\nsimilar approach for the described task, so we consider our results to be a baseline.\nThis work was created as a part of a project with an Italian automotive industry part-\nner Fiat Chrysler Automobiles. They provided us with a dataset of Mold\row project \fles\nof simulations. Since this work is covered by the non-disclosure agreement, including the\ndataset, we cannot disclose the partner's name or provide many vivid data representations,\nas the format of the data suggests. However, we will provide summary details of a dataset, a\ndescription of the used features, details of our data processing pipeline and machine learning\n(ML) models, and the achieved values of the result metrics.\n2. Injection Molding Process\n2.1. Molding Process\nAn injection molding machine (Figure 1) consists primarily of two units: a clamping unit\nand an injection unit. The clamping unit opens and closes the die and maintains the holding\npressure. The injection unit melts plastic pellets and creates melted plastic \row under either\ncontrolled speed or pressure (depending on the stage of the process). It consists of a hopper,\nheaters and a screw which propels the plastic \row. Speed and pressure are controlled by the\nrotation speed of the screw.\nThe mold is usually a steel cube split into two halves with cavities in the shape of the\nproduct and auxiliary cavities for plastic canals and a cooling system.\nA typical process of injection molding consists of the following steps:\n1. Clamping: closing the halves of the mold\n2. Injection: introduction of melted polymer \row into the cavity\n3. Dwelling: equalizing of polymer pressure through the whole cavity\n4. Cooling\n2Figure 1: Injection molding machine, picture takes from [6]\n5. Mold opening\n6. Product ejection\nThe product's shape is produced by an industrial designer in the form of a CAD \fle.\nThe engineer considers the given shape and proposes technical parameters for the molding\nmachine and corrections to the shape if needed. To do so, the engineer iteratively selects\nparameters based on his or her domain knowledge and tests it with numerical analysis of\nthe whole molding process in specialized software such as Epicor, Moldex3D or Autodesk\nMold\row.\nIn this work we consider complex thin-walled plastic parts such as car dashboards and\nbumpers.\n2.2. Numerical Simulation\nSimulation allows us to predict such things as \fll time, cooling time, and number and\ntype of defects based on technical parameters such as material, mold temperature, cooling\nsystem, \row speed, pressure pro\fles, and geometry itself.\nFor thin-walled shapes Mold\row o\u000bers the so-called dual domain method of modeling.\nThe analysis takes place on the surface of the mold cavity expressed as triangular mesh.\nSpecial connector elements are used to synchronize the results on the opposing faces.\nA typical simulation sequence is Fill + Pack + Warp + Cool analysis. Fill analysis models\npolymer \row during the injection stage. Pack analysis models the same during the dwelling\nstage. Cool analysis models heat exchange between elements of the mesh, the mold and the\ncooling system. Warp analysis predicts the types and locations of defects. One of the most\nimportant results of analysis is spatial distribution of de\rection, or displacement from the\nset shape under the load, in the \fnal product. During simulations MoldFlow solves a system\nof di\u000berential equations that describes the \ruid dynamics and heat exchange on the surface\nof the mesh using the \fnite elements method.\nAs input parameters for a simulation scenario we can, among others, emphasize these\ntechnological parameters: mold geometry, type of plastic, position of input gates and cooling\nchannels, temperature of plastic and mold, opening time of each gate, \row pressure, and\n3clamping pressure. The results of simulation are spatial distribution of \fll time for Fill\nanalysis, cooling time for Cool analysis, and weld lines and de\rection for Warp analysis.\nFigure 2 shows an example of output of the \fll time simulation for a generic dashboard\nmodel.\nFigure 2: Mold\row prediction of \fll time for generic car dashboard. Image is taken from an open information\nsource [7]\n3. Related Works\nHere we review the most common approaches that apply machine learning models to three-\ndimensional objects. Most models can be distinctively categorized by used representation of\n3D objects.\n3.1. Meshed Representation\nMeshes discretize an object's surface with a set of \rat polygons, usually triangles. It is\nthe most common representation in computer graphics and CADs.\nThe simplest approach to deal with the mesh would be to treat it as a graph and encode\ngeometrical properties into vertex and edge signals. The two main approaches to construct-\ning graph convolutions are based on spectral \fltering or local \fltering. Depending on the\ntype of specter approximation or type of local \flters, di\u000berent operations can be derived.\nNonetheless, most models allow general representation in terms of a message passing to a\nnode from its neighborhood.\nGraph networks are usually isotropic in terms of mutual orientation of nodes and are\ninvariant under permutations of the neighborhood, but mutual orientation of nodes in a mesh\n4is crucial to de\fning that mesh. Convolutional operators should thus be anisotropic in terms\nof orientation between the target and source nodes. However, any de\fnition of orientation\nvia angle would arbitrarily depend on the choice of reference angle. We thus either need to\n\fx canonical orientation in the node neighborhood for all meshes, as it is done in the case of\nimage convolutions on a regular grid. In [8], the authors consider similarly meshed surfaces\nand introduce \fxed serialization of the neighborhood by spiral trajectory from the target; the\nauthors of [9] use an attention mechanism for soft assignment of neighbors to kernel weight\nmaps.\nAs stated by Bernstein et al. in [10], the success of convolution on regular data is based\non implicitly exploited global symmetries of Euclidean spaces. 2-d convolutions, for example,\nare naturally shift equivariant. A general manifold, and meshes in particular, lacks global\nsymmetries that can be exploited, but one can de\fne some locally equivariant operations.\nCohen et al. in [11] and [12] introduce a speci\fc type of features on node tangent space and\nderive gauge equivariant convolution.\n3.2. Point Cloud Representation\nIn this approach, a 3D object expressed as a set of points in a 3D space sampled from the\nvolume of the object or from the nodes of 3d mesh polygons. It is usually produced by a 3D\nsensing method such LiDAR scanners or stereo reconstruction. One of the \frst successful DL\nmodels for point clouds was PointNet from [13], where the same (weights sharing) network is\napplied to features of each point, and then aggregated by a permutation invariant operator to\nproduce embedding of a whole cloud. Such an approach cannot grasp the local geometry of\nan object's surface. In [14] Qi et al. proposed clustering a point cloud and applying PointNet\nto each cluster, resulting in new point cloud, which is processed in same fashion until the\nwhole point cloud does not merge.\nIn [15], the authors dynamically built a k-Nearest Neighbors graph in the feature space of\neach layer and then applied a graph convolutional network to obtain new features, repeating\nthis process until the desired depth was reached.\nAlthough point cloud models are simpler and can be implemented more easily than meshed\nmodels, one of their main drawbacks is that they cannot di\u000berentiate between points that\nare close in 3D space and far in terms of geodesic distance (distance on the surface of the\nshape).\n3.3. Rasterized representation\nThe most common regular approach is to build voxel representation. The object and\nbounding space around it are divided into a three-dimensional grid of voxels. Such repre-\nsentation allows for trivial extension of 2D convolutions [16, 17]. However, complexity and\nmemory consumption for such an approach grows cubically with resolution. E\u000bective data\nstructure allows for better time and memory complexity. In [18], Wang et al. split bounding\nspace recursively in octets of smaller cubes, and use an oct-tree as an e\u000bective storage. Then\nconvolutions act on neighboring cubes on the same level of the tree, passing aggregated values\nto the coarser level.\nAnother approach would be in using 2d projections of 3d structures and then applying\ntraditional 2d convolutions. There can be single or multiple views projections of a 3d object\n5with additional 2d features, like the distances to the projection plane, and with some mech-\nanism for aligning predictions from di\u000berent projections [19]. Such an approach then allows\nuse of state-of-the-art 2d convolutional architectures for processing of obtained regular 2d\nrepresentations. And the models used in practice are relatively simple for implementation\nand training, and they are cost-e\u000bective in inference.\nIt is not quite obvious which of these representations is better suited for our task. In this\npaper we present the results for 2d projection representation as a baseline.\n4. Our Approach\n4.1. Data extraction\nThe data for further analysis were extracted from Mold\row simulations projects. For this\npurpose we export simulation data from Mold\row and then, using developed data loader\nscripts, we extract valuable data from an exported set of \fles for a particular simulation,\npreprocess it, and pack the aggregated data for all simulations in the Pandas dataframe. For\neach simulation we use the following \fles to extract data from:\n1. a .pat \fle that contains mesh data. We collect nodes ids, their coordinates and infor-\nmation for polygons as nodes ids triplets.\n2. a .txt \fle that contains a Mold\row log \fle. From this \fle we extract values of technolog-\nical parameters (such as Melt temperature, Cooling time, Duration, Filling pressure,\nAmbient temperature, Mold temperature, Time at the end of \flling), and injection\ngates information (node ids and Opening time).\n3. an .xml \fle that contains Mold\row simulation results.\nAlso during the loading process we calculate the shortest paths from nodes to gates\nconsidering polygonal mesh as a graph of nodes and edges weighted by the length of the\nedges. The shortest paths between all nodes and all gates then are calculated according to\nDijkstra's algorithm.\n4.2. Data processing pipeline\nThe general scheme of our pipeline for the analysis of extracted data is shown on Fig.\n3. This diagram shows the general pipeline idea without any actual result distribution or\na 3d geometry from our dataset, since we can't show these details due to non-disclosure\nagreement.\nThe pipeline has the following sequential steps:\n1. Pre-process point cloud data\n2. Extract features for gates\n3. Train the Gradient boosting model to predict \fll time\n4. Generate 2d feature maps from the 3d model with values\n5. Train a CNN model to predict de\rection distribution in 2d space\n6. Reproject 2d de\rection distribution back over the 3d mesh\n6Figure 3: The general scheme of data processing pipeline\n4.3. Point cloud pre-processing\nAt \frst the subset of points of a given size is randomly selected from the model. We take\nof about 1=8 of the total points count.\nFor each of the selected points we then take k nearest points for the area that will be\nused for smoothing the prediction results. In our experiments we take k=100.\n4.4. Extraction of features of gates\nFor each point we construct an N-dimensional vector of features of gates that will be used\nto predict \fll time value. Since \fll time value is dependent on the distance a plastic should\ntravel to reach a particular place, in our experiments for each point we construct features\nfrom the three nearest gates, which will mostly a\u000bect the time of the plastic to reach this\npoint. That gives us an 8-dimensional feature vector.\nAs a \frst set of values, we take distances to the three nearest gates, calculated as described\nin section 4.1.\nAs a second triplet of features for each of the three selected gates we use its opening time\nvalue.\nFinally, we calculate the three-dimensional vectors of orientation of each selected gate\nrelatively to each point and then take the values of cos(\u000b1),cos(\u000b2), as the two last features,\nwhere\u000b1and\u000b2are the angles between the orientation vectors of the \frst and second and\nthe \frst and third gates, respectively.\n4.5. Fill time prediction with the Gradient boosting model\nUsing derived feature vectors for points, we trained the Gradient boosting model to predict\nthe \fll time value. The ground truth distribution of values for this parameter is taken from\nthe data, exported from Mold\row simulations.\nThe prediction is performed for a subset of previously randomly selected points. Then for\neach selected point the predicted value is copied over a selected neighboring area. Values for\n7overlapping parts of areas are averaged. This allows us to create smooth distribution that\nre\rects the continuous nature of the \fll time value and reduces computational time.\n4.6. Fill time projection to 2d space\nFor de\rection prediction, we project a 3d point-cloud with the predicted \fll time distri-\nbution to the 2d plane. The projection plane is selected so that it minimizes the squared\nEuclidean 2-norm for the original and projected points. The projected image is 384x768\npixels in size. As a feature map for the second channel, we also obtain a 2d map of a mask\nthat represents the silhouette of the projection of the mesh, to help the model to distinguish\nthe projection from the background. This approach allows us to take into account the geo-\nmetrical features of the mesh alongside derivative features of the injector's gates in the form\nof predicted \fll time.\n4.7. De\rection prediction with the CNN model\nFor the prediction of the distribution of de\rection values over the 2d image we use a con-\nvolution neural network model based on the U-Net model [20]. The details of the architecture\nof the model are described in the appendix.\nAs a result of inference with our model, we obtain a 2d map of predicted distribution\n12x24 in size. This 2d representation is then up-scaled, with interpolation, to represent the\n2d image, 384x768 in size, of predicted de\rection distribution over the initial 2d projection.\nPrediction of 2d de\rection map in a lower resolution and then up-scaling it allows us to\nobtain a smooth result distribution that re\rects the continuous nature of the de\rection.\n4.8. In\rate 2d prediction data back to 3d mesh\nThe predicted 2d de\rection map then is re-projected back onto 3d space to produce\ndistribution over the initial point cloud. For each of 3d points, we pick up the corresponding\nvalue from the 2d plane using information about 3d-to-2d points correspondence saved on\nthe projection step.\n5. Experiments and Results\n5.1. Dataset\nWe use the dataset obtained from our aforementioned partner, which consists, after ex-\ntraction (described in section 4.1), of 158 Mold\row injection molding simulations for car\ndashboards and bumpers. Each simulation consists of 3d mesh along with Mold\row simu-\nlation parameters and results. All simulations are performed using same type of polymer,\nwithout a cooling system. Statistics of mesh parameters from the dataset are shown in Table\n1.\nTable 1: Statistics of mesh characteristics\nParameter Min Median Max\nVertices 35311 77846 110126\nEdges 212172 462420 662472\nFaces 70724 156140 220824\n8Units of \fll time values are measured in seconds (s). De\rection values are measured in\nmillimeters (mm).\n5.2. Metrics\nTo assess performance of our predictors we calculate RMSE and MAE metrics taking \fnal\nvalues of predicted distribution over a 3d point cloud after all steps, described in chapter 4.5\nand 4.8. As a \fnal result for each fold of cross-validation we present mean values of these\nmetrics averaged over test samples and values of metrics calculated on all points from all of\nthe test samples of the fold, merged into a single array.\n5.3. Cross-validation\nWe trained and tested our predictors of \fll time and de\rection using cross-validation with\n\fve folds. Parameters of cross-validation are shown in Table 2.\nTable 2: Cross-validation parameters\nParameter Value\nTotal simulation samples 158\nCross validation folds 5\nTrain samples per fold 127\nTest samples per fold 31\nBoth of \fll time and de\rection predictors are trained and tested on the same split of the\ndataset for each of the cross-validation folds.\n5.4. Fill time prediction\nAt the \frst stage, we predict the distribution of \fll time with the Gradient Boosting\nmethod using XGBoost library. The hyperparameters used for XGBoost regressor training\nare shown in Table 3.\nTable 3: XGBoost regressor hyperparameters\nParameter Value\nLearning rate 0.08\nMaximum depth 8\nNumber of estimators 200\nImportance type gain\nTable 4 shows the results of \fll time prediction per each fold and mean values of metrics\nover all folds.\nTo train our CNN model we use samples with both predicted and ground truth \fll time\ndistributions with the same target 2d maps of de\rection distribution. These maps are ob-\ntained from the ground truth de\rection distribution with the same procedure of 2d projection,\nafter which they are resized to the target dimensions of 12x24.\nAlso we augment the dataset by mirroring maps of each sample over the xandyaxes.\n9Table 4: Fill time prediction results\nFoldRMSE\nsamplesMAE\nsamplesRMSE\npointsMAE\npoints\n#1 0.7600 0.5831 0.8490 0.5234\n#2 0.5397 0.3840 0.6035 0.3781\n#3 0.6045 0.4571 0.6561 0.3897\n#4 0.5243 0.3762 0.5811 0.3597\n#5 0.6291 0.4698 0.7699 0.4543\nSummary\nMean 0.6115 0.4540 0.6919 0.4210\nTo test the de\rection predictor we use the predicted \fll time for the test sample and take\naverage of four predicted 2d de\rection maps for one original and three mirrored 2d \fll time\nmaps. After that we perform the reprojection of the results of 2d de\rection prediction back\nto the 3d point cloud, as described above, and calculate the metrics for this resulting 3d\ndistribution. Table 5 shows the results of de\rection prediction.\nTable 5: De\rection prediction results\nFoldRMSE\nsamplesMAE\nsamplesRMSE\npointsMAE\npoints\n#1 2.3837 2.0582 2.6504 1.9954\n#2 1.6406 1.2530 1.9034 1.2312\n#3 1.1052 0.8111 1.2956 0.7779\n#4 1.2364 0.8587 1.3846 0.8651\n#5 3.1082 2.7538 3.2924 2.6459\nSummary\nMean 1.8948 1.5469 2.1053 1.5031\n5.5. Simulation time comparison\nWe obtained the execution times for each phase of a simulation using our solution. The\ncon\fguration of the test system is presented in Table 6.\nTable 6: Test bench setup\nCPU Intel Xeon E5-2630 v4 40 cores 2.20GHz\nRAM 125.8 GB\nOS Ubuntu 16.04 LTS\nGPU GeForce RTX 2080 Ti\nWe measured time for three main parts of the calculations: initial data pre-processing,\n\fll time prediction and de\rection prediction. The summary results are shown in Table 7\nFor comparison with Mold\row performance we use execution time from log \fles from\nour dataset of simulations. Since these \fles provide only overall time for the Warp Analysis\n10Table 7: Summary of simulation time with our solution, in seconds\nStage Min Mean Max Std Med\nPre-processing 13.611 95.961 224.120 66.317 76.022\nFill time 1.390 3.529 5.716 1.18 3.534\nDe\rection 1.015 2.294 4.566 0.61 2.302\nTotal 16.016 101.784 232.341 67.873 81.948\nphase and for Fill Analysis combined with other types of analysis (for example, with both\nPack Analysis and Residual Stress Analysis), if they are provided, we cannot perform direct\ncomparison of times of isolated \fll time and de\rection prediction. Therefore, for the \fll\ntime phase only, we took only simulations that have execution time of Fill Analysis without\nconjunction with any other type of analysis. In our dataset Mold\row simulations were\nperformed on di\u000berent hardware, so to make the comparison more precise, we took only\nthose simulations that were performed on a hardware con\fguration as close to ours as we\ncould select (CPU of 40 and more cores).\nThere were seven such simulations in our dataset. The result is shown in Table 8.\nTable 8: Execution time for Mold\row simulations with Fill analysis only, in seconds\nMin Mean Max\n291 869.143 1103\nIt is evident that our solution is faster in passing all the stages that are needed to compute\n\fll time distribution, even if we compare the fastest Mold\row simulation, with an execution\ntime of 291 sec., with the slowest pre-processing and \fll time prediction stages with our\nsolution, which is 229 :135 sec.\nFor de\rection prediction time we took simulations that contain a Warp analysis stage\nand were performed on the most similar hardware. There were 99 simulations selected. Then\nfor each simulation we obtained the total time of execution of all stages of the simulation,\nincluding analysis stages preceding the Warp analysis (for example, the Fill analysis). Table\n9 shows the summary of the obtained time periods.\nTable 9: Total execution time summary for Warp analysis in selected Mold\row simulations, in seconds\nMin Mean Max Std 25% Med 75%\n127 1765.4 38116 4222.6 520 1161 1594.5\nWe compare these results with the total time of all stages to obtain de\rection prediction\nwith our solution. There are only nine Mold\row simulations, with a maximum execution\ntime of 177 sec. and a minimum of 127 sec., that are faster than the maximum total time\nwith our solution. These Mold\row simulations are with di\u000berent technological parameters\nand the same geometry. And our solution gives execution time for these simulations in the\ninterval from 29 :075 to 33:05 sec., which is still multiple times faster.\nOn average our solution for de\rection prediction appeared to be 1663 :616 sec. and 17 :34\n(14:17 by median) times faster than Mold\row.\n11Conclusion\nIn this work we proposed the baseline pipeline for data processing of injection molding\nsimulation parameters to predict target distributions of de\rection and \fll time over a 3d\nmesh using a surrogate modelling approach. The pipeline includes the extraction of data\nfrom Mold\row simulation projects and the prediction of the target distributions.\nWe described how and where to get valuable data from \fles of injection molding simula-\ntion, extracted from Mold\row.\nThen we proposed the algorithm for engineering of features, that will be used to train\nmodel for prediction of \fll time distribution. Derived features include information about\nlocation and direction of injector's gates that will mostly a\u000bect the time of plastic to reach\nthe particular point of a 3d mesh, relative to each point of the point cloud of a mesh, alongside\ninformation about the opening time of gates.\nAs a model for \fll time prediction we proposed using the Gradient boosting technique, and\nwe showed its performance on our data using a \fve-fold cross-validation check and obtained\nthe baseline values of MSE and RMSE metrics for the \fll time distribution prediction.\nThen we described the algorithm for engineering of features for predicting the de\rection\ndistribution that will include information about the geometry as well as parameters of gates\nin form of predicted \fll time distribution.\nFor the de\rection distribution we proposed a 2d convolutional neural network model\nand steps for features and target distibution projection to 2d dimensional space and back.\nWe showed the performance of the proposed method using the same cross-validation split\nand obtained the baseline values of MSE and RMSE metrics for the de\rection distribution\nprediction.\nFinally we measured the execution time of our solution for the steps of targets prediction\nand compared it to the time of simulations with Mold\row software. The result shows that\nour solution signi\fcantly excels the Mold\row in execution time: for about 17 times faster\ncomparing mean and for about 14 times comparing median for total time of all stages of\nthe de\rection prediction. The slowest \fll time prediction with our solution is faster than the\nquickest with Mold\row. And our slowest de\rection prediction is faster than the vast majority\nof Mold\row simulations with Warp analysis from our dataset, while for each individual\nsimulation with our solution is still multiple times faster.\nTo demonstrate the potential of the described solution, we developed a web application\nprototype that allows a user to upload molding data of a 3d mesh, set up injector's gates\nparameters, perform simulation using our data preprocessing algorithms and trained ML\nmodels, and obtain vivid visualization of its result. This prototype was presented and ap-\nproved by the management board of the Fiat Chrysler Automobiles, the Italian automotive\nmanufacturer.\nIn the further work, we will aim to improve the quality of the model by the engineering\nof more features from the available data and testing the applicability of geometrical models\nfor our task. We will also train our models on more data as it becomes available.\nOne of the nearest ways of improvement of our current model, we see using a multiple\nview approach and including information of distance from 3d point to its projection as an\nadditional geometrical feature. Another direction of model enhancement is to use ANN\nmodels that work directly with Point Cloud and Mesh or Graph representations, such as\n12DGCNN [15].\nFinally, we see one of the possible but promising applications of such surrogate modelling\napproach in using trained models as a fast objective function in the task of optimization\nof technological parameters (i.e. optimal gates placement), which could signi\fcantly help\nengineers in this task, or even automate it.\nAcknowledgements\nWe thank Vadim Leshchev1for his great contributions to the preparation of results pre-\nsentation and technical implementation of ideas of this work, programming tools, and MVP\napplication. We thank Ilnur Nuriakhmetov1for his great help in technical support of this\nproject. We also thank Anna Nikolaeva1for her great help in the scienti\fc research and\nthe scienti\fc report preparation. We thank Roman Misiutin for his great help in scienti\fc\nresearch into graph models and this paper preparation.",
      "metadata": {
        "filename": "Surrogate Modelling for Injection Molding Processes using Machine Learning.pdf",
        "hotspot_name": "PBT_Injection_Molding",
        "title": "Surrogate Modelling for Injection Molding Processes using Machine\n  Learning",
        "published_date": "2021-07-30T12:13:52Z",
        "pdf_link": "http://arxiv.org/pdf/2107.14574v1",
        "query": "injection molding PBT housing energy efficiency optimization cycle time reduction"
      }
    },
    "Towards new methods for process adjustments based on parts quality measurements": {
      "full_text": " \n15e Colloque National AIP -Priméca  1/6 La Plagne (73) – 12 au 14 avril 2017  \n Vers un p ilotage de la qualité des pièces injecté es \n \n \nPierre  Nagorny  \nUniversité Savoie Mont Blanc  \nLaboratoire SYMME, 7 Chemin de Bellevue  \nAnnecy -le-Vieux  – France  \npierre.nagorny@univ -smb.fr  \n  Eric Pairel  \nUniversité Savoie Mont Blanc  \nLaboratoire SYMME, 7 C hemin de Bellevue  \nAnnecy -le-Vieux – France  \neric.pairel@univ -smb.fr  \n  \nMaurice Pillet  \nUniversité Savoie Mont Blanc  \nLaboratoire SYMME, 7 Chemin de Bellevue  \nAnnecy -le-Vieux – France  \nmaurice.pillet@univ -smb.fr  \n \nRésumé — Le procédé d’injection des thermoplastiq ues permet \nla production de pièces complexes en grande s série s. Les exigences \nqualités sont croissantes. Il est nécessaire de réguler le procédé \nd’injection afin de conserver un point de fonctionnement. Le \npilotage du procédé ne permet pas, aujourd’hui, de  proposer \nd’ajuster l’ensemble des paramètres du procédé pour optimiser la \nqualité du produit. Afin de proposer une méthode robuste de \npilotage auto -adaptative, nous pouvons nous appuyer sur les succès \nde la modélisation par rése au de neurones . L’objectif est de piloter \nla machine cycle à cycle à partir de caractéristiques mesurées sur la \npièce  produite. L e temps de cycle  est souvent inférieur à 30 \nsecondes pour ce procédé de fabrication  ; ce qui pose le défi du \nmesurage, du traitement et de l’ajustement de s paramètres dans ce \ncourt délai. Cette présentation nous permet d’établir une étude de la \nlittérat ure sur laquelle s’appuie ra notre démarche expérimentale.  \nMots -clés— injection des thermoplastiques, pilotage, réseau de \nneurones , plan d’expériences  \nI.  INTROD UCTION  \nLe procédé d’injection des thermoplastiques permet de \nproduire des pièces complexes en grande s séries. Il demande \nune qualification longue de l’outillage  et le savoir -faire du \ntechnicien afin de régler les paramètres du procédé. C es \nparamètres  influent de manière non -linéaire sur les \ncaractéristiques de la  pièce finale. Plusieurs facteurs non \ncontrôlables agissent comme perturbations et doivent être \ncompensés par l’ajustement des paramètres . Garantir la qualité \ndes pièces  en sortie machine est crucia le car elle détermine la \nsuite des opérations de fabrication et  la pièce finale.  Le procédé \nd’injection se compose de phases interdépendantes  : les \nparamètres d’une phase influent sur la phase suivante. Dans \nl’ensemble de la littérature, il n’a jamais été proposé d’ajuster \nl’ensemble des paramètres réglables du procédé afin de garantir \nl’ensemble des caractéristiques qualités attendues du produit. \nNous investiguons une méthode de pilotage du procédé cycle à \ncycle basée sur la mesure de la qualité des pièces  produites. \nCette approche globale p eut s’appuyer sur les succès de la \nmodélisation du procédé par réseaux de neurones, sur la \nfaisabilité du mesurage  en cycle et sur l’analyse statistique afin d’optimiser la quantité à ajuster. Une approche globale \npermet tra de trouver un point de fonctionnement idéal du \nprocédé, qu’il conviendra de conserver par régulation.  \nA. Interdépendances des phases du procédé  \nNous proposons une représentation (Figure 1 ) Zig Zag  Process \ndans les deux derniers piliers définis par l’appro che Axiomatic \nDesign  [1] qui fait apparaître  les enjeux transverses du pilotage \ndu procédé  : les paramètres du procédé {Pi} et les \ncaractéristiques d es produit s lors de chacune des phases { Ci}. \nLes caractéristiques du produit fini { C} sont fonction de \nl’ensemble des phases. De plus, chaque phase est fonction des \nphases précédentes et des variables du procédé. Si un réglage \nest modifié au niveau de l’une des phases , les caractéristiques \ndu procédé pour toutes les phases  avales seront modifiées. \nDans la s uite de notre présentation, nous indiquerons à quelle \nphase  de notre représentation correspond ent les variables des \nétudes citées.  \n \n \nFigure 1.  REPRESENTATION ZIG ZAG  \n15e Colloque National AIP -Priméca  2/6 La Plagne (73) – 12 au 14 avril 2017  \n II. PILOTAGE  PAR RESEAU DE NEURON ES – ETAT DE L’ART \nAfin de diminuer la dispersion de la production, le s machines \nd’injection plastique sont mises sous contrôle automatique. \nNous distinguons les méthodes de régulations des paramètres , \nprocédé s qui visent à garantir un point de fonctionnement et \nles méthodes d’ajustement des paramètres , dont le but est de \ntrouver un point de fonctionnement optimal aux vues des \ncaractéristiques qualité s produites et des perturbations.  Les \nréseaux de neurones permettent de modéliser des systèmes \nnon linéaires aux multiples entrées et sorties avec des taux \nd'erreur s très faible s. Le procédé d'injection plastique possède \ndes paramètres liés et non linéaires. Un réseau de neurones est \nun modèle relationnel composé de nœuds interconnectés \n(neurones), liés et pondérés. Le réseau le plus utilisé en \ninjection des plastiques est le rése au à réaction par propagation \ninverse (feed -forward Back -Propagation Neural Network, \nBPNN ) défini par  Rumelhart  [2]). Celui -ci se construit par \napprentissage itératif  : pour une entrée, la sortie donnée par le \nréseau est comparée à la sortie attendue. La différence obtenue \nest alors propagée depuis les nœuds de sortie vers l’entrée en \najustant  successivement les coefficients de pondérations  de \nchaque nœud . Le jeu de donnée s de la phase d’apprentissage \npeut provenir de simulations  ou de campagne s d’essai s \nexpériment aux d'un outillage. Cette seconde proposition \npermet de prendre en compte des phénomènes qui ne sont pas \nencore modélisés dans les simulations . La réponse à une \nsollicitation sur un réseau entrainé est ensuite instantanée  car \nce n’est qu’un modèle relationnel . \nA. Régulation du  procédé  \nSchnerr -Häselbarth  et Michae li [3] propose nt le système \n«Intelligent Quality Control ». Il s’agit d’une interface \ninformatique centralisant le pilotage et les mesures réalisées \nsur l’ensemble du parc machine et qui permet de réaliser les \nessais pour la phase d’apprentissage. Un algorithme est  \ndéveloppé pour prédire la qualité des pièces produites ({C1} \nFigure 1) à partir des variables du procédé  ({P2+P3+P4} \nFigure 1) . Il s’agit ensuite de permettre l’exploitation des \nmesures pour réguler  cycle à cycle. Les réseaux récurrents \n(Recurrent Neural Networks ) de Jordan [4] et Elman [5] sont \nchoisis car ils prennent en compte les corrélations temporelles. \nDes neurones spéc ialisés forment  la couc he contextuelle du \nréseau. Ces derniers ont la particularité de prendre en entrée \nleur propre sortie, en plus de l’entrée courante de connexion \navec le reste du réseau  ; ils réagissent ainsi en utilisant la \nmémoire de leur précédente valeur. Leur sortie dé pend des \nentrées courantes et des entrées précédentes. L’apprentissage \ndu réseau est réalisé en deux phases. La phase de \nconditionnement  : les données sont entrées et le réseau \npropose une valeur en sortie. Celle -ci est ignorée. C’est la \n« couche contextue lle » du réseau JE qui mémorise et pondère \nles valeurs du réseau. Dans la seconde phase de sortie, c’est la \ncouche contextuelle qui est la source des valeurs du réseaux \n(l’entrée est nulle). Le réseau produit alors une sortie. Cette \nsortie est comparée à l a sortie attendue et les termes du réseau \nsont pondérés par propagation inverse. Ces deux étapes sont \nrépétées pour l’ensemble des données composant la base d’apprentissage. Les données d’apprentissage proviennent \nd’un plan factoriel à trois niveaux faisan t varier  trois \nparamètres du procédé : température du fondu, pression de \nmaintien et vitesse d’injection  ({P4+P3+P2} Figure 1) . 150 \npoints de mesure s de la pression dans le moule ({ C2+C1} \nFigure 1) sont enregistrés pendant les phases d’injection et de \nmain tien. La masse de la pièce est mesurée avec une précision \nde 1 milligramme et la plage d’essai couvre une variation de \nmasse de 1,4%. Le réseau est alors capable de prédire la masse \ndes pièces avec une exactitude de 86 à 95,2%, soit une \nmoyenne de 7 millig ramme d’erreur sur la masse.  Les auteurs \nretiennent une topologie de réseau 1 -10-10-1. Ils concluent \nleur étude en constatant que la topologie du réseau de neurone \nchoisi e a une influence minime sur le succès de la phase \nd’apprentissage . Avec deux couches de neuron es cachées, plus \nle modèle possède de neuron es, plus le modèle est correct . De \nplus l’organisation des nœuds cachés n’est pas importante . En \nrevanche, les auteurs observent que des réseaux possédant \ntrois couches cachées donnent des résultats légè rement moins \njustes que les réseaux à deux couches , sans proposer de \ncomparaison numérique . Les auteurs e xpliquent ce phénomène \npar un nombre trop élevé de n eurones . Par la suite, Michaeli et  \nSchreiber [6] régule nt la pression dans le m oule en pilotant  la \npression hydraulique d’injection, à l’aide d’un réseau BPNN \nentrainé sur 15 cycles d’injection du système avec différents \nréglages . La température de la matière dans le moule est \nmesurée par capteur infrarouge  ({C2} Figure 1 ) et la pres sion \nd'injection ({P3} Figure 1) est régulée  afin de suivre le \ndiagramme P ression Volume Température  de transformation \ndu matériau pendant l'ensemble du cycle . Les résultats \nexpérimentaux obtenus montrent qu’une augmentation de 20 \ndegrés de la température cause une augmentation de 0,07% de \nla masse de la pièce, ce qui semble négligeable. En \ncomparaison, la même augmentation produit une diminution \nde 1.27%  sans la régulation proposée . \nB. Ajustement des paramètres du procédé  \nLes problèmes de faisabilité d’un mes urage des pièces en cycle \nont été résolu s par la prédiction de  la qualité des produits à \npartir des variables intermédiaires mesurée sur le cycle du \nprocédé. Une mesure directe reste préférable pour  plus de  \nprécision ( voir II.B.2). \n1) Ajustement par les carac téristiques prédites  \nHaeussler et Wortberg  [7] utilisent  un réseau de neurones pour \nprédire la masse et la longueur des pièces  produi tes à partir  \ndes paramètres  du procédé . Ils entrainent un réseau 9 -21-2 à \npartir des mesures sur une p roduction de 162 cycles continu s. \nLe réseau montre des résultats en corrélations avec la mesure \nde pièces de 80% pour la masse et 88,2% pour la longueur. \nCes résultats sont comparés avec une régression non -linéaire \nqui produit des corrélations de 75% pour la masse et 79% pour \nla longueur. Ils concluent leur étude en identifiant la nécessité \nd’un réseau adaptatif pour un véritable contrôle en cycle \nindustrie l. Le réseau doit  apprendre des mesures  cycle à cycle \net ne pas répondre qu’à des données apprises.  Woll et Cooper  \n[8] compare nt ensuite  les performances d'une analyse \nstatistique MSP/ SPC avec un réseau de neurones BP NN pour  \n15e Colloque National AIP -Priméca  3/6 La Plagne (73) – 12 au 14 avril 2017  \n prédire la masse des pièces à partir de profils  de paramètres \ndiscrets (pression hydraulique, dans le moule et dans la buse) \net accepter ou non une pièce produ ite. Le réseau est entrainé \ncomme un modèle inverse du procédé  : des profils de pression \nde moule en entrée correspondent à des valeurs de paramètres \nde pression de maintien ou tempé rature du fourreau . Les \nparamètres sont alors ajustés cycle à cycle en com parant le \nprofil de pression dans le moule actuel avec le profil de \nréférence appris. L'analyse MSP  est effectuée sur la valeur de \npression maximale mesurée dans le moule tandis que le réseau \nde neurones est entrainé sur l'ensemble du profil de pression. \nLe réseau est entrainé sur des résultats simulés afin d'éviter de \nréaliser une longue campagne d'essai, la convergence du \nmodèle étant observé pour 2000 essais. L'essai est réalisé pour \nun unique matériau et ne s'intéresse qu'à la longueur de \nl'éprouvette. Les résultats montrent la capacité supérieure des \nréseaux de neurones à pré dire les non linéarités comparé  à la \nrégression linéaire multiple utilisé e par SPC. L'exactitude de \nla prédiction des mesures est augmentée de 10% et le taux de \nfaux positif est réd uit de moitié. Le réseau répond très bien \naux perturbations. L'étude conclue sur la nécessité d'entrainer \nle réseau  sur plusieurs profils , dont les températures.  \n2) Ajustement par les caractéristiques mesurées  \nIl est possible de mesurer la qualité des pièces produites dès la \nsortie de moule. Dans ces études, la diversité des \ncaractéristiques mesurées sur les pièces est limitée. La masse \nest la mesure récurrente car elle peut être réalisé e pendant le \ncycle, et ainsi être utiliser pour corriger le cycle suivant.  Les \nmesures complémentaires sont effectuées à posteriori de la \nproduction d'un lot, pour valider les performances du modèle \nde pilotage. Il serait intéressant de réaliser une caractérisation \ncomplète de la pièce obtenue pendant le temps de cycle afin de \ncorriger les pièces suivantes à partir de ces données.  Sous \nréserve d'un e durée de  mesur age des pièces inférieur e au \ntemps de cycle, le pilotage peut s'effectuer dès la pièce \nsuivante . Lau et al. [9] propose nt de vérifier les capacités de \ncréation par apprentissage d'un modèle multi entrées -sorties du \nprocédé d'i njection par réseau de neurones à propagation \ninverse. L’objectif est que le réseau suggère des modifications \ndes paramètres procédé s afin d’ajuster les dimensions de la \npièce, pour obtenir  la pièce cible en fonction de la pièce \nobtenue.  L'étude dimensionn elle porte sur une longueur, deux \nlargeurs et l'épaisseur d'un échantillon. L'apprentissage du \nréseau est réalisé sur les mesures de 100 pièces produites  en \nfaisant varier  6 paramètres  procédé s : la températ ure du \nfourreau central  et arrière, la vitesse de  fermeture  de \nl’outillage , la vitesse d ’avance de  la vis, la force de fermeture, \nla durée d'injection, la durée de refroidissement, la vitesse \nd'injection  et la  pression d'injection. L'étude souligne que \nmalgré le maintien de paramètres de production ident iques, les \nquatre dimensions étudiées varient au bout de quatre jours de \nproductions, signe de la difficulté d'obtenir une production \nstable en injection et le besoin de contrôle. Les variations \ndimensionnelles mesurées sont appliquées en entrée au réseau,  \nqui indique en sortie les valeurs du point de fonctionnement \npermettant de produire  une pièce avec ces dimensions. \nConnaissant le réglage utilisé pour produire les pièces, l'étude propose de régler les paramètres dans des proportions inverses \nafin de comp enser les variations de pièces. Ce tte démarche \nsuppose que les paramètres soient  indépendants , ce qui n'est \npas le cas. Néanmoins, après 3 itérations de réglages \nsuccessif s, le réseau parvient à converger vers une erreur \nquadratique moyenne  (RMS)  de 0, 07. Ce résultat est comparé \nà une précédente étude des auteurs [10] qui utilisai ent un \nsystème hybride à logique floue qui convergeait en 3 itérations \nvers une erreur quadratique moyenne  (RMS)  de 0, 06. La \nméthode proposée est moins performante qu'un système à \nlogique flou e et réseaux de neurones  mais plus simple à mettre \nen œuvre. De plus, une optimisation de la topologie  du réseau \nde neurones, telle que la recherche en informatique la  permet \naujourd’hui améliorerait ce résultat  (voir II I.B). \nIII. VERS UN P ILOTAGE PAR LA QUALI TE PRODUITE  \nNous définissons la  caractéristique qualité intermédiaire \ncomme un critère qualité immédiatement mesurable en sortie \nde presse  ({C1} Figure 1) , non fonctionnel, mais fortement \ncorrélé avec les caractéristiques fonctionnelles de la pièce finie.  \nLe projet S APRISTI  vise à évaluer l’intérêt de l’exploitation  de \nces caractéristique s qualité s intermédiaire s. Nous proposons \nd’identifier les caractéristiques intermédiaires mesurables sur le \nprocédé. Nous validerons  ensuite la corrélation de ces \ncaractéri stiques avec la qualité  de la pièce finie. Sur la base de \nces travaux, nous proposons de développer un modèle du \nprocédé prenant en compte l’ensemble des caractéristiques \nintermédiaires mesurables afin d’ajuster les paramètres du \nprocédé. Ce modèle pourra  être construi t par apprentissage à \nl’aide de réseaux de neurones.  Nous distinguons les \ncaractéristiques {Ci}  et les paramètres réglables {Pi} du \nprocédé pour chacune des phases présentées dans la Figure 1.  \nNotre étude nous permet d’ identifi er les caractéri stiques \nintermédiaires du procédé régulièrement retenues dans la \nlittérature (Tableau 1).  \nCaractéristiques produit s Paramètres du procédé  \nC Pièce finie  (masse , géométrie )  \nC1 Caractéristiques intermédiaires   P1 \nC2 Pression dans moule  Pression de maintie n \nP2 \nTemp érature dans le  moule   \nC3 Température du fondu  Pression d’injection  P3 \nTableau 1.  VARIABLES ET PARAMETR ES DU PROCEDE  \nA. Identification des paramètres influents  \nAfin de caractériser l’influence des paramètres réglables sur le s \ncaractéristiques des produits , nous réaliserons un plan \nd’expérience de Plackett -Burman [11] à deux niveaux . Ce plan \nnous permet  d’identifier les paramètres influents  par analyse \nstatistique, par exemple en utilisant un test de Fisher . Nous \nmesurons le maximum de caractéristiques possi bles, afin \nd’utiliser ces données pour l’apprentissage d ’un modèle  réseau \nde neurones . Nous retenons  des facteurs procédés supposés \npertinents  afin de les faire varier . L’objectif de cette phase n’est \npas de réguler le procédé autour d’un point de fonction nement \noptimal mais de faire varier l’ensemble des paramètres afin de \ncréer de la variabilité pour l’analyse statistique . Cette \nvariabilité, permettra d’identifier et de hiérarchiser  les \ncaractéristiques pertinent es, mais aussi de créer de la variabilité  \n15e Colloque National AIP -Priméca  4/6 La Plagne (73) – 12 au 14 avril 2017  \n sur les caractéristiques élémentaires … et sur les pièces finies. \nNous utiliserons ainsi cette variabilité pour valider l’hypothèse \nde l’intérêt des caractéristiques intermédiaires comme \nprédicteurs de la qualité finie et comme intermédiaire de \npilotage cou rt terme du processus.  \nB. Modélisation par apprentissage  \n1) Topologie du réseau de neurones  \nAfin de définir la topologie du réseau de neurone employé dans \nnotre étude, nous évaluerons différents nombres de couches \ncachées. Un nombre de couche trop élevé  entraine  un \nphénomène de sur -ajustement ( overfitting ) qui génère des \nerreurs dans la réponse alors qu’un nombre trop faible ne \npermet pas de modéliser précisément  le problème.   Après avoir \nentrainé différents  réseaux sur nos données, nous choisi ssons  la \nconfigurat ion qui minimise l’erreur quadratique moyenne  lors \nde l’apprentissage mais aussi lors de l’évaluation  du réseau sur  \ndes données expérimentales.  Rivals [12] propose  d’optimiser \nce nombre en deux phases. La premi ère dans laquelle le \nnombre de couches cachés est augmenté jusqu’à observer le \nsur-ajustement, puis une seconde phase dans laquelle les \nréseaux retenus  sont évalués par test de Fisher  pour déterminer  \nsi l’ensemble des nœuds cachés sont statistiquement pertinents . \nCette méthode permet une optimisation rapide de grands \nréseaux. Xu et Chen [13] évaluent différentes méthodes \nd’optimisation du nombre de couches caché es d’un réseau de \nneurones et proposent une approche reprise dans de \nnombreuses publications.  Néanmoins, la méthode proposée  ne \nprend pas en compte la quantité de bruit s des données, le  type \nde fonction d’activation ou l’algorithme d’entrainement  utilisé . \n \nFigure 2.  RESEAU 4-6-6-2 DYNAMIQUE  \n2) Réseau de neurones dynamique s \nNous proposons d’évaluer l’utilisation d’un réseau de neurone \nrécurrent (bouclé) prenant en compte la variable temporelle, \nafin d’exploiter les mesures discrètes pendant les cycles \n(température, pression, déplacement ). Ces réseaux demandent \nde postuler l ’influence  des perturbations  (bruit ) sur le procédé. \nDans notre cas, le bruit est additif avec la sortie actuelle et \nl’ensemble de s sorties passées (hypothèse NARX  : Nonlinéaire \nAutoRégressif à entrées eXogènes).  Pour illustrer la \nconstruction d’un réseau dynamique, la Figure 2 représente un \nréseau dynamique 4-6-6-2 capable de déterminer des consignes  \nde pression et température à par tir des mesures obtenues pendant le cycle dans le moule. L’échantillonnage des mesures \nréalisées dans le moule pendant le cycle  est proche de 100 \nmesures par secondes, le réseau devra être dimensionné en \nconséquence et les mesures moyennées  par intervalles . Nous \nutilisons des capteurs de pression capacitifs et des \ntherm ocouples . Les capteurs de pression ont une dérive \ninférieure à 1 bar par secondes  ce qui est satisfaisant sur des \ntemps de cycle inférieurs à 30 secondes.  \nC. Mesurage de la qualité des pièces en  cycle  \nLe procédé d’ injection est cyclique (Figure 3 ). Les différentes \nphases du procédé, comme le dosage de la matière pour la \nprochaine pièce et le refroidissement de la pièce actuelle sont \nréalisés en temps parallèles.  Afin de respecter le temps de \ncycle, le mesurage de la pièce produite doit être réalisé entre \nl’éjection de la pièce de l’outillage et l’éjectio n de la pièce \nsuivante. La caractéristique des pièces finies la plus étudiée \ndans la littérature  est la masse. Cela s'explique par la facilité de \nmesure en sortie machine  et la bonne précision des balances \n(milligramme) . La masse est une condition nécessaire pour \nvalider les pièces.  Les caractéristiques d'aspect sont en \nrevanche peu étudié es. Elles  contiennent des informations \ncapitales pour l'analy se des causes de la variabilité d'une pièce  \net intéressent le client . Le régleur utilise prioritairement l 'aspect \ndes pièces pour régler.  \nFigure 3.  CHRONOGRAMME DU CYCL E DU PROCEDE  \nLe contrôle qualité géométrique  par imagerie est appliqué pour \nles grandes séries. I l demande un équipement c alibré et \nautomatisé. Il prend  place après la production, permet d'exclure \nles pièces non conformes  et ne s'intéresse pas à l'aspect. Les \ndimensions mesurées ne sont pas utilisées pour ajuster le \nprocédé pour les cycles suivants. E n parallèle, l e manque de \nspécification et la difficulté à mesurer, en cycle,  limite \nl'utilisation  de ces caractéristiques. De plus, a ucune norme ne \nstatue actuellement sur les qualités d'aspect. Plusieurs travaux \nde recherches sont en cours  [14], [15]. Ces travaux associent \nune démarche de spécification aux développements de \nsolutions de mesures adaptées aux cy cles industriel s [16]. \nDifférentes mesures sont aujourd'hui compatibles , de par leurs \ncourtes durées, avec une utilisation en cycle industriel : masse, \nthermographie infrarouge, contrôle d'aspect, imagerie \ntridimensionnelle , durométrie . Pour obtenir des mesures fiables \net répétables cette opération  doit être automatisée. En so rtie de \nmoule, la pièce est chaude et déformable.  De nombreuses \npresses industrielles sont équipées de robot  de déchargement . \nNous proposons  de programmer la cinématique du robot afin \nde déplacer la pièce devant différ ents outils de mesures (Figure \n4). Ainsi nous  synch ronisons le mesurage de toutes les  pièces \nen sortie de moule  à des instants fixes . Nous obtiendrons des \n \n15e Colloque National AIP -Priméca  5/6 La Plagne (73) – 12 au 14 avril 2017  \n mesu res répétables  sur l’ensemble des cycles d’injection . Nous \nutilisons un outil de mesure laser par nuage de point d’une \nrésolution de 0, 050 millimètres  entre points . Nous pourrons \nainsi caractériser une variation dimensionnelle maximale de \n1,5% sur nos échantillons. En comparaison, la norme AFNOR \nNF T58 -000 [17], pour les dimensions de nos pièces, définie \nune tolérance dim ensionnelle très large de 13% en classe \nprécision . \n \n \nFigure 4.  MESURAGE DES PIECES EN CYCLE  \nNous réaliserons cette mesure dimensionnelle dès la s ortie de \nmoule.  Elle sera ensuite réalisée  après stabilisation de la pièce , \nune heure puis  une journée après la producti on, afin d’analyser \nles phénomènes de retraits. Nous complèterons nos mesures \npar imagerie thermographique  infrarouge  en sortie de moule \ndes pièces afin d’analyser les champs de température s qui \npourraient être corrélés à la qualité . Une mesure de la quali té \nd’aspect dès la sortie de moule est envisageable par prise s de \nvues photographique s et analyse d’image s. L’aspect de surface \ndes pièces sera mesuré et caractérisé post -production  sur un \ndôme de mesure  de réflectance  (Polynomial Texture Mappings ) \n[18]. L’ensemble de ces mesures nous permettra d’identifier \ndes corrélations entre caractéristiques et paramètres, ainsi  que \nd’entrainer le réseau de neurones du modèle.  \nIV. CONCLUSION  \nLe pilotage du procédé d’injection est une thématique de \nrecherche riche  et aux enjeux industriels important s. Les \nméthodes de pilotage s’appuient sur l’instrumentation de la \nmachine  et nous proposons d’automatiser le mesurage des \npièces produites . L’objectif est d’ obtenir un ajustement cycle  à \ncycle des paramètres  procédé  afin d’obtenir un produit à la \nqualité cible. Le procédé d’injection possède des paramètres \ndont les effets sur la géométrie sont non-linéaires. Les réseaux \nde neurones permettent de  modéliser par apprentissage  un \nprocédé multi entrées et sorties,  à partir de données \nexpérimentales . La littérature en pilotage du procédé \nd’injection plastique propose des études précises sur des  \nphase s uniques du processus et ne pren d pas en compte \nl’ensemble des rela tions entre les phases (Figure 1 ), ni \ntemporelles . Le lien entre les paramètres mesurables sur la \npresse et la qualité finale  des produits est peu étudiée. Enfin, les critères sensoriels des produits ne sont pas pris en compte \nalors qu’ils sont essentiels pour la qualité des produits.  La \nrecherche en métr ologie géométrique et d’aspect automatisée  \net la recherche active dans le domaine  des réseaux neuronaux  \nouvrent de larges perspectives pour le pilotage . L’apport de la \nmodélisation dyna mique par réseaux de neurones permet une \nréponse très rapide rendant co mpatible le pilotage cycle à \ncycle.  \nV. REFERENCES  \n[1] N.P. Suh, The Principles of Design, Oxford University \nPress, 1990.  \n[2] D.E. Rumelhart, J.L. McClelland, C. PDP Research \nGroup, eds., Parallel Distributed  Processing: \nExplorations in the Microstructure of Cognition, Vol. 1: \nFoundations, MIT Press , 1986.  \n[3] O. Schnerr -Häselbarth, W. Michaeli, Automation of \nonline quality control in injection moulding, \nMacromolecular Materials and Engineering , Vol. \nN°284–285, 2000,  pp. 81–85. \n[4] M.I. Jordan, D.E. Rumelhart, Forward Models: \nSupervised Learning with a Distal Teacher, Cognitive \nScience , Vol. N°16, 1992, pp.  307–354. \n[5] J.L. Elman, Finding structure in time, Cognitive Science , \nVol. N°14, 1990,  pp. 179–211. \n[6] W. Michaeli, A. Schreiber, Online control of the \ninjection molding process based on process variables, \nAdvances in Polymer Technology , Vol. N° 28, 2009,  pp. \n65–76. \n[7] J. Haeussler, J. Wortberg, Quality Assurance in Injection  \nMolding with Neural Networks, A ctes de  : Society of \nPlastics Engineers  SPE ANTEC'93 , pp 123–129, New \nOrleans , 1993 . \n[8] S.L.B. Woll, D.J. Cooper, B.V. Souder, Online pattern -\nbased part quality monitoring of the injection molding \nprocess, Polymer Engineering & Science , Vol. N°36, \n1996, p p. 1477 –1488.  \n[9] H.C.W. Lau, A. Ning, K.F. Pun, K.S. Chin, Neural \nnetworks for the dimensional control of molded parts \nbased on a reverse process model, Journal of Materials \nProcessing Technology , Vol. N°117, 2001, pp. 89–96. \n[10] H. Lau, T. n. Wong, A Fu zzy Expert System for \nComplex Closed -loop Control: A Non -Mathematical \nApproach, Expert Syst ems, Vol. N°15, 1998, pp.  98–109. \n[11] R.L. Plackett, J.P. Burman, The Design of Optimum \nMultifa ctorial Experiments, Biometrika, Vol. N°33, \n1946, pp.  305. \n[12] I. Rivals, L. Personnaz, A statistical procedure for \ndetermining the optimal number of hidden neurons of a \nneural model, Actes de :  Second International \nSymposium on Neural Computation NC'2000 , Berlin, \n2000.  \n[13] S. Xu, L. Chen, A novel approach for determining  the \noptimal number of hidden layer neurons for FNN’s and \nits application in data mining, Actes de : 5th International \nConference on Informati on Technology and Applications \nICITA 2008 , Cairns, Queensland, Australia, 2008.   \n15e Colloque National AIP -Priméca  6/6 La Plagne (73) – 12 au 14 avril 2017  \n [14] S.-F. Desage, G. Pitard, M. P illet, H. Favreliere, J.L. \nMaire, F. Frelin, S. Samper, G. Le Goïc, Syntactic \ntexture and perception for a new generic visual \nanomalies classif ication, Actes de :  The International \nConference on Quality C ontrol by Artificial Vision 2015, \nLe Creusot, France , 2015.  \n[15] G. Pitard, Métrologie et modélisation de l’aspect pour \nl’inspection qualité des surfaces, Thèse de doctorat, \nUniversité Grenoble Alpes, 2016.  \n[16] S.-F. Desage, G. Pitard, H. Favrelière, M. Pillet, J.L. \nMaire, F. Frelin, S. Samper, G. Le Goïc,  Vers une \nautomatisation du contrô le visuel des produits, Actes de :  \nQual ita' 2015, Nancy, France, 2015.  \n[17] AFNOR, NF T58 -000, (1987).  \n[18] G.L. Goïc, Qualité géométrique & aspect des surfaces : \napproches locales et globales, Thèse de doctorat, \nUniversité de Grenoble, 2012.  \n ",
      "metadata": {
        "filename": "Towards new methods for process adjustments based on parts quality measurements.pdf",
        "hotspot_name": "PBT_Injection_Molding",
        "title": "Towards new methods for process adjustments based on parts quality\n  measurements",
        "published_date": "2017-07-05T09:23:03Z",
        "pdf_link": "http://arxiv.org/pdf/1707.01765v1",
        "query": "injection molding PBT housing energy efficiency optimization cycle time reduction"
      }
    },
    "A Fast Algorithm for Parabolic PDE-based Inverse Problems Based on Laplace Trans": {
      "full_text": "A fast algorithm for parabolic PDE-based inverse problems\nbased on Laplace transforms and \rexible Krylov solvers\nTania Bakhos\u0003yArvind K. SaibabazPeter K. Kitanidisx\nAugust 27, 2018\nAbstract\nWe consider the problem of estimating parameters in large-scale weakly nonlinear inverse\nproblems for which the underlying governing equations is a linear, time-dependent, parabolic\npartial di\u000berential equation. A major challenge in solving these inverse problems using\nNewton-type methods is the computational cost associated with solving the forward problem\nand with repeated construction of the Jacobian, which represents the sensitivity of the\nmeasurements to the unknown parameters. Forming the Jacobian can be prohibitively\nexpensive because it requires repeated solutions of the forward and adjoint time-dependent\nparabolic partial di\u000berential equations corresponding to multiple sources and receivers. We\npropose an e\u000ecient method based on a Laplace transform-based exponential time integrator\ncombined with a \rexible Krylov subspace approach to solve the resulting shifted systems\nof equations e\u000eciently. Our proposed solver speeds up the computation of the forward and\nadjoint problems, thus yielding signi\fcant speedup in total inversion time. We consider an\napplication from Transient Hydraulic Tomography (THT), which is an imaging technique\nto estimate hydraulic parameters related to the subsurface from pressure measurements\nobtained by a series of pumping tests. The algorithms discussed are applied to a synthetic\nexample taken from THT to demonstrate the resulting computational gains of this proposed\nmethod.\n1 Introduction\nConsider linear parabolic partial di\u000berential equations (PDEs) of the form,\nS(x;s)@\u001e\n@t\u0000A(x;s)\u001e=f(x;t)\u001e(t= 0) =\u001e0 (1)\nalong with the associated boundary conditions, where sare the parameters to be estimated,\nf(x;t) represents the source term, \u001e(x;t) is the state variable and SandAare time-independent\noperators. We are interested in fast solutions to Equation (1) particularly in the context of inverse\nproblems [28], i.e. using measurements \u001eto estimate parameters s, whose governing forward\nequations are described by Equation (1). Problems of this kind arise in many applications\nsuch as Di\u000buse Optical Tomography [1, 11] ( A=r\u0001(Dr\u0001)\u0000\u0017\u0016a), electromagnetic inversion\n(A=r\u0002\u0016\u00001r\u0002) [35] and Transient Hydraulic Tomography ( A=r\u0001(\u0014r\u0001)) [2]. For example, in\nDi\u000buse Optical Tomography, measurements of photon \ruence are used to \\invert\" for di\u000busivity\nDand the absorption coe\u000ecient \u0016a.\nWe will focus our attention on Transient Hydraulic Tomography (THT). THT is a method of\nimaging of the subsurface that uses a series of pumping tests to estimate important hydrological\nparameters, such as conductivity and storativity. In this method, water is injected continuously\nat a constant rate in injection wells and the resulting response in pressure change is recorded at\nmeasurement wells, until steady-state is approximately reached. The drawdown curves recorded\nby each measurement location are stored and from this data, a few key time sampling points\nare identi\fed and the corresponding measured pressure data are used in an inversion algorithm\n\u0003taniab@stanford.edu\nyInstitute for Computational and Mathematical Engineering, Stanford University\nzDepartment of Electrical and Computer Engineering, Tufts University\nxDepartment of Civil and Environmental Engineering, Stanford University\n1arXiv:1409.2556v2  [math.NA]  17 May 2015to recover the aquifer parameters of interest (e.g. hydraulic conductivity and speci\fc storage).\nInstead of solving for the groundwater equations in the traditional time-stepping formulation,\nwe propose a Laplace transform-based solver. We emphasize that while we describe methods as\nrelated to THT, they can be extended to a large class of linear parabolic PDEs.\nWe focus our attention on methods based on successive linearization such as Gauss-Newton\nto solve the inverse problem. However, we note that other possible approaches include Ensemble\nKalman Filters, particle \flters, rejection samplers, Markov Chain Monte Carlo, for a good\nreview please see [10]. Popular approaches for solving inverse problems based on time-dependent\nparabolic PDEs rely on traditional time-stepping methods (such as Crank-Nicolson) that require\nthe solutions of linear systems of equations at each time step in order to implicitly march the\nsolution in time. The time step is computed based on accuracy and stability requirements. This\nprocess is inherently sequential and is computationally expensive. A second challenge in the\ncontext of inverse problems solved using Newton-type methods is that the computation of the\nJacobian or sensitivity using the adjoint state method requires storing the entire time history of\nthe forward problem. To alleviate the memory costs, previous work has considered checkpointing\nmethods to trade computational e\u000eciency for memory [5, 27, 31]. Therefore, there is a need to\ndevelop a method that is accurate and e\u000ecient, both in terms of memory and computational\ncosts.\nFor solving the problem that arises in the discretization of the forward problem, previous\nworks have noted that applying the contour integral representation of Laplace transforms leads\nto a sequence of shifted systems of linear equations which can then each be solved for indepen-\ndently. This observation can be exploited to develop a parallel algorithm for time integration\nfor parabolic PDEs [24] or an iterative Richardson method [25]. In [9], the application of Krylov\nsubspace methods that take advantage of the shift-invariance of the Krylov subspace to simulta-\nneously solve the shifted systems arising from the contour integral representation was considered.\nHowever, to the best of our knowledge, the application of these methods to inverse problems has\nnot been performed.\nContributions: In this paper, we propose a Laplace transform-based exponential time in-\ntegrator combined with a \rexible Krylov subspace approach to speed up the computation of\nthe forward and adjoint problems, thus yielding signi\fcant speedup in total inversion time. The\nmodi\fed Talbot contour is chosen to invert the Laplace transform and is discretized using the\nTrapezoidal rule to yield an exponentially convergent quadrature rule [30]. The resulting shifted\nsystems of equations are solved using a \rexible Krylov subspace solver that allows for multi-\nple preconditioners to e\u000eciently precondition the systems across the entire range of shifts [22].\nThis yields signi\fcant speedup in solving the forward problem. We propose heuristics to pick\npreconditioners and demonstrate the robustness of the proposed solver with several parameters\n(variance of the underlying conductivity \feld, measurement time samples and grid discretiza-\ntion). We show that for a certain range of measurement times, the corresponding inverse Laplace\ntransforms can be sped up simultaneously using shared preconditioners across all shifts allowing\nfor additional speedup.\nAdditionally, we derive a method for computing the Jacobian using the adjoint state technique\nbased on the Laplace transform. Computing the Jacobian this way requires several solutions of\nshifted systems of equations. Using the solver developed in our previous work [22] allows for\nsolving the forward (and adjoint) problem for multiple shifts at a cost that is comparable to\nthe cost of solving a system of equations with a single shift. It should be noted that the fast\ncomputation of the Jacobian can be used not only to accelerate the geostatistical approach but\nalso other techniques for inversion and data assimilation such as 3D Var, 4D Var \fltering and\nExtended Kalman \flter. An additional advantage for using the proposed solver to compute the\nJacobian is that the rows corresponding to di\u000berence time points can be computed independently\nwithout the need to store the entire time history. We demonstrate results using our method to\naccelerate an example problem in THT.\nOutline : The paper is organized as follows. We describe the governing equations for ground-\nwater \row and the choice of contour used to approximate the inverse Laplace transform in Section\n2. In Section 3, we discuss the \rexible Krylov subspace solver for solving the resulting shifted\nlinear systems of equations. In Section 4 we demonstrate the performance and robustness of the\nproposed solver under various test conditions. We follow this with a description of the geosta-\ntistical method for solving inverse problems and show an example taken from THT in Section\n5. Finally, in Section 6 we conclude with a summary and a discussion on future work.\n22 Forward Problem\n2.1 Transient hydraulic tomography\nIn this work we will assume that \row in the aquifer of interest can be modeled as con\fned (with\nstandard, linear elastic storage) or, if uncon\fned, can be treated appropriately using a saturated\n\row model with the standard linearized water table approximation [18]. The equations governing\ngroundwater \row through an aquifer for a given domain \n with boundary @\n is composed of the\nunion of three non-intersecting regions - @\nD,@\nNand@\nwreferring to Dirichlet, Neumann\nand linearized water table boundaries respectively.\nSs(x)@\u001e(x;t)\n@t\u0000r\u0001 (\u0014(x)r\u001e(x;t)) =f(x;t); x 2\n (2)\n\u001e(x;t) = 0; x 2@\nD\nr\u001e(x;t)\u0001n= 0; x 2@\nN\nr\u001e(x;t)\u0001n=\u0000Sy@\u001e(x;t)\n@t; x2@\nw\nwhere\u0014(x) [LT\u00001] is the hydraulic conductivity, Ss[L\u00001] is the speci\fc storage, f(x;t) [T\u00001] is\nthe pumping source, \u001e(x;t) is the hydraulic head (pressure) and Sy[\u0000] is the speci\fc yield. In\n2D aquifers, the terms \u0014(x) [L2T\u00001] andSs(x) [\u0000] are known as transmissivity and storativity\nrespectively. In this work, the pumping source is modeled as f(x;t) =q(t)\u000e(x\u0000xs), wherexs\ncorresponds to the location of the pumping source and q(t) to the pumping rate. To derive the\nweak formulation, we multiply by appropriately chosen test functions  . Integrating by parts\n(Ss@t\u001e; )\n+ (Sy@t\u001e; )@\nw+A(\u001e; ) =q(t)(\u000e(x\u0000xs); )\n (3)\nwhere (u;v)\ndef=R\n\nuvdx , (u;v)@\nwdef=R\n@\nwuvdx andA(u;v)def=R\n\n\u0014(x)ru\u0001rvdx. In our\nwork, we have used the standard linear \fnite element approach but extensions to higher order\n\fnite elements are possible. The above weak form is then discretized using \fnite elements as\n\u001eh(t) =PN\nj=1^\u001ej(t)uj(x) whereuj(x) are the \fnite basis functions, to obtain the following\nsemi-discrete system of equations\nM@t\u001eh+K\u001eh=q(t)b \u001eh;0ft= 0g=Ph\u001e0 (4)\nwhere the matrices KandMhave entries Kij=A(uj;ui),Mij= (Ssuj;ui)\n+ (Syuj;ui)@\nw\nand the vector bhas entries bj= (\u000e(x\u0000xs);uj)\n. Furthermore,Phis the orthogonal projector\nwith respect to the ( \u0001;\u0001)\ninner product. Taking the Laplace transform ^\u001e(\u0001;z) =R1\n0\u001e(\u0001;t)e\u0000ztdt,\nwe have\n(K+zM)^\u001eh(z) = ^q(z)b+M\u001eh;0 (5)\nThe \feld\u001eh(t) can be recovered by applying the inverse Laplace transform on a carefully chosen\ncontour using the formula\n\u001eh(t) =1\n2\u0019iZ\n\u0000ezt(K+zM)\u00001(^q(z)b+M\u001eh;0)dz (6)\nwhere ^q(z) denotes the Laplace transform of q(t) and \u0000 is an appropriately chosen contour. We\nwill now discuss possible choices for \u0000.\n2.2 Choice of contour\nTo compute the inverse Laplace transform, we will employ the modi\fed Talbot contour\n\u0000(\u0012) :z(\u0012) =\u001b+\u0016(\u0012cot\u0012+\u0017i\u0012)\u0000\u0019\u0014\u0012\u0014\u0019 (7)\nwhere\u0012is the contour discretization parameter. This contour was analyzed in [30] with the\ncontour parameters \u001b,\u0016and\u0017optimally chosen to improve the convergence of the quadrature\nscheme. The contour is a simple, closed curve that encloses both the eigenvalues of the gener-\nalized eigenvalue problem Kx=\u0015Mx and the singularities of ^ q(z).1In [33], it was shown that\n1In this work, we assume that ^ q(z) does not have any singularities on the positive real axis. Otherwise, the\ncontour has to be adapted to account for these singularities. This is discussed in [9].\n3optimal convergence as Nz!1 keepingt\fxed is achieved with \u001band\u0016proportional to the\nratioNz=t.\nAs in [30], we approximate the integral using the Trapezoidal rule on a uniform grid on\n[\u0000\u0019;\u0019] using evenly spaced points \u0012kspaced by 2 \u0019=Nz,\n\u001eh(t) =1\n2\u0019iZ\u0019\n\u0000\u0019ez(\u0012)tz0(\u0012)F(z(\u0012))d\u0012\u0019NzX\nk=1ez(\u0012k)tz0(\u0012k)F(z(\u0012k)) (8)\nwhere we de\fned F(z) = (K+zM)\u00001(M\u001e0+ ^q(z)b). Further, we also de\fne the weights of the\nquadrature wk=\u0000i\n2Nez(\u0012k)tz0(\u0012k). Since (K;M ) is a Hermitian de\fnite pencil and the contour\nis symmetric, we need only use half the points,\n\u001eh(t)\u0019NzX\nk=1wkF(z(\u0012k)) = 2 Real8\n<\n:Nz=2X\nk=1wkF(zk)9\n=\n;(9)\nFigure 1: Plot of contours corresponding to Nz= 20 (left) and Nz= 40 (right). Because of\nsymmetry, only half the contour plot is shown. Here Nzis the number of quadrature points in\nEquation (9)\nAn example of the contours for three di\u000berent measurement times is shown in Figure 1.\nAs mentioned earlier, in this paper we only consider sources f(x;t) =q(t)\u000e(x\u0000xs) but this\nmethod is applicable for all sources that are separable in space and time. This assumption is\ncrucial since we intend to use Krylov subspace methods for shifted systems. This requires the\nright-hand sides to be independent of the shift (except, perhaps by a multiplicative factor). If\nthe Laplace transform of q(t) is known explicitly, it can be used. Otherwise, one may consider\nthe use of Prony's method to \ft an exponential sum q(t)\u0019P\njajexp(\u0000\u0015jt) on [0;T], for which\nthe Laplace transform can be computed easily [25]. Although in this case, the right hand side is\ndependent on zk, the Krylov subspace method for shifted systems can still be applied.\nApart from the Talbot contour, other contours such as parabolas and hyperbolas have been\nproposed and analyzed. In addition, the connection between the Trapezoidal rule applied to\napproximate the integral and rational approximations to the exponential function have been\npointed out in [30]. We would like to emphasize that our fast algorithm does not depend on the\nspeci\fc choice of contour.\n2.3 Accuracy and computational cost\nThe error between \u001eNz;h(t) and the true solution \u001e(x;t) in theL2(\n) norm can be bounded\nusing the triangle inequality\nk\u001e(t)\u0000\u001eNz;h(t)k \u0014 k\u001e(t)\u0000\u001eh(t)k+k\u001eh(t)\u0000\u001eNz;h(t)k\nThe \frst term contributing to the error is due to the \fnite element discretization. As shown in\nLemma 3:1 in [29], this error is\nk\u001e(t)\u0000\u001eh(t)k\u0014Ch2\u0010\nk\u001e0k+k^fk\u0000\u0011\n4where,k^fk\u0000def= supz2\u0000j^f(z)jandCis a constant independent of h. The second contributing\nterm is the error incurred by the discretization of the inverse Laplace transform and is dependent\non the choice of parameters \u001b;\u0016and\u0017. In summary, the total error is bounded by O(e\u0000cNz+h2).\nFor the parameters chosen, c= log 3:89 [30].\nWe now analyze the computational cost of the Laplace transform method and compare it to\na standard time-stepping scheme. Let us assume that we need to compute the solution \u001e(x;t)\natNTtime-steps where NT=O(1). For a given discretization, let the number of grid points be\nNand let the cost of solving a linear system of equation be \u0016(N). The cost of computing the\nsolution atNTpoints in time using the Laplace transform methods is NzNT\u0016(N) and using time-\nstepping schemes is Nt\u0016(N). Let us now analyze the cost for a desired tolerance \". Assuming\nsecond-order discretization technique in time we have C\u0001t2\u0018\", so thatNt=T=\u0001t\u0018C0\"\u00001=2.\nHowever, for Laplace transform method the convergence is exponential, so the number of systems\nisNz\u0018C00jlog\"j. Thus, for the same accuracy we expect that the Laplace transform-based\nmethod is much more e\u000ecient. Furthermore, as has been noted in [33, 30, 32] and several\nothers, the system of equations can be solved independently for each shift and parallel computing\nresources can be leveraged for e\u000ecient solution of the Laplace transform since the computations\nare embarrassingly parallel.\nE\u000ecient methods have been proposed for solving the sequence of shifted systems arising\nfrom the discretization of the contour integral using the Trapezoidal rule. In [9], the authors\npropose a Krylov subspace method for solving the shifted system of equations. However, there is\na rich literature concerning the Krylov subspace methods for shifted systems (for a good review\nsee [26]). In practice, e\u000ecient preconditioners are needed to ensure convergence in a reasonable\nnumber of iterations. For this purpose, in this paper, we adapt the \rexible Krylov subspace\nmethods proposed in [6] and analyzed in [22].\n3 Fast solvers for shifted systems\nRecall the approximation of the inverse Laplace transform as shown in Equation (8). It leads to\na sequence of shifted systems at each time step, which can then each be solved independently.\nIn particular, for each time step, we need to solve two shifted systems of equations,\n(K+zkM)Xk= [b; M\u001e 0]; k = 1;:::;Nz=2 (10)\nwherezkare the (complex) shifts corresponding to the inverse Laplace transform contour. F(zk)\nis thus found by adding the solution to the \frst system of shifted systems to the second (times\nthe corresponding ^ q(zk)). In this section, we consider Krylov subspace methods for the solution\nof the \frst shifted systems of equations,\n(K+zkM)xk=b; k = 1;:::;Nz=2 (11)\nIt should be noted that the second system of equations can be solved for in a similar fashion.\nWe assume that the systems are non-singular for all k= 1;:::;Nz=2. Recall that because of\nEquation (9) only half the systems need to be solved. In Equation (11), the matrices KandM\nand right-hand side bare independent of the shifts zk. Krylov based methods are particularly\nattractive for this class of problems because of their shift-invariant property. The main strategy\nof Krylov solvers is to build a Krylov basis and then to search for an approximate solution in\nthe resulting reduced space. The expensive step of constructing the Krylov basis needs to be\nperformed only once since an approximation space is generated independently of the shifts. Once\nthe basis is built, the subproblem for each shift can then be solved at a much reduced cost. We\nconsider Arnoldi-based Krylov solvers, in particular the Full Orthogonalization Method (FOM)\nand the General Minimum RESidual Method (GMRES) [21].\nThe number of iterations required by the Krylov subspace solvers for convergence can be\nquite large, particularly for problems arising from realistic applications. To reduce the number\nof iterations, we use right preconditioners of the form K\u001cdef= (K+\u001cM) factorized and inverted\nusing a direct solver. For preconditioners of this form and for k= 1:::Nz=2 it can be readily\nveri\fed that,\n(K+zkM)(K+\u001cM)\u00001=I+ (zk\u0000\u001c)M(K+\u001cM)\u00001(12)\n5It can be shown that Km(MK\u00001\n\u001c;b) is equivalent to the Krylov subspace Km((K+zkM)(K+\n\u001cM)\u00001;b). This property is known as the shift-invariant property. The algorithm to solve\nmultiple shifted systems using a single shift-and-invert preconditioner is as follows: a basis for\nthe Krylov subspace Km(MK\u00001\n\u001c;b) is obtained by running msteps of the Arnoldi algorithm\non the matrix MK\u00001\n\u001cwith the starting vector b. At the end of msteps, the following Arnoldi\nrelationship holds,\n(K+zkM)K\u00001\n\u001cVm=Vm+1\u0012\u0014\nI\n0\u0015\n+ (zk\u0000\u001c)\u0016Hm\u0013\n|{z}\ndef=\u0016Hm(zk;\u001c)=Vm+1\u0016Hm(zk;\u001c) (13)\nwhereVm+1= [v1;:::;vm+1] is the Krylov basis and \u0016Hmis a (m+ 1)\u0002mmatrix formed by\nthe Arnoldi process. We use the notation Hmto denote the corresponding upper Hessenberg\nmatrix, i.e. Hm= [I;0]\u0016Hm. Having constructed the Arnoldi basis, the solution for each shift is\nobtained by searching for solutions xm(zk)2span\b\nK\u00001\n\u001cVm\t\n, written as x(zk) =K\u00001\n\u001cVmym(zk)\nwhere the vectors ym(zk) can be obtained by either by imposing a Petrov-Galerkin condition, i.e.\nrm(zk)?Vmwhich leads to the Full Orthogonalization Method for shifted systems (FOM-Sh)\nor by minimizing the residual norm krm(zk)k2over all possible vectors in the span of K\u00001\n\u001cVm\nwhich leads to the GMRES method for shifted systems (GMRES-Sh). The residual is de\fned as\nrm(zk)def=b\u0000(K+zkM)xm=Vm+1\u0000\n\fe1\u0000\u0016Hm(zk;\u001c)ym(zk)\u0001\nNumerical evidence in [22] showed that a single preconditioner might not be su\u000ecient to\ne\u000bectively precondition all systems, especially if the values of zkare spread out relative to the\nspectrum of the matrices. This is because a single preconditioner only adequately preconditions\nsystems with zkclose to\u001c. In order to improve the convergence rate across all the shifts, we\nproposed a Flexible GMRES/FOM solver [22] for shifted systems that allows us the \rexibility\nto choose di\u000berent preconditioners (of the shift-and-invert type) at each iteration. This requires\nstoring an additional set of vectors Um= [u1;:::;um]. Consider the use of a preconditioner\nat each iteration of the form K+\u001cjMwherej= 1;:::;m andjis is the iteration number.\nSystems with shifts zkclose to\u001cjconverge faster. The procedure to solve the shifted systems of\nequations follows similarly to the single preconditioner case with the following modi\fed Arnoldi\nrelationship [22],\n(K+zkM)Um=Vm+1\u0012\u0014I\n0\u0015\n+\u0016Hm(zkIm\u0000Tm)\u0013\n|{z}\ndef=\u0016H(zk;Tm)=Vm+1\u0016Hm(zk;Tm) (14)\nwhereTm= diagf\u001c1;:::;\u001cmg. The columns of the matrix Umnow no longer span a Krylov\nsubspace but rather a rational Krylov subspace. As in the case of a single preconditioner K\u00001\n\u001c, the\napproximate solution is constructed xm(zk) =Umym(zk), so thatxm(zk)2spanfUmg. Again,\nthe coe\u000ecients ym(zk) are either computed by using a Petrov-Galerkin condition, leading to the\n\rexible FOM for shifted systems (FFOM-Sh) or by minimizing the residual, leading to \rexible\nGMRES for shifted systems (FGMRES-Sh). The expensive step of computing the matrix Um\nis shared across all the systems and the cost of solving smaller subproblem (either by a Petrov-\nGalerkin projection or a residual minimization) for each shift is negligible compared to the cost\nof generating Um. The algorithm is presented in detail in Algorithm 1.\nIn Algorithm 1, the use of a di\u000berent preconditioner shift at each iteration would be too\nexpensive since a di\u000berent preconditioner ( K+\u001cjM) has to be factorized at each iteration.\nNumerical evidence shows that this is unnecessary since we need only pick a few systems with\nshifts\u001csuch that they e\u000bectively precondition the systems over the entire range of shifts. The\nrule of thumb is that systems with shifts zkclose to\u001cconverge \frst. In our previous work [22], we\nconsidered the solution of shifted systems with 200 shifts that were on the pure imaginary axis.\nWe observed that the systems with shifts closer to the origin converged more slowly. Therefore,\nto speed convergence, we chose 5 preconditioners with shifts \u001cthat were evenly spaced on a\nlog scale. The choice of shifts and number of preconditioners is entirely application dependent.\nFor the case of the shifts determined by the modi\fed Talbot contours as shown in Figure 1,\nnumerical experiments in Section 4 show the use of two preconditioners produces more favorable\n6Algorithm 1 Flexible FOM/GMRES for shifted systems\n1:GivenK,M,b,f\u001c1;:::;;\u001cmg,fz1;:::;zNz=2gand a tolerance tol,\n2:v1=b=\f,k= 1 and\fdef=kbk2\n3:for allj= 1;:::; maxit do\n4: Solve (K+\u001cjM)uj=vj\n5:sj:=Muj\n6:for alli= 1;:::;j do\n7:hij:=s\u0003\njvi\n8: Computesj:=sj\u0000hijvj\n9:end for\n10:hj+1;j:=ksjk2. Ifhj+1;j= 0 stop\n11:vj+1=sj=hj+1;j\n12: De\fneUj= [u1;:::;uj],Vj+1= [v1;:::;vj+1] and \u0016Hj=fhi;lg1\u0014i\u0014j+1;1\u0014l\u0014j\n13: ConstructTj= diagf\u001c1;:::;\u001cjg\n14: for allk= 1;:::;Nz=2do\n15: If, not converged\n16: Construct \u0016Hj(zk;Tj) =\u0014\nI\n0\u0015\n+\u0016Hj(zkIj\u0000Tj)\n17: FOM:\nHj(zk;Tj)yfom\nj=\fe1;whereHj(zk;Tj) = [I;0]\u0016Hj(zk;Tj)\n18: GMRES:\nygm\njdef= min\nyj2Cjk\fe1\u0000\u0016Hj(zk;Tj)yjk2\n19: Construct the approximate solution as xj(zk) =Ujyj(zk)\n20: end for\n21: If all systems have converged, exit\n22:end for\nresults compared to only a single preconditioner. The shifts are chosen as follows: the \frst\nshift corresponds to zkwith the smallest real part called \u0016 \u001c1, and the second shift corresponds\nto the shift with the largest real part called \u0016 \u001c2. The sequence \u001cjforj= 1;:::;m is constructed\nas follows. We choose \u001cj= \u0016\u001c1forj= 1;2;3 and\u001cj= \u0016\u001c2forj= 4;5. This procedure is\nrepeated until all the systems converge to the desired user-de\fned tolerance. This choice of\npreconditioners results in fast convergence for the range of parameters we have explored. Future\nwork will focus on the optimal choice of preconditioners.\n3.1 Properties of approximation space\nIn this section, we analyse the convergence of the \rexible Krylov subspace approximation to the\ndiscretized representation of the contour integral. The error of the Krylov subspace approach in\nEquation (14) is de\fned by [19, 17] as\nemdef=NzX\nk=1wk\u0000\n(K+zkM)\u00001b\u0000xm\u0001\n=NzX\nk=1wk\u0000\n(K+zkM)\u00001b\u0000UmH\u00001\nm(zk;Tm)\fe1\u0001\n(15)\nwherexm=UmH\u00001\nm(zk;Tm)\fe1is the approximate projected solution. Similarly, we can de\fne\nthe residual vector\nrmdef==sumNz\nk=1wk(b\u0000(K+zkM)xm) =NzX\nk=1wk\u0000\nb\u0000(K+zkM)UmH\u00001\nm(zk;Tm)\fe1\u0001\n(16)\nIt can be readily veri\fed that the approximate solution xmdef=P\nkwkUmH\u00001\nm(zk;Tm)\fe1lies in\nthe subspace span fUmgwhere\nspanfUmg= spanfv1;T1v1;:::; (Tm\u00001\u0001\u0001\u0001T1)v1g;withTk= (K+\u001ckM)\u00001(17)\n7To simplify the analysis, let us make the following change of variables: b M\u00001=2b,A \nM\u00001=2KM\u00001=2andx M1=2x. This variable change is possible since Mis a mass matrix and\ntherefore, positive de\fnite.\nThe projective space Umis independent of the shift zkand is not a typical Krylov subspace\nbut rather a rational Krylov subspace. Krylov subspaces generate sequences of vectors that are of\npolynomial form, i.e., vm+1=pm(A)v1wherep(\u0001) is a polynomial. On the other hand, rational\nKrylov subspace generates iterates that can be expressed as rational functions of the matrix, i.e.\nvm+1=ym\u00001(A)zm(A)\u00001v1, whereym\u00001is a polynomial of degree at most m\u00001 andzmis a\n\fxed polynomial of degree m. The rational Krylov method was originally proposed by Ruhe [20]\nin the context of approximating interior eigenvalues, with which appropriately chosen shifts\ncould potentially accelerate convergence of the eigenvalues in the desired region of the spectrum.\nThey have been quite successful in the context of model reduction, in order to approximate the\ntransfer function for a dynamical system [3]. The rational approach requires either knowledge\nof the shifts \u001cka priori, or this needs to be computed on-the-\ry. In this work, our choice of\nshifts is done a priori and is based on heuristics that perform well in the applications, as we will\ndemonstrate. In the context of H2-optimality reduction, an automated choice of shifts has been\nproposed in [3]. The following result characterizes the properties of the error due to the \rexible\nKrylov approach and derives an expression for the rational function that de\fnes the rational\nKrylov subspace.\nProposition 1. The erroremsatis\fes\nem2span\b\n(A+z1I)\u00001vm+1;:::; (A+zNzI)\u00001vm+1\t\n(18)\nand can be expressed in terms of a rational function as\nem=r(A)v1r(\u0015)def=NzX\nk=1wk\n\u0015+zkdet(Gm\u0000\u0015Hm)Qm\nj=1(\u0015+\u001cj)(19)\nwhereGm=I\u0000HmTmFurther, the error can be bounded as\nkemk2\u0014Nz=2X\nk=1jwkjk(A+zkI)\u00001vm+1k2j\u0011kj (20)\nwhere\u0011kdef=hm+1;m(zk\u0000\u001cm)e\u0003\nmH\u00001\nm(zk;Tm)\fe1.\nProof. Let us begin by writing\nem=NzX\nk=1wk(A+zkI)\u00001\u0000\nb\u0000(A+zkI)UmH\u00001\nm(zk;Tm)\fe1\u0001\nUsing the modi\fed Arnoldi relation in equation (14)\nb\u0000(A+zkI)UmH\u00001\nm(zk;Tm)\fe1=b\u0000Vm+1\u0016Hm(zk;Tm)Hm(zk;Tm)\u00001\fe1=\u0000vm+1\u0011k(21)\nsinceb=Vm\fe1and\u0011kas de\fned above. Therefore, we have\nem=\u0000NzX\nk=1wk\u0011k(A+zkI)\u00001vm+1\nThis proves the part of the proposition in Equation (18) the Arnoldi relations Um=Vm+1\u0016Hm\nandAUm+UmTm=Vm. Eliminating Umwe haveAVm+1\u0016Hm+Vm+1\u0016HmTm=Vm. The general\nrecurrence for the vector vmcan be written as\n(A+\u001cmI)vm+1hm+1;m=vj\u0000mX\nj=1(Avjhjm+vjhjm\u001cm)\nThe individual vectors ( A+zkI)\u00001vm+1can be expressed in terms of rational functions and n\nconvergence this yields hm+1;m= 0 following the arguments of [20] we get that\nvm+1=r(A)v1wherer(\u0015)def=I\u0000HmTm\u0000\u0015Hm\n(\u0015+\u001c1)\u0001\u0001\u0001(\u0015+\u001cm)\n8Plugging in this expression into Equation (19) we get the desired result. The inequality for the\nerror follows from the properties of vector norms.\nWe also have that the residual satis\fes rm=\u0000PNz\nk=1wk\u0011kvm+1and therefore,krmk2\u0014P\nkjwkjj\u0011kj. The quantityj\u0011kj=jhm+1;m(zk\u0000\u001cm)e\u0003\nmH\u00001\nm(zk;Tm)\fe1jis established as an a\nposteriori estimate of error of the \rexible Krylov approach [22]. The result in Equation (19)\nsuggests that an e\u000ecient choice of shifts \u001ckfor the preconditioners may result in a smaller error\nemaftermiterations. In [22] analysis is provided for when the a posteriori measure is small\nand showed that it is related to the convergence of the eigenvalues of the matrix KM\u00001(or\nalternatively, M\u00001=2KM\u00001=2). Although we do not have an automated procedure to determine\nthe shifts, empirical evidence supports that our heuristic choice of preconditioner shifts \u001ckleads\nto fast convergence for a wide range of parameters of the underlying PDE. This is discussed in\nthe following section.\n4 Numerical Experiments\n4.1 Problem set-up\nIn this section we demonstrate using numerical examples the computational gains of the \rexible\nKrylov solver and its robustness to changing various parameters in the forward problem. We\nconsider a 2D depth-averaged aquifer with horizontal con\fning layers in a square domain sat-\nisfying the di\u000berential equation (2) with zero Dirichlet boundary conditions on all boundaries.\nThe equations are discretized using standard linear \fnite elements implemented using FEniCS\n[14, 15, 16] using Python as the user-interface. The domain size is [0 ;L]2whereL= 100 [m].\nFor our numerical experiments, we consider two transmissivity \felds : the \frst is a random\n\feld generated using an exponential covariance kernel, i.e.\n\u0014(x;y) =\u0012exp(\u0000r) (22)\nwithr=kx\u0000yk2=L. For the second transmissivity \feld we use a scaled version of Franke's\nfunction [4] which is a smooth function obtained by the weighted sum of four exponentials.\nThe natural log of the \felds are displayed in Figure 2. The exponential \feld is rougher than\nthe Franke \feld and as a result we expect the iterative solver to work hard to converge to the\ndesired tolerance. In both cases, the mean transmissivity was chosen to be \u0016K= 10\u00004[m2/s]\nand in these examples storativity was chosen to be constant with Ss= 10\u00005[-]. We consider\none pumping source located at (50 ;50) pumping at a constant rate of 0 :85 L/s.\nWe compare the results of two di\u000berent solvers - `Single' corresponds to the Krylov solver\nwith a single preconditioner ( K+\u001cM) where,\u001c= arg min Real( \u001bk) fork= 1;:::;Nz, i.e. the\nshift corresponding to the smallest real part and `Flexible' using the \rexible Krylov approach\ndescribed in section 3. The procedure to choose the shifts for the preconditioners is also described\nthere.\n4.2 Results\nTo demonstrate the robustness of our solver, we test it with respect to di\u000berent parameters -\nvariance of the \feld, number of times at which the solution needs to be computed and the number\nof grid points in the domain. First, we vary the variance of the log transmissivity \feld and note\nthat higher variances correspond to more ill-conditioned systems. We report the number of\niterations taken and the CPU time required for the solvers to converge to a relative tolerance\nof 10\u000010. The number of shifts Nzwas chosen to be 40 and therefore the number of systems\nrequired to be solved is Nz=2 = 20. The results are displayed in Table 1. The last column in the\ntable is an estimate of the maximum condition number of K+\u001bMacross all shifts. This is a\nlower bound to the condition number in the 1-norm [7, 8]. We see that the \felds corresponding\nto larger variances have more ill-conditioned systems. Since Franke \feld is smoother than the\nrandom \feld, it is expected that the resulting linear systems are less ill-conditioned and therefore,\nit would take fewer number of iterations to converge to the desired tolerance. Furthermore, the\nnumber of iterations required by both `Single' and `Flexible' preconditioning increases with\nincreasing variance, however the number of iterations using `Flexible' preconditioning does not\n9Figure 2: Example log transmissivity \felds generated by (left) sample drawn from a random\n\feld with exponential covariance kernel and (right) scaled version of Franke's function. The\nvariance for both \felds in the \fgure is 1 :6.\nFigure 3: Drawdown curve measured at location (70 ;70) with the source at (50 ;50) pumping at\na constant rate of 0 :85 L/s. The squares indicate sampling times 1, 5 and 20 min that were used\nin the results in Table 2.\nincrease signi\fcantly. By contrast, the use of `Single' preconditioning is insu\u000ecient because it\ndoes not adequately precondition all the systems and the cost can grow signi\fcantly.\nFranke Field\nVarianceSingle Flexible\u0014pseudoIter. CPU Time [s] Iter. CPU Time [s]\n0.8 73 8.6 40 4.6 1:2\u0002106\n1.6 95 13.0 48 5.8 1:5\u0002107\n3.5 125 21.3 54 7.1 1:3\u0002109\nRandom Field\nIter. CPU Time [s] Iter. CPU Time [s]\n0.8 89 11.6 45 5.5 1:1\u0002106\n1.6 126 20.1 53 6.8 9:9\u0002106\n3.5 139 28.9 54 7.3 9:8\u0002108\nTable 1: Iteration count and CPU time for solving the system of equations (11). The solution was\ncomputed at measurement time = 5 min. Two di\u000berent log transmissivity \felds were considered.\nAn estimate for condition number shows that the \felds with higher variance corresponded to\nlarger condition number. The proposed `Flexible' solver performs better than `Single', both in\nterms of iterations and CPU time.\n10Next, we consider the performance of the solvers in computing the solution at three di\u000berent\nmeasurement times. The number of iterations and the time required by the solver for solving for\nthe hydraulic head at various times is computed and displayed in Table 2. It is observed that the\nnumber of iterations required for the systems corresponding to the early times to converge are\nlarger, even though the condition number decreases. The reason for the increase in iterations can\nbe explained by examining the the contours obtained from di\u000berent measurement times (Figure\n1). It is the presence of the scaling factor 1 =tmultiplying the shifts zkthat cluster the shifts\nfor larger times, see Equation (7). The clustering of the shifts allows for faster convergence\nbecause it allows for better preconditioning (both `Single' and `Flexible') of all the systems since\nthe systems with shifts closer to \u001cwill converge more rapidly than those further away. This is\nconsistent with the analysis done in [22].\nFranke Field\nMeas. Time [min]Single Flexible\u0014pseudoIter. CPU Time [s] Iter. CPU Time [s]\n1 134 22.6 55 7.8 4:3\u0002106\n5 95 13.0 48 5.8 1:5\u0002107\n20 58 5.8 33 3.4 5:6\u0002107\nRandom Field\nIter. CPU Time [s] Iter. CPU Time [s]\n1 145 26.2 59 8.3 3:0\u0002106\n5 126 20.1 53 6.8 9:9\u0002106\n20 92 12 45 5.4 3:8\u0002107\nTable 2: Iteration count and time for for solving the system of equations (11). Two di\u000berent log\ntransmissivity \felds were considered and the solution was computed at di\u000berent measurement\ntimes. Here, variance of the log transmissivity was \fxed to be 1 :6. An estimate for condition\nnumber shows that the systems at earlier measurement time corresponded to larger condition\nnumber. The proposed `Flexible' solver performs better than `Single', both in terms of iterations\nand CPU time.\nThus far, we have only looked at solving for each time point independently, however our solver\nallows for simultaneously solving for the solutions at multiple times. We consider 40 uniformly\nspaced time points between 40 min and 60 min. These measurements correspond to `late-time'\nmeasurements (see Figure 3). These systems are solved simultaneously using `Flexible' precondi-\ntioning. To pick the preconditioner shifts, we \frst sort all the shifts arising from the discretized\ncontour from the 40 time instances by their real parts and we choose two preconditioners \u001c1\ncorresponding to the overall shift with the largest real part and \u001c2corresponding to the overall\nshift with the smallest real part. We observe that the maximum number of iterations was 27\nacross all shifts and all the 40 time sample points, which is roughly the same cost of solving for\none single time, i.e., the computational cost associated with solving the problem for multiple\ntimes is not signi\fcantly higher than the cost of solving the system of equations for a single time.\nMore investigation is needed to extend the solvers to be able to compute the entire time history\nsimultaneously.\nGrid sizeSingle Flexible\nIter. Time [s] Iter. Time [s]\n41296 3.6 49 1.5\n101295 13.0 48 5.8\n201294 52.8 45 25.1\n301292 260.2 44 116.2\n401291 611.7 44 268.1\nTable 3: Iteration count and time respectively for computing pressure \feld using increasing\nnumber of grid points. The log transmissivity \feld is Franke \feld with a variance of 1 :6.\nAll of the above experiments have been conducted on a grid of size 1012. To show the\n11independence of the iteration count with respect to the grid size, we consider grids ranging\nfrom 412to 4012. In all the experiments, the log transmissivity \feld is the Franke \feld with\na variance of 1 :6. Table 3 lists the iteration count of the solver with increasing grid size. For\nboth the solvers `Single' and 'Flexible', the iteration count is independent of the number of grid\npoints.\nIn summary, we can conclude that our solver is robust to various parameters such as \feld\nsmoothness and variance, as well as the computation of the solutions at di\u000berent measured times.\nAdditionally, for `late enough' times, the proposed \rexible solver further improves e\u000eciency as\nit allows for the solution of the resulting systems of equations simultaneously. Therefore, this\nsolver can be used within the framework of inverse problems to achieve signi\fcant speedup.\n5 Inverse Problem\n5.1 Geostatistical approach\nWe consider the geostatistical approach as described in [12, 13], which is one of the prevalent\napproaches for solving the inverse problem. The objective is to determine aquifer properties\n(here log transmissivity) given discrete head measurements. In the geostatistical approach the\nunknowns are modeled as Gaussian random \felds and the Bayesian approach is used to infer\nthe posterior probability density function of the unknowns as the product of two parts: the\nlikelihood of the measured data or the `data mis\ft' and the prior distribution of the parameters\nwhich represents the assumed spatial correlation of the parameters. Denote by s(x)2RNsthe\nvector corresponding to the discretization of the function to be estimated with s\u0018N (X\f;Q )\nwhereXis aNs\u0002pknown matrix, \farepunknown drift coe\u000ecients and Qis a covariance matrix\nwith entries Qij=\u0014(xi;xj) where\u0014is a generalized covariance kernel. The covariance kernel\ncontains information about the degree of smoothness of the random \feld and the correlation\nbetween two points. Popular choices of covariance kernels include the Mat\u0013 ern covariance family,\nwhich includes the exponential and Gaussian covariance kernels [34]. The measurement equation\nis expressed as,\ny=h(s) +\u000f \u000f\u0018N(0;R) (23)\nwherey2RNydenotes the hydraulic head measurements and \u000frepresents the measurement\nerror which includes both the error resulting from data collection and the errors in evaluating\nh(s). The matrices Q;R andXare structural parameters whose values can be optimized using a\nrestricted maximum likelihood approach. More details on the choice of these parameters can be\nfound in [12]. The best estimate is obtained by computing the maximum a posteriori estimate\n(MAP). This is equivalent to solving the following optimization problem,\narg min\n^s;^\f1\n2ky\u0000h(s)k2\nR\u00001+1\n2ks\u0000X\fk2\nQ\u00001\nwhere, the objective function to be minimized is the negative logarithm of the posterior prob-\nability density function p(s;\fjy). To solve the nonlinear optimization problem, Gauss-Newton\nalgorithm is used. The procedure is described in Algorithm 2.\nEvery iteration in algorithm 2 requires the computation matrices JkQJT\nkandQJT\nk. Storing\nthe dense covariance matrix Qwhile computing QJT\nkcan be expensive, both in terms of memory\ncosts for storage and computational costs, particularly when the number of grid points is large.\nFor covariance matrices that are translation invariant or stationary, the associated cost for storing\nQand computing matrix vector products QJT\nkcan be reduced. A Fast Fourier Transform\n(FFT) method can be used if the grid is regular. If the grid is not regular, the Hierarchical\nmatrix approach can be used [23]. The resulting computations are O(NyNslogNs) instead of\nO(NyN2\ns).\n5.2 Sensitivity computation\nAnother step that can be very expensive in the geostatistical method for inversion is the com-\nputation of the Jacobian, or sensitivity matrix. When traditional time stepping algorithms are\n12Algorithm 2 Quasi-linear Geostatistical approach\nwhile not converged do\nCompute the Ny\u0002NsJacobianJas,\nJk=@h\n@s\f\f\f\f\ns=sk(24)\nSolve the following system of equations,\n\u0012JkQJT\nk+R JkX\n(JkX)T0\u0013\u0012\u0018k+1\n\fk+1\u0013\n=\u0012y\u0000h(sk) +Jksk\n0\u0013\n(25)\nsk+1is computed by,\nsk+1=X\fk+1+QJT\nk\u0018k+1 (26)\nend while\nused for simulating the time-dependent forward problem, computing the sensitivity of the mea-\nsurement operator with respect to parameters to be inverted for is accomplished by the adjoint\nstate method. This involves cross-correlation of two \felds at the same time, one obtained by\nforward recursion and the other by backward recursion. Typically, the forward recursion is per-\nformed \frst, however the entire time history up to the desired time must be accessible during the\nbackward recursion. For small scale problems, one typically stores the entire time history. How-\never, for large-scale problems arising from \fnely discretized problems the storage requirements\nmay be so large that it may not be able to be stored in RAM so one must resort to disk stor-\nage, in which case memory access then becomes the limiting factor. A standard approach is to\nuse checkpointing [5, 27, 31] which trades computational complexity (by a factor logarithmic in\nnumber of steps) while reducing the memory complexity to a few state bu\u000bers (also logarithmic\nin number of steps).\nUsing the Laplace transform approach to compute the solution of the forward problem at time\ntavoids the expensive computation of the time history. In this section, we derive expressions\nfor computing the sensitivity of the measurements with respect to the parameters also using the\nLaplace transform. In addition to the solution of the forward problem \u001e(x;t), the algorithm\nrequires computing an adjoint \feld  (x;t) solving for which also leads to a shifted system of\nequations, and can be e\u000eciently done using the \rexible Krylov subspace method described in\nsection 3.\nWe now derive expressions for the sensitivity. Let Ssbe parametrized by sSand\u0014be\nparametrized by sK. Since we want the reconstruction of Ssand\u0014to be positive, it is common\nto consider a log-transformation Ss=esSand\u0014=esK. Consider the functional that we would\nlike to compute the sensitivity of\nI(t;sK;sS)def=Z\n\n\u000e(x\u0000xm)\u001e(x;t)dx\nwhich corresponds to a point measurement at the measurement location. We are interested in\ncomputing the sensitivity of the functional Iwith respect to the parameters sKandsS. We\nstart by making the following transformation\nI(t;sK;sS) =1\n2\u0019iZ\n\nZ\n\u0000ezt\u000e(x\u0000xm)^\u001e(x;z)dzdx\nTo compute the variation \u000eIwith respect to the functions sKandsS, we use the standard adjoint\n\feld approach. First, take the Laplace transform of Equations (2) and multiply throughout by\ntest functions ^ (x;z), then integrate by parts and apply appropriate boundary conditions to\nget, Z\n\nesKr^\u001e\u0001r^ dx+zZ\n@\nwSy^\u001e^ dx+zZ\n\nesS^\u001e^ dx=Z\n\n^q^ dx\n13Taking the variation \u000e^\u001e, we have\nZ\n\nesKr\u000e^\u001e\u0001r^ dx+zZ\n@\nwSy\u000e^\u001e^ dx+zZ\n\nesS\u000e^\u001e^ dx\n=\u0000Z\n\n\u0010\nesK\u000esKr^\u001e\u0001r^ +zesS\u000esS^\u001e\u0001r \u0011\ndx (27)\nThe adjoint \feld ^ is chosen to satisfy the following set of di\u000berential equations, which is similar\nto the Laplace transformed version of Equations (2) with the forcing term corresponding to a\nmeasurement operator at the measurement location xm.\n\u0000r\u0001\u0010\nesKr^ (x;z)\u0011\n+zesS^ (x;z) =\u0000\u000e(x\u0000xm) x2\n (28)\n^ (x;z) = 0; x 2@\nD\nr^ (x;z)\u0001n= 0; x 2@\nN\nesKr^ (x;z)\u0001n=\u0000zSy^ ; x 2@\nw\nSimilarly, we multiply equation (28) by \u000e^\u001eand integrate by parts to obtain,\nZ\n\nesKr\u000e^\u001e\u0001r^ dx+zZ\n@\nwSy\u000e^\u001e^ dx+zZ\n\nesS\u000e^\u001e^ dx=Z\n\n\u000e(x\u0000xm)\u000e^\u001e(x;z)dx\nEquating the right hand sides from the Equation (27) and the equation above, we get\n\u000eI=1\n2\u0019iZ\n\u0000Z\n\n\u000e(x\u0000xm)\u000e^\u001e(x;z)dzdx (29)\n=1\n2\u0019iZ\n\u0000Z\n\nezt\u0010\nesK\u000esKr^\u001e\u0001r^ +zesS\u000esS^\u001e\u0001^ \u0011\ndzdx (30)\nFinally, the integral w.r.t zcan be discretized using the same contour integral approach described\nin Section 2.2\n\u000eINz=NzX\nk=1wkZ\n\n\u0010\nesK\u000esKr^\u001ek\u0001r^ k+esS\u000esS^\u001ek\u0001^ k\u0011\ndx (31)\nAs an example of this calculation, we plot the sensitivity at t= 5 [min] (made dimensionless)\nwith respect to log transmissivity. The \feld here was chosen to be a constant \feld, hence the\nsymmetry observed between the measurement location (40 ;50) and the point source (50 ;50).\nThe results are shown for time t= 5 [min]. This is displayed in Figure 4.\n5.3 Application: Transient Hydraulic Tomography\nThe objective is to reconstruct log transmissivity of an aquifer given measurements of the hy-\ndraulic head at several observation locations. The governing equations are the groundwater\nequations for a 2D con\fned aquifer, as described in Section 4. The `true' \feld is chosen to be\nthat generated by the exponential kernel as shown in Figure 2 also provided in Equation (22).\nThe parameters RandXare chosen to be R= 10\u00007IandX= [1;:::; 1]Trespectively. We do\nnot investigate the optimality of these parameters, i.e. those that yield the best reconstruction,\nsince the goal was to demonstrate the performance of our proposed solver in solving inverse prob-\nlems. We additionally added model error by generating measurements using Crank-Nicolson to\nsolve the governing equations using the `true' log transmissivity \feld. We have three measure-\nment points: at t= 8;10 and 20 [min] and 36 measurement locations spaced evenly in the square\n[20;80]\u0002[20;80]. We introduce a 2% error in the measurements. The inverse problem is solved\nusing these measurements. Storativity was held constant at Ss= 10\u00005[-] and the domain was\na square domain of length L= 100 [m]. The single pumping source, located at (50 ;50), has\na pumping rate of 0 :85 L/s. We use the \rexible Krylov solver with Nz= 20 for solving the\nforward and adjoint problems on a problem with 101 \u0002101 grid points. The results are shown\nin Figure 5. The error in the relative L 2norm is 0:16 within the area of measurements, i.e. the\n[20;80]\u0002[20;80] m2box, and the total error in the relative L 2norm is 0:36 for the whole aquifer.\n14Figure 4: Sensitivity (made dimensionless) with respect to log transmissivity. The \feld here was\nchosen to be a constant \feld, hence the symmetry observed between the measurement location\n(40;50) and the point source (50 ;50).\nFigure 5: True transmissivity \feld (left) and the estimates transmissivity \feld (right). The\nnumber of grid points is 101 ;124. The error in the relative L 2norm is 0:11 within the area of\nmeasurements, i.e. the [20 ;80]\u0002[20;80] m2box, and the total error in the relative L 2norm is\n0:35 for the whole aquifer.\nFigure 6 shows the time required for the various solvers. `Direct' refers to solving each of the\nshifted systems using a direct solver, and FGMRES-Sh is the solver using 2 preconditioners per\ntime.Nzwas chosen to be 20. Note that the slope of the graph corresponding to Crank-Nicolson\nis dependent on the particular application and on the time step used. The e\u000eciency of Crank-\nNicolson relative to FGMRES-Sh depends on the time at which the solution is to be evaluated\nas well as the size of the time steps taken. For our application, the speedup using FGMRES-Sh\nas opposed to Crank-Nicolson to solve the forward problem was signi\fcant, as demonstrated in\nthe \fgure. The computational gain is more dramatic in the calculation of the Jacobian since\nthis solution is required for multiple sources and receivers.\nLet us assume that we have Nssources,Nmreceivers and collect NTtime measurements.\nThe total number of systems that need to be solved are NsNmNT. Assuming the application of\nthe preconditioner can be represented by \u0016(N) and the number of iterations using the Krylov\nsolvers are Niter. The total cost of computing the Jacobian is NmNsNiterNT\u0016(N). Using a\ntime-stepping scheme with Nttime steps (and therefore, the solution of Ntsystem of equations)\nsimilarly costs NmNsNt\u0016(N). The costs are comparable when Nt\u0018NiterNT. In Section 2.3 we\nthat for increasing accuracy, the number of systems to be solved increase only logarithmically\nfor Laplace transform-based methods whereas it has a square-root growth for time-stepping\nschemes. For a few time measurement points NTor when the number of time steps required\nby the time-stepping schemes is large, we expect that the Laplace transform-based methods are\n15Figure 6: Comparison between Crank-Nicolson and the Laplace Transform method using direct\nsolver and GMRES for shifted systems. Note that the slope of the line corresponding to Crank-\nNicolson depends on the time step that is used.\nmuch more e\u000ecient. This is also con\frmed by Figure 6.\n6 Conclusion and future work\nWe have proposed a fast method for solving large-scale nonlinear inverse problems based on\nparabolic PDE. While we have focused on THT, it can be applied to a general class of time-\ndependent PDEs, for which the Laplace transform can be applied. The resulting system of\nequations are solved e\u000eciently using a \rexible Krylov approach, previously used to accelerate\noscillatory hydraulic tomography. For small number of measurement times, our solver is compu-\ntationally more e\u000ecient than standard time-stepping schemes, especially when very small time\nsteps required for stability. We have applied the solver to synthetic problems arising in THT.\nSince the computation of the Jacobian is often the bottleneck in solving large-scale inverse prob-\nlems, our approach for computing the Jacobian based on the Laplace transform greatly improves\nthe storage and computational cost.\nThere are several avenues for future work. In our work, the measurement times are considered\nknown a priori and are few in number. However, the number of measurement times and the\nsampling rate can be chosen to better improve the accuracy of the reconstruction. For example,\nchoosing the sampling points as the roots of Laguerre polynomials may be used to improve the\naccuracy of the integral corresponding to the data mismatch. Regarding the \rexible Krylov\nsolver, theoretic insight into the properties of the rational Krylov subspace generated Umcan\nbetter guide the user to pick preconditioners to e\u000eciently precondition the shifted systems,\nincluding those corresponding to multiple times. It remains to be demonstrated, if based on\nour approach, we can develop a solver that can solve for the entire time history. Another\ncomputational bottleneck we would like to consider is the solution of the shifted system in\nEquation (11) with multiple right hand sides, corresponding to di\u000berent source and measurement\nlocations. One may either try block approaches, or recycling strategies for systems with multiple\nshifts and multiple right hand sides.\n7 Acknowledgments\nThis work was supported by NSF award 1215742, \\Collaborative Research: Fundamental Re-\nsearch on Oscillatory Flow in Hydrogeology.\" We would also like to thank James Lambers and\nAnil Damle for their careful reading of the manuscript and useful suggestions.\n16",
      "metadata": {
        "filename": "A Fast Algorithm for Parabolic PDE-based Inverse Problems Based on Laplace Trans.pdf",
        "hotspot_name": "PCBA_Production",
        "title": "A Fast Algorithm for Parabolic PDE-based Inverse Problems Based on\n  Laplace Transforms and Flexible Krylov Solvers",
        "published_date": "2014-09-09T00:23:41Z",
        "pdf_link": "http://arxiv.org/pdf/1409.2556v2",
        "query": "THT and SMD soldering PCBA production energy efficiency improvement quantitative efficiency gains"
      }
    },
    "Convolutional Recurrent Reconstructive Network for Spatiotemporal Anomaly Detect": {
      "full_text": "IEEE TRANSACTIONS ON CYBERNETICS 1\nConvolutional Recurrent Reconstructive Network\nfor Spatiotemporal Anomaly Detection\nin Solder Paste Inspection\nYong-Ho Yoo, Ue-Hwan Kim, and Jong-Hwan Kim, Fellow, IEEE\nAbstract —Surface mount technology (SMT) is a process for\nproducing printed circuit boards. Solder paste printer (SPP),\npackage mounter, and solder reﬂow oven are used for SMT.\nThe board on which the solder paste is deposited from the\nSPP is monitored by solder paste inspector (SPI). If SPP\nmalfunctions due to the printer defects, the SPP produces\ndefective products, and then abnormal patterns are detected\nby SPI. In this paper, we propose a convolutional recurrent\nreconstructive network (CRRN), which decomposes the anomaly\npatterns generated by the printer defects, from SPI data. CRRN\nlearns only normal data and detects anomaly pattern through\nreconstruction error. CRRN consists of a spatial encoder (S-\nEncoder), a spatiotemporal encoder and decoder (ST-Encoder-\nDecoder), and a spatial decoder (S-Decoder). The ST-Encoder-\nDecoder consists of multiple convolutional spatiotemporal memo-\nries (CSTMs) with ST-Attention mechanism. CSTM is developed\nto extract spatiotemporal patterns efﬁciently. Additionally, a\nspatiotemporal attention (ST-Attention) mechanism is designed\nto facilitate transmitting information from the ST-Encoder to the\nST-Decoder, which can solve the long-term dependency problem.\nWe demonstrate the proposed CRRN outperforms the other\nconventional models in anomaly detection. Moreover, we show\nthe discriminative power of the anomaly map decomposed by\nthe proposed CRRN through the printer defect classiﬁcation.\nIndex Terms —Attention mechanism, convolutional LSTM, one-\nclass anomaly detection, recurrent auto-encoder, spatiotemporal\ndata, surface mount technology.\nI. I NTRODUCTION\nSmart factories represent fully connected and self-\noptimizing systems, which evolve from traditional automation\nfactories. Smart factories collect the state data of sub-systems\nusing IoT sensors and control sub-systems analyzing the\ncollected data in real time. One of key technologies in smart\nfactories is Surface Mount Technology (SMT) for automatic\nPrinted Circuit Board (PCB) production. SMT in general\nconsists of ﬁve steps: 1) solder paste printing, 2) solder paste\ninspection, 3) mounting chips, 4) chip alignment inspection\nand 5) reﬂow oven to attach chips to solder pastes. The process\nalternates between a task step and an inspection step, which\nis to guarantee a high throughput.\nThis work was supported by the Industrial Strategic Technology Devel-\nopment Program (10077589, Machine Learning Based SMT Process Opti-\nmization System Development) funded By the Ministry of Trade, Industry &\nEnergy (MOTIE, Korea).\nY .-H. Yoo is with the School of Electrical Engineering, KAIST, Daejeon,\n34141, Republic of Korea, also with Koh Young Technology, Inc., Yongin-si,\nGyeonggi-do, 16864, Republic of Korea (e-mail: yh.yoo@kohyoung.com).\nU.-H. Kim, and J.-H. Kim are with the School of Electrical Engi-\nneering, KAIST, Daejeon, 34141, Republic of Korea (e-mail: fuhkim,\njohkim g@rit.kaist.ac.kr).According to a recent analysis, 50-70% of PCB defects\noccur during the solder printing step [1]. The solder paste\nprinter (SPP) causes defects due to two factors: randomness\nand defects in the printer itself. SPI detects the PCB defects\nin advance and prevents a decrease in the throughput. SPI\ninspects each PCB by measuring the volume of the solder\npaste on each PCB pad. If the deposited solder paste is\nexcessive, two or more pads are connected through the solder\npaste causing solder bridging. Conversely, if the deposited\nsolder paste is insufﬁcient, the reliability of the solder joint\ndeteriorates or electrical opens could appear. Therefore, SPP\nis designed to deposit solder paste on each pad within the\nspeciﬁcation limit of the solder paste volume. If the deposited\nvolume of the pad is out of the speciﬁcation limit, SPI judges\nthe pad as an excessive or insufﬁcient one.\nTraditionally, the decision thresholds for excessiveness and\ninsufﬁciency are determined with the assumption that the\ndeposited volumes on the pads with the same aperture shape\nfollow a normal distribution. With the assumption, SPI groups\nthe deposited pads by the same aperture shape, and calculates\nthe mean and variance of the volumes per each group. Then,\nthe pads whose solder paste volumes are far from the mean\nare regarded as anomaly pads. This approach, however, can\nhardly identify the PCB defects caused by the printer defects.\nThe printer malfunction increases the ratio of the anomaly\npads, which makes the normal distribution biased.\nTo overcome the above-mentioned limitation, we propose\na Convolutional Recurrent Reconstructive Network (CRRN)\nthat detects anomaly pads without the normal distribution\nassumption. CRRN, as a type of convolutional recurrent au-\ntoencoder (CRAE), consists of a spatial encoder (S-Encoder),\na spatiotemporal encoder and decoder (ST-Encoder-Decoder),\nand a spatial decoder (S-Decoder). By training CRRN using\nonly normal data, CRRN can reconstructs normal data even\nif it receives anomaly data. Thus, the anomaly map can\nbe decomposed from the reconstruction error between the\nreconstructed data and the input data.\nTwo main features proposed in this paper characterize\nCRRN from conventional CRAEs: Convolutional Spatiotem-\nporal Memory (CSTM) and spatiotemporal attention (ST-\nAttention) mechanism. First, CSTM helps CRRN to utilize\nboth the spatial and temporal information whereas conven-\ntional CRAEs exploits only temporal information. A CSTM\ncell directly delivers spatial information to other CSTM cells\nwithout incrementing the number of parameters. Next, the\nST-Attention mechanism effectively handles long-term depen-arXiv:1908.08204v1  [eess.IV]  22 Aug 2019IEEE TRANSACTIONS ON CYBERNETICS 2\ndency problem by transmitting the ST-Attention map extracted\nin the encoder to the decoder in CRRN, which acts as a\nshortcut path between the encoder and decoder.\nThe main contributions of our work are as follows.\n1) We develop CSTM that boosts the performance of the\nproposed CRRN by exploiting both spatial and temporal\ninformation.\n2) We design the ST-Attention mechanism to deal with\nlong-term dependency.\n3) We formulate the spatiotemporal anomaly detection of\nthe SPI data using CRRN.\n4) We conduct a series of experiments and analyze the\nexperiment results thoroughly to verify the performance\nof CRRN in anomaly detection.\nThe rest of the paper is structured as follows. In Section\nII, we discuss previous works related to CRRN. Section III\ndelineates the characteristics of SPI data. Section IV presents\nthe proposed CRRN and Section V veriﬁes the performance\nof CRRN in anomaly detection. Concluding remarks follow\nin Section VI.\nII. R ELATED WORKS\nA. Anomaly Detection\nAnomaly detection can be classiﬁed into spatial anomaly\ndetection, temporal anomaly detection, and spatiotemporal\nanomaly detection depending on the data types it handles.\nFirst, spatial anomaly detection identiﬁes anomaly regions\nin the spatial data such as images. Recent studies for the\nspatial anomaly detection mainly utilize unsupervised learning\nalgorithms such as the generative adversarial network [2], and\nthe convolutional auto-encoder [3]–[7]. Application areas of\nspatial anomaly detection include medicine and manufactur-\ning. In the medical ﬁeld, anomaly detection highlights the\ndisease regions in medical images [2], [3]. Anomaly detection\nin manufacturing identiﬁes defective parts of products using\nthe product images [6], [7].\nNext, temporal anomaly detection identiﬁes the anomaly\nregions in the time-varying data. One-class SVM [8] and\nrecurrent auto-encoder (RAE) [9]–[13] are main algorithms\nused for the temporal anomaly detection. In the ﬁnance ﬁeld,\nresearchers have studied to minimize the damage caused\nby unexpected patterns. Unusual power consumption can be\npredicted by analyzing the current power consumption [9]. A\ncrime such as credit card fraud can be detected in advance\n[10]. In security, there are studies on intrusion detection using\nsystem logs or temporal network signals [8], [11], [12]. In\nhealth-care, modeling biological data such as Electrocardiog-\nraphy (ECG) signals can be utilized to detect the abnormal\nsignals of the body in advance. [13].\nSpatiotemporal anomaly detection identiﬁes the anomaly re-\ngions in the spationtemporal data. Recently, the combinations\nof RNN with CNN have been widely applied to extract both of\nthe spatial and temporal patterns [14]–[16]. Intelligent trafﬁc\nsystem utilizes the video streams for preventing accidents\nor predicting trafﬁc jams [17]. Video-surveillance monitors\ntemporal images and then detects the abnormal events [18]–\n[20]. In this paper, we focus on the spatiotemporal anomaly\ndetection of SPI data obtained during SMT process.\nFig. 1: Visualization of the 3D SPI data.\nB. SMT Optimization\nThe quality of the soldering can be enhanced by optimiz-\ning the key factors [21], [22]: squeegee pressure, squeegee\nspeed, and stencil cleaning interval. By analyzing the trend\nof soldering, the stencil cleaning interval can be automati-\ncally optimized [23], [24]. Similarly, the optimization of SPP\nparameters improves the quality of the end-products [25]. In\naddition, there are studies diagnosing the quality of soldering\nand identifying soldering defects [26]–[28]. Their studies are\nto diagnose soldering states of a single PCB. However, there\nare limits to applying to the SPP diagnosis, since the patterns\nof the SPP defects may appear over multiple PCBs. In this\npaper, we propose the CRRN that can decompose anomaly\nmaps using SPI data of multiple PCBs to diagnose the SPP\ndefects. In particular, we decompose the anomaly map caused\nby the printer defects.\nIII. C HARACTERISTICS OF SPI D ATA\nSeveral packages, mounted on the board, are ﬁxed to the\nboard with the deposited solder paste. Thus, the position of\nthe deposited solder paste should match the position of the\nmetal pad in the package. As shown in Fig. 1, the SPI data,\nmeasuring the volume of the solder paste, contains a spatial\npattern due to the different spatial locations of the packages\nand various shapes of pads. The solder paste volume of each\npad depends on the package type. Moreover, even in the same\npackage, the solder paste volume of each pad varies according\nto the relative position in the package.\nAlso, SPI data display a temporal pattern in addition to\nthe spatial pattern. The temporal pattern originates from two\nfactors: the stencil cleaning interval of SPP and the two\nsqueegee blades of SPP. Firstly, SPP regularly cleans up the\nresidual solder paste of the stencil to prevent anomaly pads.\nDuring mass production of PCBs using SPP, residual solder\npaste accumulates beneath the stencil and inside the stencil\napertures. The residual solder paste beneath the stencil causes\na solder smudging problem and the residual solder paste inside\nthe stencil apertures causes an aperture blockage problem.\nTo resolve the problems, the stencil cleaning is periodically\nperformed to remove the remaining solder paste and the\nperiodic cleaning generates the temporal pattern of the SPI\ndata.\nSecond, SPP utilizes two squeegee blades to maximize\nprinting efﬁciency. One blade deposits the solder paste by\nrolling solder paste on the stencil in the forward direction,IEEE TRANSACTIONS ON CYBERNETICS 3\n0 20 40 60 80 100\nTimestep405060708090Volume\n(a)\n0 20 40 60 80 100\nTimestep2\n02Normalized volume\n(b)\n0 20 40 60 80 100\nTimestep2\n1\n0123Normalized volume\n(c)\nFig. 2: (a) The proﬁles of the three arbitrarily selected solder paste\nvolumes over timestep, (b) the proﬁle of the normalized volume over\ntimestep, and (c) the proﬁle of the average normalized volume over\ntimestep. Timestep indicates the number in the order that the PCB\nproducts are produced.\nwhile the other blade deposits the solder paste in the backward\ndirection. The two blades perform the deposition of solder\npaste in a regular manner and show two distinctive character-\nistics. As a result, the SPI data display temporal patterns along\nthe two directions of the blades.\nFig. 2(a) shows the volume of three arbitrarily selected\npads over timestep that indicates the order that the PCB\nproducts are produced. The graph on the right side in the\nﬁgure shows the distributions of the solder paste volume\nfor each pad. Each distribution shows different distributions\ndue to different spatial features such as the pad location\nand the shape. The normalization of the solder paste volume\ndistribution eliminates the spatial pattern as shown in Fig. 2(b).\nFig. 2(c) plots the average of normalized distributions over\ntimestep. As shown in Fig. 2(c), the average of normalized\ndistributions has periodicity, and the period corresponds to the\nstencil cleaning interval. Moreover, the average of normalized\ndistributions moves up and down in order due to the two\ndirections of squeegee blades in SPP.IV. C ONVOLUTIONAL RECURRENT RECONSTRUCTIVE\nNETWORK\nIn this section, we describe the proposed Convolutional\nRecurrent Reconstructive Network (CRRN) for anomaly de-\ntection in spatiotemporal data. Fig. 3 displays the overall\narchitecture of CRRN. CRRN is a type of an encoder-decoder\nmodel. The encoder consists of a spatial encoder (S-Encoder)\nthat extracts spatial features from the spatiotemporal input,\nand a spatiotemporal encoder (ST-Encoder) that extracts spa-\ntiotemporal features from a sequence of the spatial features.\nSimilarly, the decoder of CRRN consists of an S-Encoder,\na spatiotemporal decoder (ST-Decoder) that decodes spatial\nfeatures of each timestep, and a spatial decoder (S-Decoder)\nthat reconstructs the original data.\nA. Spatial Encoder and Decoder\nThe S-Encoder shown in Fig. 4(a), extracts the spatial\nfeature, ^Xt2RNc\u0002Nh\u0002Nwfrom the original spatial data,\nXt2RNin\nc\u0002Nin\nh\u0002Nin\nw, whereNc(Nin\nc),Nh(Nin\nh), andNw\n(Nin\nw) respectively denote the numbers of channel, height,\nand width of ^Xt(Xt). The S-Encoder contains multiple\nmodules and each module consists of a convolution layer,\na batch normalization layer, and a ReLU activation layer.\nThe convolution layer converts high dimension feature maps\ninto low dimension feature maps. The batch normalization\nprevents the covariance shift by normalizing the values of each\nlayer [29]. The ReLU activation layer imposes non-linearity\nto enhance the modeling capability of CRRN.\nThe S-Decoder shown in Fig. 4(b), operates in the reverse\nmanner compared to the S-Encoder. The S-Decoder generates\nthe reconstructed spatial data, X0\nt2RNout\nc\u0002Nout\nh\u0002Nout\nwfrom\nthe low-dimensional spatial feature, ^X0\nt2RNc\u0002Nh\u0002Nw. The\ndimension of Xtis equal to that of X0\nt. The S-Decoder\nalso consists of multiple modules. Unlike the S-Encoder, the\nS-Decoder uses a deconvolutional operation known as the\ntransposed convolution instead of the convolutional operation.\nThe deconvolution reconstructs a high dimensional feature\nmaps from a low dimension feature maps. The last module\nof the S-Decoder does not include the ReLU activation layer,\nthus ensuring that the generated output has a valid range.\nB. Spatiotemporal Encoder and Decoder\n1) Convolutional Spatiotemporal Memory (CSTM): We de-\nvelop the proposed CSTM based on Convolutional LSTM\n(ConvLSTM) and Spatiotemporal LSTM (ST-LSTM). In con-\ntrast to the vanilla LSTM which can hardly extract spatial\nfeatures, the convolutional operation [14] enables the ConvL-\nSTM to extract both of the temporal and spatial information\nsimultaneously, while using less parameters than that of the\nLSTM. We describe the operation of ConvLSTM for the\nformulation of CSTM as follows:\nil\nt=\u001b(Wi\u0003Hl\u00001\nt+Ui\u0003Hl\nt\u00001+Wc\u000eCl\nt\u00001); (1a)\nfl\nt=\u001b(Wf\u0003Hl\u00001\nt+Uf\u0003Hl\nt\u00001+Wf\u000eCl\nt\u00001); (1b)\nCl\nt=fl\nt\u000eCl\nt\u00001+il\nt\u000etanh(Wg\u0003Hl\u00001\nt+Uc\u0003Hl\nt\u00001);(1c)\nol\nt=\u001b(Wo\u0003Hl\nt\u00001+Uo\u0003Hl\nt\u00001+Wo\u000eCl\nt); (1d)\nHl\nt=ol\nt\u000etanh(Cl\nt) (1e)IEEE TRANSACTIONS ON CYBERNETICS 4\nST-Encoder-Decoder\n܂܆\nST-Encoder\nST-Decoder\nCSTM CSTM CSTMCSTM CSTM CSTMSTA\nCSTMSTASTA\nSTASTA\nSTACSTMSTA\nSTA\nCSTMCSTM\n܆෡૚ ܆෡૛ ܆෡܆܂ ෡૜ ܆෡૛\n܆૚ ܆૛܆෡૚ᇱ܆෡૛ᇱ܆෡܂ᇱ܆૚ᇱ܆૛ᇱ ܂܆ᇱ\n܆૜ ܆૛ߠ்ி ߠ்ி1െߠ்ி 1െߠ்ிS-Decoder\nS-Encoder\nFig. 3: The overall structure of the proposed CRRN model. STA stands for ST-Attention.\nSpatial Encoder\nConvBNReLU\nConvBNReLU\n܆෡ܜ܆ܜ\n(a)\nSpatial Decoder\nTrans.\nConvBNReLU\nTrans.\nConvBN\nܜ܆ᇱ܆෡ܜᇱ\n(b)\nFig. 4: The structures of (a) the spatial encoder, and (b) the spatial\ndecoder.\nConvolutional Spatio-Temporal Memory\n  𝐂𝐭ି𝟏𝐥it\nft 𝐂𝐭𝐥gtSpatiotemporal cellOutput Gate Input Gate\nInput Modulation Gate\nForget Gate\n  𝐂𝐭𝐥ି𝟏\n𝐇𝐭𝐥ି𝟏\n𝐇𝐭ି𝟏𝐥\n𝐇𝐭𝐥ot\nFig. 5: Convolutional Spatiotemporal Memory.where the superscript landtdenote a layer and a timestep,\nrespectively, il\nt,fl\nt, andol\ntdenote an input gate, forget gate,\nand output gate, respectively, H0\ntis equal to ^Xt,\u0003is a\nconvolutional operator and \u000eis a Hadamard product. The\ncell gate,Cl\ntis a function of Cl\nt\u00001, which helps the ﬂow of\nthe temporal information. However, the Cl\ntdoes not rely on\nCl\u00001\ntdirectly, which causes a part of the spatial information\nof the input feature to be lost. Spatiotemporal LSTM (ST-\nLSTM) is designed to facilitate the ﬂows of the spatiotem-\nporal information [16]. The ST-LSTM has another cell gate\nresponsible for the spatial information ﬂow, as well as the\ncell gate,Cl\ntresponsible for the temporal information ﬂow.\nHowever, the ST-LSTM requires twice as many parameters as\nthe ConvLSTM.\nFig. 5 shows the proposed CSTM. The CSTM can effec-\ntively capture the spatiotemporal pattern while maintaining the\nsimilar number of parameters of the ConvLSTM. The CSTM\nreplaces the cell gate, Cl\nt2RNc\u0002Nh\u0002Nwin (1c), with the\nfollowing operation:\nCl\nt=fl\nt\u000eW1\u00021[Cl\nt\u00001;Cl\u00001\nt] +il\nt\u000egl\nt (2)\nwhere the two cell gates, Cl\u00001\nt andCl\nt\u00001, each with Nc\nchannels, are concatenated in a channel-wise manner. W1\u000212\nRNc\u00022Ncis the weight matrix of the one by one convolutional\noperation for reducing the number of channels back to half.\nThe one-by-one convolution, whose kernel size, Nkis 1,\nhas the advantage of adjusting the number of channels while\nkeeping the dimension of the feature map.\n2) ST-Encoder and ST-Decoder: The ST-Encoder-Decoder\npart of CRRN employs the proposed ST-Encoder and ST-\nDecoder. For readability, we denote the hidden states of\nthe ST-Encoder and ST-Decoder by El\nt2RNc\u0002Nh\u0002NwandIEEE TRANSACTIONS ON CYBERNETICS 5\nST-Attention in ST-Decoder ST-Attention in ST-Encoder\nܜ۳\n۳ ෠ܜ\nܜۯ\nܜۯ\nܜۯ\nܜۯ\nܘ܍ܚܜۯ\t\n۲෡ܜ\nܜ۲ \nܜۯ\nܜۯ\nܜۯ\nܘ܍ܚܜۯ\t\nFig. 6: ST-Attention mechanism in ST-Encoder and ST-Decoder.\nDl\nt2RNc\u0002Nh\u0002Nw, respectively, instead of Hl\nt. The hidden\nstate,El\ntin the ST-Encoder is updated as follows:\nEl\nt=CSTM (El\u00001\nt;El\nt\u00001): (3)\nThe hidden state of the last timestep, El\nTis copied to the\nhidden state, Dl\nTas shown in Fig. 3. The hidden state, Dl\ntin\nthe ST-Decoder is updated as follows:\nDl\nt=CSTM (Dl\u00001\nt;Dl\nt+1) (4)\nwhere the hidden state of the top layer, DL\ntis denoted by ^Xt.\nFor training the ST-Encoder-Decoder model, we adopt the\nscheduled sampling [30] for feeding inputs. The scheduled\nsampling probabilistically selects between the generated out-\nput of the previous time step, X0\ntand the ground truth of the\nprevious time step, Xt. The scheduled sampling is formulated\nas follows:\nXin\nt=\u0012TFXt+ (1\u0000\u0012TF)X0\nt (5)\nwhereXin\ntis the input of the model, XtandX0\ntare ground\ntruth input and generated input, respectively and \u0012TF2f0;1g\nis a sampling mask that chooses between XtandX0\nt.\u0012TF\nfollows a Bernoulli distribution, P(\u0012TF) =\u000f\u0012TF(1\u0000\u000f)1\u0000\u0012TF\nwhere\u000fis the probability of selecting Xt. In the early state\nof learning, \u000fis set high and gradually decreased over the\ntraining epochs. The scheduled sampling technique can solve\nthe exposure bias problem by guiding the learning process\nsimilar to the test process.\nIn addition, we apply a denoising auto-encoder technique\nto prevent CRRN from acting like an identity function. We\nadd random noise to the inputs during the training phase.\nMoreover, we set a part of input elements to zero at the training\nphase. The denoising auto-encoder enhances the resilience of\nthe original data without the noises [31].\n3) Spatiotemporal Attention Mechanism: As the length of\nthe sequence increases, the number of hidden states to pass\nto reach from the hidden state of the encoder to the corre-\nsponding hidden state of the decoder increases. This causes\nlong-term dependency problem. To resolve the problem, we\npropose a spatiotemporal attention (ST-Attention) mechanism\nthat directly transmits the information of the corresponding\nhidden states between the encoder and decoder, regardless of\nthe sequence length. The ST-Attention mechanism extracts the\nST-Attention map that represents the feature of the hidden state\nof the encoder, and then reuses the ST-Attention map when\ngenerating the corresponding hidden state in the decoder. Since\nthe ST-Attention map acts as a bridge between the encoder\nand the decoder, the proposed ST-Attention mechanism can\neffectively solve the long-term dependency problem.The procedure of the ST-Attention mechanism is shown in\nFig. 6. The ST-Attention mechanism extracts the spatial feature\nofEtthat affects the next hidden state, Et+1. For this, we\ndeﬁne the ST-Attention map, At2R1\u0002Nh\u0002Nwthat is applied\nto the hidden state, Etbefore transmitting to next hidden state,\nEt+1as follows:\nAt=tanh(WA\u0003Et) (6)\nwhereWA2R1\u0002Nc\u0002Nk\u0002Nkrepresents the weight matrices\nof the convolutional operation and Nkdenotes the kernel size\nof the convolution. The hidden state, EtwithNcchannels\nis converted to the ST-Attention map, Atwith 1 channel. At\nis replicated in channel-wise manner to match the number of\nchannels with that of Et. The replicated ST-Attention map,\nArep\ntis applied to the hidden state, Etas follows:\n^Et=Et+Arep\nt: (7)\nIn the decoder, the ST-Attention map, Atobtained in the\nencoder is applied to the hidden states in the reverse way. We\nsubtractArep\ntfrom ^Dtand obtainDtas follows:\nDt=^Dt\u0000Arep\nt: (8)\nWe reuse the Atobtained from the encoder. Thus, Atacts as a\nshortcut path between the encoder and the decoder. The reason\nfor reversing the operation of the attention map in the encoder\nand decoder is to use the last hidden state of the encoder, ET\nas the initial hidden state of the decoder, DT.\nThe last hidden state, ETis a feature that contains global\ninformation of the input sequence. The longer the length\nof the sequence, the more difﬁcult for the ﬁxed-size hidden\nstate to encode the information of the input sequence due to\nincreased contexts. If the ST-Attention mechanism is applied,\nthe local information of each input can be efﬁciently transmit-\nted through the ST-Attention map, which makes the decoder\ndirectly access the corresponding ST-Attention map when\ngenerating each output. Since local information is captured\nin the ST-Attention map, global information can be captured\nintensively in the last hidden state of the encoder, Et.\nV. E XPERIMENTS\nWe evaluated the performance of the proposed CRRN\nfor the anomaly detection through three experiments. Two\nexperiments were conducted to verify the performance of\nthe anomaly map decomposition and one experiment was\nconducted to classify the SPP defects using the decomposed\nanomaly map. For the anomaly map decomposition, we used\ntwo layers for all of the S-Encoder, S-Deocder and ST-\nEncoder-Decoder. The number of channels in the input and\noutput layers were set to 2, and those of the rest layers were\nset to 64. The kernel size was set to 5\u00025for all convolu-\ntional layers. Under the experimental setting, we compared\nthe performance of the proposed CRRN with those of the\nstatistical method and the CRAE consisting of ConvLSTMs. In\naddition, we compared the performance without and with the\nST-Attention mechanism. We denoted CRRN and CRAE with-\nout the ST-Attention mechanisms as CRRN na and CRAEna,\nrespectively.IEEE TRANSACTIONS ON CYBERNETICS 6\nIn Experiment 1, we evaluated the performance for synthetic\nSPI data whose anomaly pads were generated by adding noises\nto randomly selected pads. In Experiment 2, real data obtained\nfrom the defective SPP were used to decompose the anomaly\nmap. In addition to the two experiments, we demonstrated the\ndiscriminative power of the anomaly map generated from the\nCRRN for a SPP defect classiﬁcation task.\nA. Experiment 1\nFig. 7(a) shows the process of generating the synthetic SPI\ndata including the anomaly pads. The synthetic SPI data, Xt\nin\nwas generated by adding a randomly generated anomaly map,\n\u000fgenerated\nt to the normal SPI data, Xnormal\nt as follows:\nXin\nt=Xnormal\nt +\u000fgenerated\nt\n=Xnormal\nt +Mlabel\nt\u0001MN(\u0016;\u001b2)\nt\n=Xnormal\nt +Mpad\nt\u0001Manomaly\nt\u0001MN(\u0016;\u001b2)\nt (9)\nwhere dimensions of Xin\nt,\u000fgenerated\nt ,Xnormal\nt ,Mlabel\nt,\nMN(\u0016;\u001b2)\nt ,Mpad\nt, andManomaly\nt are all RNin\nc\u0002Nin\nh\u0002Nin\nw.\n\u000fgenerated\nt is expressed as product of the Mlabel\nt and\nMN(\u0016;\u001b2)\nt .Mlabel\nt indicates whether each pad is an anomaly\nor not.Mlabel\nt is determined by the product of Mpad\nt and\nManomaly\nt .Mpad\ntindicates the map where the pads actually\nexist in the SPI data, and Manomaly\nt is a randomly generated\nbinary map. Each element in MN(\u0016;\u001b2)\nt is sampled from\nGaussian noise, N(\u0016;\u001b2).\nFig. 7(b) shows the process of detecting the anomaly pads\nthrough the statistical method. By assuming that the volumes\nof pads with the same shape follow the Gaussian distributions,\nthe pads whose volumes deviate from the mean are regarded as\nthe excessive or insufﬁcient pads. Fig. 7(c) shows the process\nof detecting the anomaly pads through the deep-learning\nbased model. Since the model is trained to reconstruct the\nnormal data even if abnormal data are fed into the model, the\nreconstructed output is regarded as the normal SPI data. The\ndecomposed anomaly map, \u000fdecomposed\nt is the reconstruction\nerror that is calculated by subtracting Xout\ntfromXin\nt.\nIn this experiment, we generated the random mask,\nMN(\u0016;\u001b2)\nt with\u0016=[5;\u00005]and\u001b=0:1to cover a broad\nrange of the noises. In addition to generating different scales of\nthe noises, we generated Manomaly\nt by sweeping the anomaly\nratio,Pa= [10;20;30;40;50]% to compare the performance\naccording to the anomaly pad ratio in the SPI data. From the\nanomaly map, \u000fdecomposed\nt , the pads whose values are larger\nthan the decision threshold are judged as the anomaly pads.\nBy sweeping over the decision thresholds, the precision-\nrecall (PR) curves of the statistical method and deep-learning\nbased methods (CRAE na, CRRNna, CRAE, and CRRN) are\nplotted in Fig. 8. The sub-ﬁgures in the ﬁgure from left to\nright show the PR curves according to the anomaly ratio,\nParanging from 10% to50%. The upper and lower rows in\nthe ﬁgure represent the PR curves of detecting the excessive\nand insufﬁcient pads, respectively. When Pais10%, the\nanomaly detection accuracy of the statistical method is similar\nto those of deep-learning based methods. As the anomaly ratio\n࢔࢏࢚ࢄࢊࢋ࢚ࢇ࢘ࢋ࢔ࢋࢍࣕ࢒ࢇ࢓࢘࢕࢔࢚ࢄൌ࢒ࢇ࢓࢘࢕࢔࢚ࢄ൅ࢊࢋ࢚ࢇ࢘ࢋ࢔ࢋࢍ࢚ࣕ\n(a)\n࢔࢏࢚ࢄ࢔࢏࢚ࢄ∈\t࢖࢛࢕࢘ࡳ ૚ ࡼ࢔࢏࢚ࢄ∈\t࢖࢛࢕࢘ࡳ ૚\n࢔࢏࢚ࢄ∈\t࢖࢛࢕࢘ࡳ ૛ ࡼ࢔࢏࢚ࢄ∈\t࢖࢛࢕࢘ࡳ ૛ࢊࢋ࢙࢕࢖࢓࢕ࢉࢋࢊ࢚ࣕ∈\t࢖࢛࢕࢘ࡳ ૚\nࢊࢋ࢙࢕࢖࢓࢕ࢉࢋࢊ࢚ࣕ∈\t࢖࢛࢕࢘ࡳ ૛\n(b)\n࢔࢏࢚ࢄModel\n࢚࢛࢕࢚ࢄ\nࢊࢋ࢙࢕࢖࢓࢕ࢉࢋࢊ࢚ࣕൌ࢔࢏࢚ࢄെ࢚࢛࢕࢚ࢄ\n(c)\nFig. 7: Experiment 1. (a) Anomaly pad generation for evaluating\nthe performance of anomaly detection. (b) Anomaly detection using\nthe statistical method. (c) Anomaly detection using the deep-learning\nbased method.\nincreases, however, the performance of the statistical method\ndrops sharply. The reason for this is described in the following.\nIn the statistical method, the pads are grouped according\nto the shape of pads. Fig. 9 shows the volume histogram of\nthe pads in one of the groups. The upper and lower graphs in\nthe ﬁgure indicate the histograms of the ground truth and the\ngenerated output, respectively. If the anomaly ratio is relatively\nsmall, the volumes of the anomaly pads are in the tails of\nthe entire volume distribution. However, as the anomaly ratio\nincreases, the anomaly pads, which were minority of the entire\nvolume, become dominant and the Gaussian distribution shifts\ntoward the anomaly pads. Statistical method assumes that\nnormal pads are dominant so that the pads located at the tails\nof the Gaussian distribution are regarded as the anomaly pads,\nwhich causes performance degradation as the anomaly ratio\nincreases. On the other hand, the deep-learning based methods\nnot only are superior to the statistical method, but also show\nthe improved performance. Among the deep-learning based\nmethods, the models with the ST-Attention mechanism are\nbetter than the models without the ST-Attention mechanism.\nMoreover, the accuracy of the proposed CRRN is higher than\nthat of CRAE. The results of Experiment 1 are summarized\nin Fig. 10 and Table 1.\nB. Experiment 2\nWe collected the anomaly SPI data, which were affected\nby the SPP defects that cause the SPP to malfunction. During\nsolder paste printing, the squeegee blade in the SPP passesIEEE TRANSACTIONS ON CYBERNETICS 7\n0.0 0.2 0.4 0.6 0.8 1.0\nRecall0.20.40.60.81.0PrecisionExcessive anomaly (Pa=10%)\nStatistical\nCRAE\\a\nCRRN\\a\nCRAE\nCRRN\n0.0 0.2 0.4 0.6 0.8 1.0\nRecall0.20.40.60.81.0PrecisionInsufficient anomaly (Pa=10%)\nStatistical\nCRAE\\a\nCRRN\\a\nCRAE\nCRRN0.0 0.2 0.4 0.6 0.8 1.0\nRecall0.20.30.40.50.60.70.80.91.0Excessive anomaly (Pa=20%)\n0.0 0.2 0.4 0.6 0.8 1.0\nRecall0.20.40.60.81.0Insufficient anomaly (Pa=20%)0.0 0.2 0.4 0.6 0.8 1.0\nRecall0.30.40.50.60.70.80.91.0Excessive anomaly (Pa=30%)\n0.0 0.2 0.4 0.6 0.8 1.0\nRecall0.30.40.50.60.70.80.91.0Insufficient anomaly (Pa=30%)0.0 0.2 0.4 0.6 0.8 1.0\nRecall0.40.50.60.70.80.91.0Excessive anomaly (Pa=40%)\n0.0 0.2 0.4 0.6 0.8 1.0\nRecall0.40.50.60.70.80.91.0Insufficient anomaly (Pa=40%)0.0 0.2 0.4 0.6 0.8 1.0\nRecall0.50.60.70.80.91.0Excessive anomaly (Pa=50%)\n0.0 0.2 0.4 0.6 0.8 1.0\nRecall0.50.60.70.80.91.0Insufficient anomaly (Pa=50%)\nFig. 8: Precision-recall curves for anomaly detection (Upper and lower graphs represent excessive and insufﬁcient pad detection results,\nrespectively.\n2.5\n 0.0 2.5 5.0010203040Number of pads\n(Ideal)Normal\nAbnormal\n2.5\n 0.0 2.5 5.0\nPa=10%010203040Number of pads\n(Statistical)Normal\nAbnormal2.5\n 0.0 2.5 5.0010203040\n2.5\n 0.0 2.5 5.0\nPa=20%0102030402.5\n 0.0 2.5 5.0010203040\n2.5\n 0.0 2.5 5.0\nPa=30%0102030402.5\n 0.0 2.5 5.0010203040\n2.5\n 0.0 2.5 5.0\nPa=40%0102030402.5\n 0.0 2.5 5.0010203040\n2.5\n 0.0 2.5 5.0\nPa=50%010203040\nFig. 9: The solder paste volume histograms of normal and abnormal pads on the ground truth (upper graphs), and on the generated output\n(lower graphs).\nTABLE I: F1scores according to anomaly ratio, Pa. The symbol\nna indicates that the ST-Attention mechanism is not applied.\n(a)F1score of excessive anomaly.\nPa 10% 20% 30% 40% 50%\nStatistical 0.78 0.74 0.67 0.58 0.59\nCRAE na 0.65 0.67 0.69 0.72 0.76\nCRRN na 0.64 0.69 0.73 0.76 0.79\nCRAE 0.77 0.80 0.83 0.84 0.86\nCRRN 0.77 0.80 0.83 0.85 0.86\n(b)F1score of insufﬁcient anomaly.\nPa 10% 20% 30% 40% 50%\nStatistical 0.79 0.75 0.67 0.57 0.59\nCRAE na 0.61 0.64 0.67 0.70 0.74\nCRRN na 0.62 0.67 0.72 0.75 0.78\nCRAE 0.75 0.79 0.82 0.84 0.86\nCRRN 0.76 0.80 0.82 0.84 0.86\n0.1 0.2 0.3 0.4 0.5\nPa0.650.700.750.800.85F1 score\nExcessive anomaly\nStatistical\nCRAE\\a\nCRRN\\a\nCRAE\nCRRN\n0.1 0.2 0.3 0.4 0.5\nPa0.600.650.700.750.800.85\nInsufficient anomalyFig. 10: The changes of F1score according to the changes of anomaly\nratio, Pa:\nover the stencil, which deposits the solder paste in the aperture\nof the stencil. Fig. 11 shows the anomaly SPI data generated\nby the ﬁve printer defects, which are described in detail below.IEEE TRANSACTIONS ON CYBERNETICS 8\n(a)\n (b)\n (c)\n(d)\n (e)\nFig. 11: The anomaly SPI data affected by the ﬁve printer defects that\ncause the SPP to malfunction. (a) squeegee blade defect, (b) support\ndefect, (c) removed area of solder paste, (d) solder no kneading,\nand (e) clamp defect. The arrow in (a) indicates the direction of the\nsqueegee blade defect. In (b), the solder paste in red represents the\nexcessively deposited solder paste as much as the board subsides due\nto the support defect.\n\u000fSqueegee blade defect : It happens when squeegee blade\nhas cracks. The cracked portion of the squeegee blade\nmakes the solder paste deposited more than other regions.\nIn the SPP, there are two types of squeegee blades with\ndifferent directions, forward and backward directions. We\ndeliberately created the squeegee blade defects only on\nthe forward squeegee blade.\n\u000fSupport defect : If the board is not maintained ﬂat during\nprinting, the solder paste is deposited excessively on the\nbroad region of the board.\n\u000fRemoved area of solder paste : Repeated printing is\nlikely to result in depositing the solder paste insufﬁ-\nciently. In the trail of the squeegee blade, a lot of solder\npaste is used for the dense parts of the apertures, so that\nthe solder paste of these parts is insufﬁciently deposited.\n\u000fSolder no kneading : Before printing, solder paste should\nbe well kneaded with solder powder and ﬂux. Insufﬁcient\nkneading produces insufﬁcient patterns over a large area.\nThe solder no kneading usually occurs at the early stage\nof production and gradually disappears.\n\u000fClamp defect : Clamp is used to ﬁx both sides of the\nboard. If the board is not properly secured, insufﬁcient\nor excessive solder paste is deposited to the edge of the\nboard.\nWe used the collected SPI data as a benchmark dataset,\nwhich is formulated as follows:\nXin\nt=Xnormal\nt +\u000fgenerated\nt\n=Xnormal\nt +Mlabel\nt\u0001f(t) (10)\nwhereMlabel\nt is the spatial anomaly area representing anomaly\npads in the SPI data. We had gradually increased the degree of\nthe defect in the SPP over timestep. An anomaly score, f(t)\nrepresents the degree of the defect respectively for squeegee\nblade defect, support defect, support defect, removal area ofsolder paste, solder no kneading, and clamp defect. f(t)is\napproximated as a scalar value for each time step as follows:\nf(t) =(\n\u001b(10t=T\u00005);iftis odd;\n0; otherwise,(11a)\nf(t) =\u001b(10t=T\u00005); (11b)\nf(t) =\u0000\u001b(10t=T\u00005); (11c)\nf(t) = 1\u0000\u001b(10t=T\u00005); (11d)\nf(t) =\u001b(10t=T\u00005)or\u0000\u001b((10t=T\u00005); (11e)\nwhereTis the total number of PCB products generated.\nThe effects of defects were not noticeable in early PCB\nproduction, and as the production progresses, the effects of\ndefects appeared in the SPI data. Thus, we approximated the\nanomaly score using the sigmoid function.\nFig. 12 displays the proﬁle of the anomaly score, f(t)for\neach defect type over timestep (upper graph of each subﬁgure).\nUsing the selected threshold of the highest F1score obtained\nfrom Experiment 1, we evaluated the recall for each timestep\n(lower graph of each subﬁgure). We compared CRRN with the\nstatistical method. The larger the anomaly score, the easier the\ndetection is, because the volumes of the anomaly pads become\nsigniﬁcantly different from those of the normal pads. Thus, the\nrecall ratio increases according to the increase of the anomaly\nscore. In the statistical model, however, despite anomaly score\nincreases, the recall tends to be saturated. This phenomenon is\ncaused by the shift of the Gaussian distribution as described in\nExperiment 1. On the other hand, the recall of CRRN tends to\nincrease as the anomaly score increases. Since CRRN judges\nthe anomaly by considering volumes of the whole pads, the\nperformance is superior to the statistical method that judges the\nanomaly by using volumes of pads belonging to each group.\nFig. 13 shows the results of anomaly map decomposition for\neach defect, and each sub-ﬁgure shows the original SPI data,\nthe reconstructed outputs, and the reconstruction errors for\neach row from the top. The anomaly pattern in the anomaly\nmap becomes clear with the increase of the anomaly score.\nC. Experiment 3\nThe anomaly maps decomposed through CRRN can be used\nas features for classifying the SPP defects. We performed the\ndefect classiﬁcation task regarding the ﬁve defects introduced\nin Experiment 2. The decomposed anomaly map was splited\ninto two channels, excessive and insufﬁcient channels. The\nexcessive channel was obtained by max pooling to preserve\nexcessive patterns. On the other hand, insufﬁcient channel was\nobtained through min pooling to preserve the insufﬁcient pat-\ntern. Each channel was binarized using the decision threshold\nused in Experiment 1. Fig. 14 shows the binarized channel\nfor each defect where the excessive channel and insufﬁcient\nchannel are marked in red and blue, respectively. Since the SPP\ncould have multiple defects, a combination of defect patterns\nmay exist in the anomaly map.\nFig. 15 shows a CNN model for the defect classiﬁcation.\nThe CNN model consists of convolutional layers for feature\nextraction and fully-connected layers for classiﬁcation. In the\nconvolutional layers, a pretrained Resnet-18 or Inception-v4IEEE TRANSACTIONS ON CYBERNETICS 9\n012Anomaly score\n(Excessive)\n0 20 40 60 80 100\nTimestep0.00.20.4Recall\n(Excessive)Statistical\nCRRN\n(a) Squeegee blade defect\n012Anomaly score\n(Excessive)\n0 20 40 60 80 100\nTimestep0.000.250.500.75Recall\n(Excessive)Statistical\nCRRN\n(b) Support defect\n2\n1\n0Anomaly score\n(Insufficient)\n0 20 40 60 80 100\nTimestep0.00.20.4Recall\n(Insufficient)Statistical\nCRRN\n(c) Removal area of the solder paste\n2\n1\n0Anomaly score\n(Insufficient)\n0 20 40 60 80 100\nTimestep0.00.20.40.6Recall\n(Insufficient)Statistical\nCRRN\n(d) Solder no kneading\n012Anomaly score\n(Excessive)\n0 20 40 60 80 100\nTimestep0.00.20.4Recall\n(Excessive)Statistical\nCRRN\n(e) Clamp defect\nFig. 12: Each sub-ﬁgure shows the proﬁle of the anomaly score over\ntimestep, f(t), and the recall of each defect over timestep for each\nrow from the top.\n20 40 60 80 100\nTimestep(a) Squeegee blade defect\n20 40 60 80 100\nTimestep\n(b) Support defect\n20 40 60 80 100\nTimestep\n(c) Removal area of the solder paste\n20 40 60 80 100\nTimestep\n(d) Solder no kneading\n20 40 60 80 100\nTimestep\n(e) Clamp defect\nFig. 13: Each sub-ﬁgure shows the original SPI data, the recon-\nstructed outputs, and the decomposed anomaly map obtained by\nsubtracting the reconstructed outputs from the original SPI data for\neach row from the top.IEEE TRANSACTIONS ON CYBERNETICS 10\n(a)\n (b)\n (c)\n(d)\n (e)\nFig. 14: Binary anomaly maps of (a) squeegee blade defect, (b)\nsupport defect, (c) removal area of solder paste, (d) solder no\nkneading, and (e) clamp defect.\nConvolutional\nlayersFC layers GAP Sigmoid BCE\nFig. 15: SPP defect classiﬁcation model.\nmodel was loaded and ﬁne-tuned. Since the size of the SPI\ndata depends on the type of the PCB, we applied a global\naverage pooling (GAP) [32] between the convolutional layers\nand the fully-connected layers. The number of neurons in the\ninput layer of the fully connected layers was 512 for Resnet-\n18, and 1,536 for Inception-v4. In both models, the numbers\nof neurons in the hidden layer and in the output layer were set\nto 100 and 5, respectively. For multi-label classiﬁcation, the\noutput of the fully connected layer passed through a sigmoid\nfunction and the binary cross entropy was adopted as the loss\nfunction.\nWe compared the performances of the classiﬁcation models\nbased on Resnet-18 and Inception-v4. The mean average\nprecision (mAP) and the exact match ratio (EMR) were used\nas performance metrics. The mAP is the mean of the average\nprecision for each defect. EMR is the ratio of the case where\nthe target and the output are completely matched. In the case\nof mAP, Resnet-18 showed 91:3%, and Inception-v4 showed\n93:8%. In the case of EMR, Resnet-18 showed 71:7%and\nInception-v4 showed 74:8%. The accuracy of Inception-v4 was\ngenerally superior to that of Resnet-18. Through the experi-\nments, it was veriﬁed that anomaly maps decomposed through\nCRRN can be used as features for classifying defects. Note\nthat the experiment was intended to verify the discriminative\npower of the anomaly map, and thus further work related\nto the classiﬁcation model can improve the accuracy of the\nclassiﬁcation.\nVI. C ONCLUSION\nIn this paper, we proposed the CRRN model to decom-\npose anomaly patterns of SPI data caused by the printer\ndefects that make the SPP malfunction. The CRRN consists\nof S-Encoder, ST-Encoder-Decoder, and S-Decoder. The ST-\nEncoder-Decoder consists of multiple CSTMs with the ST-\nAttention mechanism. CSTM has a spatiotemporal cell that cancapture both spatial and temporal patterns. We also designed\nthe ST-Attention mechanism to generate consistent outputs\nby dealing with the long-term dependency problem. The ST-\nAttention serves as a shortcut path for connecting the encoder\nand the decoder in CRRN. Since most of SPI data are normal,\nwe trained CRRN using only normal SPI data. Using the\ntrained CRRN, the anomaly map was decomposed from the\nSPI data with anomaly defects based on the reconstruction\nerror. To verify the performance of CRRN, we compared\nCRRN with the statistical method and other deep learning-\nbased methods. Through three experiments, we demonstrated\nthe superior anomaly detection performance of CRRN to other\nmethods. In addition, we proved that the anomaly map can be\napplied to the defect classiﬁcation of the SPP.",
      "metadata": {
        "filename": "Convolutional Recurrent Reconstructive Network for Spatiotemporal Anomaly Detect.pdf",
        "hotspot_name": "PCBA_Production",
        "title": "Convolutional Recurrent Reconstructive Network for Spatiotemporal\n  Anomaly Detection in Solder Paste Inspection",
        "published_date": "2019-08-22T05:18:41Z",
        "pdf_link": "http://arxiv.org/pdf/1908.08204v1",
        "query": "THT and SMD soldering PCBA production energy efficiency improvement quantitative efficiency gains"
      }
    },
    "DVQI_ A Multi-task_ Hardware-integrated Artificial Intelligence System for Autom": {
      "full_text": "DVQI: A Multi-task, Hardware-integrated Artificial Intelligence System for\nAutomated Visual Inspection in Electronics Manufacturing\nAudrey G. Chung1, Francis Li1, Jeremy Ward1, Andrew Hryniowski1,2,3, Alexander Wong1,2,3\n1DarwinAI Corp., Waterloo, ON\n2Vision and Image Processing Research Group, University of Waterloo\n3Waterloo Artificial Intelligence Institute, Waterloo, ON\n{audrey, francis, jeremy.ward, andrew, alex }@darwinai.ca\nAbstract\nAs electronics manufacturers continue to face pressure to\nincrease production efficiency amid difficulties with supply\nchains and labour shortages, many printed circuit board as-\nsembly (PCBA) manufacturers have begun to invest in au-\ntomation and technological innovations to remain compet-\nitive. One such method is to leverage artificial intelligence\n(AI) to greatly augment existing manufacturing processes. In\nthis paper, we present the DarwinAI Visual Quality Inspec-\ntion (DVQI) system, a hardware-integration artificial intelli-\ngence system for the automated inspection of printed circuit\nboard assembly defects in an electronics manufacturing envi-\nronment. The DVQI system enables multi-task inspection via\nminimal programming and setup for manufacturing engineers\nwhile improving cycle time relative to manual inspection. We\nalso present a case study of the deployed DVQI system’s per-\nformance and impact for a top electronics manufacturer.\nIntroduction\nAs electronics manufacturers aim to increase production\nefficiency amid difficulties with supply chains and labour\nshortages, many printed circuit board assembly (PCBA)\nmanufacturers have begun to invest in automation and tech-\nnological innovations to remain competitive. However, tra-\nditional automated inspection methods in the PCBA indus-\ntry are costly to setup, and are difficult to extend to the later\nsteps of the manufacturing process.\nPrinted circuit board assembly (PCBA) manufacturing is\nthe process of placing and soldering electronic components\non a printed circuit board (PCB). For a given board, this pro-\ncess is composed of many steps performed sequentially in a\nmanufacturing line. While exact steps may vary, at a high\nlevel a typical line will consist of:\n1. Solder application: solder is applied on the bare PCB\n2. Pick and place: surface mount devices (SMD) are placed\non the board\n3. Reflow oven: solder is melted and SMD components are\nsoldered onto the board\n4. Through-hole assembly: through-hole components are\nplaced onto the board\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: The proposed DVQI system in a conveyored con-\nfiguration for inline inspection between two transport con-\nveyors.\n5. Wave solder: through-hole components are soldered onto\nthe board\n6. Final assembly: connectors, wires, additional mechanical\ncomponents are assembled onto the board\n7. Box-build: if applicable, the board is assembled into its\nchassis\nAt any given step above defects may occur and make its way\nin to the final PCBA. Typically, a high-volume PCBA man-\nufacturer would use an automated optical inspection (AOI)\nsystem right after the reflow oven to catch potential defects\nwhen placing and soldering the SMT components. While\nthese AOIs can be effective at detecting defects, they mostly\nrely on traditional machine vision methods and hence are\ntimely to setup and prone to false positives. This makes AOIs\ndifficult to use in low-volume high-mix use cases, where dif-\nferent types of boards are frequently being made and the\nlarge overhead in programming the AOI per board becomes\ntoo costly. At the same time, AOIs are highly specialized in\ninspecting SMT components. The PCBA can become quite\ncomplex in the later stages after wave solder which the AOI\nis unable to inspect due to a large variation in electronic\ncomponent height, occlusions, and positioning.\nAs such, inspection for such high-variance, mixed-\nassembly PCBA scenarios are often conducted by hu-\nman operators manually. However, not only is this time-arXiv:2312.09232v1  [cs.CV]  14 Dec 2023Figure 2: An overview of the inspection workflow using the proposed DarwinAI Visual Quality Inspection (DVQI) system.\nconsuming it is also prone to varying levels of inspection\nperformance as the level of performance achieved by human\ninspectors can differ significantly (See et al. 2017; Klamklay\nand Bishu 1998). A study by Sandia National Laboratories\nfound that human inspectors miss 20% to 30% of defects\nacross multiple types of inspection tasks (See 2021).\nFor these reasons there is a need for a flexible system that\ncan inspect mixed-assembly PCBAs that is both highly ac-\ncurate while easy to setup. Recent advances in AI allows\nfor such a system, where deep learning can be applied to\nhelp solve various defect detection challenges with PCBs\nand PCBAs (Ling and Isa 2023; Kim et al. 2021; Zhang et al.\n2022; Westphal and Seitz 2021; Bhatt et al. 2021; Yang et al.\n2020). In this work we introduce DarwinAI Visual Qual-\nity Inspection (DVQI) system, a hardware-integrated artifi-\ncial intelligence system which fulfills the needs of mixed-\nassembly PCBA inspection through automated multi-task\nvisual inspection powered by deep learning. Figure 1 shows\nthe proposed DVQI system in a conveyored configuration\nbetween two transport conveyors.\nDVQI System\nThe proposed DarwinAI Visual Quality Inspection (DVQI)\nsystem is a hardware-integrated artificial intelligence system\ndesigned to automatically perform multiple visual inspec-\ntion tasks across a variety of printed circuit board assembly\ndefects in an electronics manufacturing environment. Fig-\nure 2 presents an overview of the inspection workflow using\nthe proposed DVQI system, where a PCBA is inspected for\ndefects via component detection and defect identification,\nand presented back to an operator for review.\nMore specifically, the DVQI system comprises an ultra-\nwide-field-of-view, extended depth-of-field imaging systemthat captures high-resolution images of assembled printed\ncircuit boards, and leverages a suite of high-performance\ndeep neural networks to conduct various automated inspec-\ntion tasks.\nThe DVQI system is configurable for both inline inspec-\ntion (i.e., integrated into a PCBA manufacturing line) and\nstand-alone inspection (i.e., operated by an inspector asyn-\nchronous from the manufacturing lines).\nThe DVQI system can be setup to inspect any assembled\nPCB configuration with minimal effort by simply insert-\ning a ground-truth assembled PCB (referred to as a golden\nboard) and initiating the automatic golden board learning\nprocess using a tailored component detection neural network\n(see the first subsection for details). Once set up, inspection\ncan be performed on subsequent PCBAs by capturing high-\nresolution images of each board. These images, along with\nthe reference images captured of the golden board, are then\nfed into a suite of customized deep neural networks trained\non multiple inspection tasks to automatically compare the\nsample PCBAs against the golden board to detect and iden-\ntify a plethora of possible defect types (see second subsec-\ntion for details). These detected defects are then automati-\ncally logged and processed by the system for accountability\nand reporting purposes.\nA human operator is then provided with alerts and visual-\nizations of all detected defects (where the defects are, what\ntype of defects they are, which board they belonged to, etc.)\nso that defective PCBAs can be scraped or reworked accord-\ningly, and the manufacturing line can be adjusted to prevent\nfurther defects. The human operator is also given the option\nto provide active feedback to the DVQI system on whether\nit has made the right decisions. Doing so allows the system\nto continuously learn and improve over time as the system\ncontinues to be used (see the third subsection).Figure 3: The DVQI system takes image(s) of a PCBA to be inspected (left) and automatically detects and creates bounding\nboxes around every electronic component (right) as well as identify their component type via a highly efficient component\ndetection deep neural network.\nFigure 4: The DVQI system minimizes the time required to\nprogram a new PCBA for inspection, and only requires the\ndimensions (length and width) of the PCBA during setup.\nMinimal Programming & Setup\nIn traditional visual inspection systems, each new PCBA de-\nsign needs to be manually programmed into the system in\norder for the system to be able to perform visual inspection\non that board. More specifically, an operator needs to manu-\nally draw a bounding box around each electronic component\non a golden board (a board with no defects), then program\nin a custom heuristic for determining whether that compo-\nnent is defective or not. Given the presence of hundreds to\nthousands of components on each PCBA, this programmingprocess can take as much as a day. To minimize the need\nto program a new PCBA board type and go from a day to\njust a few minutes, the proposed DVQI system automati-\ncally learns a profile of a golden board and enables visual\ninspection immediately after via the following approach. As\nshown in Figure 4, operators only need to input the dimen-\nsions (length and width) of the PCBA during the set up and\nprogramming of a new PCBA inspection.\nFirst, during the golden board learning process, an ef-\nficient deep neural network (Li et al. 2022) automatically\ndetects and creates bounding boxes around every electronic\ncomponent on the golden board. More specifically, the neu-\nral network possesses a one-stage double-condensing atten-\ntion condenser object detection architecture design achieved\nvia a generative neural network design strategy, named gen-\nerative synthesis (Wong et al. 2019), to automatically find\nthe optimal balance between operational efficiency require-\nments for real-time, high-throughput inference and compo-\nnent detection accuracy. We harness double-condensing at-\ntention condensers (Wong et al. 2023a) in the generative de-\nsign strategy as they allow for highly condensed feature em-\nbeddings to achieve selective attention in a highly computa-\ntional and efficient manner.\nFollowing automatic bounding box creation, a digital pro-\nfile is created for each component automatically based on its\ncomponent crop. This digital profile includes component de-\ntails, such as component type (e.g., SMD resistor, integrated\ncircuit, etc.), that are leveraged by the DVQI system to bet-\nter constrain the search space of defects during multi-task\ninspection. The use of these digital profiles of components\nallows for the elimination of the need to manually createa) Reference component b) Inspected component\nFigure 5: Examples of the different defects that can be de-\ntected by the proposed DVQI system via a highly efficient\nmulti-head deep neural network designed for multi-task in-\nspection. From top to bottom: 1) presence / absence of a\ncomponent, 2) skewed placement of a component, 3) wrong\ncomponent used, and 4) reversed polarity placement of a\ncomponent.\ncustom heuristics, while also significantly improving the ro-\nbustness of the system as manual heuristics have significant\nchallenges in handling a wide variety of variations in both\ndefect and non-defect scenarios.\nMulti-task Inspection\nDuring operational inspection the golden board for the ap-\npropriate PCBA is retrieved, and the digital profiles of\nall electronic components of the golden board are loaded\nto support the automatic visual inspection of the sample\nPCBA. More specifically, an efficient multi-head, double-\ncondensing attention condenser neural network, generated\nusing the aforementioned generative network architecture\ndesign strategy (Wong et al. 2019), is leveraged to per-\nform multiple inspection tasks on each electronic compo-\nnent to identify a variety of different types of defects. Here,\nsimilar to the component detection network, we also har-\nness double-condensing attention condensers (Wong et al.\n2023a) in the generative design strategy to achieve selective\nattention to improve representational capabilities, which is\nquite important especially for multi-task applications, while\nachieving high computational and architectural efficiency.\nSimilar to (Wong et al. 2023b) in terms of handling mul-\ntiple tasks, the proposed network shares a common double-\ncondensing attention condenser backbone for feature extrac-tion, with each network head responsible for detecting a\ndifferent defect: 1) presence / absence of a component, 2)\nskewed placement of a component, 3) wrong component\nused, and 4) reverse polarity placement of a component (see\nFigure 5 for examples of each defect).\nThe key advantage of leveraging a deep neural network\nfor automatically tackling such inspection tasks is that it not\nonly eliminates the need for manual programming of indi-\nvidual components, but also allows for: 1) significant im-\nprovements in robustness during inspection as it can bet-\nter handle high diversity and variations in defect and non-\ndefect scenarios, 2) it allows for the identification of defect\ntypes for a given component (while heuristics only allow for\nidentifying whether a defect occurs but not what type), and\n3) it can be improved over time as new training data and\nnew operator feedback on inspection decisions are collected.\nTo train the deep neural networks within the DVQI system\n(component detection and multi-task inspection), we pre-\npared a large-scale, carefully curated proprietary data com-\nprising of around half a million fully annotated electronic\ncomponents across a wide variety of component types and\ndefect types. Currently, the DVQI system has an inspection\naccuracy of 97.8%, with a false positive (or overkill) rate of\n1.7% and a false negative (or escape) rate of 3.7%.\nOnce all the defect types and locations are automatically\nidentified, they are displayed using a graphical user interface\n(GUI) for the operator to visualize and validate if needed.\nThe DVQI system allows the operators to easily adjust the\nperformance of the system during inspection by fine-tuning\nthe sensitivity per inspected component, while also enabling\nthe operators to provide feedback on whether the deep neural\nnetwork made the correct decision and leverage this infor-\nmation to improve the performance of the neural networks\nover time. Furthermore, the DVQI system also allows op-\nerators to add alternate components to standard components\nfound on the golden board that may be used by the manufac-\nturer during the PCBA manufacturing process. This is espe-\ncially important in recent years, with manufacturers switch-\ning to different functionally-equivalent electronic compo-\nnents from different vendors due to supply chain issues.\nThis on-site continuous learning strategy leverages human-\nin-the-loop improvements that lead to the system performing\nincreasingly better through use and allows the human oper-\nator to garner greater trust in the system over time.\nEase-of-Use Considerations\nThe DVQI system is first and foremost easy for operators\nto use. From setting up and programming the system to in-\nspect a new PCBA to visualizing and validating defects in\nreal-time to reviewing past inspections, operators can use\nthe graphical user interface (GUI) to help streamline their\ninspection workflow.\nThe DVQI system displays the inspection results and al-\nlows the operator to visualize and validate the detected de-\nfects as necessary. As shown in Figure 6, a list of all detected\ndefects is provided in the left panel of the GUI, detected\ndefects are clearly indicated on the image(s) of the sample\nPCBA via red bounding boxes, and component crop compar-\nisons against the reference component on the golden boardFigure 6: The DVQI system displays detected defects via a graphical user interface (GUI), allowing the operator to visualize\nand validate defects as necessary. The operator is also able to provide feedback on whether the deep neural network made the\ncorrect decision, and this information is used to improve the performance of the neural networks over time.\n(and any alternate components, if applicable) are available\nin the right panel for easy visual verification. The operator\nis also able to provide feedback on whether the deep neu-\nral network made the correct decision in the left panel, and\nthis information is used to improve the performance of the\nneural networks over time.\nLastly, the DVQI system allows for operators to both eas-\nily monitor the manufacturing line in real-time via the live\ndashboard and retroactively review past inspections via the\nworklist (see Figure 7). The live dashboard continually up-\ndates with the most recent inspection results, and provides\nan easy way for operators to see which boards contain de-\nfects via heuristic green (good, not defective) and red (bad,\ndefective) colour-coding. The live dashboard also shows\nthumbnails of the detected defects for quick validation, and\nthis view is typically used by operators as PCBAs are ac-\ntively being inspected.\nThe worklist displays a list view of all inspection results,\nand enables operators to quickly find and review past inspec-\ntions. To help with this, the worklist allows the user to search\nby board serial number search, work order, or AI model, and\nhas filtering capabilities (e.g., by date and time, manufactur-\ning line, inspections that contain defects, etc.). This view is\noften most useful for users other than the operator who are\nnot directly overseeing the manufacturing line and operat-\ning the inspection system, such as manufacturing engineers,\nrework technicians, and plant managers.Improved Cycle Time\nThe DVQI system shows a clear improvement to inspec-\ntion cycle time relative to the manual inspection of PCBAs.\nWhile manual inspection of PCBAs can take anywhere from\na few minutes to over 10 minutes for more densely populated\nPCBAs. In comparison, the DVQI system’s inspection cycle\ntime is than one minute. When configured for stand-alone\ninspection, the inspection cycle time of the DVQI system is\napproximately 50 seconds, including the time spent physi-\ncally loading and unloading the PCBA into the system.\nDuring inline inspection, the DVQI system leverages\nstandard SMEMA (The Hermes Standard Initiative et al.\n2019) communication signals via its conveyor base and the\napproximate inspection cycle times are shown in Table 1. In\nthe case study presented in the following section, these inline\ninspection cycle times were sufficient to keep up with the\nmanufacturing line whether the DVQI system is deployed.\nPCBA Size Inspection Cycle Time\n8” x 8” ∼20 s\n10” x 14” ∼25 s\n20” x 20” ∼40 s\nTable 1: Approximate inspection cycle times of the DVQI\nsystem when configured for inline inspection.Figure 7: The DVQI system allows for operators to both easily monitor the manufacturing line in real-time via the live dash-\nboard, and retroactively review past inspections via the worklist. The live dashboard (top) continually updates with the most\nrecent inspection results, and provides an easy way for operators to see which boards contain defects as PCBAs are actively\nbeing inspected. The worklist (bottom) is used to easily review past inspection results, and has board serial number search and\nfiltering capabilities.Case Study: Deployment in Electronics\nManufacturing\nWe present a case study of the DarwinAI Visual Quality\nInspection (DVQI) system’s deployment at a top electron-\nics manufacturer, where the system has been deployed for\nover a year. During the first 12 months of deployment, the\nDVQI system inspected approximately 55.5 million compo-\nnents across over 90k boards. The manufacturer has con-\ntinued to use the DVQI system beyond the first year of de-\nployment, and is actively using the system as part of their\nstandard manufacturing process.\nFor the purpose of this paper, we specifically leverage the\nDVQI system performance over a one month evaluation pe-\nriod during the first year of deployment to provide a detailed\nanalysis and summarize the impact of the system. During\nthis evaluation period, the DVQI system was deployed at\none of the manufacturer’s surface-mount technology (SMT)\nlines, and inspected PCBAs coming out of a reflow oven\nfor SMD component placement defects. An average of 388\nboards were inspected daily and a total of 9,218 boards were\ninspected, with 473 defective boards caught over the course\nof the one month evaluation period.\nSystem Performance Across Deployed\nEnvironments\nDuring the evaluation period, a total of 5.7 million compo-\nnents were inspected for component placement defects, and\nthe DVQI system achieved a false positive or overkill rate\nof 0.11%. This is notably less than our internal false posi-\ntive rate of 1.72%, indicating that our continuous learning\nstrategy of human-in-the-loop improvements can indeed al-\nlow for boosted inspection performance after the system has\nbeen deployed.\nWe also assessed the availability and reliability (of the\nDVQI system in a deployed environment, as defined below:\nAvailability (%) =Uptime (min)−Downtime (min)\nUptime (min)\nReliability (hrs) =Total uptime\n#of breakdowns\nwhere uptime excludes any planned system maintenance and\ndowntime refers to when the system is unavailable for in-\nspection. Note that system reliability is measured as the\nmean time between failures (MTBF). During the evaluation\nperiod, the DVQI system achieved a system availability of\n99.6%, and a MTBF of 356 hrs.\nImpact for Manufacturers\nLastly, we present the impact of the DVQI for the manufac-\nturer during the one month evaluation period, and the antic-\nipated return on investment (ROI) of using this system for\na full year. The manufacturer saw notable benefits both in\nterms of first piece savings via labour costs of manual in-\nspection, and cost avoidance via reducing waste and rework.\nIn terms of first piece savings, the manufacturer estimated\nan average of 15 first piece inspections per day, each tak-\ning 15 minutes and resulting in 225 minutes of manual in-\nspection at a labour rate of $33.78/hr. Over the course of ayear, the manufacturer anticipated $32,760 of savings from\nlabour costs associated with manually inspecting the first\npiece of each PCBA batch. Similarly, the manufacturer es-\ntimated $4,769 of cost avoidance by reducing waste and re-\nwork (i.e., cost saved by preventing multiple defects from\nbeing manufactured) per month, and anticipated $56,228 in\ncost avoidance over the course of a year. Together, the manu-\nfacturer estimated $89,988/year in added value by using the\nDVQI system on a single manufacturing line.\nDiscussion\nIn this work, we introduced the DarwinAI Visual Quality\nInspection (DVQI) system, a hardware-integration artificial\nintelligence system for the automated inspection of printed\ncircuit board assembly defects in an electronics manufactur-\ning environment. The DVQI system was designed to conduct\nautomatic multi-task inspection of PCB boards for a multi-\ntude of different electronic component defects in an accurate\nyet high-throughput manner via a highly efficient double-\ncondensing attention condenser network created via genera-\ntive network architecture search. The system was further de-\nsigned to require minimal operator programming and setup\nthrough the use of a highly efficient deep neural network\nalso created via generative network architecture search. Fi-\nnally, DVQI provides easy-to-use GUI mechanisms to facil-\nitate operator review of detected defects and user feedback\nthat enables continuous learning to be conducted to improve\ninspection performance by the deep neural networks over\ntime, allowing the system to adapt to the manufacturing en-\nvironment and operational needs.\nA case study was conducted to investigate and evaluate\nthe DVQI system in a deployment scenario at a top elec-\ntronics manufacturer, which at the time of this paper has\nnow inspected approximately 55.5 million electronic com-\nponents across over 90k boards. A number of key findings\nwere observed during the case study. First, the DVQI sys-\ntem was qualitatively evaluated to be intuitive to the user\nand easy to use to setup boards, taking on average 30 min-\nutes or less to program the system to perform a new PCBA\ninspection. As well, the defect detection models were robust\nto small variations in the boards and generalized to compo-\nnents never seen before by the deep neural networks during\ntraining. Lastly, the deployed DVQI system has a lower false\npositive rate (0.11%) than the default DVQI system perfor-\nmance (false positive rate of 1.7%), indicating that the sys-\ntem’s inspection performance improves over time via user\nfeedback during deployment.\nFuture system improvements include adding additional\ndefect types to be detected by the system by extending the\nmulti-task deep neural network, such as solder joint inspec-\ntion and foreign object detection. In summary, it was ob-\nserved that the DVQI system provides significant benefits\nwithin a deployed scenario, saving an estimated $90k per\nmanufacturing line per year in labour costs and prevented\nwaste and rework, and enabling electronics manufacturers\nto increase production efficiency and remain competitive.",
      "metadata": {
        "filename": "DVQI_ A Multi-task_ Hardware-integrated Artificial Intelligence System for Autom.pdf",
        "hotspot_name": "PCBA_Production",
        "title": "DVQI: A Multi-task, Hardware-integrated Artificial Intelligence System\n  for Automated Visual Inspection in Electronics Manufacturing",
        "published_date": "2023-12-14T18:56:54Z",
        "pdf_link": "http://arxiv.org/pdf/2312.09232v1",
        "query": "THT and SMD soldering PCBA production energy efficiency improvement quantitative efficiency gains"
      }
    },
    "Grounding Stylistic Domain Generalization with Quantitative Domain Shift Measure": {
      "full_text": "Grounding Stylistic Domain Generalization with Quantitative Domain Shift\nMeasures and Synthetic Scene Images\nYiran Luo1*Joshua Feinglass1Tejas Gokhale2Kuan-Cheng Lee1Chitta Baral1Yezhou Yang1\n1Arizona State University2University of Maryland, Baltimore County\n1{yluo97, jfeingla, klee201, chitta, yz.yang }@asu.edu2gokhale@umbc.edu\nAbstract\nDomain Generalization (DG) is a challenging task in\nmachine learning that requires a coherent ability to com-\nprehend shifts across various domains through extraction\nof domain-invariant features. DG performance is typically\nevaluated by performing image classification in domains\nof various image styles. However, current methodology\nlacks quantitative understanding about shifts in stylistic do-\nmain, and relies on a vast amount of pre-training data,\nsuch as ImageNet1K, which are predominantly in photo-\nrealistic style with weakly supervised class labels. Such\na data-driven practice could potentially result in spurious\ncorrelation and inflated performance on DG benchmarks.\nIn this paper, we introduce a new 3-part DG paradigm to\naddress these risks. We first introduce two new quantita-\ntive measures ICV andIDD to describe domain shifts in\nterms of consistency of classes within one domain and sim-\nilarity between two stylistic domains. We then present Su-\nperMarioDomains (SMD) , a novel synthetic multi-domain\ndataset sampled from video game scenes with more consis-\ntent classes and sufficient dissimilarity compared to Ima-\ngeNet1K. We demonstrate our DG method SMOS . SMOS\nuses SMD to first train a precursor model, which is then\nused to ground the training on a DG benchmark. We\nobserve that SMOS+SMD altogether contributes to state-\nof-the-art performance across five DG benchmarks, gain-\ning large improvements to performances on abstract do-\nmains along with on-par or slight improvements to those on\nphoto-realistic domains. Our qualitative analysis suggests\nthat these improvements can be attributed to reduced dis-\ntributional divergence between originally distant domains.\nOur data are available at https://github.com/\nfpsluozi/SMD-SMOS .\n1. Introduction\nThe generalizability of deep neural networks in computer\nvision is a crucial task that is still challenging [3, 18].\n*Corresponding author\nLossC EPrecursor\nModelNES SNES N64 W ii\nPrecursor\nModelDomain\nGeneralization\nModel\nPhoto Art Cartoon SketchTraining Domains Test Domain SuperMarioDomains\nDJ S LossC EPredictionDomain\nGeneralization\nModelFigure 1. Top: We define two quantitative measures ICV and\nIDD to describe stylistic domain shifts in image datasets for Do-\nmain Generalization (DG). We find that the vast ImageNet1K,\ncommonly used for pre-training DG models, has inconsistent class\nlabels and is already similar in style with photo-realistic domains\nfound in multiple benchmarks. Therefore, we compile a novel syn-\nthetic dataset SuperMarioDomains (SMD) as referential stylis-\ntic domains with consistent scene class labels and sufficient dis-\nsimilarity from existing domains. Bottom: We present our DG\napproach SMOS that leverages the unique domain shifts in our\nnewSMD dataset. We first train a Precursor Model using SMD\nand cross entropy Loss CE. We then utilize the trained Precursor\nModel to ground the training of the DG model with training do-\nmains from the benchmark, optimizing the empirical loss of both\ncross entropy Loss CEand Jensen-Shannon Divergence DJSbe-\ntween the Precursor Model and the DG Model.\nHence, the specific task of Domain Generalization (DG)\nhas been defined with the aim of improving the generaliza-\ntion of models by singling out distribution shifts among data\nthat belong to independent and identically distributed (i.i.d)\ndomains [22, 44]. The evaluation of a DG model consists\n1arXiv:2405.15961v1  [cs.CV]  24 May 2024of performing supervised image classification in a multi-\nsource leave-one-out scenario, where one domain is held\nout as an unseen test set, and other domains are for training.\nThe crucial DG strategy is to learn domain-invariant\nfeatures from the training domains such as SagNet [42],\nCORAL [53], and DANN [17]. More DG benchmarks of\nmore perplexing stylistic domains and more fine-grained\nclasses [4, 5, 31, 46, 57] have also been developed for more\ncomprehensive evaluation of generalizability. However, we\nnotice potential risks in the current methodology. Most DG\nmethods solely rely on initializing their backbone models\npre-trained with vast weakly supervised image collections,\ne.g. ImageNet1K [13], which overwhelmingly resemble\n[38, 55] photo-styled domains in multiple DG benchmarks.\nWe observe that many DG methods [2, 9, 42, 53, 56] reduce\nvarious forms of qualitative distances among domains, but\nlack quantitative understanding of the specific differences\namongst the domains of image styles. Without clarified\nand unbiased understanding of the common ground among\ntraining domain examples, generalization onto an unseen\ndomain would potentially be based upon un-grounded spu-\nrious evidence, resulting in inflated DG performance.\nIn this paper, we present a new 3-fold paradigm for Do-\nmain Generalization. First, we define 2 quantitative mea-\nsures for stylistic domain shifts based on Jensen-Shannon\nDivergence [40] - Intra-Class Variation (ICV) within an\nindividual domain, and Inter-Domain Dissimilarity (IDD)\nbetween two domains. With these 2 measures, as shown in\nFigure 1 in the top, we confirm that ImageNet1K is biased\ntoward domains of photorealistic styles, but also has incon-\nsistent representations within its individual class categories.\nWe then construct a new multidomain dataset dubbed\nSuperMarioDomains (SMD)1. SMD features 4 domains\nrepresenting multiple generations of video game graphic\nstyles, ranging from low-resolution pixels to 3D-rendered\npolygon-rich graphics. All domains share 4 classes of in-\ngame type of scenes that appear consistently throughout\nthe Mario franchise. Unlike ImageNet1K, SMD’s domains\nmaintain variable stylistic distances from ImageNet1K in\nterms of IDD, while having more consistently labeled ex-\namples than ImageNet1K’s in terms of ICV .\nOur proposed DG method SMOS is shown in the bot-\ntom of Figure 1. SMOS first trains a precursor model with\ndomains in SMD. It then trains a DG model grounded by\nthe distribution represented by the SMD-trained precursor\n1As of Mar. 20, 2024, the official content guidelines of Nintendo, who\nowns the copyright of all games involved in SMD, has clearly stated they\n’will not object to your use of gameplay footage and/or screenshots cap-\ntured from games for which Nintendo owns the copyright ... for appropri-\nate video and image sharing sites’ [41]. We will follow this guideline and\npublish our work in two forms. Extracted feature vectors and pre-trained\nmodels will be readily available under the MIT license. The SMD dataset\nwith its raw images will be accessible after agreeing to using the dataset for\nfair-use research purposes only under the Open Database License (ODbL).Dataset Domains Classes Size\nImageNet1K [13] 1 image style 1K objects 1.3M\nPlaces365 [65] 1 image style 365 scenes 1.8M\nPACS [36] 4 image styles 7 objects 10K\nVLCS [29] 4 photo sources 5 objects 10K\nTerraIncognita [5] 4 locations 10 animals 25K\nOfficeHome [57] 4 image styles 65 objects 16K\nDomainNet [46] 6 image styles 345 objects 587K\nSMD (ours) 4 game graphics 4 scenes 82K\nTable 1. Statistics of image classification datasets involved in DG.\nmodel. We apply SMOS and find overall improvements on\nmultiple DG benchmarks. SMOS contributes to the most\nsignificant improvements when targeting abstract-styled do-\nmains, for example +7.3%on Sketch of PACS [36], +3.6%\non Clipart of OfficeHome [57], or +8.1%on Clipart of Do-\nmainNet [57] over the baselines of MIRO [9]. SMOS also\nmaintains performance on par with baselines when target-\ning photo-realistic domains, and in some cases, e.g. PACS,\neven has improvements. We also observe that SMOS is\nable to improve extraction of domain-invariant features and\ngeneralization over domain shifts in quality, as we find that\noriginally distant stylistic domains are now projected within\nconsiderably smaller distributional divergence.\nOur main contributions in this paper are as follows.\n1. We propose ICV and IDD as measures for stylistic do-\nmain shifts in DG benchmarks. Using our measures, we\nfind that real-world datasets like ImageNet1K used as\npre-training data for DG may not be ideal for obtaining\ndomain-invariant features due to inconsistent classes and\novert stylistic similarity with training domains.\n2. We introduce a new synthetic dataset SuperMari-\noDomains as a precursor dataset for DG, incorporat-\ning unique features of consistent classes of video game\nscenes across stylistic domains in video game graphics\nthat are dissimilar to ImageNet1K.\n3. We present our DG method SMOS. SMOS utilizes\nSuperMarioDomains to obtain a precursor model that\ngrounds the training process on DG benchmarks. We\nshow that SMOS is capable of obtaining top perfor-\nmance on multiple DG benchmarks, in particular, via\nlarge improvements on targeting abstract-styled do-\nmains together with on-par or slight improvements on\nphotorealistic-styled domains.\n2. Related Works\nSynthetic Datasets with Domain Shifts. Synthetic data\nhas long been used in various disciplines in computer vi-\nsion [1, 7, 11, 45, 54, 60]. Recently, we have seen that more\nsynthetic datasets of distribution shifting domains are being\nassembled to encourage more robust algorithms in different\n2tasks. We have benchmarks studying adaptation between\nsynthesized and real-life objects in Syn2Real [47] or SYN-\nTHIA [50]. Super-CLEVR [39] studies more robust visual\nreasoning skills by constructing domains w.r.t. visual ques-\ntion answering settings. More specifically, we draw great\ninspiration from the GTA V-Cityscapes challenge [10, 49],\nadapting from a vast collection of video game landscapes\nto real-world scenarios for segmentation. All these works\nshow that synthetic datasets, though lacking full realism,\nmay help provide great insight into domain shifts.\nDomain Generalization Benchmarks. Early DG bench-\nmarks such as Office [51] or VLCS [29] focus solely on\nphoto-realistic images. Since the time when single-style\nbias was first exposed by DeCAF [14], more fine-grained\nimage styles have been introduced to the mix of image\ndomains, and we have seen a steady increase in scale for\nDG datasets, including OfficeHome [57], PACS [36], Ter-\nraIncognita [5], SVIRO [12], WILDS [31], Camelyon17\n[4], and NICO++ [63]. Currently, the DomainNet dataset\n[46] is the largest with 587K images, featuring 6 stylistic\ndomains and 345 object categories.\nDomain Generalization Approaches. Techniques to\ntackle the problem of domain shift in Domain General-\nization have been rapidly developed over the years. Re-\nsearchers are no longer restricted to straightforward ap-\nproaches such as finding linear [26] representations with\ntechniques like interpolation [16, 20, 59, 61] or nonlinear\n[15] representations in-between domains. In many circum-\nstances, simple methods, such as variants of empirical risk\nminimization [2, 23], can produce high performance on\npopular domain generalization benchmarks. But recent ap-\nproaches to domain generalization can involve adaptation\nand combination of deep neural network models such as\nDANN [17] and CDANN [37], leveraging Meta-Learning\n[34] or Adversarial-Learning [19, 48, 58] to transfer model\nparameters, using an ensemble of different architectures\n[38], training strategies [28], or regularization methods to\nimprove generalizability like SWAD [8] or MIRO [9].\n3. Preliminaries\nIn this paper, we focus on the Domain Generalization\n(DG) performance of image classification by conducting\nMulti-Source Domain Generalization (MSDG), where we\nevaluate a model’s performance with leave-one-out cross-\nvalidation . In formal terms, a DG dataset Dis divided\nintoNdomains {d1, ...dN}. Each individual domain di\ncontains images Xdipaired with their class labels Ydi.\nAll domains share the same set of image labels. We\ntrain a model Mwith the training set (training domains)\nDtrthat uses all data from all-but-one domains Dtr=\n{(Xd1, Yd1), ...(XdN−1, YdN−1)}. The trained model is\nthen evaluated using unseen data from the one held-out test\n(target) domain as the test set Dte={(XdN, YdN)}. No-ticeably, unlike Domain Adaptation [68], the one test do-\nmainDtein DG is not involved in any training and does not\ncontain samples that overlap with the training domains.\nA typical DG model M=f◦gconsists of two core\ncomponents: a feature extractor (featurizer) fas the back-\nbone network, followed by a single-layered linear classifier\ng. Although more complicated methods have been devel-\noped over the years, the fundamental approach of Empir-\nical Risk Minimization (ERM ) [56] remains relevant. In\nthe context of DG, the goal of ERM is simply minimizing\nthe classification loss averaged over the N−1training do-\nmains given training data xi, yi∈ Dtr:\nLERM =1\nN−1X\nxi,yi∈DtrLCE(f◦g(xi), yi).(1)\nAlthough straightforward, the latest baseline ERM [23],\nwhich applies cross-entropy as the training loss LCE, is\nshown to rank high in many DG benchmarks [22, 33, 55].\n4. Analyses on Domain Shift of Stylistic Do-\nmains and Pre-training Data\nRisk in Pre-training Data Used for DG. Most DG meth-\nods do not train their model Mentirely from scratch.\nThe model’s initial weights are commonly transferred from\nan existing model, typically pre-trained on ImageNet1K\n[13]. Researchers [55, 63] have observed that there is\nan apparent stylistic resemblance between ImageNet1K\nand many domains from various DG benchmarks, such as\nthe Photo domain in PACS[36], the RealWorld domain in\nOfficeHome[57], or even all 4 domains in VLCS [29].\nSuch a common practice of only depending on a vast\nreal-world image dataset can be risky. Qualitatively, the im-\nages used for pre-training lack shift in style to adapt from,\ne.g. ImageNet1K are dominated by real-world photos. In\naddition, the images in ImageNet1K may be inconsistent\nwithin the same class; e.g., images of a class of fish may\nor may not include a person holding it. Hence, when we\nperform on DG benchmarks by using a model pre-trained\nonly by such data, we may find it difficult to learn coherent\nstyle-invariant presentations in order to generalize over dis-\ntributional shifts among domains, especially when we have\nto train with the predominating photo-realistic domains and\ntest on an unseen abstract-styled domain.\nQuantitative Measures of Stylistic Domain Shift. We\nwould next like to compare the advantages of different DG\npre-training data like ImageNet1K. Inspired by previous\nworks which examine image similarity [21, 64], we define\ntwo intuitive measures for domain shift in DG: Intra-Class\nVariation (ICV) to implicate the class-wise presentation in-\nconsistency within one specific domain, and Inter-Domain\nDissimilarity (IDD) to implicate the distance in represen-\ntation distributions across two domains of styles.\n30.0000.0050.0100.0150.0200.0250.0300.0350.040\nNESSNESN64WiiPhotoArtCartoonSketchVOC2007LabelMeSUN09Caltech101ArtClipartProductRealWorldLocation100Location38Location43Location46ClipartRealSketchInfographPaintingQuickdrawImageNet1KIntra-Class VariationSMD (Ours)PACSVLCSOfficeHomeTerraIncognitaDomainNetImagetNet1KFigure 2. Intra-Class Variation (ICV) for each domain in featured datasets. A low ICV indicates that the classes are more consistent in\nterms of colors, as in NES of SMD, Sketch of PACS, and Quickdraw of DomainNet. Meanwhile, the classes in ImageNet1K, which is\ncommonly used for pre-training in DG, are implicated to be as inconsistent as those in photo- or art-styled domains, e.g. Photo and Art of\nPACS, LabelMe of VLCS, Art and RealWorld of OfficeHome, as well as Real of DomainNet. Average of 3 trials.\n00.10.20.30.40.50.6\nNESSNESN64WiiPhotoArtCartoonSketchVOC2007LabelMeSUN09Caltech101ArtClipartProductRealWorldLocation100Location38Location43Location46ClipartRealSketchInfographPaintingQuickdrawInter-Domian Dis. vs. ImageNet1KSMD (Ours)PACSVLCSOfficeHomeTerraIncognitaDomainNet\nFigure 3. Inter-Domain Dissimilarity (IDD) of ImageNet1K vs. each domain in featured datasets. IDD of ImageNet1K vs. itself is\npresumably 0. Since ImageNet1K is dominantly real-world photographs, those with smaller IDD implicate stronger resemblance to the\nstyle of photo-realism, e.g. all 4 domains of VLCS, or Photo of PACS. Highly abstract and simplistic styles, such as Sketch of PACS and\nQuickdraw of DomainNet, are shown in very large IDD values on the flip side.\nBoth measures utilize the symmetric Jensen-Shannon\nDivergence (JSD) [40]. Concretely, let PandQbe the es-\ntimated probability distributions of the RGB channels and\nbins (3 ×256 dimensions), making the outcome set a vector\nof length 768. The JSD between PandQis defined as:\nDJS(P, Q) =1\n2(DKL(P||M) +DKL(Q||M)),(2)\nwhere Mis the average mixed distribution:\nM=1\n2(P+Q), (3)and the KL divergence between two distributions PandQ:\nDKL(P||Q) =X\nxP(x) logP(x)\nQ(x), (4)\nwhere P(x)andQ(x)are the probabilities of image in-\nstance xin distributions PandQrespectively. DJSranges\nbetween 0 and 1, with 0 indicating identical distributions\nand 1 indicating completely dissimilar distributions.\nForIntra-Class Variation of a given image domain, Pi\nandQirepresent distributions of two equal splits of samples\n4Figure 4. A qualitative overview of our SuperMarioDomains(SMD) dataset, consisting of video frames from actual game footage cate-\ngorized into 4 distinctive scene classes and 4 image style domains. Columns from left to right: The four image domains, named after the\nconsole hardware on which each game runs - NES,SNES ,N64, and Wii.Rows from top to bottom: The four classes of in-game scenes\n- Overworld, Underground, Aquatic, and Castle. These synthetic image styles of SMD are drastically different from those in existing DG\nbenchmarks, such as realistic photographs, pencil sketches, or oil paintings.\nbelonging to the same class i. The ICV w.r.t. the domain D\nofnclasses is the average over all its class-wise JSDs:\nICV (D) =1\nnnX\ni=1DJS(Pi, Qi). (5)\nForInter-Domain Dissimilarity between two domains\nD1andD2, the metric straightforwardly computes the JSD\nbetween respective distributions PD1andPD2:\nIDD (D1,D2) =DJS(PD1, PD2). (6)\nFor fairness, we assume that all image presentations\nfollow Gaussian distributions with regard to probabilities\nof raw RGB values within bins of [0,255], normalized\nby the uniform mean [0.5,0.5,0.5]and standard deviation\n[0.5,0.5,0.5]. In addition to ImageNet1K, we choose to\nstudy the domains in the following DG benchmarks: PACS,\nVLCS, OfficeHome, TerraIncognita, and DomainNet. The\nICV for each domain studied is averaged over 3 trials.\nIn Figure 2, we first compare the ICV of ImageNet1K\nwith that of every domain in the chosen DG benchmarks.\nWe discover that, in ImagetNet1K, image representations\nwithin the same class can be as diversely distributed asthose learning samples from photo- or art-styled domains\nin DG benchmarks. Meanwhile, domains of highly abstract\nstyles, such as Sketch in PACS and Quickdraw in Domain-\nNet, have drastically low ICV values compared to other do-\nmains in their respective benchmarks. According to Table\n1, since ImageNet1K also has at least twice as many classes\ncompared to DG domains, generalization with known sam-\nples solely from high-ICV classes would be difficult to learn\ndomain-invariant features, especially when the test domain\nsamples have vastly low variation in feature space.\nFrom Figure 3, we now have a clear view of Ima-\ngeNet1K’s similarity to existing styles in multiple DG\nbenchmarks. This can be observed, for example, from the\nnext-to-0 IDD values of all four domains of VLCS, the\nPhoto and Art domains of PACS, or the Location38 domain\nof TerraIncognita. In contrast, styles of abstract color pat-\nterns or textures, such as the Sketch domain of PACS and\nthe Quickdraw domain of DomainNet, have large IDD val-\nues which suggests large distances from ImageNet1K.\n5. Methodology\nThe SuperMarioDomains Dataset. Taking inspirations\nfrom previous efforts to construct synthetic datasets with\n5Linear\ngSPrecursor\nFeaturizer\nf S\nLinear\ngDG\nFeaturizer\nf\nℒJ SPACS, Of ficeHome, DomainNet, ...\nℒE R MTraining Domains Dt r\nPrecursor\nFeaturizer\nf SSuperMarioDomains\nPrecursor Domains DS\nℒSFigure 5. The pipeline of our SMOS method. The feature extrac-\ntion backbones fandfShave an identical structure. fis initialized\nwith ImageNet1K pre-trained weights. Left: We first train the Pre-\ncursor Model fS◦gSto learn scene style shifts with SMD. Right:\nWe then perform DG training with training domains from a DG\nbenchmark (e.g. PACS), tuning the DG model f◦gwhile being\ngrounded to the SMD-trained Precursor fSby optimizing LJS.\nenvironments rendered in video games[7, 49, 54], we com-\npile a multi-domain dataset SuperMarioDomains (SMD)\nfrom video game scenes that can be leveraged as precur-\nsor data for Domain Generalization experiments. As its\nname suggests, our SMD dataset feature synthetic in-game\nscenes captured from multiple games in the Mario fran-\nchise, with image style domains encompass pixelated mo-\nsaic 2D graphics in only 50 colors, all the way to high-\npolygon 3D graphics in 32-bit colors. We land on 4 classes\nof distinct in-game scenes that consistently appear in all\nMario games of our choice. Each scene class has its own\nunique traits defined by the combination of terrains, objects,\nand texture. To ensure high consistency in image styles and\nclass labels, we sample our images from video frames of\nactual game play footage, - neighboring frames of the same\ngame segment share the same scene class. An overview of\nthe samples in SMD is available in Figure 4.\nQuantitatively, SMD is designed to counterbalance the\nstylistic biases of ImageNet1K by having domains that are\nlow in ICV and relatively high in IDD . On the left of Fig-\nure 2, we find that the 4 synthetic-styled domains in SMD\nhave much higher class consistency than ImageNet1K, as\nin generally lower ICVs. Also, in Figure 3, SMD’s domains\nalso keep a series of evenly placed domain shifts in terms of\nIDD from the dominating style of ImageNet1K, with NES\nbeing the most distant domain while N64 and Wii not as\nclose as Photo of PACS or Caltech101 of VLCS.\nThe SMOS DG Method. Seeing the unique stylistic do-\nmain shifts in SMD, we are motivated to design a new\nDG method with better domain-invariant feature extrac-\ntion, learning the domain shifts first from isolated class-\nconsistent samples in SMD. We present the Scene-grounded\nMinimal d Omain Shift (SMOS ) method to best leverage the\nunique domain shifts in SMD. Figure 5 demonstrates theSMOS pipeline. We first train a Precursor Model MS=\nfS◦gSwith a Precursor Feature Extractor fSto learn about\nthe domain shifts of SMD. Sequentially, we train an iden-\ntically structured model M=f◦gwith training domains\nfrom a DG benchmark, while simultaneously grounded by\nminimizing the Jensen-Shannon Divergence between distri-\nbutions of the corresponding feature extractors fSandf.\nFormally, the distribution of the Precursor Model MS\nis learned with the synthetic scene data DS, namely from\nSMD. Given NSstylistic domains of scene image-label\npairs xj, yj∈ D S, we optimize the cross-entropy loss for\nupdating MSin a similar form with ERM:\nLS=1\nNSX\nxj,yj∈DSLCE(fS◦gS(xj), yj). (7)\nTogether with Ntraining domain images xi∈ D tr,\nSMOS additionally grounds the training of the DG model\nMby minimizing the JSD from the corresponding Precur-\nsor Feature Extractor fS:\nLJS=DJS(fS(xi), f(xi)). (8)\nSMOS optimizes the empirical loss during benchmarking:\nLSMOS =LS+LERM +λLJS, (9)\nwhere the coefficient λis a hyper-parameter that controls\nthe grounding factor.\n6. Experiments and Results\nExperiment setups. We use ResNet50 [25] pre-trained\nwith ImageNet1K [13] as the single default backbone for\nfeature extractor networks. Our implementation is based\non the source code of DomainBed [23] and MIRO [9]. We\nevaluate the performance of SMOS on the following DG\nbenchmarks: PACS [36], OfficeHome [57], VLCS [29],\nTerraIncongnita [5], and DomainNet [46].\nTwo variants of SMOS are designed to initialize MSdif-\nferently - SMOS−whose fSis initialized from scratch via\nKaiming Initialization [24], and SMOS+whose fSis ini-\ntialized with ImageNet1K pre-trained weights.\nWe apply the same data augmentations by ERM [23] and\nMIRO [9] - each training set image is cropped with ran-\ndom size and aspect ratio, resized to 224×224 pixels, ap-\nplied random horizontal flips, applied random color jitter,\napplied grayscale with 10% probability, and normalized us-\ning the ImageNet channel means and standard deviations.\nFor hyperparameters, we use a batch size of 16, and the\nAdam optimizer[30] for all experiments. The hyperparam-\neters listed in Table 4 are consistent with those used in the\nMIRO paper [9], where for each benchmark, we use a dif-\nferent combination of hyperparameters. We divide all do-\nmains in SMD by a 4-to-1 training-test ratio to obtain the\n6MethodPACS OfficeHome DomainNet\nACS→ PAS→ PAC→Avg.ACP→ RAP→Avg.CSIPQ→ RSIPQ→ CRSIP→Avg.Photo Cartoon Sketch RealWorld Clipart Real Clipart Quickdraw\nERM [56] 97.0±0.1 79.8±1.0 74.7±0.7 84.2 77.4±0.2 53.2±1.2 67.6 60.9±0.1 50.8±0.2 10.5±0.1 44.0\nMIRO [9] 97.3±0.2 80.5±0.5 75.9±1.0 85.4 81.1±0.4 53.8±0.8 70.5 63.4±0.2 54.2±0.8 11.3±0.0 44.3\nSMOS−(ours) 98.1±0.0 84.8±0.2 81.3±0.4 88.7 80.7±0.3 57.5±0.3 71.0 63.9±0.3 60.3±0.1 13.9±0.0 44.5\nSMOS+(ours) 98.1±0.1 84.8±0.3 83.2±0.6 89.4 80.8±0.4 58.6±0.4 71.6 64.0±0.2 62.3±0.0 14.7±0.0 45.3\nTable 2. DG performance on individual target domains. SMOS gains the greatest improvements on abstract-styled domains while main-\ntaining its performance on photo-styled domains. The letters that precede →denote initial letters of the training domains in the respective\nbenchmarks. The best results per setting are shown in bold. Average of 3 trials.\nMethod PACS VLCS OfficeHome TerraIncognita DomainNet Avg.\nMixstyle†[67] 85.2 ±0.377.9 ±0.5 60.4 ±0.3 44.0 ±0.7 34.0 ±0.1 60.3\nGroupDRO†[52] 84.4 ±0.876.7 ±0.6 66.0 ±0.7 43.2 ±1.1 33.3 ±0.2 60.7\nIRM†[33] 83.5 ±0.878.5 ±0.5 64.3 ±2.2 47.6 ±0.8 33.9 ±2.8 61.6\nARM†[62] 85.1 ±0.477.6 ±0.3 64.8 ±0.3 45.5 ±0.3 35.5 ±0.2 61.7\nVREx†[32] 84.9 ±0.678.3 ±0.2 66.4 ±0.6 46.4 ±0.6 33.6 ±2.9 61.9\nCDANN†[37] 82.6 ±0.977.5 ±0.1 65.8 ±1.3 45.8 ±1.6 38.3 ±0.3 62.0\nDANN†[17] 83.6 ±0.478.6 ±0.4 65.9 ±0.6 46.7 ±0.5 38.3 ±0.1 62.6\nRSC†[27] 85.2 ±0.977.1 ±0.5 65.5 ±0.9 46.6 ±1.0 38.9 ±0.5 62.7\nMTL†[6] 84.6 ±0.577.2 ±0.4 66.4 ±0.5 45.6 ±1.2 40.6 ±0.1 62.9\nMixup†[59] 84.6 ±0.677.4 ±0.6 68.1 ±0.3 47.9 ±0.8 39.2 ±0.1 63.4\nMLDG†[35] 84.9 ±1.077.2 ±0.4 66.8 ±0.6 47.7 ±0.9 41.2 ±0.1 63.6\nERM†[56] 84.2 ±0.177.3 ±0.1 67.6 ±0.2 47.8 ±0.6 44.0 ±0.1 64.2\nSagNet†[43] 86.3 ±0.277.8 ±0.5 68.1 ±0.1 48.6 ±1.0 40.3 ±0.1 64.2\nCORAL†[53] 86.2 ±0.378.8 ±0.6 68.7 ±0.3 47.6 ±1.0 41.5 ±0.1 64.5\nCCFP [28] 86.6 ±0.278.9 ±0.3 68.9 ±0.1 48.6 ±0.4 41.7 ±0.0 64.8\nMIRO†[9] 85.4 ±0.479.0 ±0.0 70.5 ±0.4 50.4 ±1.1 44.3 ±0.2 65.9\nSMOS−(ours) 88.7 ±0.279.7 ±0.1 71.0 ±0.0 55.5 ±0.8 44.5 ±0.0 67.9\nSMOS+(ours) 89.4 ±0.379.8 ±0.1 71.6 ±0.1 55.4 ±0.4 45.3 ±0.0 68.3\nTable 3. Average leave-one-out cross-validation performances on multiple DG benchmarks.†are baseline results reported in [9]. The best\nresults per DG benchmark are highlighted in bold. All accuracies and errors are averaged from 3 trials.\nHyperparameter PACS OH DN TI VLCS\nLearning rate 3e-5 3e-5 3e-5 3e-5 1e-5\nDropout 0.0 0.1 0.1 0.0 0.5\nWeight decay 0.0 1e-6 1e-4 0.0 0.0\nSteps 5000 5000 15000 5000 5000\nλ 0.15 0.1 0.1 0.1 0.01\nTable 4. Hyperparameters for DG experiments. OH, DN, and TI\nrespectively stand for OfficeHome, DomainNet, and TerraIncog-\nnita.λis our grounding coefficient for SMOS as in Equation 9.\nprecursor model MSin SMOS. All experiments are con-\nducted using 2 NVIDIA V100 GPUs.\nDomain Generalization Performance with SMOS. We\npresent the most significant improvements of SMOS on the\nindividual target domains in Table 2. The key takeaway\nis that our SMOS method demonstrates its best strength\nwhen generalizing onto more abstract-styled domains, e.g.\nSketch in PACS, on which baseline methods such as ERMand MIRO struggle to improve. Using the unique domain\nshifts in SMD, our SMOS+variant achieves large improve-\nments over the state-of-the-art MIRO method by +3.7%\nand+7.3%on Cartoon and Sketch in PACS, +3.6%on\nClipart in OfficeHome, and +8.1%and+3.4%on Clipart\nand Quickdraw in DomainNet. We also show that SMOS is\nable to maintain its performance on photo-realistic domains\non par within the MIRO baseline’s error range. In some\ncases, such as PACS and DomainNet, SMOS can even gain\nslightly better performance than the MIRO baseline.\nA comprehensive comparison of a larger selection of\nDG methods on multiple benchmarks is shown in Table 3.\nWe show that our 2 SMOS variants achieve better overall\nperformance on the chosen DG benchmarks, thanks to the\nparticular improvements on abstract-styled domains. Our\nSMOS+variant surpasses the baseline MIRO by +4.0%on\nPACS, +0.9%on VLCS, +1.1%on OfficeHome, +4.9%\non TerraIncognita, and +1.0%on DomainNet.\nQualitative Analysis of Domain-invariant Feature Ex-\n700.10.20.30.40.50.6\nRaw RGBERMMIROSMOS- (ours)SMOS+ (ours)Inter-Domain Dissimilarity (IDD)Sketch vs. PhotoSketch vs. ArtSketch vs. CartoonFigure 6. Test vs. training domain IDDs resulted from different\nDG methods when targeting the Sketch domain of PACS.\nMethodPrecursor Benchmark Avg.\nTrain. Data DS PACS OfficeHome\nERM - 84.2 67.6\nMIRO - 85.4 70.5\nSMOS−DomainNet 87.7 65.2\nPlaces365 87.4 66.5\nSMD (ours) 88.7 71.0\nSMOS+DomainNet 87.9 70.1\nPlaces365 87.5 66.5\nSMD (ours) 89.4 71.6\nTable 5. DG performance of using substitute precursor data in\nSMOS−and SMOS+. All substitute precursor data are downsam-\npled to the same size of SMD (80k). Average of 3 trials.\ntraction. We further validate SMOS’ generalizability,\nshowing it helps project originally distant domains to\ncloser distributional proximity, which qualitatively impli-\ncates improvements in extracting domain-invariant features\nand thus better grounded DG performance. We apply our\ndivergence-based IDD measure between domains that be-\nlong to the same benchmark, comparing distributions of\nrepresentations extracted by different DG methods.\nFigure 6 exemplifies the case from the perspective of the\nSketch domain in PACS. In raw RGB distributions, Sketch\nis shown to be highly distant from other domains except\nCartoon. When conducting DG image classification tests\non Sketch, baseline DG methods such as ERM and MIRO\nare shown to incrementally lower the distributional diver-\ngence from Photo and Art, but at the cost of Cartoon which\nis originally the most similar to Sketch in RGB. Our meth-\nods SMOS−and SMOS+, along with our SMD precursor\ndata, universally lower the divergences with all non-Sketch\ndomains of PACS. Our methods are thus able to extract\ndomain-agnostic representations within much closer distri-\nbutional proximity in respective feature spaces, presented\nas further improved DG performance on Sketch.\nAblation Study of Substitute Precursor Data withSMOS. We also experiment with using substitute image\nclassification datasets as precursor data ( DSas in Figure\n5) under the SMOS method, while SMD is not involved.\nFor substitutes, we choose the benchmark dataset Domain-\nNet [46] which, on its own, has more different domains of\nimage styles and more classes compared to SMD, but also\nfar higher variance in both ICV and IDD than SMD (shown\nin Figure 1). We also choose the scene-based Places365-\nStandard (Places365) dataset [66] that has 300+ classes of\nscenes similar to SMD, but only in one photo-realism style\nidentical to ImageNet1K. We randomly downsample the\nsubstitute precursor data to the same size of SMD (80k) at\neach trial. We use the same 4-to-1 training-test split ratio\nwhen obtaining alternative precursor models. We perform\nbenchmarking on PACS and OfficeHome.\nTable 5 shows the benchmarking performances of using\ndifferent precursor data in the SMOS paradigm. We find\nthat SMOS retains the best DG performance when it uti-\nlizes SMD’s unique stylistic domain shifts and scene labels.\nIn contrast, we see that training the precursor model with\nImageNet1K-like image styles (Places365) or less consis-\ntent stylistic domains (DomainNet) leads to lower improve-\nments, if not worse performance, than the baseline methods.\n7. Conclusions\nIn this work, we introduce a new paradigm for Domain Gen-\neralization (DG) on three facets. We define two new mea-\nsures, ICV and IDD, to quantitatively understand distribu-\ntional shifts in stylistic domains based on Jensen-Shannon\nDivergence. We then present a novel precursor dataset Su-\nperMarioDomains (SMD) sampled from scenes in video\ngames, featuring more consistent categorical classes and\ndissimilar domains compared to ImageNet1K. We also\ndemonstrate our new DG method SMOS that leverages\nthe unique features of SMD as means to ground the train-\ning on DG benchmarks. We find that SMOS along with\nSMD reaches top performance on multiple DG benchmarks\nthrough significant improvements on abstract-styled target\ndomains. SMOS has also been shown to qualitatively im-\nprove domain-invariant feature extraction by bringing dis-\ntant domains within closer divergence in learned feature\nspace. In the future, we would like to explore the applica-\ntion of our methodology on other tasks, such as Controllable\nText-to-Image Generation or Visual Question Answering.\nAcknowledgements\nThe authors acknowledge Research Computing at Arizona\nState University for providing HPC resources and support\nfor this work. This work was supported by NSF RI grants\n#1750082 and #2132724. The views and opinions of the\nauthors expressed herein do not necessarily state or reflect\nthose of the funding agencies and employers.\n8",
      "metadata": {
        "filename": "Grounding Stylistic Domain Generalization with Quantitative Domain Shift Measure.pdf",
        "hotspot_name": "PCBA_Production",
        "title": "Grounding Stylistic Domain Generalization with Quantitative Domain Shift\n  Measures and Synthetic Scene Images",
        "published_date": "2024-05-24T22:13:31Z",
        "pdf_link": "http://arxiv.org/pdf/2405.15961v1",
        "query": "THT and SMD soldering PCBA production energy efficiency improvement quantitative efficiency gains"
      }
    },
    "Miniaturized liquid metal composite circuits with energy harvesting coils for ba": {
      "full_text": "Journal XX Journal Title \nJournal XX (XXXX) XXXXXX  https://doi.org/XXXX/XXXX \nxxxx-xxxx/xx/xxxxxx  1 © xxxx Journal X \n Miniaturized liquid metal composite circuits \nwith energy harvesting coils for battery-free \nbioelectronics and optogenetics \n \nDenis Rocha1, Pedro Lopes1, Paulo Peixoto1, Aníbal de Almeida1, Mahmoud Tavakoli1 \n1 Soft and Printed Microelectronics Lab, University of Coimbra, Portugal \n \nAbstract \nOver the past years, rapid progress has been made on soft-matter electronics for wearable and implantable \ndevices, for bioelectronics and optogenetics. Liquid Metal (LM) based electronics were especially popular, due \nto their long-term durability, when subject to repetitive strain cycles. However, one major limitation has been \nthe need for tethering bioelectronics circuits to external power, or the use of rigid bulky batteries. This has \nmotivated a growing interest in wireless energy transfer, which demands circuit miniaturization. However, \nminiaturization of LM circuits is challenging due to low LM-substrate adhesion, LM smearing, and challenges \non microchip-interfacing. In this article, we address these challenges by high-resolution laser-assisted \nmicropatterning of biphasic LM composites and vapor-assisted LM microchip soldering.Through development \nof a search algorithm for optimization of the biphasic ink coil performance, we designed and implemented \nmicro coils with trace spacing of 50 µmthat can harvest a significant amount of energy (178 mW/cm2) through \nnear field inductive coupling. We show miniaturized soft-matter circuits with integrated SMD chips such as \nNFC chips, capacitors, and LEDs that are implemented in a few minutes through laser patterning, and vapor-\nassisted soldering. In the context of optogenetics, where lightweight, miniaturized systems are needed to \nprovide optical stimulation, soft coils stand out in terms of their improved conformability and flexibility. Thus, \nthis article explores the applications of soft coils in wearable and implantable devices, with a specific focus on \ntheir use in optogenetics.  \nKeywords:  soft electronics, soft fabrication, wearables, wireless power harvesting, optogenetics  \n \nIntroduction \nIn recent years, the field of wearable and implantable devices has witnessed significant advancements, revolutionizing \nthe monitoring and treatment of individuals14, as well as enabling groundbreaking discoveries in animal studies57. However, \na common challenge faced by these devices is the issue of tethering or reliance on bulky batteries, which often hinders their \nusability and convenience8. This limitation has sparked immense interest in the realm of energy harvesting solutions, motivating \nresearch on energy harvesting from light911, mechanical motion through piezoelectric or triboelectric devices1214, or far field \nantennas15. Among these approaches, the use of magnetic resonance for energy transfer has garnered significant attention due \nto its superior power output and the fact that it is not dependent on external light or mechanical pressure16. While magnetic \nresonance coupling is typically performed at short distances, recent studies17 have demonstrated the potential to extend the \nrange of power transfer to a few meters. \nPrevious works have focused on flexible circuits with copper-based conductors to produce the coils necessary for \nenergy harvesting. However, the next generation of wearable, and especially implantable devices, demand soft-matter \nelectronics, with stiffness close to that of human organs. If these soft-matter circuits are as well stretchable, they can deal with \nthe strain applied to them due to the dynamic morphology of the human organs, accommodating the mechanical deformations \nexperienced during daily movements1. Although copper/Au serpentine-shaped circuits18have shown some degree of flexibility, \nand high electrical conductivity, challenges remain such as fabrication difficulties, the need for clean room lithography, limited Journal XX (XXXX) XXXXXX Author et al  \n 2  \n stretchability, and lack of intrinsic softness19,20. \nWithin the broader context of soft coil applications, this article places special emphasis on a specific scenario: \nOptogenetics. Optogenetics has risen to prominence as a potent new paradigm for understanding how neurons connect and give \nrise to more complex brain functions. This enabled rapid scientific progress, leading to the belief that direct clinical use of this \ntechnology is forthcoming, potentially leading to improved treatments for a variety of neurological illnesses.21 Recent \napplications of optogenetics in neuroscience involve manipulating or monitoring electrophysiological functions and disease \nmodels,22,23 pacing the heart24 or treatment of bladder dysfunctions.7 \nConventional means for delivering light to targeted regions in behaving animals uses optical fibers.24 They are inserted \nusing stereotactic equipment, and external fixtures, surgical glues, cement, and sutures are used to hold them in place. 2527 \nHowever, the high moduli, stiff materials used in these systems are fundamentally mismatched with the soft, compliant tissues \nof the brain and peripheral nerves.28 Micromotions brought on by mechanical forces transmitted through the related \nhardware29,30 as well as those resulting from natural, biological movements can degrade the biotic/abiotic interface by causing \ncellular damage and the formation of glial scars.31 The tethers inevitably affect the animals by restricting their range of motion \nand altering their behaviours in ways difficult to quantify, which hinders the reproducibility of the investigations and clouds \nthe interpretation of the data obtained.24 As a replacement for tethered and battery-operated devices, the possibilities of wireless, \nbattery-free and fully implantable devices have recently been explored7,15. The result is lightweight neural interfaces with small \nform factors that enable long-term functionality and better mechanical adaptation to soft neural tissue28 . However, to provide \nsome degree of stretchability to the devices, recent approaches still rely on deterministic circuit architectures, such as serpentine \ngeometries. The natural body motion of a mouse can lead to compression and elongations up to 60% on implanted devices32. \nStill, the serpentine probe of the head-mounted devices32 developed recently sustains stretching only up to 30%. Moreover, \ntheir fabrication process requires expensive cleanroom lithography with over ten fabrication steps19. \nTo wirelessly power an optogenetic stimulator different types of wireless energy harvesting strategies are available \nsuch as ultrasound33, far-field RF radiation (frequency of 2.4 GHz)15,34, or near-field power transfer, such as magnetic \nresonance35,36. This article  focus is on magnetic resonance, since with high-quality factor (Q) resonators, efficient power \ntransfer can be accomplished over longer distances (up to a few meters)16,17. Moreover, within the frequency range of 100 \nkHz37,38 to 200 MHz39 , the harvested energy is little affected , as well as \nby the existence of obstructions, thanks to the non-radiative nature of the harvesting technique. This is true even when the line \nof sight between the two resonators is entirely blocked16. A more broader use case for wearable wirelessly powered \nbioelectronics would be its application in the clinical setting for neuromodulation, where one can consider a device that works \nby magnetic resonance at close range. Thus, the energy requirements differ from the application in long-range optogenetic \nstudies, given that in the latter the magnetic resonance principle has to be extended to allow a larger distance between the \nreceiver and transmitter antennas on the outside of the cages8,32.\nGiven the need of interfacing biological tissues in both of the applications mentioned, recent progress has been made \nin the direction of achieving better conformability and comfort, thus enhanced biocompatibility. In light of this, one can think \nof implants composed not only by soft polymers in the dielectric encapsulation layers, but also soft electrically conductive \npolymers as the basis of the electrical circuitry, thus achieving what we term  To accomplish this, it is \nnecessary to develop \"soft coils,\" i.e., coils that are not made of rigid conductor materials such as copper, but of soft and \nstretchable inks.  \nLithography-free approaches have been explored for the rapid prototyping of flexible and stretchable electronics.  \nThese include the use of nanowires, which provide stretchability and mechanical durability while avoiding complex fabrication \nprocesses40. However, nanowires can suffer from long-term stability issues, such as oxidation or fatigue under repeated \nmechanical stress, which may limit their reliability41. Metal nanoparticles are another option, enabling high-conductivity \ncircuits with simple processing techniques, though they may require additional steps for stability and integration42. Moreover, \nmetal nanoparticles can exhibit issues like agglomeration, which can degrade their electrical performance over time, and their \nrelatively low mechanical robustness may limit their use in highly deformable applications43. Additionally, conducting \npolymers 44 present a flexible and stretchable alternative, providing ease of fabrication, although their conductivity often falls \nshort compared to metallic conductors. While each of these approaches has its merits, the biphasic liquid metal-based ink used \nin this work and on refs.19,20,45 offers an advantageous combination of ease of patterning, high conductivity, and mechanical \nadaptability, overcoming many of the limitations found in previous methods. \nOne promising approach is the use of a composite of Eutectic Gallium-Indium (EGaIn) with stretchable polymers, \nwhich provides both high electrical conductivity and mechanical deformability19,20,45. Renowned for its high electrical \nconductivity and unique liquid form at room temperature, EGaIn offers several advantages for the realization of soft electronics. \nHowever, due to its liquid phase, EGaIn is difficult to deposit, and pattern, and is smearing to touch.46 Seminal works on laser Journal XX (XXXX) XXXXXX Author et al  \n 3  \n patterning and laser sintering of liquid metals47,48, were followed by higher resolution patterning49 for visually imperceptible \nelectrodes. Still, as the LM is smearing and can easily cause short circuits, achieving a high patterning resolution in ref.49 \nrequired multiple steps of metal deposition and patterning and access to clean room lithography. On the other hand, electrical \nconductivity limitations are often observed in liquid metal-based materials for antenna applications, prior research has focused \non the fabrication of thick metal layers to reduce sheet resistance and thereby enhance overall performance5052. These studies \nillustrate that increasing the thickness of the metal layer is an effective strategy for overcoming the intrinsic low conductivity \nof liquid metals. However, increasing the thickness of the metal layer can be a problem for liquid metal-free printed coils due \nto natural deformations on the body, which can compromise their long-term durability. Furthermore, methods based on liquid \nmetal selective wetting on Cu/Au seed layers50,53 have enabled high resolution liquid metal based stretchable circuits. However, \nthere is a limitation on the achievable circuit thickness, as the thickness of the deposited liquid metal depends on the width of \nthe seed trace, thus reducing the control over the applied liquid metal thickness. The trace thickness and resolution are as well \nlimited due to the high probability of short circuits between the adjacent liquid metal traces, which is further problematic during \nmicrochip interfacing. Therefore, liquid metal wetting techniques are not yet able to simultaneously achieve high resolution \npatterning, high trace thickness, and reliable microchip interfacing. On the other hand, the techniques developed in this work \nnot only address these problems, they as well eliminates the need for clean room deposition and lithography, and manual \ndeposition processes, such as liquid metal wetting or HCl vapour for SMDs interfacing, making it lower-cost, more scalable \nand more reproducible compared to previous works.   \nWe previously developed versions of digitally printable biphasic composites based on liquid metal and conductive \nsilver composites which have been extensively discussed in ref.19,20,45 The digital printing technique enables rapid fabrication \nof biphasic ink composites, offering a cost-effective and customizable approach to circuit production. Despite these \nadvancements, there are still challenges that need to be addressed to achieve a fully integrated and battery-free circuit with \noptimized performance. The resolution of current digital printing techniques may not be sufficient to develop compact circuits \nrequired for miniaturized applications. Advancements in rapid laser patterning of liquid metal composites with metal nanowires \nhave improved mechanical stability and electrical conductivity. Recent research40,54 has also demonstrated enhanced electrical \nperformance and flexibility through the integration of these materials. Other studies55 reveal high-resolution laser-patterning \ntechniques that maintain mechanical integrity. Moreover, laser patterning stands out53,56,57, as a technique for high-resolution \npatterning. However, implementation of wireless optogenetic devices demands for a combination of high-resolution patterning, \nmicrochip integration, and high conductivity traces, which remains a challenge.  \nIn this work, by combining the use of biphasic liquid metal-based composites, high-resolution laser patterning, and \nthe vapor-soldering technique19, we demonstrate miniaturized hybrid circuits based on liquid metal, that can be used for \noptogenetics applications. This research focuses on rapid prototyping of soft-matter coils for energy harvesting through \nmagnetic resonance coupling. To afford the desired low stiffness and stretchability for the circuits, we use a highly stretchable \nand digitally printable biphasic liquid metal-based conductive composite20,58. Liquid metals, including Eutectic Gallium Indium \n(EGaIn) have proven to provide the best trade-off between electrical conductivity and mechanical deformability. Using digital \nfabrication techniques such as laser patterning, we demonstrate simultaneous fabrication of coils along with other parts of \ncircuits in minutes and with accessible equipment, resulting in cost-effective and rapid implementation of customized circuits \nthat can be adapted according to the user and application19,20.  This work builds over these previous works, by patterning a \nbiphasic ink that, despite containing liquid metal, has a non-smearing behaviour and therefore its deposition and patterning is \nstraightforward. Moreover, it is as well demonstrated interfacing miniaturized surface mount silicon chips, that are necessary \nfor functional energy-autonomous implants capable of harvesting power from their surroundings through wireless power \ntransfer38. We also investigate the optimal geometry to maximize energy harvesting, given the maximum patterning resolution, \nconductivity of the biphasic inkand minimizing its overall size due to the space restrictions associated with implantable devices.\nThis article outlines the methodology for fabricating these soft coils, evaluates their performance, and discusses the \nchallenges and future directions for integrating fully autonomous, wireless energy-harvesting systems in wearable and \nimplantable bioelectronic devices.\n \n \nResults \nFabrication of soft circuits with integrated energy harvesting coil and microchip assembly.  In order to fabricate the battery-\nfree soft circuit with integrated microchip, three main steps should be performed: deposition and patterning of the soft \nconductive circuit, integration of the SMD components, and encapsulation of the circuit with a biocompatible polymer.  Figure \n2A summarizes this process. Here we use a biphasic EGaIn-Ag composite previously developed by our team20 as the conductive Journal XX (XXXX) XXXXXX Author et al  \n 4  \n ink.  As patterning options, various methods exist such as extrusion printing, laser patterning, or stencil printing. Previously we \ndemonstrated that the ink can be digitally printed through extrusion printing 20. While this permits rapid additive manufacturing \nof circuits, the printing resolution of ~3 00µm (trace width and spacing) is limited for miniaturized coils7,15. Therefore, we used \na laser patterning technique in order to obtain more compact coils. This permitted improving the resolution significantly to \ntrace widths of 150 µm and spacing between turns of 50 µm, and consequently fitting more coil turns in the lim ited space. \n Two similar biphasic EGaIn-Ag composite inks were used. These composite inks were studied in the articles20,51, \nthey have only a difference in the polymer used as substrate. The AgEgaIn- TPU (up to ~1.08×10^6 S/m),  was also employed \nin this study due to its slightly higher electrical conductivity than AgEgaIn- SIS (up to ~4.56×10^5 S/m), which may be \ntoluene59,60. This results in \nbetter packing of the composite, thus improved percolation.  However, the SIS based ink has an already proven good reliable \nperformance on the gluing of the chip-pad interface19, which is expected since the SIS hyperelastic binder, has excellent \nadhesive properties. Therefore, as we showed in previous works, when the circuits are subjected to the solvent-assisted \nsoldering process, the substrate fuses with ink, which in turn glues the chip's pad to the circuit, all due to the SIS phase transition.  \nThe process starts with the application of a thin sacrificial film of poly(vinyl alcohol) (PVA) (~ 300 µm) on a flat \nsurface, such as a glass plate, using a thin film applicator (supplementary fig. 1A). This layer serves as the sacrificial layer that \npermits easy circuit release. On top of it, layers of ink are deposited using the same thin-film applicator or a metallic roller with \ntape spacers (supplementary fig. 1B).  We used a Master Oscillator Power Amplifier (MOPA) fiber laser with Infrared (1064 \nnm) wavelength for patterning the circuit (supplementary fig. 1C). This is a low-cost laser system commonly used in print \nshops to engrave metals. As the laser wavelength mostly affects metals, the PVA layer under the ablation area remains intact. \nAfter patterning the circuit, a solution of SIS hyperplastic binder is poured over the patterned circuit, then, after curing and the \ncircuit is peeled off (supplementary fig. 1D). After dissolving the PVA layer, the circuit is turned upside down and SMD \ncomponents are placed using a manual or automatic pick & place machine (supplementary fig. 1E).  \nThe soldering is performed using a vapor-assisted soldering technique recently developed by our team19.  A polymer-\ngel transition is induced on the ink and on the SIS substrate by exposing the circuits to a solvent vapor chamber for 45 min \n(supplementary fig. 1F). During the gel state, the conductive pads of the microchip package penetrate and adhere to the \nconductive ink. The bottom side of the package also adheres to the underlying substrate. And since the chip penetrates the \nsubstrate, it is completely surrounded on all four sides by the adhesive polymer. Providing at the same time self-soldering and \nself-encapsulation of the electronics. Finally, the circuit is encapsulated by PDMS, with the help of a 3D printed mold \n(supplementary fig. 1G). \n \nCoil design optimization and tuning. First showcased by Nikola Tesla61, near-field power transfer utilizes non-radiative \nmethod relies on inductive coupling, \nestablishing a connection between a transmitting coil and a receiving coil62. This strategy provides a highly efficient mean of \ntransmitting power over small distances, making it feasible for practical implementations in commercial contexts such as \nwireless charging of electric vehicles63 and powering of consumer electronics64. \nAlthough designed for near range, the efficiency of power transfer and the achievable distance can be substantially improved \nby utilizing transmitting and receiving coils (resonators) that are finely tuned to establish a robust magnetic resonant \ncoupling16,17. Thus, coil design optimization and tuning to resonate in the same frequency of the transmitter are crucial (13.56 \nMHz). In this section, a description of the methodologies followed for the optimization of the coil design, and its subsequent \ntuning, will be laid out.\n \nDesign optimization.    In the context of magnetic resonance, the coil can be regarded as an inductor65,66, thus we can evaluate \nits power harvesting capabilities in terms of its quality factor,\n-factor of an inductor is a measure of how close the inductor is to an ideal \ninductor, i.e. one that has no parasitic resistance. As can be seen in fig. 3A, the receiver coil in the fabricated devices can be \ndefined by 5 parameters: overall width, overall length, number of turns, trace width and trace spacing. These parameters and \ntheir relationships define the inductance and resistance of the coil. To optimize them, calculations were performed using the \nformulas from ref.67 \nFor wearable \napplications and for implants, on the other hand, we desired them to be as small as possible, to have the minimum possible \nimpact on the body. So, given the size constraint and considering the fabrication resolution limitations, one can optimize design \nparameters of the coil such as number of turns, tracewidth and trace spacing. \nAn exhaustive search algorithm was developed to thoroughly investigate all potential combinations of values for the \nnumber of turns, trace width and trace spacing of the coil. The algorithm systematically assessed the performance of the coil \n(which is reflected in its Q-factor) by rigorously evaluating each possible combination of values in a defined interval of values \nfor trace width and number of turns. The formulas implemented in the code for the inductance and resistance calculations are Journal XX (XXXX) XXXXXX Author et al  \n 5  \n present in supplementary formulas 3 and 4. The results were displayed in the form of 2D plots (inductance/resistance/Q-factor \nversus trace width and number of coils) and intensity maps of the best combination of design values. Subsequently, the obtained \ndata was analysed and visualized to gain insights into the coil's performance across a range of value combinations. This \nmethodology enabled a meticulous examination of the parameter space, aiding in the selection of the most suitable design for \nenhanced per race width and \nspacing). \nIn the search for the optimized values, we imposed conditions such as the exclusion of parameters combinations that \nresulted in: Q values below 25; resistance values above 3 ohms; inductance values below 0.7  and inner diameters smaller \nthan 2 mm. In the end, all these criteria were combined and evaluated in a single plot (fig.3Bi). To visualize the results of the \nsearch algorithm, intensity maps and 2D plots were created (figs.3B).  As one can see in fig.3Bi there is a solution that \nmaximizes the Q-\nQ-value there is a parameter space where the differences in this value are marginal. Thus, a more holistic approach should be \ntaken. \n Starting by the inductance value (fig.3Bii), one can see that it increases with the increase of the number of turns since \nit can concentrate more of the electromagnetic field. On the other hand, maintaining the number of turns fixed, the inductance \ndecreases with the increase of trace width, which can be explained by the fact that the turns get more on the outer contour of \nthe coil, which in turn decreases the coil  fill ratio6567. This insight is of major importance when designing small coils, which \ndue to their space constraint have difficulty in achieving high inductance values, thus reducing the trace width helps to improve \nthe Q-value of a coil. However, the trace width can only be reduced to some extent until it stops being beneficial65,67 , because \nof the fact the resistance also increases with its reduction (fig.3Biii). On the other hand, miniaturized coils have the benefit of \nmuch lower trace lengths for the same number of turns when compared to coils of bigger sizes.  \ns inner diameter, i.e. the empty space inside the coil (measured on \nits last inner turn). This parameter is also associated with the coil fill ratio6567. This parameter gains substantial importance \nwhen designing miniaturized coils since often it will be the space where the electronics will stay, so depending on the circuit \ncomplexity this space can be adjusted. In fig.3Biv, one can see that, as expected, the increase inthe number of turns or the trace \nwidth will always result in an increase i -value \nuntil inner diameters of around 1 - 2 mm6567.  \n \nLC resonator tuning.  Once the optimum parameters for manufacturing the coils had been found, it was necessary to find the \nright tuning for them. Coil tuning for magnetic resonance refers to the process of matching the electrical properties of the coil \nto the desired resonant frequency 16,17. It involves optimizing the coil's inductance (L) and capacitance (C) values to create a \nresonant LC circuit. Tuning is necessary to maximize the efficiency of the coil. It can be made simply by adding capacitors in \nparallel between the coil and the load16,17. \nThe exact values of the required capacitance were determined using a vector network analyzer (VNA) by two different \nmethods: direct measurements, where the coil was directly connected to the VNA (figs.3E) and indirect measurements, where \nthe coil was not connected to the VNA, but was placed on top of a copper coil with similar dimensions, being this one connected \nto the VNA (fig.3D). \nWith the direct measurements, we were able to get the S11 response of the system coil and tuning capacitors for \nstimulation in optogenetic application) was also added in parallel. Since it can be seen as a capacitor itself by the coil, its \ncapacitance is also important to ensure the finding of the correct values for the capacitors to be added. \nWith the direct measurements we obtained the curve of inductance as a function of frequency (Fig.3Ei) and the curve \nof impedance moduli as a function of frequency (Fig.3Eii). By experimenting with different values for the tuning capacitors, \nthe optimum tuning can be obtained, i.e. when the peak of the curve is at 13.56 MHz, in the case of the curve of the impedance \nmoduli versus frequency. For the case of the inductance versus frequency curve, it is when the point of zero inductance is \naligned with 13.56 MHz, i.e., the point that minimizes the inductive reactance65,66. This method was mainly used for the smaller \ndesigns, as their small factor means that they cannot cause any measurable changes in the magnetic field of the receiver coil, \nwhich is connected to the VNA in the direct measurement method.\n \n \nPerformance comparison of copper versus soft coils.  For the evaluation of the performance of copper versus soft coil, four \ncoils were selected from supplementary table 2. The coils selected were the ID1, ID2, ID3, and ID4 (fig.4A). ID1 is a \nrace race \nrace race \ntrace race spacing and overall size of 13 x 7.6 mm.  \nThe antennas ID1 and ID2 were selected as high-performance counterparts for comparison with the best soft antennas \n~7.02 × 105 Sm-1 which is  Journal XX (XXXX) XXXXXX Author et al  \n 6  \n <80× compared to the copper. Therefore, achieving the same efficiency of commercial coils with printed coils is very difficult. \nNevertheless, printed coils are attractive due to fabrication advantages, and the possibility of development of soft, and ultrathin \nelectronics that are necessary for wearable and implantable devices. \nA metric was created to compare the performance of the coils, named the figure-of-merit of miniaturization, \nFOMminiaturization : \nBeing L the inductance (µH) and Vpp the peak -to-peak voltage (V). The A represents the area (mm2\nfill ratio. The resistance represented by R is expressed in ohms. \nFOMminiaturization  serves as a comparator metric which was designed for finding the best coil design when the physical \nspace (for implantation, in this case) is restricted. Three main variables are involved: coil dimension, coil inductance, and coil \nresistance. In an ideal world, one would strive for the largest possible coil given the space limitation, with the highest possible \ninductance and lowest possible electrical resistance. However, for example, when one tries to improve the Q-factor by \nincreasing the inductance through increasing the number of turns or decreasing the trace width, an inevitable increase in the \nelectrical resistance will happen, which has a negative effect on the Q-factor, hindering the desired gains. The same applies for \nwhen one tries to increase the Q-factor by increasing the overall dimensions of the coil. The size limitation for a coil implanted \n, ref.32) is higher than on its back (where implants exceeding 200 mm2 have been \ndemonstrated32), thus, a head-mounted coil will be a lower burden for the mice than a back-implanted one. Hence, a metric was \nneeded for comparison between coils of different sizes. Therefore, the objective of the FOM miniaturization  is to highlight which \ncoil designs provide a more efficient power harvesting performance with the smallest possible footprint. The FOM miniaturization  \nformula was created so that it values high values for inductance and a peak-to-peak voltage in the same proportion that awards \nlow areas and coil fill ratios, but not overly emphasizes on the resistance. Given that when looking to fabricate liquid metal \nstretchable/deformable coils, a compromise has already been established between electrical conductivity and device \ndeformability. \nTo obtain the data of the plots in fig. 4E-G, three setups were used: (fig.4B,4C, 4D). Setup 1 consisted of connecting \na half-bridge circuit connected to the coil to use it as a rectifier to test the response of the coils to different loads. In this setup, \nthe receiver coils were in close contact with the transmitter coils. Setup 2 had the receiver coils directly connected to the \noscilloscope to evaluate the variation of the peak-peak voltage of the AC wave induced by the transmitter coil. Here the distance \nbetween the transmitter and receiver coil was changed with the help of an F-shaped clamp clip and this change in distance was \nmeasured with a laser meter. Setup 3 was broadly similar to experimental setup 2, with the exception that a piece of tissue from \na C57/BL6 mouse was inserted between the transmitter and receiver. This is to simulate a case in which the receiver is \nimplanted, and the transmitter is placed over the skin as a wearable textile or printed circuit (fig. 4D). Therefore, the implant \ncoil was placed in contact with the tissue.  \nIf we analyze the graphs, we can first see from Fig. 4E that the ID1 and ID2 antennas were able to achieve a maximum \nharvest power of more than 180 mW. In these measurements, the coils were in close contact with the transmitter coil. Coil ID3 \nhas the same design as ID2, so these two coils are useful for analyzing the difference in power harvesting between soft antennas \nand copper-based antennas. The resistance of coil ID3, which was made with conductive ink, was \nresistance of its copper counterpart, ID2, was So, although the resistance of the coil was increased by 9 times, ID3 \nwas still able to harvest more than 140 mW at its peak. Also, this is ~25% less than the copper coil, it is still a very good value, \nconsidering the fact that the coil is printed using an ink. Various optimizations were performed to achieve a coil with this value \nof electrical resistance. These iterations are listed in the supplementary table 3-5. The improvements that were added to the \nmanufacturing method are also listed there. The coil ID4 had a different design from the one of antennas ID2 and ID3, however, \nit had an inductance (1.37 µH) close to what the antennas ID2 and ID3 had achieved. But, since its coil length was also bigger \nof around 100 mW, substantially lower than the other three. \nIn terms of the effects of distance between the transmitter and receiver coil, as one can see in the plot of figure 4F, \nantennas ID1-3 started at the same voltage level ( 21 V), whereas ID4 already started at a lower voltage (\nharvested voltage values seem to have an exponential decay with the distance. After 1.5 cm, none of them demonstrated a peak-\nto-peak voltage above 1.5 V. Which was expected due to the size of the transmitting coil, which was comparable to the ones of \nthe receiving coils. \nRegarding how the fabricated coils are affected by the presence of tissue in close proximity (fig.4G), i.e. the capacity \nto simulate the coil being implanted under the skin and the transmitter coil as wearable on top of it, by analyzing the plot of fig. \n4G is perceptible that between 0.2  1.2 cm of distance from the transmitter coil, the differences in power harvesting with and \nwithout tissue seem to be negligible.  Journal XX (XXXX) XXXXXX Author et al  \n 7  \n capabilities. The biggest difference occurred when the tissue was in close contact with both the receiver coil and the transmitter \ncoil. In this case, the receiver coil is only capable of harvesting about 14 V, while the measured voltage without the tissue was \n18 V. This difference is partly due to the fact that the thickness of the tissue did not allow the coils to be as close together as \nis possible when it is not present. Nonetheless, due to the non-radiative nature of magnetic resonant coupling, it was already \ndielectric properties as well as by the existence of obstructions16,37,38. \nTo demonstrate the capabilities of liquid metal composite-based coils for wireless battery-free optogenetics, we \nperformed long-range experiments in a 35 x 25 cm cage with a double loop copper coil wrapped around for wireless power \ntransmission (fig. 5E-G). Here we added the concept of multi-agent optogenetic control (illustrated in figure 1), consisting of \nwireless energy transfer in a cage of mice, where all of the optogenetic implantables can be controlled on-the-fly. That is, \nmultiple battery-free passi\nthe energy harness by the coils in different locations of the cage are available in figure 5G.\n \n \nDiscussion \nIn this work, we demonstrated materials and methods for rapid prototyping of soft-matter implantable coils for energy \nharvesting for optogenetic neuromodulation. We used a liquid metal-based ink to achieve a high electrical conductivity, and a \nlaser patterning technique to achieve rapid and low-cost prototyping. Through several evolutions and optimizations of the \nfabrication method (supplementary tables 3-5), it became possible to fabricate soft coils with 150 µm trace widths and 50 µm \ntrace spacings. This permitted the development of smaller circuits. The overall size of the first device produced was reduced to \n 1/7 of the first coil produced by digital printing, from 600 mm2 (first prototype) to 78.5 mm2 (final prototype), resulting in \na final device smaller than 1 cm2. \nThe improvements achieved in this work on the fabrication method are also of great value for the adaptation of the \ndesigned devices to other applications such as photometry for monitoring neuronal dynamics in the deep brain6,68 and local \ntissue oximetry for continuous sensing of local haemoglobin dynamics5. Other clinical applications in the area of \nneuromodulation for other organs can be as well envisioned, for example, for the application of optogenetics for cardiovascular \nmedicine28,69. \nTowards wireless optogenetics for neuroscience research. The miniaturized implant of fig. 5E can be used for wireless \noptogenetic stimulation in behavioral experiments. Its footprint (19 x 11 mm) has dimensions comparable to the ones used in \nref.32 to implant copper-based rigid devices in small mice breeds such as C57BL/6, which are widely used in animal research.  \nHowever, the implants developed by ref.32 are fabricated on the basis of rigid conductors such as copper and flexible \nsubstrates such as polyimide (Dupont Kapton®, which has a Young´s Modulus of  ~2.5 GPa (ref.70)),\nYoung´s Modulus is  1 -10 kPa (ref.71). The use of Cu-based circuits results in devices that lack tolerance to strain and as well \nlack mechanical compliance with the host, making it uncomfortable, thus failing to adapt to the animal body (e.g. mice C57BL/6 \nor Balb/c, breeds used predominantly in optogenetics research) much less to their large range of movements. This will inevitably \ncause stress in the animals24,28, which in turn will invalidate the conclusions obtained from the animal experiments conducted. \nOn the other hand, the implants that we developed tackle this challenge through enhanced mechanical performance due to their \nstretchability/deformability, without resorting to lithography techniques which are expensive and require the use of cleanrooms. \nTo our knowledge, this is the first demonstration of miniaturized liquid metal-based coils being wirelessly powered in \na long-range setting, in a cage with dimensions comparable to those required to perform wireless optogenetics experiments in \nfreely moving mice. Finally, further development will include a probe system with micro-sized LEDs to transmit electricity \nand light into the brain, as in ref.32. \n \nTowards optogenetic sacral neuromodulation. The miniaturized implant in fig. 2C can be developed as an optogenetic \nstimulator for sacral neuromodulation in overactive bladder (OAB) patients. This requires a probe, as OAB stimulators are \ntypically subdermal and use probes to conduct electricity to inner nerves, making the surgery minimally invasive72. \nAlternatively, placing the stimulator near the nerves complicates communication and increases invasiveness. For optogenetic \nstimulation, the probe can either conduct electricity to a µLED or use an optical fiber to conduct light. The implant can be \nwirelessly powered and communicate via inductive coupling, requiring the patient to wear a powering patch, or it can have a \nrechargeable battery charged by the patch. This work lays the foundation for developing fully soft optogenetic implants for \nneuromodulati  \n \nWhile the developed devices are promising for optogenetic applications, it is important to address the biocompatibility of the \nmaterials used in the implants. In this case, the liquid metal (Eutectic Gallium-Indium, EGaIn) is fully encapsulated in PDMS, \nrather than being directly embedded in the tissue. Therefore, the primary material to study regarding biocompatibility should Journal XX (XXXX) XXXXXX Author et al  \n 8  \n be PDMS, which has been extensively researched and shown to have favorable biocompatibility properties for use in biomedical \napplications73. For instance, studies have demonstrated that PDMS exhibits good compatibility with various cell types and is \nwell-tolerated in vivo. However, the potential cytotoxic effects of the liquid metal itself, particularly with prolonged exposure, \nshould not be overlooked as they may pose challenges in the long-term functionality of implantable devices74,75. \n \nMethods \nPVA synthesis:  The PVA solution is obtained by mixing 5g of PVA powder (Selvol 125, SEKISU) with 50 mL of H2O. The mixture was stirred at 90 °C using \na hot plate until it became a clear and homogeneous solution. \n \nSIS-based inks fabrication:  The first ink experimented with was a bi-phasic ternary Ag-In-Ga ink developed by ref.20 The AgEGaIn-SIS ink is synthesized by \nmixing EGaIn, Ag micro-flakes, and styrene isoprene block copolymers (SIS). The ink is prepared by first dissolving SIS block copolymers (15 wt % SIS) in \na toluene solution and mixing it in a planetary mixer (30 min at 500 rpm). For each 5g of BCP solution, 6.2g of Ag flakes (Silflake 071 Technic Inc.) were \nadded. However, the proportion of EGaIn:Ag was varied, and tests using ratios of 1, 2, and 4 were made in an attempt to incre\nconductivity. The EGaIn solution was prepared by mixing 75.5 wt% Gallium and 24.5 wt% Indium and letting it naturally dissolve for 12 hours. After the \naddition of Ag and EGaIn, the solution goes again to the planetary mixer for 3 min at 2000 rpm. \n \nTPU-based inks fabrication:  The AgEGaIn-TPU ink was synthesized by mixing EGaIn, Ag micro-flakes, and thermoplastic polyurethane (TPU), instead of \nSIS. The first step of the ink synthesis is dissolving the TPU filament (Ninjaflex TPU 1.75mm 3DNF0817510). A mix of tetrahydrofuran (THF, Sigma Aldrich) \nwith dimethylformamide (DMF, Sigma Aldrich) is used as solvent, and the ratio between 2-mTHF and n-DMF is 4:1. The dissolution of TPU filament (15 wt \n% TPU) in THF:DMF takes roughly about 4 days at ambient conditions if one just let it dissolve naturally. For each 1 g of TPU solution, one must add 1.75 g \nof Ag micro-flakes and 1.75 g of EGaIn. Initially, every time a component was added, the solution would go to the mixer (3 min at 2000 rpm). However, when \nthe solution only went to the mixer one time (after all components were added), the final ink seemed to be less viscous, thus easier to spread. Hence, this \nmethod was used primarily. Also, the ratio of EGaIn to Ag micro-flakes was 1:1 in the first ink produced, \ndifferent EGaIn:Ag ratios were experimented (more specifically 2:1 and 3:1). \n \nSIS substrate preparation: The SIS solution is prepared by dissolving SIS (Styrene 14%, Sigma Aldrich) in toluene with a ratio of toluene:SIS of 2:1 and \nmixing it for 30 min at 500 rpm.  \n \nPDMS synthesis: The PDMS used was Sylgard 184 (Dow Corning Corporation). For 10 g of PDMS used, 1 g of curing agent is required. The solutionis mixed \nfor 3 min at 2000 rpm and then degassed for 4 min at 2200 rpm.\n \nToluene vapor exposure: The circuit is placed in an enclosed chamber that contains a paper soaked with toluene, thus generating toluene vapor there. Different \ntreatment times were experimented (more information on supplementary table 3-5). \n \nPVA and ink deposition:  The PVA layer is deposited with a thin film applicator and it is cure in the oven at 60 ºC for about 30 min. Afterwards, two sheets of \nmetallic tape must be placed on the sides of the glass (supplementary fig. 3.2b). They serve as spacers to then apply 2 or 3 layers of conductive ink with a \nspatula. Between layers, one must let the ink dry in the oven for a few minutes. \n \nLaser patterning:  A thin film applicator is used to apply a thin layer of conductive ink to the substrate. A pulsed fiber laser (1064 nm wavelength) then \nselectively removes the ink to separate the circuit traces. The polymer substrate beneath the ablation area is unharmed since this laser wavelength affects mostly \nmetals. This proves to be a successful technique for creating high-resolution circuits with trace spacings as small as 50µm.  The parameters used for ablation \ninvolved hatchings with a line width of 0.01 mm both in 0º and 90º directions, with the marking of the contour. The power of the laser was set to 100%, its \nspeed to 1000 mm/s with a loop count of 10. The frequency used was 50 kHz with a Q Pulse Width of 40 ns. \n \nFlexible PCB fabrication : The process started by applying a layer of photosensitive paint (POSITIV 20) to a fPCB. Then, the next step was laser patterning of \nthe circuit (pulsed fiber laser with 1064 nm wavelength), uncovering the photoresist where one needs the copper to be etched. Next, copper etching of the \nexposed parts using ferric chloride in a hot plate (AGIMATICN) for 45 min at 40°C. Finally, to strip the rest of the photoresist the fPCB was washed with \nacetone. \n \nElectronic device components : The RF harvesting module was built with a tuner capacitor and the dynamic NFC-accessible EEPROM (1.8 x 2.6 mm, NTAG5, \nNTP5210, NXP) which provides internal AC-DC rectification, as well as, user-selectable fixed output voltages. Its power consumption in NFC passive \ncommunication is 0.66 mW. An older version of the NTP5210 IC, the NFC Forum Type 2 NTAG I²C 1K (NT3H1101W0FHK. NXP), was also used. For the \nrelated circuitry components, such as decoupling capacitors, LEDs, and resistors, the 0402 package was selected to minimize the overall footprint of the \ndevices. An SMD red LED (150040RS73240, Wurth Elektronik) was used as the light source in the various device designs, due to its low forward voltage of \n2V. \n \nWireless RF power transmitter:  The model CLRC663 plus NFC Frontend Development Kit (OM26630FDK) from NXP was used. It is a multi-protocol NFC \nfrontend with a maximum output voltage of 1.9W. \n \nCoil optimization calculations:  These calculations and plots were obtained with the aid of MATLAB software. Formulas used are present in supplementary \nmaterials (supplementary formulas 3 and 4). \n \nVIA stretchability evaluation:   \nand 16-bit DAQ (NI USB 6002) and a multimeter (gwInstek gdm-8351). \nTissues for the performance comparison tests:  The tissue was collected from the mice (Charles River) used for a biocompatibility study realized by CNC \n(Centro de Neurociências e Biologia Celular) from the University of Coimbra. All experiments were carried out with the approv al of their animal ethics Journal XX (XXXX) XXXXXX Author et al  \n 9  \n committee (Orgão Responsável pelo Bem -Estar dos Animais (ORBEA)), the approval of the Direcão -Geral de Alimentacão e Veterinária (DGAV), and in \naccordance with EU directives regarding animal use in research. The tissue was obtained from 2 - 5 months old C57/BL6 mice, had a thickness of  0.1 mm \nand a size of  16 x 18 mm. \n \nAbbreviations \nBlock co-polymers (BCP) \nFlexible printed circuit board (fPCB) \nSIS-based polymeric Ag and EGaIn conductive inks  (AgEGaIn-SIS) \nTPU-based polymeric Ag and EGaIn conductive inks  (AgEGaIn-TPU) \nLiquid Metal (LM)  \nStyrene-isoprene-styrene (SIS)  \nLight-Emitting Diode (LED)  \nEutectic Gallium-Indium (EGaIn)\nVector Network Analyzer (VNA) \nOveractive bladder (OAB) \n \n ",
      "metadata": {
        "filename": "Miniaturized liquid metal composite circuits with energy harvesting coils for ba.pdf",
        "hotspot_name": "PCBA_Production",
        "title": "Miniaturized liquid metal composite circuits with energy harvesting\n  coils for battery-free bioelectronics and optogenetics",
        "published_date": "2025-01-19T11:30:46Z",
        "pdf_link": "http://arxiv.org/pdf/2501.11016v1",
        "query": "THT and SMD soldering PCBA production energy efficiency improvement quantitative efficiency gains"
      }
    },
    "Optimization simulation of reflow welding based on prediction of regional center": {
      "full_text": " 1 基于区域中心温度场预测的回流焊接优化仿真  \n隋远，  卜凡洋，邵子龙，闫伟* \n（山东师范大学信息科学与工程学院，山东  济南 250014） \n摘要：实现集成电子产品回流焊接 前，对回焊炉焊接区域中心温控曲线模拟仿真，有助于选择 适当\n的回焊工艺参数，提高回流焊接工艺整体效率及产品质量。根据热传导规律以及比热容公式 ，得到\n焊接区域中心温度曲线关于炉内温度分布函数在传送带位移上的一阶常微分方程， 对于温差较小的\n间隙，使用 Sigmo id函数，得到平滑的区 间温度过渡曲线；对于温差较大的间隙， 利用指数函数和一\n次函数进行线性组合 ，迫近实际凹函数 ，从而得到完整的炉内温度分布函数 。通过求解常微分方程\n得到焊接 参数，并通过计算预测温度场与真实温度分布数 间的均方误差 优化模型参数， 得到一组符\n合制程界限的最优工艺参数 。同时，根据上述建立的基于区域中心温度场预测方法 ，针对特定工业\n生产场景下的实际需求设计了一套 回流焊接优化策略：给定温度参数下速度区间预测策略， 锡膏融\n化回流面积最小参数区间预测策略， 锡膏融化回流面积左右最对称参数区间预测。 仿真结果表明采\n用该方法 得到的温度场预测结果与实际 传感器数据高度吻合 ，具有极强的相关性 。该方法可以很好\n的帮助选择适 当的工艺参数 ，优化生产过程 ，减少设备调试实践， 优化生产产品焊点质量。  \n关键词： 回流焊接；炉温曲线预测 ；机理模型；常微分方程模型；工艺优化；  \n中图分类号： TH164                          文献标识码：A    \nOptimiza tion simulation of reflow welding based on prediction of regional center  temperatur e field  \nSui Yuan，Bu Fan-yang， Shao  Zi-long，Yan  Wei(corresponding author*)  \n(School of Infor mation Science and Engineering， Shandong Normal  University ， Jinan  Shandong  \n250014， China ) \nABSTRACT ：Before reflow soldering of integrated electronic  products, the  numerical simulation of \ntemperature co ntrol curve  of reflo w fu rnace is crucial  for select ing proper parameters and improv ing the \noverall ef ficiency of reflow soldering process and product quality. According to the heat conduction law \nand the  specific heat  capacity formula, the first- order ordi nary differ ential eq uation of the central tempera-\nture curve of the welding area with respect to the t emperature distribution function in the furnace on the \nconveyor belt displacement is obtained. For the  gap with small temperature difference, the sigmoid f unc-\ntion is us ed to o btain a smooth interval temperature transition curve; For the gap with large temp erature \ndifference, the linear combination of exponential function and primary function is used to appro ach the \nactual concave function, so as to obtain the c omplete temperatu re distribution function in the furnace. The \nwelding parameters are obtained by s olving the ordinary differential equation, and a set of optimal process \nparameters consistent with the  proc ess boundary are obtained by calculating the mea n square er ror betwe en \nthe predicted temperature field and the real temperature distribution. At the same time, according to the \nabove established prediction method based on the regional center temperature  field, a set of reflow optimi-\nzation strategies are de signed f or the actual needs of specific industrial production scenarios: speed interval \nprediction  strategy under given temperature parameters, minimum parameter interval prediction strategy of \nsolder  pastes meltin g reflow area, and the most symmetrical parameter interval prediction of solder paste \nmelting reflow area. The simulation results show that the temperature field prediction results obtained by \nthis method are highly consistent with the actual  sensor data, and have strong correlation. This method c an \nhelp to select  appropriate process parameters, optimize the production process, reduce equipment  commis-\nsioning practice and optimize the solder joint quality of production products.  \nKEYWO RDS：reflow soldering ; furnace temperature curve;  mechanism modelin g; ordinary dif-\nferential equation model ; process optimization ; \n \n 基金项目：本课题得到国家自然科学基金(No.62002207, No.6207229 0, No. 62073201) 、山东\n省自然科学基金(No.ZR2020MA102) 资助   2 1 引言  \n元件焊接技术是集成电路板等电子产品生\n产过程中的一项重要工艺，通过加热元器件达\n到锡膏熔点后， 在液态锡表面张力及 助焊剂的\n作用下锡液回流到元 器件引脚上形成焊点 ，进\n而完成将线路板焊盘和元件焊接成整体 的任\n务。  \n实际焊接过程中，工业 界往往采用 回流焊\n接工艺，即使用回流焊炉 设备，将待焊接元器\n件传送经过多个不同温区，使锡粉完成由固态\n到液态再到固态的转换，此过程中锡膏在助焊\n剂等材料 的催化下，融化（锡膏熔点： 217º C）\n形成一层 薄薄的锡珠，在其表面张力 的作用下，\n聚集在元器件焊点表面，经冷却区 制冷凝固，\n实现焊接 。回流焊接作为集成电路板生产中的\n关键工序， 合理的温度曲线设置是保证回流焊\n接质量的关键 [1]。回流焊接的控制实质上是对\n温度工艺 参数的控制[2]，不恰当的温 度工艺参\n数设置会使 PCB板出现焊接不全、 虚焊、元件\n翘立、焊锡球过多等焊接缺陷， 影响产品质量\n[14]。 \n目前，关于温度场工艺参数设定大多通过\n多次重复 实验测试 进行控制和调整 ，测试人员\n使用传感器等设备获取温区 温控曲线，凭借操\n作人员经验观察得出这条曲线反映 的能量作用\n量以及能量作用点，并依次调整温度场工艺参\n数。产品质量的好坏直接受到操作员经验的影\n响。现有的仅通过实验 获得符合工艺 要求的最\n佳温度的方法不仅低效 ，而且在每次实验 仅只\n能获得一组炉温曲线的数据，样本量小， 测试\n结果缺乏 普适性，极大的浪费了人力财力物力。  \n为优化回流焊接工艺参数的调控方法，国\n内外学者 进行了相关研究。目前回流焊温度曲\n线仿真与预测系统主要以F.Sarvar [6]为代表，\n提出了部分以机理模型为主要切入点的优化 方\n法[9] [10]，徐宗煌[8] 提出了一套基于牛顿冷却\n定律的微分方程的炉温曲线优化模型; 席晨曦\n[11]在微分方程的基础上引入 模拟退火算法 辅\n助炉温曲线优化设计；国内的龚雨兵通过数值\n建模与仿真 提出了一种优化的回流焊温度曲线\n控制；饶庶民 [7]在回焊炉温度控制模块的基础\n上，分析了实际温度场变化 规律，使用有限元\n分析软件对 回焊炉内的温度进行仿真分析 ，可\n以很好的 对温度场进行拟合预测并开发了一套适合回焊炉曲线分析的软件；对于 实际回流焊\n接温度曲线设置， 姜海峡 [15]论述了回流焊接\n温度曲线的设置与测试方法，通过对比分析 ，\n可调整参数至更加理想的回流焊接温度曲线冯\n志刚 [3]量化了回流焊接工艺参数对温控曲线\n的影响；张辉华[5]提出了一套面向 混装氮气回\n焊炉的温度曲线控制方案。 \n为解决传统实验测试 方法的弊端，本文采\n用机理模型进行分析研究，提出一种回流焊接\n区域中心 温度场预测 模型，实现区域中心温度\n场工艺参数的预测优化，并针对特定工业生产\n场景下的实际需求设计了 一套回流焊接优化策\n略：温度参数已知 情况的速度区间预测策略 ，\n锡膏融化回流面积最小参数区间预测策略 ，锡\n膏融化回流面积左右最对称参数区间预测 ，能\n够极大的优化生产过程、节省设备调试时间、\n优化生产产品焊点质量。  \n2. 基于常微分方程的回流焊接区域中心温度\n曲线预测模型  \n2.1 模型定义 \n通常情况下，回 流焊炉内部设置若干个小\n温区，它们从功能上可分成 4个大温区：预热\n区、恒温区、回流区、冷却区（如图 1所 示 ）。\n某回焊炉内有 11个小温区及炉前区域和炉后\n区域（如图 1所示），每个小温区长度为 30.5 \ncm，相邻小温区之间有 5 cm的间隙。其中 小\n温区是指具 有加热功能的某一连续加热区间，\n间隙是指没有加热源的某一连续区间 ，大温区\n是指由小温区和间隙组合而成的某一连续区\n间。 实验条件下的回焊炉实际尺寸如表 1所示。 \n \n图1  回焊炉截面示意图 \n表1   某回焊炉内含有 11个小温区及炉\n前区域和炉后区域的具体尺寸  \n炉内位置  区间长度\n(cm) 开始位置\n(cm) 结束位置 (cm) \n炉前区域  25 0 25 \n小温区 1 30.5 25 55.5  3 间隙 1 5 55.5 60.5 \n小温区 2 30.5 60.5 91 \n间隙 2 5 91 96 \n小温区 3 30.5 96 126.5 \n间隙 3 5 126.5  131.5  \n小温区 4 30.5 131.5  162 \n间隙 4 5 162 167 \n小温区 5 30.5 167 197.5  \n间隙 5 5 197.5  202.5  \n小温区 6 30.5 202.5  233 \n间隙 6 5 233 238 \n小温区 7 30.5 238 268.5  \n间隙 7 5 268.5  273.5  \n小温区 8 30.5 273.5  304 \n间隙 8 5 304 309 \n小温区 9 30.5 309 339.5  \n间隙 9 5 339.5  344.5  \n小温区 10 30.5 344.5  375 \n间隙 10 5 375 380 \n小温区 11 30.5 380 410.5  \n炉后区域  25 410.5  435.5  \n参数可调节范围如表 2某回焊炉的可调节\n参数范围所示。  \n表2   某回焊炉的可调节参数范围 \n 符号  默认参数  可调节范围  \n小温区 1~5 TT1  175℃  165℃~185℃  \n小温区 6 TT2  195℃  185℃~205℃  \n小温区 7 TT3  235℃  225℃~245℃  \n小温区 8~9 TT4  255℃  245℃ ~265 ℃ \n小温区10~11及 \n外界空气温度 TT5  25℃  25℃  \n传送带速度  V 70cm/min  65 ~100 cm/min  \n在设定各温区的温度和传送带的过炉速 度\n后，可以通过温度传感器测试某些位置上焊接\n区域中心的温度 ，称之为炉温曲线（即焊接区\n域中心温度曲线） 。 实际生产时可以通过调节各\n温区的设定温度和传送带的过炉速度来控制产\n品质量。在上述实验设定温度的基础上， 各小\n温区设定温度可以进行± 10ºC范围内的调整。\n调整时要求小温区 1~5中的温度保持一致 ，小\n温区 8~9中的温度保持一致 ，小温区10~11中\n的温度保持 25ºC。 传送带的过炉速度调 节范围\n为65~100 cm/ min。 \n在回焊炉电路板焊接生产中， 各温区中心温度场变化应满足一定要求 ，即制程界限（见\n表3）。  \n表3   区域中心温度场变化 制程界限  \n界限名称  最低值  最高值  单位  \n（1）温度上升斜率  0 3 ºC/s \n（2）温度下降斜率  -3 0 ºC/s \n（3）温度上升 至\n150ºC~190 ºC的时间  60 120 s \n（4）温度大于 217ºC的时间  40 90 s \n（5）峰值温度  240 250 ºC \n2.2 模型建立 \n 假设焊接区域中心看作 质点、焊 接系数受\n温度的影响忽略不计、各 加热区设定温度即是\n对应区域炉内温度、焊接过程不考虑热对流。根据热传导规律 （1）以及比热容公式 （2）对\n比得出，小温区焊炉内环境温度与焊接中心区\n域温度的一阶常微分方程 (3)： \n           dTqkdx=−                   (1) \n   Qqm dT=⋅                  (2) \n          [() () ]dT kTx fxdx c v−= −⋅      (3) \n (3)中， ()Tx是炉内环境温度场分布函数，\n()fx是实验测得焊接区域中心温度场分布函\n数。进一步设k\nc−为焊接系数 Q，得到( 4)： \n           [() () ]dT QTx fxdx v= −       (4) \n2.2.1 回流焊炉炉内温度场分布函数 ()Tx  \n由于回焊炉相邻小温区之间炉内环境温度\n场分布符合 Sigmoid函数( 5)，利用此规律，得\n到平滑的温度过渡曲线如图2所示，曲线两端\n是恒温区，中间是无加热源的炉内环境温度区，\n此时 Sigmoid函数经过平移变换、伸缩变换后\n得到 ()Tx解析式( 6)： \n           1()1xSxe−=+              (5) \n()2() ( )\n1xxxTTTx T x x x\ne+−−−= + ≤≤\n+后前后温区前温区\n后 前温区前   (6) \n针对小温区与小温区之间温差过大的特殊\n间隙，利用(4)所述一阶常微分方程推理可知，\n焊炉内环境温度 ()Tx与焊接中心区域温度 4 ()fx温差越小， 其焊炉内环境温度 ()Tx的一阶\n导数越小 ，因此在 TT4=255 ℃，TT5=25℃且环\n境温度也为 25℃的条件下， 凹函数存在 下降趋\n势，由于温差 的不断缩小，()Tx的一阶导 数也\n不断减小 ，此时焊炉内环境 温度 ()Tx必然为凹\n函数。利用位移与焊炉内环境温度 ()Tx的直角\n坐标系易知坐标4 [, ]x TT前，5 [, ]x TT后，代入一阶\n线性函数可得( 7)，代入指数函数可得( 8)，由\n(7)(8)可以得到 炉内环境温度的线性表达式(9)。 \n     15\n54() ( )T x k x x TT\nTT TTkxx=−+\n−=−后\n后前  (7) \n         ()\n452\n45\n4\nln( ) ln( )()\nln( ) lnbx\nTT TTxxxTx Ae\nTT TTkxx\nTTA\ne−\n−= ∗\n− =−\n=\n前\n后前后前   (8) \n        12 () () ( 1 ) ()Tx pT x p T x=⋅ +− ⋅  (9) \n \n图2  两端是恒温区 ，中间是无加热源的炉内\n环境温度区间过渡曲线  \n通过遍历 参数 p，可以确定当 0.8p=时方\n差最小，不同 参数 p下炉温曲线与拟合 曲线方\n差见表4。 \n表4   炉温曲线与拟合数据 在不同 p下方差  \np 0.6 0.7 0.8 0.9 1 \n方差  10161.56 6897.62  4907.24  6333.51  6742.85  \n 因此，得到炉内环境温度场分布函数如下\n表5： 表5   炉内环境温度场分布 函数  \n炉内位置  温度场分布函数表达式  \n炉前区域  5TT  \n小温区\n1~5 1TT  \n间隙 5 21\n1 197.5 202.5()21xTT TTTT\ne+−−−+\n+ \n小温区 6 2TT  \n间隙 6 32\n2 233 238()21xTT TTTT\ne+−−−+\n+ \n小温区 7 3TT  \n间隙 7 43\n3 268.5 273.5()21xTT TTTT\ne+−−−+\n+ \n小温区\n8~9 4TT  \n间隙 9~小\n温区 11 45\n45ln( ) ln( )\n4 339.5 410.5\nln( ) ln( )\n339.5 410.5\n540.2\n0.8 ( 410.5)339.5 410.5TT TTx\nTT TTxTTe\ne\nTT TTx TT−\n−\n−\n−⋅ ⋅+\n−⋅ ⋅− +−前\n \n炉后区域  5TT  \n2.2.2 最优焊接系数 Q预测  \n利用传感器等工具， 测得得焊接区域中心\n温度曲线 ()fx。将 ()fx与炉内环境温度分布\n()Tx代入公式( 4)中，设置各个小温区的温度参\n数以及传送带过炉速度， 并将前一区间 预测的\n()fx的温度末值赋值 给后一区间预测的温度\n初始值，使用四阶龙格库塔法 ，显式迭代非线\n性常微分方程， 求解出各个位置的解常微分方\n程的函数值 。 \n通过遍历焊接系数 Q，利用作方差等方法\n挑选出与实验中测得焊接区域中心温度曲线\nf(x)最为相近的一组温度曲线 ，确定并评价最优\n焊接系数并绘制预测曲线。实验显示得出\n0.021 Q=−时方差最小 ，不同参数 Q下炉温曲\n线与拟合 曲线方差见表6。  5 表6   炉温曲线与拟合数据 在不同 Q下方差  \nQ 0.0200 0.0205 0.0210 0.0215 0.0220 \n方差  3346.01  2975.17  2912.57  3141.50  3646.0 5 \n \n3. 回流焊接方法优化策略  \n根据 2提出的基于常微分方程的 回流焊接\n区域中心 温度场预测 模型，针对特定工 业生产\n场景下的实际需求设计了 一套回流焊接优化 策\n略如 下 ：（ 1）预测设定温度参数下的速度区间 ；\n（2）预测锡膏融化时回流面积最小参数区 间；\n（3）预测锡膏融化时回流面积左右最对称参数\n区间；  \n3.1 预测设定温度参数下的速度区间  \n 各温区温度设定条件下， 利用建立的 焊接\n区域中心温度 场预测模型，对速度从小到大每\n隔0.1cm/min 进行遍历 ，在所有温区温度确定、\n焊接中心温度曲线唯一的条件下可以找出符合表3制程界限的最大传送带过炉速度 ，加快工\n业生产速度。制 程界限如下： \n对于界限条 件（1）、（ 2），要求升降温速度\n不超过 3 ºC/s，判断是否满足：  \n        \n1||3nnTTpt+−= ≤∆          (10)  \n对于界限条件（ 3），找到 150℃和 190℃对\n应的1t和2t，判断是否满足 ： \n21 60 120 tt≤−≤             (11) \n对于界限条件（ 4），找到焊锡熔点 温度对\n应的1t和2t，判断是否满足 ： \n         21 40 | | 90 tt≤−≤        (12) \n对于界限条件（5 ），找到 ()fx的最大值\nmaxT，判断是否满足 ： \n max 240 250 T≤≤            (13) \n通过枚举速度 ，将焊接区域中心温度曲线\n()Tx离散化抽样保存到数组中 ，判断是否满 足\n制程界限( 10)~(13)，记录速度区间并输出 。 \n3.2 预测锡膏融化时回流面积最小参数区间  \n 各温区温度设定条件下， 利用建立的 焊接\n区域中心温度 场预测模型，对速度 v、各温区\n温度设定值1234,,,TT TT TT TT 进行枚举，计算对\n应的焊接 区域中心温度场变化 ()fx，在所有温\n区温度确定、焊接中心温度曲线唯一 、焊接中心温度曲线符合制程界限的条件下， 计算温度\n大于 217℃的部分 的阴影面积2\n1()x\nxs f x dx=∫，经\n过抽样离散后2\n1()x\nxs f x dx=∑ ，寻找使得阴影面\n积最小的各小温区温度以及传送带速度并输\n出。  \n3.3 预测锡膏融化时回流面积左右最对称参数\n区间  \n 各温区温度设定条件下， 利用建立的 焊接\n区域中心温度 场预测模型，对速度 v、各温区\n温度设定值1234,,,TT TT TT TT 进行枚举，计算对\n应的焊接 区域中心温度场变化 ()fx。 \n 通过对焊接中心温度曲线的离散化抽样，\n利用离散化抽 样的方法 按时间间隔 t=0.5s抽样\n得到焊接中心温度曲线的离散化数组 ，通过遍\n历数组的首部与尾部 ，寻找出 ( ) 217fx=℃临界\n点时刻的12,tt，计算临界点到中心点12\n2ttn+=\n两侧对称区间每对 炉温值 的方差\n()2\n12 1[( ) ]n\nnnfx f x−∑ ，从中选择 即符合制程条\n件又能使方差最小，对称面积最小 的最优解。  \n \n4. 仿真与分析  \n为了证明 基于区域中心温度场预测的回流\n焊接模型的有效性，在Matlab环境下进行仿真\n实验测试。 炉内环境温度变化 ()Tx和某次实验\n中测得的焊接区域中心温度场变化 ()fx，如图\n3所示，传送带的过炉速度为70cm/min，炉温\n各温区温度设定如下： 175ºC（小温区 1~5）、\n195º C（小温区 6）、235ºC（小温区 7）、 255º\nC（小温区 8~9）及 25ºC（小温区 10~11）。 \n  6 图3  炉内环境温度变化 和某次实验中测得的\n焊接区域中心温度场变化 示意图  \n图4为随机实验中，焊接区域中心温度曲\n线与不同焊接系数 Q预测的对比图 ，发现当\n0.021Q=时，预测效果最佳。当最佳焊接系数\nQ值确定后， 任意条件下的焊接区域中心的温\n度变化情况都可以确定。  \n \n图4  焊接区域中心温度曲线与不同焊接系数\nQ预测的对比图  \n为进一步 说明本方法的可靠性，使用皮尔\n逊系数，由表7可知当 0.021Q=时，拟合数据\n与真实炉温曲线 高度吻合，皮尔逊系数 高达\n99%，表现出极强 的相关性 ，预测结果如图 5\n所示。  \n表7   实际的炉温曲线与拟合出的炉温曲线\n在不同 Q下的方差与皮尔逊相关系数  \nQ 0.0200 0.0205 0.0210 0.0215 0.0220 \n方差  3346.01  2975.17  2912.57  3141.50  3646. 05 \nPear-\nson 0.99923  0.99902 0.99874  0.99797  0.99692  \n \n图5  某次实验焊 接区域中 心温度曲线与最优\n焊接系数 0.021 Q=−温度预测的对比图  \n5. 结论  \n 传统回流 焊接工艺参数设定时多采用实验\n测试的方法，存在着低效、耗时、设定结果强\n依赖经验 等问题，因此，本研究采用一种机理\n模型对该问题进行分析研究，提出一种回流焊\n接区域中心温度场预测模型，实现焊接区域中\n心温度场 工艺参数的预测优化：根据热传导规\n律以及比热容公式， 得到焊接区 域中心温度曲\n线关于炉内温度分布函数在传送带位 移上的一\n阶常微分方程 ，对于温差较小的间隙， 使用\nSigmoid函数，得到平滑的区间温度 过渡曲线；\n对于温差较 大的间隙 ，利用指数函数和一次函\n数进行线性组合 ，迫近实际凹函数 ，从而得到\n完整的炉内温度分布函数。通过求解常微分方程得到焊接参数 ，并通过计算预测温度场与真\n实温度分布数间的均方误差优化模型参数 ，得\n到一组符合制程界限的最优工艺参数。  \n同时，根据上述建立的基于区域中心 温度\n场预测方法 ，针对特定工业生产场景下的实际\n需求设计了一套回流焊接优化策略：给定温度参数下速度区间预测策略 ，锡膏融化回流面积\n最小参数区间预测策略， 锡膏融化回流面积左\n右最对称参数区间预测。  \n仿真结果 表明本方法得到的区域中心温度\n场预测结果 与实际传感器 数据高度吻合 ，具有\n很强的相关性。 因此，本方法可以 极大的优化\n回流焊接 生产过程 ，节省设备调试实践， 优化\n生产产品焊点质量。  \n \n \n参考文献 ： \n[1] 陈善. 基于 ANSYS仿真的热轧辊瞬态温度\n场分析[J]. 农业装备与车辆工程 ， 2020，  \n58(6): 137- 140. \n[2] 潘开林， 周德俭， 覃匡宇 . SMT再流焊接\n工艺预测与仿真技术研究现状 [J]. 电子工\n艺技术， 2000(5):  185-187. \n[3] 冯志刚， 郁鼎文， 朱云鹤 . 回流焊工艺参\n数对温度曲线的影响 [J]. 电子工艺技术 ， \n2004，  25(6): 243 -246， 251. \n[4] 冯志刚， 郁鼎文，朱云鹤 .PCB的结构特征\n对回流温度曲线的影响研究[J]. 电子元件与\n材料，  2004，(12) :36 -39. \n基金项目：本 课题得到国家自然科学基金(No.62002207 ， No.62072290 ， No. 62073201) 、山东\n省自然科学基金(No .ZR2020MA102) 资助   7 [5] 张辉华， 黎全英， 邴继兵 . 混装氮气回流\n焊接技术研究[J]. 电子工艺技术，  2019，  \n40(3): 143- 147+156.  \n[6] Sarvar F， Conway P P.Effective modeling of \nthe reflow pro -cess:use of a  modeling tool for \nproduct and process  design[J] .IEEE Transac-\ntions on components ， packag ing and man-\nufacturing  technology， 1998， (21) :126- 133. \n[7] 饶庶民. 无铅热风回 流炉温度控制系统的\n设计与研究[D]. 哈尔滨工业大学 ，2008.  \n[8] 徐宗煌， 徐剑莆， 李世龙，林慧雅.回焊炉\n电路板焊接炉温曲线优化模型[J]. 沈阳大学\n学报(自然科学版 )，2021， 33(03):279- 286. \n[9] 孙沛源， 吴双琪， 吉风池.回流焊炉温曲线\n优化设计[J]. 机电信息 ，2021(14):46- 49+52. \n[10] 丛铭智， 李琪，刘斌，王杰铃， 刘靖宇.一\n种基于机理预测的 PCB板回流焊炉温控制\n方法研究[J]. 电子技术与软件工程，2020(24):67- 69. \n[11] 席晨馨. 基于微分方程 +模拟退火算法的炉\n温曲线优化设计[A]. 国家新闻出版广电总\n局中国新闻文化促进会学术期刊专业委员\n会.2020年第四届国际科技创新与教育发展\n学术会议论文集（卷一） [C].国家新闻出版\n广电总局中国新闻文化促进会学术期刊专业委员会 :香港新世纪文化出版社有限公\n司，2020:3.  \n[12] 宋会良.基于回流炉温度曲线测试及分析[J].\n电子测试， 2018(13):63- 65. \n[13] 岳江浩.回焊炉传热温度曲线[J]. 科学技术\n创新， 2021(12):48- 50. \n[14] 李恒.回流焊接工艺及常见质量缺陷的改进\n方法[J].内燃机与配件 ，2020(08):121- 123. \n[15] 姜海峡\n.关于回流焊接温度曲线设置的研究\n[J].新技术新工艺，2019(08):64- 67. \n \n ",
      "metadata": {
        "filename": "Optimization simulation of reflow welding based on prediction of regional center.pdf",
        "hotspot_name": "PCBA_Production",
        "title": "Optimization simulation of reflow welding based on prediction of\n  regional center temperature field",
        "published_date": "2022-06-21T05:31:50Z",
        "pdf_link": "http://arxiv.org/pdf/2206.10119v1",
        "query": "THT and SMD soldering PCBA production energy efficiency improvement quantitative efficiency gains"
      }
    },
    "Polymerase_nicking enzyme powered dual-template multi-cycled G-triplex machine f": {
      "full_text": "Polymerase/nickingenzymepowered\ndual-templatemulti-cycledG-triplexmachine\nforHIV-1determination\nQiuyueDuan,aQiYan,aYuqiHuang,aWenxiuZhang,aShuhuiZhao,aGangYi*a\naKeyLaboratoryofClinicalLaboratoryDiagnostics(MinistryofEducationofChina),\nDepartmentofLaboratoryMedicine,ChongqingMedicalUniversity,Chongqing,400016,P.\nR.China.\nKeywords:DNAnanomachine,HIV,multi-cycledamplificationstrategy,G-triplex,trace\nbiomarkeranalysis\n1.Summary\nWeproposedadual-templatemulti-cycledDNAnanomachinedrivenbypolymerase/nicking\nenzymewithhighefficiency.Thereactionsystemsimplyconsistsoftwotemplates(T1,T2)\nandtwoenzymes(KFpolymerase,Nb.BbvCI).Thetwotemplatesaresimilarinstructure\n(X-X’-Y,Y-Y’-C):primerrecognitionregion,primeranaloguegenerationregion,output\nregion(3’-5’),andthere’sanickingsitebetweeneachtworegions.OutputofT1istheprimer\nofT2andG-richfragment(G3)isdesignedasthefinalproducts.InthepresenceofHIV-1,\nnumerousofG3weregeneratedowingtothemulti-cycledamplificationstrategyandformed\nintoG-triplex/ThTcomplexaftertheadditionofthioflavinT(ThT),whichgreatlyenhanced\nthefluorescenceintensityassignalreporterinthelabel-freesensingstrategy.Adynamic\nresponserangeof50fM-2nMforHIV-1genedetectioncanbeachievedthroughthis\nmulti-cycledG-triplexmachine,andbenefitfromthehighefficiencyamplificationstrategy,\nenzymaticreactioncanbecompletedwithin45minutesfollowedbyfluorescence\nmeasurement.Inaddition,analysisofothertargetscanbeachieved byreplacingthe\ntemplatesequence.Thusthere’sacertainapplicationpotentialfortracebiomarkeranalysisin\nthisstrategy.\n2.Introduction\nIncreasingnumberofstudiesshedlightonthesignificanceofmonitoringdiseasesrelated\ntraceamountbiomarkers[1-4](eg.nucleicacid,proteinandsmallmolecule)forthereason\nthatdeterminingtheconcentrationofamoleculepreciselycanbevitalforearlydiagnosis,\ntreatment,monitoring,andprognosisassessmentofadisease[5-8].Asweknow,biomarkers\n*Authorforcorrespondence(yigang666@cqmu.edu.cn).\n†Presentaddress:KeyLaboratoryofClinicalLaboratoryDiagnostics(MinistryofEducationofChina),Departmentof\nLaboratoryMedicine,ChongqingMedicalUniversity,Chongqing,400016,P.R.China.arefoundinlow-picomolarconcentrationsorevenfemtomolarconcentrationsinclinical\nsamples[9-11].Variousstrategiesservetheunmetneedforultrasensitivedeterminationof\nthesebiologicalmoleculeswithgreataccuracyandspecificity.Forinstance,microarray\nanalysis,PCR(polymerasechainreaction),LAMP(loop-mediatedisothermalamplification)\nreactionandsoon[12-15].Howeverdrawbackssuchasinconvenientoperationprocedure,\ntime-consuminganddependantofcostlyequipmentstillremain,whichleadstolimited\napplicationandtoughdevelopmentinordinarylaboratory.\nSeveralbiosensingstrategieswithpromisingamplificationmethodshaveshowngood\nperformanceinbioanalysis,especiallytheemergingofmultipleDNAmachineswithdifferent\nfunctions,suchasDNAwalkers[16],DNAswitches[17-18],DNArobots[19]andDNA\ncoppiers[20-21].PreciseandintelligentcontroloftheoperationinDNAmachinesunder\nspecificconditionsisanecessaryconditionforitsconstruction.Thatmeans,aDNAmachine\nhastorespondquicklyandpowerfulinexistenceoftargetanalyte,whileneverworkwithno\nstimulate.AndtheamplificationefficiencyisofgreatsignificanceforaDNAmachine.For\nbetterperformance,researchersintegratedseveralamplificationreactionsintoonestrategyto\nimprovetheamplificationefficiency[22-25].However,wefoundthatintegrationofdifferent\namplificationmethodsmeetsdifficultiessuchashighbackgroundcausedbynon-specific\namplificationincomplexreactionmixture,extendedreactiontimeandtroublesomeoperation\nonaccountofcomplicatedthermalcycle.ThuspriorDNAmachinewithidealamplification\nefficiencyundersimpleoperationisingreatdemand.\nPolymerase/nickingendonucleasefueledisothermalexponentialamplificationwithhigh\nefficiencyhasattachedresearchers’interestsasanidealcandidatetoconstructvariousDNA\nmachine-basedanalyticalstrategies.DNAmachinesdrivenbyspecificpolymeraseand\nendonucleaserunprogrammaticallyandaccuratelyowingtopreciseparingoffourbasesin\ndoublehelixandhighefficientextendingornickingfunctionofenzymes[26-28].After\nspecificrecognitionoftargetanalyteandtemplatestrandorrecognitionprobe,enzymesin\nthesemethodsactlike“engine”ofthemachine,drivingthemachineinhighspeed\nautomatically[28-29].\nBenefitingfromtheremarkablespecificityofWatson-Crickbasepairingrule,nucleicacid\nplayedavitalroleinvariousmachine-likeamplificationstrategiesforitsprogrammable\nsequence[30-33].RecentreportsfoundstableDNAsecondarystructurenamedG-triplexby\nshorteningthelengthofG-quadruplexsequencetoa13-mersequencefoldingintothree\nG-tractswithstructuresimilartoG-quadruplexandconductsthesamebiofunctionsuchas\ncatalysisandlight-upfluorescence[33-34].OvercomingthelimitedmodulationofG4caused\nbytherelativelylonger22-mersequence,theG-triplexiseasiertobegeneratedandfreedoff\nfromthetemplate.AndG-triplextemplatebasedpolymerization/nickingreactionsystem\nbringshighersignalamplificationefficiencyforfastergenerationspeedofG-triplex[35].\nMoreover,combiningwithThT,theG-triplexincreasesfluorescenceintensityevenmore\ndramaticallythanG-quadruplex[34].Duetoinherentadvantagessuchascheap,stable,and\neasytosynthesizeofnucleicacid,thisnon-Watson-CrickDNAsecondarystructurewith3\nG-tractshasbeenwidelyusedinapplicationindevelopmentofbiosensingplatforms[35-36].\nInspiredbytheabovework,asimpledual-templatemulti-cycledG-triplexmachinebased\nonpolymerase/nickingenzymefueledisothermalexponentialamplificationforultrasensitive\nbioanalysiswasconstructed.TheDNAmachinesimplyconsistedoftwotemplates(T1,T2)\nandtwoenzymes(KFploymerase,Nb.BbvCI),andG-triplex/ThTcomplexwasusedassignal\nreporter.ConsideringtheinfectionofHIVworldwide,highmortalityofacquired\nimmunodeficiencysyndrome(AIDS)andincreasingrequirementsforearlydiagnosis,\ntherapyandpreventionofvirus’spropagation[37-38],HIV-1genewaschosenastarget\nanalyte.WithHIV-1bindingtoT1,amountsofoutputstrandsYproducedinextending\nreactionofT1staredupnewreactioncycleofT1asfeedbackamplificationandseriesof\ndownstreampolymerization/nickingreactionofT2automatically.Asaresult,massiveG3\nwereproducedthroughthismulti-cycledG-triplexmachineandformedintoG-triplex/ThT\ncomplexwiththeexistenceoftheinexpensivewater-solublefluorogenicdyeThT[39].Owing\ntothesemerits,anewone-potandlabel-freeG-triplexmachineforfastandultrasensitive\ndeterminationofHIV-1withlowbackgroundwasconstructed.3.MaterialsandMethods\n3.1.Reagentsandmaterials\nHPLCpurifiedoligonucleotidesusedinthisresearch(AslistedinTableS1†)weresupplied\nbySangonBiotechInc.(Shanghai,China),andfullydissolvedintheiceboxwithTEbuffer\n(10mMTrisHCl,1mMEDTA,pH8.0)intoastorageconcentrationof10μM.1×TEbuffer,\n10mMdeoxynucleotidetriphosphates(dNTPs),6×DNAloadingbuffer,30%acrylamide/bis\nsolution,ammoniumpersulfate(APS)wereobtainedfromSangonBiotechnologyCo.Ltd\n(Shanghai,China).N,N,N’,N’-tetramethylethylenediamine(TEMED)and20bpDNALadder\n(DyePlus)usedforpolyacrylamidegelelectrophoresiswasacquiredfromTaKaRaBiotech\n(Dalian,China).GoldViewwasacquiredfromSolarbioLIFESCIENCES(Beijing,China).\nThioflavinT(ThT)waspurchasedfromBBILIFESCIENCESCORPERATION(Shanghai,\nChina)andresolvedwithultrapurewaterinto100μMforfluorescentmeasurement.\nNb.BbvCIendonucleaseandKlenowfragment(3’-5’exo-)polymerase(KFpolymerase)used\ninthisstudywereservedbyNewEnglandBiolabs(Beijing,China).Ultrapurewaterusedfor\nsolutionpreparingwasobtainedfromaMilliporewaterpurificationsystemwitharesistivity\nof18.2MΩcm.HumanserumsamplesforrecoveryexperimentwereofferedbytheFirst\nAffiliatedHospitalofChongqingMedicalUniversity.\n3.2.Apparatusandinstruments\nACaryEclipseFluorescenceSpectrophotometer(AgilentTechnologies,PaloAlto,CA)was\nusedforallfluorescencespectrameasurementsusingaquartzfluorescencecuvette(optical\npathlengthof1.0cm)atanexcitationwavelengthof442nm.And10nmwassetasboth\nexcitationandemissionslitwidths.TheDYY-6Celectrophoresisanalyzer(LiuyiInstrument\nCompany,China)andBio-RadChemDocXRS(Bio-Rad,USA)wereusedforpolyacrylamide\ngelelectrophoresis.\n3.3.FluorescencespectrophotometeranalysisofHIV-1gene\nAconventionaloperationofthemulti-cycledG-triplexmachinewasperformedinaone-step\nwayasdescribedbelow.Toreducethebackgroundsignal,thereactionliquidwasdivided\nintotwopartsnamedmixtureAandmixtureB.MixtureAincludedvariousconcentrationsof\nHIV-1gene,twotemplates(T1andT2),dNTPsandNEBbuffer2.MixtureBcontainedKF\npolymerase,Nb.BbvCIandNEBbuffer2.Andthetwopartswerefullymixedinanicebox\nbeforea37℃incubationfor45minatafinalvolumeof10μl.Afterthat,themixturewas\nheatedat85℃for10mintoinactivatetheenzymes.Finally,5μMThT,50mMKCland\nultrapurewaterwereaddedintomakea50μlvolumesolutionforfluorescence\nmeasurement.\n3.4.GelElectrophoresisanalysis\nThemulti-cycledG-triplexmachinewasanalyzedby12%nativepolyacrylamidegel\nelectrophoresis(nPAGE)in1×TBEbufferataconstantvoltageof120Vfor45min.\nGoldViewwasusedforgelstainingsubsequently,andafteranexposuretotheultraviolet\nlight,thedyedgelwasvisualizedviaagelimagingsystem(Bio-RadLaboratories,USA).3.5.Circulardichroism(CD)measurements\nCirculardichroism(CD)measurementswerecarriedoutontheChirascanCDspectrometer\n(AppliedPhotophysicsLtd.,UK)ina1mmpathlengthquartzcuvette.Threescanswere\nperformedforeachsampleatroomtemperatureunderthefollowingparameters:range200–\n500nm,bandwidth1nm,scanningspeed200nm/minandaresponsetimeof0.5s[34].\n4.Resultsanddiscussion\n4.1.Principleofthedual-templatemulti-cycledG-triplex\nmachine\nPrincipleoftheG-triplexmachineisillustratedinscheme1.Theone-potlabel-freeG-triplex\nmachinewasformedbypolymerase/nickingenzymefueledmulti-cycledexponential\namplification.ThetwotemplatesnamedT1,T2weresimilarinstructure(X-X’-Y,Y-Y’-C):\nprimerrecognitionregion,primeranaloguegenerationregion,outputregion(3’-5’),and\nthere’sanickingsitebetweeneverytworegions.WiththeexistenceofHIV-1gene,HIV-1\ngenehybridizedwithT1andswitchedonthepolymerase/nickingendonucleasepowered\nDNAmachineautomatically.Inbrief,thetargetanalyteHIV-1DNAtriggeredcycle1with\ngenerationofmanyX’andY.Cycle2wasinducedbyplentyofX’whichledtomore\nproductionofYsubsequently.ConsiderableamountsofYstartedupcycle3asprimerofT2\nandbroughtmassiveamountsofY’andG3.Finally,G3emergedinlargenumberowningto\nthefeedbackamplificationconductedbyY’incycle4.Greatlyenhancedfluorescence\nintensitycouldbeobtainedwithnumerousG3formingintoG-triplex/ThTcomplex.\nScheme1.Principleofthedual-templatemulti-cycledG-triplexmachineforHIV-1determination.\n4.2.CharacterizationofG-triplexCDandfluorescencespectrophotometeranalysiswasusedforcharacterizationaboutthe\nstructureandbiofunctionofG-triplex.AsdepictedinFigure1A,there’reapositivepeakat\n265nmandanegativepeakat240nminspectrumofG-triplex,whichmatchthetypical\nparallelstandarrangement,suggestingformationofparallelG-triplex[34].Andthiscanbe\nwelldistinguishedbythecharacteristicofdouble-strandedDNAandsingle-strandedDNA\nwhichpossessapositivepeakat280nm.[40]Thetwopeaksremainedunchangedafterthe\nadditionofThT,andanewnegativepeakappearedintheCDspectrumat425nm,indicating\nanintercalationmodeofbindingwithoutchangeofparallelstructureintheG-triplex/ThT\ncomplex.Figure1Billustratedthatfluorescencesignalwasextremelylowinthepresenceof\neitherThTorG-triplex,whilegreatlyenhancedfluorescenceintensitycouldbemeasuredin\nG-triplex/ThTcomplex.TheseresultsaregoodproofoftheformationofG-triplexandits\nbiologicalfunctioninenhancingfluorescenceintensityofThT.\nFigure1.CharacterizationofG-triplex.(A)CDspectraof5μMG-triplex,100μMThTandthemixtureof\nG-triplexandThT.(B)Fluorescencespectrophotometeranalysisof1μMG-triplex,5μMThT,andthemixture\nofG-triplexandThT(25mMTris–HClbuffer(pH7.4)containing50mMKCl).\n4.3.FeasibilityoftheG-triplexmachine\nToevaluatefeasibilityoftheG-triplexmachine,fluorescencespectrumanalysisandnPAGE\nwascarriedon.Figure2AwasthediagramofnPAGE.Lane1,Lane2andLane3wereHIV-1\nDNA,T1andT2.Lane7andLane8werefinalproductsG3and20bpDNAmarker.Wecan\nseeproductsbandinlane5wasthesameastheG3bandinlane7,thismanifesteda\nsuccessfulconstructionoftheamplificationstrategy.TheG3banddidn’tappearinlane4or\nlane6forabsenceofNb.BbvCIinlane4,andlackofHIV-1DNAinlane6,thusthe\npolymerization/nickingreactioncanhardlycarryout.Theresultscertificatedthatthe\nmulti-cycledamplificationstrategyhasbeenestablishedsuccessfully.\nInordertofurtherprovetheconstructionofthisG-triplexmachine,fluorescencespectrum\nanalysiswasputtedon.Inaddition,weconstructedatraditionaltwo-cycledstrategywitha\ntemplatenamedS1,andconductedafluorescencespectrumanalysisofthistwo-cycled\nstrategycomparedwithourmulti-cycledstrategytowards1nMHIV-1DNA.Asdepictedin\nFigure2B,there’sonlyweakfluorescenceinblank,andahighfluorescenceintensitywas\ndetectedinourmulti-cycledstrategywithasignal-to-noiseratio(SNR)uptonearly11times,\nseetheredcurve.Moreover,fluorescenceintensityinthetwo-cycledstrategywasmuchlowerthanthatinourstrategy,seethepinkcurve.Theseresultsrevealedthatconstructionof\nthedual-templatemulti-cycledG-triplexmachinewassuccessfulanditownedadmirable\nperformance.\nFigure2.Feasibilityofthedual-templatemulti-cycledG-triplexmachine.(A)Verificationofthedual-template\nmulti-cycledamplificationstrategythroughthe12%nPAGE.Line1,HIV-1;Line2,T1;Line3,T2;Line4,\nT1+T2+HIV-1+KF;Line5,T1+T2+HIV-1+KF+Nb.BbvCI;Line6,T1+T2+KF+Nb.BbvCI;Line7,G3;Line8,20bpDNA\nmarker(Sampleconcentrationsare2μMinLine1,Line2,Line3and3μMinLine7.ConcentrationofT2and\nT1are1μMand100nMseparatelyinLine4,5and6.(B)Fluorescencespectroscopyanalysisofthe\ndual-templatemulti-cycledG-triplexmachinecomparedtotwo-cycledamplificationstrategy.Curvea,blankof\nthemulti-cycledamplificationstrategy;Curveb,signalofthemulti-cycledamplificationstrategytowards1nM\nHIV-1DNA;Curvec,blankoftwo-cycledamplificationstrategy,Curved,signaloftwo-cycledamplification\nstrategytowards1nMHIV-1DNA.\n4.4.Optimizationofexperimentalconditions\nExplorationofoptimalreactionconditionsincludingmolarratiobetweenT1andT2(T1/T2),\nthereactiontime,dosageofKFpolymeraseandNb.BbvCIwereconductedforacquisitionof\noptimalperformance.Asweknow,manypolymerase/nickingreactionsystemmeetsthehigh\nbackgroundsignalcausedbynon-specificamplificationofexcessivetemplates,andwefound\nthatinappropriatemolarratiooftemplatesledtohighbackgroundsignalaswell.Thus,we\ngaveprioritytooptimizationofT1/T2.AseriesofT1/T2wassetunderatotalconcentrationof\n100nMtogetthemostpowerfulDNAmachine.AsshowninFigure3A,theSNRhasbeen\ngreatlyimprovedwithT1/T2reducedfrom1/10to1/160,andreachedthehighestlevelunder\namolarratioof1/80.WhenT1/T2decreasedto1/160,theSNRdroppedalittle,thismightbe\nassociatewithaslightlydeclineofthesignalamplificationefficiencyduetofewerT1inthe\nreactionsystem.Therefore,1/80wassetasthemostsuitableT1/T2forsubsequent\nexperiments.\nWenextinvestigatedtheenzymaticreactiontime.AsshowninFigure3B,extensionofthe\nreactiontimeimprovedtheSNRinthebeginning,andtheidealSNRappearedatthereaction\ntimeof45minutes.Asthereactiontimeprolonged,theSNRdroppeddramatically.Thus,\n45minwasadoptedforthemulti-cycledamplificationreactiontimeaccordingly.\nKFpolymeraseistheenergyproviderofthisDNAmachine.Weanalyzedperformanceof\nthesensoratdifferentdosageofKFpolymerasesubsequently.AsillustratedinFigure4A,\nwiththedosageofKFpolymeraseincreasedto0.15U,thefluorescencesignalenhanced\nsignificantlyandthebackgroundremainedlow.FurtherincreasingoftheKFpolymerase\nheightenedthebackgroundrapidlyanddecreasedtheSNRaspresentedinFigure4B.The\nresultsindicatedthatworkingefficiencyoftheG-triplexmachinehadreachedthebestwithdosageofKFpolymeraseat0.15U,sowedeterminedthesatisfieddosageofKFpolymerase\nwas0.15U.\nFigure3.Optimizationof(A)T1/T2.(B)thereactiontime(1nMHIV-1,T1/T2at1/80,0.15UKF,5UNb.BbvCI).\nErrorbarsindicatesstandarddeviationoftestsforthreetimes.\nFinally,anotherpartofengineinthisG-triplexmachine,dosageoftheNb.BbvCIwas\nexplored.InFigure4C,fluorescenceintensityimprovedasthedosageofNb.BbvCIincreased,\nandreachedthehighestpointat5U.WithNb.BbvCIdosageroseto6U,fluorescence\nintensitydroppedabit.ThismightduetooversaturationoftheNb.BbvCI.Wecanseethat\ntheSNRelevatedfastinviewoftheincreasingenergyprovidedbyNb.BbvCI,andthen\nleveledoffat5UasshowninFigure4Daccordingly.Therefore,5Uwasconsideredasthe\noptimalNb.BbvCIdosagetoensurewellperformanceofthesensor.\nFigure4.Optimizationexperiment.(A)EffectsofdifferentdosageofKFpolymeraseonthefluorescence\nintensity(1nMHIV-1,T1/T2at1/80).(B)SNRtowardsdifferentdosageofKFpolymerase.(C)InfluenceofthedosageofNb.BbvCI(1nMHIV-1,T1/T2at1/80,0.15UKF).(D)SNRtowardsdifferentdosageofNb.BbvCI.\nErrorbarsindicatesstandarddeviationoftestsforthreetimes.\n4.5.Analyticalperformanceofthebiosensingstrategy\nSensitivity,selectivityandstabilityareprimarilyincludedinanalyticalperformanceofa\nsensor.Herein,optimalconditionswereusedforexploringoftheanalyticalperformance.\nFirstofall,differentconcentrationsofHIV-1gene(50fM-2nM)wereusedforfluorescence\nintensitymeasurementsinevaluationofthesensitivity.AsshowninFigure5A,higher\nfluorescenceintensitywasmeasuredwithtargetgeneincreasedgradually.Thiscanbe\nattributedtothefactthatmoreHIV-1DNAproducedmoreintermediatestrandYand\ninducedmassiveproductionofG3,whichgreatlyenhancedthefluorescencesignalofThT.\nAndthere’salinearrelationshipbetweenlogarithmoftheHIV-1geneconcentration(C)and\nthecorrespondingfluorescenceintensity(F)inanHIV-1DNAconcentrationrangeof50fM-2\nnMasdescribedinFigure5B.CorrespondingregressionequationisF=41.20lgC+209.50\nwithacorrelationcoefficientof0.9971.Accordingtothe3σrule,thelimitofdetection(LOD)\nwascalculatedtobe30.95fM.Andthishasbeenaprioranalysisabilitycomparedwithmany\nreportedsensingstrategies(TableS2†).Goodperformanceofthisdetectionstrategybenefited\nfromthesuperiormagnificationeffectofthemulti-cycledamplificationstrategyinthis\nconstructedDNAmachineandthehighefficiencyofthecooperativeenzyme-drivenreaction.\nFigure5.Sensitivityofthemulti-cycledG-triplexmachine.(A)Fluorescencespectrumofthedual-template\nmulti-cycledDNAmachinewithtargetHIVindifferentamounts:0,50fM,100fM,1pM,10pM,100pM,1nM,\n2nM.(B)PlotthelinearrelationshipbetweenthefluorescenceintensityandtheconcentrationofHIV-1from\n50fMto2nM.Errorbarsindicatesstandarddeviationoftestsforthreetimes.\nTofurtherstudytheselectivityofthisintelligentDNAmachine,anon-complementary\nnucleicacidandthreemismatchedstrands(single-basemismatched,two-basemismatched,\nandthree-basemismatchedDNA)namedrDNA,DNA1,DNA2andDNA3wereusedfor\nfluorescencemeasurementsinthesameconcentration.Figure6Aillustratedfluorescence\nresponseofthesenucleicacidandHIV-1inthesameassaycondition.TherDNAledto\nextremelylowfluorescenceresponseandsignalresponseofDNA1,DNA2,DNA3weremuch\nlowerthanthesignificantfluorescenceintensityroseinHIV-1geneaswell.Thisclearly\nrevealedthatthedevelopedDNAmachinepossessedhighspecificityintargetDNA\nidentifying.\nFinally,weanalyzedthestabilityofthesensingstrategybyplacingthesampleat4℃for7\ndays.Duringtheseven-daystorageperiod,thepeakintensityofthefluorescencespectrum\ndidnotchangesignificantly.AsshowninFigure6B,theRSDvalueoftheseven-daytestperiodwas3.96%,indicatingasuperiorstabilityofthismulti-cycledG-triplexmachine\nduringstorage.\nFigure6.(A)Specificityofthedual-templatemulti-cycledG-triplexmachineinanalysisofHIV-1.(B)Stabilityof\ntheconstructedG-triplexmachineina7-daystorageunder4℃.Errorbarsindicatesstandarddeviationof\ntestsforthreetimes.\n4.6.Interferencestudies\nConsideringthecomplicatedcompositionofactualsample,recoveryexperimentwascarried\noutin20timesdilutedserumforevaluationoftheutilizationpotential.Different\nconcentrations(100fM,10pM,1nM)ofHIV-1genewereaddedtothedilutedserum,and\ncorrespondingfluorescenceintensityweremeasuredthen.AsdemonstratedinTable1,\nrecoveryratesofthesamplesrangedfrom96.42%to104.93%withRSDof0.84%,1.86%and\n1.79%separately.Theabovedatashowedthatinterferenceoftheactualsamplewas\ninsignificant,revealingthepowerfulapplicabilityandcertainapplicationpotentialforclinical\ndiagnosticsofthismethod.\nTable1.RecoverytestoftargetDNAinhumanserumdilutedfor20times.\nSpikedsample\nconcentrationDetected\nconcentrationRecovery RSD\n100fM 104.93fM 104.93% 0.84%\n10pM 9.64pM 96.42% 1.86%\n1nM 1.02nM 102.15% 1.79%\n5.ConclusionWehaveproposedanewone-potandlabel-freeG-triplexmachineforfastandultrasensitive\ndeterminationofHIV-1withlowbackground.Theconstructedsensingstrategyintroduced\nmultipleamplificationcycleandshortenedthereactiontimesothatthereactioncanbe\ncompletedwithin45minutes.Moreover,thissimplereactionsystemdidn’tneedcomplicated\ntemperaturecycle,troublesomeoperationorcostlyequipment.Itperformedinaone-step\nwayinsimplereactionsystem(twotemplatesandtwoenzymes),thusdecreasing\nnon-specificamplificationandmaintainingalowbackground.Atthesametime,the\nestablishedpolymerase/nickingendonucleasedrivenG-triplexmachinepossesseshigher\nefficiencyensuringexcellentsensitivity,andrealizinganultrasensitivedetectionofHIV-1\nDNAaslowas30.95fMwithawidelinearresponserangeof50fM-2nM.Inaddition,\nutilizationoftheG-triplex/ThTcomplexasafluorescentprobedidn’trequireextra\nmodificationofnucleicacids,therebyreducingthecost.Furthermore,byreplacingsequence\nofthetargetrecognitionregionintemplate,detectionofanyotherobjectscanbeachievedas\nwell.Thisstrategyprovidesanewideaforthedetectionoftracesubstances.\nFundingStatement\nThisworkwassupportedbytheScienceandTechnologyResearchProgramofChongqingYuzhong\nDistrictScienceandTechnologyCommission(GrantNo.20180127)andtheSpecialFundProjectKey\nLaboratoryofClinicalLaboratoryDiagnostics(MinistryofEducation).\nCompetingInterests\nWedeclarewehavenocompetinginterests.",
      "metadata": {
        "filename": "Polymerase_nicking enzyme powered dual-template multi-cycled G-triplex machine f.pdf",
        "hotspot_name": "PCBA_Production",
        "title": "Polymerase/nicking enzyme powered dual-template multi-cycled G-triplex\n  machine for HIV-1 determination",
        "published_date": "2020-06-28T09:14:10Z",
        "pdf_link": "http://arxiv.org/pdf/2006.15548v1",
        "query": "THT and SMD soldering PCBA production energy efficiency improvement quantitative efficiency gains"
      }
    },
    "Separation of γ_π_0 showers at high energies": {
      "full_text": "arXiv:hep-ex/9610005v3  14 Oct 1996TAUP 2369-96\nSeptember 25, 1996\nSeparation of γ/π0Showers at High Energies\nJ. Grunhaus and S. Kananov\nSchool of Physics and Astronomy, Raymond and Beverly\nSackler Faculty of Exact Sciences, Tel Aviv University,\nTel Aviv 69978, ISRAEL\nSubmitted to Nuclear Instruments and Methods.\nAbstract\nWe have designed and carried out simulation studies of a two layer Sho wer\nMaximum Detector diagonally oﬀ-set (SMD-dos) optimized for the se para-\ntion ofπ0showers from γshowers in the 30 to 150 GeV energy range. For\n90%γacceptance the SMD-dos yields π0rejection eﬃciencies of 92 ±4 %,\n87±4 % and 32 ±2 %, respectively, for 30, 50 and 150 GeV incident energies.\nWe ﬁnd that the SMD-dos is superior to a conventional geometry sin gle-layer\nor mutiple-layer shower maximum detector (SMD), of equal granula rity, by\nan average factor of ∼1.5 over the 50 to 150 GeV energy range. We also\nﬁnd, the unexpected result, that the SMD-dos gives better π0rejection, for\nthe same number of channels, than a SMD. At hadron - hadron collide rs the\nsignature of choice for the detection of the Higgs particle, in the ma ss range\nof 120 to 160 GeV, is via the decay H→γγ. The addition of a SMD-dos to\nthe planned detectors at the LHC would signiﬁcantly reduce the bac kground\nto theγsignal coming from proliﬁc π0production.1 Introduction\nThechallenging taskofseparating π0initiatedshowers from γinitiatedshow-\ners, at high incident energies, has over the past few years attrac ted a lot of\ninterest and motivated extensive R and D work [1, 2, 3, 4, 5, 6, 7]. Th e sched-\nuled building of the LHC machine has added new interest to the γ/π0sepa-\nration problem. The main motivation for building the LHC machine is the\nquest for the Higgs particle. The Standard Model has weathered e xtremely\nwell strenuousexperimental scrutinyduringthepastdecades; h owever, itstill\nlacks experimental support for the predicted mass generating Hig gs mecha-\nnism. Theonlyviableexplanationofthemassesoftheelementarypar ticlesin\nthe framework of the Standard Model is the Higgsmechanism. The t wo main\ncollaborations approved for operation at the LHC, ATLAS [8] and CM S[9]\nstate in their Technical Reports the paramount importance of the searches\nfor the Higgs particle at the LHC machine. The upgrading of LEP to LE P2\nwill allow the search for the Higgs particle up to a mass of about 100 Ge V.\nSearches for the Higgs particle at higher masses will have to wait for the\noperation of the LHC machine.\nWehavedesigned andcarriedoutsimulationstudies ofatwo layer Sho wer\nMaximum Detector diagonally oﬀ-set (SMD-dos) optimized for the se para-\ntion ofπ0showers from γshowers in the 30 to 150 GeV energy range. A\ncomparison of the SMD-dos with a conventional geometry single-lay er and\nmutiple-layer shower maximum detector (SMD), having the same gra nular-\nity, shows thatthe SMD-dosyields higher π0rejection eﬃciency intheenergy\nrange studied.\nWe have also carried out a comparison of the SMD-dos versus a single\nlayer SMD with both detectors having the same number of channels. We ﬁnd\nthe unexpected result that the SMD-dos yields better π0rejection eﬃciency\nthan the SMD.\nThe electromagnetic showers were generated andstudied using th e Monte\nCarlo program GEANT 3.15 implementing a 100 KeV cut for γ’s and elec-\ntrons. The shower maximum detector, either a SMD-dos or a SMD, w as\nembedded inside a E.M. calorimeter, consisting of a mixture of Pb and s cin-\ntillator (CH) having a radiation length of 0.8 cm. The performance of t he\nSMD-dos was optimized by carrying out extensive Monte Carlo studie s of\ntheγ/π0separation eﬃciency as a function of the following design parame-\nters: number of scintillator layers, thickness of scintillator layer, g ranularity\n1(cell size) and position of the SMD-dos or SMD inside the E.M. calorimet er.\nTypically, samples of 500 showers were generated for each conﬁgu ration of\ndesign parameters and incident energy. The distance of the E.M. ca lorimeter\nfrom the interaction region was kept ﬁxed at 150 cm, the design dist ance of\nthe CMS detector approved to operate at the LHC machine, and th e showers\nwere generated with the shower initiating particle, a γorπ0, normally inci-\ndent on the E.M. calorimeter. We have studied the stand-alone capa bilities\nof the SMD-dos; however, since the algorithms developed assume t hat the\nenergy of the shower, in question, is known, the SMD-dos must be u sed with\nan E.M. calorimeter to determine the total energy of the shower.\nAt hadron - hadron colliders the signature of choice for the detect ion of\nthe Higgs particle, in the mass range of 120 to 160 GeV, is via the deca y\nH→γγ[10]. These searches will encounter high γbackgrounds coming\nfrom proliﬁc π0production which decay into two γ’s. The attainment of\ngoodγ/π0separation is of crucial importance to the success of the propose d\nsearches. The incorporation of a SMD-dos into the planned detect ors at the\nLHC would appreciably reduce the γbackground.\n2 The Geometry of the Diagonally Oﬀ-set\nShower Maximum Detector\nThe geometry of the SMD-dos, shown in Figure 1, consists of 2 ident ical\nscintillator layers, placed one behind the other. Each layer is subdivid ed\ninto square cells which are individually read out and the energy deposit ed\nin each cell is recorded. The 2 layers are diagonally oﬀ-set with respe ct to\neach other by half a cell, so that the intersection of any four adjac ent cells\nin one layer corresponds to the center of a cell in the other layer. S tudies of\ntheγ/π0separation eﬃciency of granular detectors show that the separa tion\neﬃciency decreases asafunctionofthedistance of theshower ce nter fromthe\nnearest intersection. Hence increasing the number of intersectio ns per unit\narea enhances the separation eﬃciency. The area of each cell, in th e front\nlayer, overlaps symmetrically one quarter of the area of each of fo ur adjacent\ncells in the back layer. And likewise the area of each cell, in the back laye r,\noverlaps symmetrically one quarter of the area of each of four adj acent cells\nin the front layer. Therefore, the energy collected by a particular cell is\n2subdivided unto 4 cells giving a substantial increase in granularity.\nFigure 1: Layout of the two layers of the SMD-dos. The cells of the f ront\nlayer are shown in solid lines and the cells of the back layer are shown in\ndashed lines. One back layer cell is highlighted to show the four quart er cells\nin the front layer which overlap it.\nIn the singular case that all the energy deposited by a shower is con ﬁned,\nin a particular layer, to one cell it is impossible with a conventional one- layer\nor multiple-layer SMD to determine whether the shower was initiated b y a\nγorπ0. However, the unique geometry of the SMD-dos, which subdivides\nthe energy collected by a particular cell, in one layer, unto four cells in the\n3other layer, does furnish some degree of γ/π0separation eﬃciency even in\nthis singular case.\n3 Deﬁnition of Variables Used in the Algo-\nrithms\nThe energy and position information collected, per event, by the sc intillator\ncells is used to construct 10 variables.\n a)\nimaxjmax b)\n c)\nimaxjmax d)\nFigure 2: Display of cell conﬁgurations used in the determination of Emax,\nE4,E8andE12.\nThese variablesembody theexperimental informationwhich goesint o the\nγ/π0separation algorithms. The information collected in the ﬁrst layer is\nused to construct 5 variables and likewise the information from the s econd\nlayer is used to construct the other 5 variables. The detailed descr iption and\nconstruction of these 10 variables is detailed in this section. In Figur e 2 are\ndisplayed the cell conﬁgurations used for the determination of Emax,E4,E8\nandE12.\n4•< r >, the average energy weighted radius of the shower. The deter-\nmination of < r >from the data collected by the cells in the layer is\ndiscussed in the following paragraph.\n•Emax, the maximum energy deposited in a single cell, in the layer. See\nFig. 2a.\n•E8, the sum of energies deposited in the eight cells which surround the\ncell with the maximum energy. E8is normalized to Etotal. See Fig. 2b.\n•E4, the sum of energies deposited in a 4 cell square which includes the\ncellswiththemaximumandnexttomaximumenergy. E4isnormalized\ntoEtotal. See Fig. 2c.\n•E12, the sum of energies deposited in the 12 cells which surround the 4\ncell square described in the preceding item. E12is normalized to Etotal.\nSee Fig. 2d.\nThe energy information gathered by each cell is read out separate ly and\nis labelled, Eij, where the x and y coordinates of the particular cell, in\nthe two dimensional layer, are denoted, respectively, by the iandjsub-\nscripts. The average energy weighted radius, < r >, is calculated for each\nlayer separately. First the coordinates of the energy weighted ce nter of\nthe shower, x0andy0, are calculated as follows: x0=/summationtext\ni,jEijxi//summationtext\ni,jEij\nandy0=/summationtext\ni,jEijyi//summationtext\ni,jEij, where the iandjsummations are carried out\nover all the cells that have collected energy and for i < j, to avoid dou-\nble counting. Using the x0andy0values, we determine the distances rij\nbetween the center of the ijcell and the shower energy weighted center,\nrij=/radicalBig\n(x0−xi)2+(y0−yj)2. The average energy weighted radius of the\nshower,< r >, is determined as follows:\n< r >=/summationtext\ni,jEijrij/summationtext\ni,jEij.\nInformation from both layers is used to construct the 2 layer corr elation\nvariables. In the following we diﬀerentiate between the data gather ed in the\nlayer that recorded the cell with the maximum energy, the leadinglayer, and\nlabel them with the superscript, l, and the data from the other layer, the\nnon-leading layer,and label them with the superscript, n.\n5The following 6 variables embody the 2 layer correlation information us ed\nin the algorithms: the correlations between Emaxfrom one layer and E4from\nthe other layer are contained in the variables C1andC2. Similarly, the\ncorrelations between E4andE8are contained in the variables C3andC4\nand, ﬁnally, the correlations between E8andE12are contained in C5andC6.\n•C1is the diﬀerence, En\n4−El\nmax, normalized to Etotal.\n•C2is the diﬀerence, El\n4−En\nmax, normalized to Etotal. Notice that C2is\nsimilar to C1with the roles of the 2 layers interchanged.\n•C3is the diﬀerence En\n4−El\n8normalized to Etotal.\n•C4is the diﬀerence El\n4−En\n8normalized to Etotal.C4is similar to C3\nwith the roles of the 2 layers interchanged.\n•C5is the diﬀerence En\n8−El\n12normalized to Etotal.\n•C6is the diﬀerence El\n8−En\n12normalized to Etotal.C6is similar to C5\nwith the roles of the 2 layers interchanged.\n4 Analysis\nThe study of the separation of γ/π0showers is quantiﬁed by determining the\nrejection eﬃciency of π0initiated showers as a function of the acceptance\nofγinitiated showers. Eight variables are used in these analyses: the six\ncorrelations, C1,.. ,C6and the two average energy weighted radii, < r >land\n< r >n.\nFor every conﬁguration of design parameters and incident energy stud-\nied, two samples of showers, one initiated by γ’s and the other by π0’s, were\ngenerated and propagated through the E.M. calorimeter and the S MD-dos.\nThe shower information collected by the SMD-dos was used to const ruct\nthe 8 variables previously described. A program which uses the Simula ted\nAnnealing Algorithm was utilized to delineate an 8-dimensional volume, c or-\nresponding to the 8 variables used, which contains a ﬁxed percenta ge of the\nγinitiated showers while keeping to a minimum the number of π0initiated\nshowers. The delineation of this volume sets upper or lower cutoﬀ va lues for\neach of the 8 variables. The π0rejection eﬃciency is equal to the percentage\n6ofπ0initiated showers which are excluded from the delineated 8-dimension al\nvolume.\nThe shower information collected by the SMD was also processed usin g\nthe same analysis chain detailed above. However, since for the SMD n o\ncorrelation information is available, the following variables were used: E4,\nE8,E12and< r >.\nThe computer ﬂow chart of the program used along with a short des crip-\ntion of those aspects of the program which are speciﬁc to the γ/π0separation\nproblem can be found elsewhere [4]. In Fig. 3 are shown the projectio ns, for\n50 and 100 GeV, of the 8-dimensional volume of γinitiated showers and\nπ0initiated showers, separately, unto the plane spanned by the corr elation\nvariables C1andC3.\n C3 a) C1\n b)\n C3 C1\n c)\n C3 C1\n d)\n C3 C1 g-showers  po-showers\n00.10.20.30.40.50.6\n0.4 0.6 0.8 100.10.20.30.40.50.6\n0.4 0.6 0.8 1\n00.10.20.30.40.50.6\n0.4 0.6 0.8 100.10.20.30.40.50.6\n0.4 0.6 0.8 1\nFigure 3: Scatterplots of the correlation variables C3versusC1for showers\ninitiated by γ’s andπ0’s. a) 50 GeV γshowers and b) 50 GeV π0showers. c)\n100 GeV γshowers and d) 100 GeV π0showers. The respective cutoﬀ values\nofC1andC3, for 90% γacceptance, at 50 GeV and 100 GeV, are shown by\ndashed rectangles on the lower right corner of each ﬁgure.\nThe cutoﬀ values of C1andC3for 90% acceptance of the γinitiated\n7showers deﬁne the rectangles shown in the lower right corners. Fo r 50 GeV\nincident energy the π0initiated showers overwhelmingly fall outside the ac-\nceptance rectangle giving high π0rejection eﬃciency while for 100 GeV inci-\ndent energy the number of π0initiated showers excluded fromthe acceptance\nrectangle is smaller hence giving lower π0rejection eﬃciency .\nThe cutoﬀ values for diﬀerent incident energies are given in Table 1.\nThese cutoﬀ values as well as all other results presented in this pap er, for\nthe SMD-dos and SMD, were determined from samples of 500 shower s each\nwhich were generated under the following conditions:\n•The distance of the E.M. calorimeter from the interaction region was\nﬁxed at 150 cm.\n•The shower initiating particle, γorπ0, was incident normally on the\nE.M. calorimeter. However, the direction of the initiating particle was\nspread randomly within a small angular range corresponding to an ar ea\nof a few cells of the SMD-dos or SMD.\n•Theπ0rejection eﬃciency was calculated for 90% γacceptance.\n•The SMD-dos or SMD was positioned at a depth of 6 X0’s inside the\nE.M. calorimeter.\n•Thickness of the scintillator layers set at 4 mm.\n•The nominal granularity, cell size, is 1 ×1 cm2. In the event that a\ndiﬀerent cell size was used, it is explicitly stated.\nEinC1≤C2≤C3≥C4≥C5≤C6≤/angbracketleftr/angbracketrightl≤/angbracketleftr/angbracketrightn≤\n300.180.700.650.210.650.170.580.80\n500.170.650.670.220.630.150.550.82\n800.170.680.700.200.620.160.530.85\n1000.160.700.710.220.650.180.500.88\n1200.150.670.710.230.600.140.500.90\n1500.150.640.730.230.580.150.480.91\nTable 1: Cutoﬀ values of the six correlation variables, C1,.. ,C6, and the two\naverage energy weighted radii, < r >land< r >n, for incident energies of\n30 to 150 GeV.\n85 Results and Discussion\nTheπ0rejection eﬃciency as a function of the incident energy of the show -\nering particle is shown in Fig. 4a for both the SMD-dos and SMD. Both\ndetectors have equal granularity, 1 ×1 cm2cells. The π0rejection eﬃciency\nof the SMD-dos and the SMD decrease as a function of energy; how ever, the\nπ0rejection eﬃciency of the SMD-dos is higher than that of the SMD ov er\nthe whole energy range. The ratio of the rejection eﬃciency of the SMD-\ndos to the SMD, the ‘advantage factor’, is plotted as function of en ergy in\nFig. 4b.\na)po rejection %\nb)Advantage Factor\n  Incident energy , GeV0102030405060708090100\n11.11.21.31.41.51.61.7\n20 40 60 80 100 120 140 160\nFigure4: a)The π0rejectioneﬃciency of theSMD-dos( •)anda conventional\nSMD (/trianglesolid) as a function of incident energy. b)The ‘advantage factor’ ( /squaresolid) of\nthe SMD-dos over the SMD as a function of incident energy.\n9The value of the ‘advantage factor’ is close to 1.0 at 30 GeV and it the n\nrises to a maximum value of 1.63 at 80 GeV. With increasing energy the\nvalue of the ‘advantage factor’ decreases gradually reaching the value of 1.39\nat 150 GeV, the highest energy studied. In Table 2 are summarized t he\ntheπ0rejection eﬃciencies of the SMD-dos and SMD, of equal granularity ,\nfor various incident energies. The ‘advantage factor’ has been ca lculated for\neach incident energy. The following parameters, which are relevant toγ/π0\nseparation problems are given, per energy, in Table 2: Θ 12, the minimum\nopening angle in the lab system of the π0→γγdecay;d1,2, the distance\nbetween the two γcenters at the position of the SMD-dos or SMD and the\nratiod1,2/∆, where ∆ is the length of cell side, 1 cm.\nEinπ0Rejection Θ12d1,2d1,2π0Rejection ‘advantage\n[GeV]SMD-dos [%] [mrad][cm]∆SMD [%] factor’\n30 92±4 9.31.431.43 87±41.06±0.07\n40 90±4 7.01.071.07 80±41.13±0.08\n50 87±4 5.60.860.86 62±31.40±0.09\n80 65±3 3.50.550.55 40±21.63±0.11\n100 54±3 2.80.430.43 34±21.59±0.13\n120 39±2 2.30.360.36 26±21.50±0.14\n150 32±2 1.90.290.29 23±21.39±0.14\nTable 2: The π0rejection eﬃciency of the SMD-dos and SMD, of equal\ngranularity, is presented for diﬀerent values of incident energy. T he following\nparameters, which are explained in the text, are given per incident e nergy:\nΘ12,d1,2, the ratio d1,2/∆ and the ‘advantage factor’.\nThe dependence of the π0rejection eﬃciency of the SMD-dos and SMD\non the granularity of the detector was studied at 50 and 100 GeV inc ident\nenergies. The results of studies are shown in Figure 5 and are summa rized\nin Table 3. The expected increase in π0rejection eﬃciency with increasing\ndetector granularity for both SMD-dos and SMD is seen in Fig. 5a.\nFor 50 GeV incident energy the π0rejection eﬃciency of the SMD-dos\nand the SMD converge as the granularity increases. The ‘advantag e factor’,\nplotted in Fig. 5b, is seen to fall rapidly from a value of 1.71 for granula rity\nof 7×7 to a value of 1.09 for a granularity of 14 ×14. The ‘advantage factor’\nfor granularity of 20 ×20 is consistent with 1.0. For 100 GeV incident energy\n10theπ0rejection eﬃciency of both the SMD-dos and the SMD increase with\nincreasing granularity; however, the π0rejection eﬃciency of the SMD-dos\nis higher over the whole energy range. The ‘advantage factor’ has a value of\n1.4 for 7×7 granularity, it attains the maximum value of 1.7 for granularity\n14×14 and it then decreases to 1.5 for 20 ×20 granularity.\n7x7 10x10 14x14 20x20a)po rejection %\n7x7 10x10 14x14 20x20b)\n GranularityAdvantage  Factor2030405060708090\n11.11.21.31.41.51.61.71.8\nFigure 5: a) The π0rejection eﬃciency, as a function of detector granularity,\nof the SMD-dos ( △) and a conventional SMD ( /square) at 50 GeV incident energy;\nat 100 GeV, SMD-dos ( /trianglesolid) and a conventional SMD ( /squaresolid). b) The ‘advantage\nfactor’ as a function of detector granularity at 50 GeV ( ◦) and 100 GeV ( •).\nThe granularity is that of a 10 ×10 cm2layer divided into n×nsquare cells.\n11From the data given in Table 3 it is possible to compare the π0rejection\neﬃciency performance of the SMD-dos versus the SMD for the sam e number\nof channels. For 50 GeV incident energy a SMD-dos detector which h as\ntwo 7×7 layers needs 98 channels in order to read out its 98 cells while a\nSMD detector of 10 ×10 granularity needs 100 channels. From Table 3 we\ndetermine that the SMD-dosis a factor of 1.16 better, 72%vs 62%. A10×10\nSMD-dos is better than a 14 ×14 SMD by a factor of 1.07. For 100 GeV\nincident energy we ﬁnd that a 10 ×10 SMD-dos is superior to a 14 ×14 SMD\nby a factor of 1.38. Also a 14 ×14 SMD-dos is a factor of 1.18 better than a\n20×20 SMD.\nEnergy = 50GeV\nNumber ∆d1,2π0Rejection π0Rejection ‘advantage\nof cells [cm]∆SMD-dos [%] SMD [%] factor’\n7×71.430.60 72±3 42±21.71±0.11\n10×101.00.86 87±4 62±31.40±0.09\n14×140.721.20 88±4 81±41.09±0.07\n20×200.501.72 91±4 88±41.03±0.07\nEnergy = 100 GeV\nNumber ∆d1,2π0Rejection π0Rejection ‘advantage\nof cells [cm]∆SMD-dos [%] SMD [%] factor’\n7×71.430.30 36±2 26±21.38±0.13\n10×101.00.43 54±2 34±31.59±0.13\n14×140.720.60 67±2 39±31.71±0.12\n20×200.50.86 87±3 57±41.52±0.11\nTable 3: The π0rejection eﬃciencies of the SMD-dos and SMD, the ‘ad-\nvantage factor’s, the values of ∆ and the ratio d1,2/∆, are given for diﬀerent\ndetector granularities, for 50 and 100 GeV incident energies.\nThe advantage of using 2 layers which are diagonally oﬀ-set with resp ect\nto each other as oppossed to using a single layer, given that the tot al number\nof cells of each detector is the same, can be understood as follows: theγ/π0\nseparation power of a granular detector is strongly dependent on the distance\nof the shower center from an intersection. The success rate of a ny available\nγ/π0separation algorithm is higher when the deposited shower energy is\ndistributed among many cells. As previously stated, if all the deposit ed\n12energy is collected by a single cell it is impossible to determine the identit y\nof the showering particle. The distance between intersections for a SMD-dos\nis smaller by a factor of 1 /√\n2 than that of a SMD when both detectors have\nequal number of cells. Therefore, a showering particle incident on a SMD-dos\ndetector will, on the average, have its shower center nearer to an intersection\nthan if it were incident on a SMD detector.\n a)po rejection %\nb)\n d1,2/DAdvantage Factor2030405060708090100\n11.11.21.31.41.51.61.71.8\n0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8\nFigure6: a)The π0rejectioneﬃciencyoftheSMD-dos( •)andaconventional\nSMD (◦) as a function of the ratio, d1,2/∆. b) The ‘advantage factor’ as a\nfunction of the ratio, d1,2/∆.\nThe ratio of the 2 variables, d1,2, the distance between the two γcenters\nat the position of the SMD-dosor SMD and ∆, the length of cell side, d1,2/∆,\nis a useful a measure of the γ/π0separation diﬃculty. The γ/π0separation\n13becomes more diﬃculty as the ratio d1,2/∆ tends to small values while it is\nsigniﬁcantly easier as the ratio tends to values above 1.0. The behav iour of\ntheπ0rejection eﬃciency and the ‘advantage factor’ as a function of d1,2/∆\nare shown in Figure 6. The π0rejection eﬃciency, as expected, increases for\nboth the SMD-dos and the SMD as the value of this ratio increases. W e\nnotice that the ‘advantage factor’ has an appreciable value only in t he range\nof from∼0.2 to∼1. Hence we ﬁnd that when the γ/π0separation is very\ndiﬃculty, values smaller than ∼0.2, than the eﬃciencies of the SMD-dos and\nSMD are comparable and likewise when the separation becomes easy, values\nhigher than ∼1.0, than again no advantage of SMD-dos over SMD.\nIf we express, d1,2in terms of the incident energy, Einc, mass of the π0\nand,L, distance to SMD-dos or SMD from the interaction point, then we\ncan write the following expression:\nd1,2\n∆=2×mπ×L\nEinc×∆.\nHence the ratio, d1,2/∆, is a function of 3 independent variables, ∆, L\nandEinc. So for instance if we ﬁx the value of 2 of these variables we can read\noﬀ from Figure 6 the permissible range of the third variable, for a des iredπ0\nrejection eﬃciency. Figure 6 can be used, in general, in order to get theπ0\nrejection eﬃciency and ‘advantage factor’ for diﬀerent combinat ions of the 3\nvariables not directly studied in this paper. However, it would be prud ent to\nlimit the use of these ﬁgures to values of the 3 variables which are with in a\nfactor of 2 of the ranges explicitly studied in this paper.\n6 Conclusions\nWe have designed and extensively studied a new geometry SMD consis ting\nof two layers of scintillator which are diagonally oﬀ-set with respect t o each\nother. The layers of scintillator are subdivided into square cells. This geome-\ntry has the virtue of increasing substantially the number of interse ctions, per\nunit area, seen by theincoming shower. Acomparison ofthe SMD-do swith a\nconventional geometry single-layer and mutiple-layer SMD, having t he same\ngranularity, shows that the SMD-dos yields higher π0rejection eﬃciencies.\nFor 90%γacceptance the SMD-dos yields π0rejection eﬃciencies of 92 ±4 %,\n87±4 % and 32 ±2 %, respectively, for 30, 50 and 150 GeV incident energies.\n14We ﬁnd that the SMD-dos is superior to the SMD, of equal granularit y, by\nan average factor of ∼1.5 over the 50 to 150 GeV energy range. We also ﬁnd\nthat the SMD-dos gives better π0rejection, for the same number of channels,\nthan a SMD. At 100 GeV a 10 ×10 cells per layer SMD-dos, which utilizes\n200channels, is a factorof ∼1.4better thana 14 ×14 cells, single layer SMD,\nwhich utilizes an equal number of channels. The π0rejection eﬃciency of the\nSMD-dos and SMD and the ‘advantage factor’ are plotted as a func tion of\nthe ratio, d1,2/∆. With the help of these plots it is possible to investigate\nthe relative importance of the 3 variables, ∆, LandEinc, over a range which\ngoes beyond the work presented in this paper.\nAt hadron - hadron colliders the signature of choice for the detect ion of\nthe Higgs particle, in the mass range of 120 to 160 GeV, is via the deca y\nH→γγ. The addition of a SMD-dos to the planned detectors at the LHC\nwouldsigniﬁcantlyreducethebackgroundtothe γsignalcomingfromproliﬁc\nπ0production.",
      "metadata": {
        "filename": "Separation of γ_π_0 showers at high energies.pdf",
        "hotspot_name": "PCBA_Production",
        "title": "Separation of γ/π^0 showers at high energies",
        "published_date": "1996-10-08T13:22:55Z",
        "pdf_link": "http://arxiv.org/pdf/hep-ex/9610005v3",
        "query": "THT and SMD soldering PCBA production energy efficiency improvement quantitative efficiency gains"
      }
    },
    "SolderNet_ Towards Trustworthy Visual Inspection of Solder Joints in Electronics": {
      "full_text": "SolderNet: Towards Trustworthy Visual Inspection of Solder Joints in Electronics\nManufacturing Using Explainable Artiﬁcial Intelligence\nHayden Gunraj1,2, Paul Guerrier3, Sheldon Fernandez2, and Alexander Wong1,2\n1Vision and Image Processing Research Group, University of Waterloo, Waterloo, Canada\n2DarwinAI Corp., Waterloo, Canada\n3Moog Inc., New York, USA\nfhayden.gunraj, a28wong g@uwaterloo.ca\nAbstract\nIn electronics manufacturing, solder joint defects are a com-\nmon problem affecting a variety of printed circuit board com-\nponents. To identify and correct solder joint defects, the sol-\nder joints on a circuit board are typically inspected man-\nually by trained human inspectors, which is a very time-\nconsuming and error-prone process. To improve both inspec-\ntion efﬁciency and accuracy, in this work we describe an ex-\nplainable deep learning-based visual quality inspection sys-\ntem tailored for visual inspection of solder joints in elec-\ntronics manufacturing environments. At the core of this sys-\ntem is an explainable solder joint defect identiﬁcation system\ncalled SolderNet which we design and implement with trust\nand transparency in mind. While several challenges remain\nbefore the full system can be developed and deployed, this\nstudy presents important progress towards trustworthy visual\ninspection of solder joints in electronics manufacturing.1\nIntroduction\nIn electronics manufacturing, solder joint defects are a com-\nmon problem affecting both surface-mounted and through-\nhole components of printed circuit boards (PCBs) (see Fig-\nure 1 for example solder joint defects). Defects introduced in\nthe soldering process can lead to electrical issues and faulty\nparts, especially if not caught early in the process. This is\nparticularly concerning in critical applications such as the\naerospace and medical industries, where defective PCBs can\ncause catastrophic failures in critical systems. If defects are\ncaught early, the solder can be reworked to minimize po-\ntential issues later in the manufacturing process and avoid\nunnecessary electronic waste.\nTo identify and correct solder joint defects, the solder\njoints on a PCB are often visually inspected by trained in-\nspectors. However, human inspectors are estimated to make\nvisual inspection errors in 20-30% of cases, and the perfor-\nmance of human inspectors varies considerably depending\non experience, mental fatigue, defect type, frequency of de-\nfect occurrence, and a variety of other factors (See et al.\n2017; Klamklay and Bishu 1998). Manual inspection is also\nCopyright © 2023, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\n1This paper consists of general capabilities information that is\nnot deﬁned as controlled technical data under ITAR Part 120.10 or\nEAR Part 772.\nFigure 1: Example images of solder joint defects from the\ndataset examined in this study: (A) fractured joint, (B) cold\njoint, (C) burns, (D) ﬂux residue, (E) poor wetting, and (F)\ndisturbed solder.\nexpensive and time-consuming, often requiring multiple in-\nspectors to achieve reasonable throughput.\nIn contrast to manual inspection, automated visual in-\nspection of solder joints offers many beneﬁts, such as high\nthroughput, high performance, and zero fatigue. However,\nautomating this task is technically challenging due to the\nsmall size of the joints, the wide variety of possible joint\nand defect types, limited computing resources, limited in-\nspection time, and the need for high performance. Deep neu-\nral networks offer an attractive solution to this problem, as\nthey have been successfully applied to detection and inspec-\ntion tasks in a variety of manufacturing scenarios (Kim et al.\n2021; Zhang et al. 2022; Westphal and Seitz 2021; Bhatt\net al. 2021; Yang et al. 2020). While deep neural networks\nare capable of achieving human-level performance in vi-\nsual inspection tasks, they have several drawbacks in critical\nmanufacturing scenarios:\n1.Computational complexity : high-performance neural\nnetworks are often compute-hungry and require special-\nized hardware for fast inference. When computing re-\nsources are limited (as in many manufacturing scenar-\nios), such networks can be slow and reduce throughput.arXiv:2211.10274v1  [cs.CV]  18 Nov 2022Figure 2: Overview of the proposed solder joint inspection system. This study focuses on developing the core defect identiﬁca-\ntion system (SolderNet), which comprises the defect identiﬁcation and XAI modules.\n2.Lack of explanations : neural networks typically operate\nas black boxes which provide a prediction but cannot ex-\nplain how the prediction was made. In critical inspection\nscenarios, determining why a network made a particular\nprediction falls to the human inspector, which reduces the\npotential throughput beneﬁts offered by these networks.\n3.Unclear reliability : the development of neural networks\nusually involves testing using data not seen by the net-\nwork. Despite this, a common challenge when deploying\ndeep neural networks is understanding when a model’s\npredictions can be trusted and which scenarios a model\nis likely to fail in.\nIn this work, we outline a trustworthy, explainable deep\nlearning-driven solder joint inspection system for electron-\nics manufacturing. The proposed system is capable of han-\ndling both through-hole and surface-mounted components\nand can provide explanations of its decision in order to facil-\nitate manual review when necessary. We perform a number\nof experiments to implement and test the core functional-\nity of this system with a focus on practical considerations\nfor manufacturing settings. Moreover, we leverage a mix\nof quantitative explainability and trust quantiﬁcation tech-\nniques to further analyze the behaviour and trustworthiness\nof the system, identify gaps in the model development pro-\ncess, and provide insights during the inspection process.\nMethods\nThe proposed system consists of several stages which are\ndesigned to provide high performance, high throughput and\nhigh reliability. Figure 2 illustrates the inspection system,\nwhich proceeds via the following steps:\n1. Images of a PCB are captured by an imaging system.\n2. PCB images are passed through a solder joint detector\nwhich identiﬁes and extracts solder joint images.3. Solder joint images are passed through a defect identi-\nﬁcation network which categorizes solder joints as de-\nfective, possibly defective, or non-defective and activates\nthe appropriate indicator.\n4. For boards with possibly defective joints, an XAI algo-\nrithm is used to generate explanations of the network’s\npredictions.\n5. A human operator reviews the possibly defective joints\nand their XAI visualizations to determine which are truly\ndefective and which are not defective.\n6. All defective solder joints are reworked.\nIn this study, we focus on defect identiﬁcation, explain-\nability, and trustworthiness. These three aspects of the sys-\ntem are described in detail in the following subsections.\nSolder Joint Defect Identiﬁcation\nWe focus on convolutional neural networks (CNNs) to per-\nform the task of solder joint defect identiﬁcation. CNNs are\na ubiquitous technology in image identiﬁcation tasks, and\nrecent advances in network design have enabled the creation\nof high-performance networks which remain computation-\nally efﬁcient. We examined both efﬁciency-focused archi-\ntectures and performance-focused architectures to examine\ntheir respective trade-offs between network complexity and\nnetwork performance. Speciﬁcally, we tested Attend-NeXt\n(Small and Large) (Wong et al. 2022), MobileNet (V2 and\nV3 Small)(Sandler et al. 2018; Howard et al. 2019), Shuf-\nﬂeNetV2 (Ma et al. 2018), and ConvNeXt (Tiny, Small, and\nBase) (Liu et al. 2022). Table 1 shows the number of pa-\nrameters and ﬂoating-point operations (FLOPs) for each of\nthe network architectures examined in this work. We train\neach of the aforementioned architectures as binary classi-\nﬁers which provide a conﬁdence score on the interval [0,\n1] indicating the network’s conﬁdence that an input imagerepresents a defective solder joint. In deployment, this al-\nlows us to deﬁne conﬁdence regions which correspond to\ndeﬁnite defects (requiring repair), possible defects (requir-\ning review), and deﬁnite non-defects (no action required).\nQuantitative Explainability\nFollowing defect identiﬁcation, as can been seen in Figure 2\nsome solder joints may require review by a human opera-\ntor given a level of uncertainty during the solder joint de-\nfect identiﬁcation process with respect to whether that joint\nis indeed defective. In such scenarios, while the identiﬁca-\ntion network’s prediction and conﬁdence provide useful in-\nformation to the operator, the operator must still identify the\nparticular defects (or lack thereof) in the solder joint images\nto determine if it is a false defect detection (in which case\ninspection is complete) or if it is indeed a true defect de-\ntection (in which case the solder needs to be reworked). To\nmake this task easier and faster for operators, we propose a\nquantitative explainability module which identiﬁes the crit-\nical factors in an image which led to the neural network’s\ndecision in a quantitative manner, allowing the operator to\nrapidly identify the locations of potential defects and vali-\ndate the model’s predictions.\nTo this end, we introduce an extended form of GSIn-\nquire (Lin et al. 2019) to provide visual explanations of\na neural network’s decision-making process. We choose\nGSInquire as the core approach to extend upon because\nit identiﬁes speciﬁc critical factors that quantitatively im-\npact the decisions made by the deep neural network, in\ncontrast to other explainability methods such as Grad-\nCAM (Selvaraju et al. 2017), Expected Gradients (Erion\net al. 2021), LIME (Ribeiro, Singh, and Guestrin 2016), and\nSHAP (Lundberg and Lee 2017) that generate only qualita-\ntive heatmaps depicting relative importance.\nIn its original form, GSInquire examines a network’s ac-\ntivation signals in response to an input image and uses them\nto identify critical factors within the image which impact the\nnetwork’s decision in a quantitatively signiﬁcant way. These\ncritical factors may then be projected into the same space as\nthe image to produce a visual interpretation. Building upon\nGSInquire, we introduce an extension which determines the\nrelative importance of different aspects within the critical\nfactors of a given image. This allows for more ﬁne-grained\ninterpretation of the critical factors, as we can now see not\nonly which critical factors contribute the most to the neural\nnetwork’s decision-making process but also which aspects\nof a particular critical factor are most important. We present\nthe relative importance of different aspects within the critical\nfactors as regional heatmap overlays within the boundaries\nof their corresponding critical factors, as shown in Figure 3.\nSecond-order Explainability\nIn addition to providing visual explanations in deployment\nsettings, visual explainability is also a valuable model val-\nidation tool during development. While quantitative met-\nrics such as accuracy provide important measures of a deep\nneural network’s performance, they do not provide informa-\ntion regarding how decisions are made. To address this gap\nin performance analysis, visual XAI enables auditing of aArchitecture Parameters (M) FLOPs (G)\nAttend-NeXt Small 1.632 0.423\nAttend-NeXt Large 3.897 0.861\nMobileNetV2 2.225 0.409\nMobileNetV3 Small 1.519 0.076\nShufﬂeNetV2 1.255 0.193\nConvNeXt Tiny 27.821 5.832\nConvNeXt Small 49.455 11.364\nConvNeXt Base 87.567 20.084\nTable 1: Comparison of architectural and computational\ncomplexity for each of the tested network architectures.\nFLOPs were measured with a 256 \u0002256 3-channel input and\na batch size of 1.\nmodel’s decisions during development to ensure that they\nare based on relevant visual indicators and elucidate poten-\ntial biases in the training data, which may then be used to\nguide improvements to the training framework. However, re-\nviewing visual explanations manually is a time-consuming\ntask, particularly for large-scale image datasets with many\nclasses or high intra-class variability. Manual review may\nalso be inﬂuenced by human biases, and it can be challeng-\ning if not intractable to mentally conceptualize key trends\nand patterns based on the individual explanations at hand.\nTo facilitate auditing of the model and dataset during\ndevelopment, we introduce the concept of second-order\nexplainable artiﬁcial intelligence (SOXAI) which extends\nthe concept of XAI from the sample level to the dataset\nlevel. Rather than manually reviewing visual explanations\nto explore patterns in a model’s decision-making behaviour,\nSOXAI aims to reveal these patterns automatically through\nanalysis of the relationships between quantitative explana-\ntions. This allows for rapid identiﬁcation of the most com-\nmon visual concepts leveraged by a model when making\nits decisions, and can reveal obvious model and dataset bi-\nases. This can also increase transparency by helping users\nunderstand what a model has learned and what it has not.\nIn essence, SOXAI enables us to ”explain explainability” by\nproviding higher-level interpretations of the behaviours of\ndeep neural networks.\nIn this study, we formulate SOXAI as an embedding prob-\nlem: given an image Iand corresponding quantitative ex-\nplanation\u000b, we deﬁne an embedding f: (I;\u000b)!RN\nwhich embeds the regions of Iindicated by \u000b. Performing\nthis embedding for all images in a dataset allows for simi-\nlar embeddings to be grouped together with secondary algo-\nrithms. To generate the visualizations presented in this work,\nwe leverage t-distributed stochastic neighbour embedding (t-\nSNE) (van der Maaten and Hinton 2008) to group the em-\nbeddings and map them to a 2-dimensional space.\nTrust Quantiﬁcation\nIn industrial applications, it is important to understand how\ntrustworthy a model is in order for it to be deployed as an\nautomation tool. Quantifying trust allows for models to be\ncompared in terms of their trustworthiness during the model\ndevelopment process, and may help guide decisions as toArchitecture Latency (s) Accuracy (%) Overkill (%) Escape (%) NetTrustScore\nAttend-NeXt Small 0.275 86.6 8.4 5.0 0.864\nAttend-NeXt Large 0.430 91.1 5.0 3.9 0.904\nMobileNetV2 4.082 88.5 7.8 3.7 0.878\nMobileNetV3 Small 1.647 88.3 7.1 4.6 0.883\nShufﬂeNetV2 1.775 87.0 7.1 5.9 0.867\nConvNeXt Tiny 5.975 88.8 7.2 3.9 0.887\nConvNeXt Small 12.027 89.6 5.8 4.6 0.890\nConvNeXt Base 17.575 88.8 6.3 4.8 0.889\nTable 2: Comparison of quantitative performance metrics for each of the network architectures on the solder joint test dataset.\nLatency was measured on an ARM Cortex-A72 with a 256 \u0002256 3-channel input and a batch size of 1, and is reported as the\naverage of 100 measurements taken after 20 warm-up runs. Accuracy, overkill, and escape were calculated at a conﬁdence\nthreshold of 0.5. Best results highlighted in bold .\nthe level of human supervision and review required once a\nmodel is deployed. Recent studies by Wong et al. (Wong,\nHryniowski, and Wang 2020; Wong, Wang, and Hryniowski\n2020) and Hyrniowski et al. (Hryniowski, Wong, and Wang\n2021) introduced several human-interpretable approaches to\ntrust quantiﬁcation which allow for a model’s overall trust-\nworthiness to be analyzed. These techniques are based on\nthe notion of question-answer trust, in which a question x\n(i.e., ”is this solder joint defective?”) is answered by both\na modelMand an oracle O. The model’s answer and con-\nﬁdence in its answer are compared to the oracle’s answer\nto compute a question-answer trust score which reﬂects the\ntrustworthiness of the model’s answer.\nTo evaluate SolderNet’s trustworthiness, we make use of\ntwo key trust metrics which summarize a model’s overall\nquestion-answer trust:\n1.Trust matrix : a matrix of expected question-answer\ntrusts for each possible model-oracle answer pair, and\n2.NetTrustScore : a scalar metric summarizing the overall\ntrustworthiness of a deep neural network across all pos-\nsible predictions.\nExperimental Setup\nDataset To build and evaluate the proposed solder joint\ndefect identiﬁcation and explanation framework, we lever-\nage a dataset comprising 2690 images of both through-hole\nand surface-mount joints acquired by Moog Inc. Images\nwere acquired using a microscope camera and were labelled\nas either defective joints (1644 images) or non-defective\njoints (1046 images) by an experienced inspector. Notably,\nlabelling was performed off-line without time constraints,\nthereby minimizing label noise in the resulting data. The\ndataset includes a variety of solder defects such as fractured\nsolder joints, cold joints, burns, ﬂux residue, poor wetting,\nand disturbed solder as illustrated in Figure 1. We used a\n60%/20%/20% split stratiﬁed by class to divide this dataset\ninto training, validation, and test sets, respectively.\nPreprocessing Images are preprocessed by resizing them\nto 256\u0002256 pixels and mapping their original unsigned 8-\nbit integer pixel values to the range [0, 1] through division\nby 255. Additionally, the following data augmentations were\nused during training, each applied with a 50% probability:random rotation in the range [0\u000e, 45\u000e], random horizontal\nand vertical ﬂipping, random translation of \u000610% in each\ndirection, random brightness jitter of \u000620%, and random\ncontrast jitter of\u000620%.\nTraining All models examined in this work were pre-\ntrained on ImageNet-1k (Deng et al. 2009). Following (Ku-\nmar et al. 2022), each model’s fully-connected layers were\ntrained for 100 epochs followed by full-model ﬁne-tuning\nfor 1000 epochs. Binary cross-entropy loss and an AdamW\noptimizer (Loshchilov and Hutter 2019) with (\f1;\f2) =\n(0:9;0:999) and weight decay of 1 \u000210\u00004were used in all\nexperiments. We used a batch size of 128 and a learning rate\nof 1\u000210\u00003to train the fully-connected layers, followed by\nfull-network ﬁne-tuning with a batch size of 128, an initial\nlearning rate of 5\u000210\u00004, and cosine learning rate decay.\nEvaluation To evaluate performance, we report accuracy,\noverkill rate (number of false-positives divided by number of\nsamples), and escape rate (number of false-negatives divided\nby number of samples) on the the holdout test set. We chose\nthese metrics due to the manufacturing context of this work;\nmanufacturers care more about the absolute rates of false-\npositives and false-negatives rather than the proportional\nrates. Additionally, we report inference latency on an ARM\nCortex-A72 processor, as well as NetTrustScore (Wong,\nWang, and Hryniowski 2020) to evaluate the trustworthiness\nof each model. Lastly, visual explanations and trust quantiﬁ-\ncation plots were qualitatively evaluated.\nResults\nQuantitative Results\nQuantitative performance metrics for each of the tested net-\nwork architectures are shown in Table 2. All tested archi-\ntectures achieve high performance, with Attend-NeXt Large\nachieving the highest accuracy (91.1%) and lowest overkill\nrate (5.0%), MobileNetV2 achieving the lowest escape\nrate (3.7%), and Attend-NeXt Small achieving the lowest\nlatency (0.275 s).\nAn intuitive interpretation of these metrics can be ob-\ntained by considering a scenario where 100 solder joints\nare to be inspected in a deployment setting. Considering the\nmetrics of Attend-NeXt Large, we see that it would takeFigure 3: Images of solder joint defects (left) and corre-\nsponding visual explanations (right) from the dataset exam-\nined in this study: (A) poor wetting, (B) solder splash, (C)\nforeign object in solder, and (D) pad and board damage.\n43 s to inspect the set of joints and about 91/100 joints\nwould be correctly classiﬁed. Of the remaining 9 misclas-\nsiﬁed joints, about 5 would be misclassiﬁed as defective and\nabout 4 would be misclassiﬁed as non-defective. However, it\nis important to note that this interpretation is only meaning-\nful if the proportions of defective and non-defective joints\nin the test data are representative of the true proportions in\ndeployment. In this study, defective solder joints are over-\nrepresented in the test data, and as such we might expect\nhigher overkill rates and reduced escape rates in practice.\nExplainability Results\nIn this section we present and describe defect explanation\nand localization results obtained through the proposed ex-\nplainability module. Figure 3 illustrates defective solder\njoints and the corresponding explanations of an Attend-\nNeXt Large model’s predictions for these images. In each\ncase, the XAI algorithm identiﬁed the critical factors that\nquantitatively drove the resulting decision (white outline)\nand the relative importance of aspects within each critical\nfactor (semi-transparent regional heatmaps) indicating the\ncritical factors used by the model to make its prediction. In\nall ﬁve cases, we observe that GSInquire identiﬁes the key\nareas of interest driving the network’s decision-making pro-\ncess and further localizes speciﬁc features via the regional\nheatmaps. These cases are described in more detail below.Case A This image illustrates poor wetting, where a glob-\nule of solder which has not adequately joined with the solder\npad can be seen. Additionally, the solder pad itself appears\nlumpy and discoloured which indicates possible residue or\ncontamination. Examining the visual explanation, we see\nthat the model correctly focuses on the non-wetted regions\nof the joint when making its prediction.\nCase B In this example, the soldering process has intro-\nduced a solder splash which can be seen in the top-right\ncorner of the image. Examining the visual explanation, we\nsee that the model correctly identiﬁed this solder splash as\nthe defect and effectively ignored the solder joint (which is\nwell-formed) when making its decision.\nCase C In this image, a foreign object (a piece of ﬁbre)\nwas inadvertently embedded in the solder when the joint was\ncreated. This foreign object is captured in its entirety by vi-\nsual explanation, with the ﬁbre itself being highlighted as\nthe most important aspect in the explanation.\nCase D Extensive damage to the pad and board are shown\nin this example, where a large piece has chipped off of\nthe solder pad and extensive damage to the board’s surface\naround the joint can be seen. While both of these aspects\nwould constitute a defective joint, we see in the visual ex-\nplanation that the model focuses on the damage to the solder\npad while still including the surface damage.\nSecond-order Explainability Results\nSOXAI visualizations are produced by placing the quantita-\ntive explanations produced by GSInquire across the wealth\nof data at hand at their corresponding 2-dimensional embed-\nding locations following t-SNE. The resulting image illus-\ntrates groups of similar quantitative explanations which can\nmore easily be examined for semantic groupings and com-\nmon trends and patterns.\nFigure 4 illustrates a SOXAI visualization for an Attend-\nNeXt Large model. As shown in the ﬁgure, SOXAI auto-\nmatically groups explanations with similar characteristics,\nmaking it easier to ﬁnd trends in the visual explanations.\nFor example, in Figure 4 (A), we see that a homogeneous\nset of overhang defects has been tightly grouped. The rela-\ntively large size of this group indicates that this particular\ndefect is well-recognized by the model but may be over-\nrepresented. In contrast, Figure 4 (B) shows a group of\nlifted leads which exhibit greater diversity but may be under-\nrepresented. In the neighbourhood of Figure 4 (C) we ob-\nserve a large variety of through-hole defects, with (C) high-\nlighting a group of wetting defects. The large size of the\nthrough-hole group paired with the intra-group variability\nindicates that through-hole joints and their defects are well-\nrepresented in the dataset and well-recognized by the model.\nTrust Analysis\nThe NetTrustScores for the models examined in this work\nare shown in the rightmost column of Table 2. These scores\ngive an overall measure of how trustworthy each model’s\npredictions are. As shown, there is little disparity in trustFigure 4: Second-order visual explainability illustrating various types of solder joint defects as viewed by the network: (A) side\noverhang, (B) lifted/unsoldered leads, and (C) wetting defects.\nFigure 5: Trust matrix of Attend-NeXt Large.\namongst the models examined in this work, with Attend-\nNeXt Large achieving the highest score of 0.904 and Attend-\nNeXt Small having the lowest score of 0.864.\nTo explore trust in more detail, Figure 5 shows the trust\nmatrix for Attend-NeXt Large. Notably, we show this ma-\ntrix as an illustrative example since the trust matrices for the\nother architectures have a similar pattern. To interpret the\ntrust matrix, consider that each entry indicates the expected\nquestion-answer trust for the given ground-truth/prediction\npair. As such, higher values are better in all cells. Examin-\ning Figure 5, we see that the diagonal entries (i.e., correct\nprediction scenarios) exhibit high trust. However, the off-\ndiagonal entries (i.e., incorrect prediction scenarios) exhibit\nextremely low trust, indicating that the model is overconﬁ-\ndent when it makes incorrect predictions. This is problem-\natic in deployment scenarios, as it makes it more difﬁcult to\nidentify uncertain model predictions in order to ﬂag them for\nmanual review. To alleviate this problem, techniques such as\nlabel smoothing and mixup regularization (Carratino et al.\n2020) could be used to soften the image labels during train-\ning and encourage intermediate conﬁdence scores.Discussion\nIn this work, we described a design for a trustworthy, ex-\nplainable solder joint inspection system for use in electron-\nics manufacturing. While this system has yet to be fully\nimplemented, we present important progress towards trust-\nworthy, explainable solder joint inspection which forms the\ncore of the proposed system. Moreover, we discuss practi-\ncal considerations for building and evaluating such a system\nand show how trust quantiﬁcation, quantitative explainabil-\nity, and second-order explainability can be leveraged to ana-\nlyze the trustworthiness of the system and identify biases or\ngaps in the data and model development process as well as\nprovide insights during inspection.\nThe image data analyzed in this study varies considerably\nin terms of camera viewpoint, magniﬁcation, and resolu-\ntion. In practice, a standardized imaging system would be\nrequired, however implementing an adequate imaging sys-\ntem is technically challenging due to the fact that different\ntypes of solder joints may need to be imaged at different an-\ngles, exposures, or resolutions in order to capture the major-\nity of possible solder defects. In this study, we have assumed\nthat such a system canbe designed, but the speciﬁc details\nof how to do so are left to future work.\nWhen deploying a system such as the one described in\nthis study, it is important to monitor and validate the sys-\ntem’s performance in the ﬁeld in order to identify and correct\nany issues that arise. No amount of ofﬂine testing can fully\nsimulate a system’s real-world performance, and so collect-\ning prediction and performance data in the ﬁeld is critical\nto evaluation. Additionally, false predictions observed in the\nﬁeld can be collected, curated, and used to ﬁne-tune the sys-\ntem in order to reduce overkill and escape rates. Such con-\ntinuous monitoring also helps to identify and mitigate drift\nin the system (for example, due to drift in camera calibration\nor other imaging parameters).Acknowledgments\nWe would like to thank Saeejith Nair for measuring the in-\nference latencies of the architectures examined in this work.",
      "metadata": {
        "filename": "SolderNet_ Towards Trustworthy Visual Inspection of Solder Joints in Electronics.pdf",
        "hotspot_name": "PCBA_Production",
        "title": "SolderNet: Towards Trustworthy Visual Inspection of Solder Joints in\n  Electronics Manufacturing Using Explainable Artificial Intelligence",
        "published_date": "2022-11-18T15:02:59Z",
        "pdf_link": "http://arxiv.org/pdf/2211.10274v1",
        "query": "THT and SMD soldering PCBA production energy efficiency improvement quantitative efficiency gains"
      }
    },
    "YOLO algorithm with hybrid attention feature pyramid network for solder joint de": {
      "full_text": "YOLO ALGORITHM WITH HYBRID ATTENTION FEATURE\nPYRAMID NETWORK FOR SOLDER JOINT DEFECT DETECTION\nLI ANGa,b, SITI KHATIJAH NOR ABDU RAHIMa, RASEEDA HAMZAH∗c,\nRAIHAH AMINUDDINc, AND GAO YOUSHENGa,b\naCollege of Computing, Informatics and Mathematics, Universiti Teknologi MARA (UiTM), Shah\nAlam, Selangor, Malaysia\ne-mail address : 2022667284@student.uitm.edu.my, sitik781@uitm.edu.my\nbCollege of Information Engineering, Jiujiang Vocational University, Jiu Jiang, Jiang Xi, China\ne-mail address : 2022667284@student.uitm.edu.my\ncCollege of Computing, Informatics and Mathematics, Universiti Teknologi MARA (UiTM) Melaka\nBranch\ne-mail address : raseeda@uitm.edu.my, raihah1@uitm.edu.my\nAbstract. Traditional manual detection for solder joint defect is no longer applied during\nindustrial production due to low efficiency, inconsistent evaluation, high cost and lack of\nreal-time data. A new approach has been proposed to address the issues of low accuracy,\nhigh false detection rates and computational cost of solder joint defect detection in surface\nmount technology of industrial scenarios. The proposed solution is a hybrid attention\nmechanism designed specifically for the solder joint defect detection algorithm to improve\nquality control in the manufacturing process by increasing the accuracy while reducing the\ncomputational cost. The hybrid attention mechanism comprises a proposed enhanced multi-\nhead self-attention and coordinate attention mechanisms increase the ability of attention\nnetworks to perceive contextual information and enhances the utilization range of network\nfeatures. The coordinate attention mechanism enhances the connection between different\nchannels and reduces location information loss. The hybrid attention mechanism enhances\nthe capability of the network to perceive long-distance position information and learn local\nfeatures. The improved algorithm model has good detection ability for solder joint defect\ndetection, with mAP reaching 91.5%, 4.3% higher than the You Only Look Once version\n5 algorithm and better than other comparative algorithms. Compared to other versions,\nmean Average Precision, Precision, Recall, and Frame per Seconds indicators have also\nimproved. The improvement of detection accuracy can be achieved while meeting real-time\ndetection requirements.\nKey words and phrases: Solder joint defect, feature fusion, self-attention mechanism, feature pyramid\nnetwork.\n*Corresponding author.\nPreprint submitted to\nLogical Methods in Computer Science© L. Ang, SKNA. Rahim, R. Hamzah, R. Aminuddin, and G. Y ousheng\nCC⃝ Creative CommonsarXiv:2401.01214v1  [cs.CV]  2 Jan 20242 L. ANG, SKNA. RAHIM, R. HAMZAH, R. AMINUDDIN, AND G. YOUSHENG\n1.Introduction\nSurface mount device (SMD) pins are prone to lead welding defects in automatic production,\nsuch as insufficient defect, and foot shifting defect, as in Figure 1. In solder joint defect\ndetection, traditional manual detection is no longer adapted to the development of industrial\nproduction. Manual detection is low efficiency, inconsistent evaluation, high cost and lack of\nreal-time data.\nComputer vision is a combination of computer hardware and software working together\nwith industrial cameras and source of lights for capturing an image. It is utilized in a variety\nof industrial scenes to automate manufacturing and improve product quality. The solder\njoint defect detection system based on computer vision has the characteristics of real-time,\ncontinuous, and contact-less. This approach can take the place of manual detection and\nenhance the accuracy of results. At present, computer vision has been frequently employed in\ndefect detection. Therefore, the use of computer vision for detecting solder joint defects has\nbecome a mainstream trend. In recent years, deep learning technology, that is one stream of\ncomputer vision has been developed rapidly. The research being developed for automated\nsolder joint defect detection is still lacking. The methods for solder joint defect detection can\nbe separated into three main groups, feature-based methods [WX18,XLLW21], statistical\nmethods [YCL+18,CCW+21], and deep learning methods [CCW+18,DMES18,CT21a]. The\ndeep learning method can learn effective information and rules from the solder joint image\ndue to its CNN structure [LHL+22]. It can solve the problem that defective features are\ndifficult to extract by artificially designed rules. The structures of deep learning neural\nnetwork (DNN) can be seen in two folds: one is single-stage and the other is two-stage\nnetworks. Although the two-stage DNN is more accurate compared to the first stage, the\nshallow feature needs to be carefully utilized to avoid missing information during the feature\nextraction stage that will lead to lower detection rate. Its real-time performance is also poor,\nand it is not suitable for an industrial production environment. On the other hand, the\none-stage method represented has good real-time performance and fast detection speed, but\nthe detection effect is not good for small defect area and low-resolution images [HJW23a]. In\nthe feature extraction module of defect detection, the target feature information is lost too\nmuch, and the detection rate of small defect is not ideal, leading to serious issues of missed\ndetection [ZS21]. The deep learning method uses deep neural network to extract features,\nbut as the network layers of the deep neural network deepen, some shallow information\nis easily lost, leading to missed detection of small-sized object. To address this issue, the\nmulti-scale feature fusion method is adopted to fuse deep and shallow features in the feature\nextraction process, enhancing information transmission between different network layers.\nTherefore, optimizing feature fusion methods can promote the improvement of detection\naccuracy for small-sized object.\nFeature Pyramid Network (FPN) [LDG+17] obtains feature images of different scales\nthrough multiple up-sampling of the input image. It integrates the abstract semantic\ninformation extracted from the high-level with the specific details, like the low-level contour\ntexture in the feature extraction process from top to bottom to fulfill the goal of feature\nextraction enhancement. However, although FPN has systematically extracted the low-level\nand high-level features, its feature fusion capability still cannot meet the requirements,\nmaking it difficult to retain shallow features information.\nTo address the missing information between high-level and low-level features, Liu\net al. [LQQ+18] designed Path Aggregation Network (PANet) to connect a bottom-upYOLO ALGORITHM WITH HAFPN FOR SOLDER JOINT DEFECT DETECTION 3\n(a)\n (b)\n (c)\nFigure 1: (a) Insufficient, (b) Foot shifting, and (c) Qualified solder joint samples in dataset\nenhancement path at the bottom of the feature pyramid. This process is done to shorten\nthe transmission path of information fusion, to feed the fusion network location data\nwith fine-grained features to increase the feature pyramid architecture’s detection capacity.\nBidirectional Feature Pyramid Network (BiFPN) [TPL20] is founded on the PANet which the\nnode with only one input is removed to reduce the amount of parameter calculation. Directly\nconnect the input and output layers of features through an additional skip transmission path\nto enhance the fusion ability of shallow features. BiFPN assigns weights to each layer of\nadaptive learning, and through the allocation of weights, the network perceives the importance\nof different levels. Multi scale feature fusion is widely used in small object detection. It\nsignificantly improves the detection performance of small objects by combining high-level\nsemantic information with low-level detailed information. However, the construction of\nthe FPN is mainly divided into cross-layer connection and parallel branch. Although the\nmechanism increases the performance, it adds additional parameter calculation and storage\nspace. Therefore, the investigation of designing a pyramid feature network architecture that\nis able to enhance the feature fusion capability of the defect detector is required. We propose\na hybrid attention mechanism to improve the feature fusion ability of feature pyramid\nnetworks. We applied the enhanced FPN to the YOLOv5 detection model. This paper\ndesigns comparative experiments and ablation experiments to verify the effectiveness of the\nproposed method on the solder joint defect dataset. The overall process flowchart of the\npaper is shown in Figure 2.\nThe main work and innovative points of this paper are as follows.\n(1) We propose a novel enhanced multi-head self-attention mechanism (EMSA) to\nenhance the ability of the network to perceive contextual information, improve the network\nutilization range of features, and enable the network to have more robust nonlinear expression\ncapabilities.\n(2) We combine a coordinate attention mechanism (CA) with the EMSA to design a\nhybrid attention mechanism (HAM) network, which solves the problem of shallow feature loss\nin feature pyramid networks, increases the capacity of the network to perceive long-distance\nposition information and learn local features.\n(3) The hybrid attention mechanism improves FPN and improves its ability to fuse\nfunctions and information transfer between network channels.\n(4) The improved FPN is applied to the YOLOv5 detection model, which improves the\nsolder joint defect detection ability of YOLOv5, significantly solving the low detection rate\nissue of small defects, while enhancing the universal applicability of the defect detection\nmodel.4 L. ANG, SKNA. RAHIM, R. HAMZAH, R. AMINUDDIN, AND G. YOUSHENG\nFigure 2: The overall process flowchart.\n2.RELATED WORK\n2.1.Feature Pyramid Network. Feature Pyramid Network (FPN) [LDG+17] is a feature\nfusion method commonly used for object detection, which is a network model for extracting\npyramid feature representations. It is usually used in the feature fusion stage of object\ndetection. After performing a bottom-up feature extraction operation on the backbone\nnetwork, the FPN is connected to the corresponding layer’s feature maps from top to bottom\nand horizontally, sequentially combining the two adjacent layers in the backbone network’s\nfeature hierarchy to construct a feature pyramid. Although FPN is simple and effective,\nsome aspects still have shortcomings. Before the feature fusion at each layer, there is a\nsemantic gap between different layers, and direct fusion will have a negative impact on the\nrepresentation ability of multi-scale features. During feature fusion, the feature information\nat the high-level of the pyramid network will be lost during the upsampling process.\nPath Aggregation Network (PANet) [LQQ+18] structure has been improved based on\nFPN and is extensively employed in the YOLO object detection frameworks and its variant\nframeworks. This network has two feature fusion paths, namely top-down and bottom-up.\nThis approach reduces the fusion distance between deep and shallow features. optimize theYOLO ALGORITHM WITH HAFPN FOR SOLDER JOINT DEFECT DETECTION 5\nfeature fusion method of FPN network to a certain extent, improve the object detection\neffect. However, due to the addition of a bottom-up path, low-level feature information\nwill still be lost due to the deepening of network layers, and the additional paths increase\ncomputational complexity and network parameters, reducing the detection speed of the\nnetwork model [YG22, LWC23]. Bi-directional Feature Pyramid Network (BIFPN) [13]\nintroduces jump connections, which uses the jump connections to transfer information\nbetween input and output layers of features. Because the operation is in the same layer,\nthis method can combine more features with fewer parameters. In order to accomplish more\nfeature fusion, BIFPN calculates the same layer parameters more than once, treating each\ntwo-way path as one feature network layer.\nAdaptive Spatial Feature Fusion (ASFF) [LHW19] was proposed in 2019 as a feature\nfusion algorithm with adaptive capabilities. It can adaptively obtain important information\nthrough weight selection, improving the effectiveness of feature fusion. Being able to solve the\ninconsistency problem between features of different sizes in the feature pyramid by learning\nthe connections between different feature maps. It has the advantages of easy implementation,\ncheap computational, and wide applicability. Quan et al. [QZZT23] proposed a Centralized\nFeature Pyramid (CFP), it is based on global explicit centralized feature rules and can be\nused in object detection models. This scheme proposes a generalized intra layer feature\nadjustment method that uses lightweight multi-layer perceptron (MLP) to capture full\nlength distance correlations, and emphasizes the use of intra layer feature rules, which can\neffectively obtain comprehensive but differentiated feature representations. CFP network can\neffectively improve the object detection capabilities of YOLOv5 and YOLOX. It improved\nmAP values by 1.4% on the public dataset MS-COCO, but it’s computational complexity is\nrelatively high.\nFPN is commonly employed in several instances involving defect detection. Chen et\nal. [CT21b] used YOLOv3 for SMD LED chip defect detection, using basic FPN as a feature\nfusion module. It has a reasonable detection rate for missing components, missing wire, and\nreverse polarity defects but a low detection rate for Surface defects. The reason is that the\nsize of the surfaces defect is relatively small and the distribution position is uncertain, so it\nis difficult to detect. Yang et al. [YG22] used YOLOv5 for steel surface defect detection,\nusing Path Aggregation Feature Pyramid Network (PAFPN) as a feature fusion module\nto detect six types of defects on the steel surface, which achieved good real-time detection\nresults, but had a low detection rate for small defect targets. The Precision value is 0.752,\nand the mAP value is 0.827. Du et al. [DWL+23] used enhanced YOLOv5 for PCB defect\ndetection, using BiFPN as a feature fusion module to detect surface defects on PCBs. The\nmAP50 index reached 95.3%, but the mAP value was lower for the smaller defects of the\nmission hole and open circuit. The mission hole defect is the hole effect formed in the solder\npad socket on the PCB due to a lack of solder. The Open Circuit defect refers to the defect\nwhere the circuit on the PCB is disconnected.\nHan et al. [HJW23b] designed a YOLO improvement scheme that replaces the original\nPAFPN with BiFPN, and uses the self-attention mechanism to embed the up sampling and\ndown sampling processing modules in BiFPN, improving the detection rate of the model\nin surface defect detection tasks. However, the ability to detect smaller defects is weak.\nTherefore, in order to improve the detection performance of defect detection networks, it is\nnecessary to design an enhanced attention mechanism to improve the feature fusion ability\nof FPN, thereby reducing the missed detection rate of small-sized defects. In recent years,\nmany studies have utilized attention mechanisms to enhance the detection capabilities of6 L. ANG, SKNA. RAHIM, R. HAMZAH, R. AMINUDDIN, AND G. YOUSHENG\ndefect detection frameworks. Attention mechanism is a mechanism that enables neural\nnetworks to focus their attention on a specific object.\n2.2.Attention Mechanism. The numerous input information includes critical and ir-\nrelevant information required by the task. The attention mechanism can focus on these\nkey information while filtering irrelevant information. The inspiration for the attention\nmechanism comes from the vision system in humans, which can quickly browse images,\nlocate the target area of interest, and enhance attention to the target area, thereby obtaining\nimportant information in that area and suppressing interference from other unrelated areas.\nHu et al. proposed an attention module called Squeeze and Stimulation (SE) [HSS18]. This\nattention module adaptively corrects the weight parameters of each channel by mining the\ninter-dependencies between feature channels so that the network can pay attention to more\ncritical feature information. Woo et al. [WPLK18] extended the spatial dimension and\ndesigned the Convolutional Block Attention Module (CBAM). Its sequential construction\nof the Channel Attention Module (CAM) and the Spatial Attention Module (SAM) en-\nhances the network’s capacity to separate and reinforce feature information. The Efficient\nChannel Attention (ECA) module [WWZ+20] uses one-dimensional convolutional operations\nto extract dependency relationships between channels, achieving cross channel interaction.\nIt solves the problem of SE’s insufficient extraction of dependency relationships between\nchannels due to compression dimensionality reduction. ECA has a low computational\ncomplexity and has little impact on the speed of the network.\nZhang et al. [LWC23] embedded ECA into the feature fusion network in YOLOv5 for\nsolar cell surface defect detection, enhancing PAFPN’s ability to fuse solar cell surface defect\nfeatures and further improving the defect detection rate. The mAP50 value on the dataset\nreached 84.23%. However, ECA incurs significant computational overhead for smaller feature\nmaps. In order to better detect surface defects on steel, Qian et al. [QWYL22] introduced\nthe CA mechanism into the detection network. The mAP value is 79.23%, while the Recall\nvalue is only 62.4%. The CA mechanism requires the calculation of attention weights for the\nentire feature map, so it cannot capture long-distance dependencies. Gathering semantic\ninformation for long-distance dependencies is crucial for small-area detection. On the other\nside, the Vision Transformer (ViT) [DBK+21] relies entirely on self-attention to capture long-\ndistance global relationships and has better accuracy than Convolutional Neural Network\n(CNN). ViT was introduced into computer vision in 2020, and much research has proven its\nperformance in the field of vision.\n2.3.Vision Transformer. Vision Transformer has achieved good performance in the field\nof computer vision because it uses the multi-head self-attention (MSA) mechanism. The\nMSA mechanism is a feature extraction method different from CNN, which can establish\nglobal dependencies and expand the receptive field of images. Compared to CNN, Its\nreceptive area is larger, and it can gather more contextual information. However, some vital\ninformation needed for the detection is removed due to inefficient filtering. Prior knowledge\nof feature localization, translation invariance, and image scale is not utilized. The ViT\nability to capture adequate information is weaker than CNN, and it cannot utilize the prior\nknowledge of feature localization, translation invariance, and image scale of the image itself.\nThe model design of ViT adopts a scaling dot product attention mechanism. ViT firstYOLO ALGORITHM WITH HAFPN FOR SOLDER JOINT DEFECT DETECTION 7\ndivides the image into non-overlapping, fixed-size image blocks and flattens the image blocks\ninto one-dimensional vectors for linear projection to achieve feature extraction.\nAnother type of Transformer is the Swin Transformer. Swin Transformer [LLC+21]\nutilizes local attention and the displacement window multi-head self-attention mechanism\n(SW-MSA) to achieve interaction between local and global features, achieving good results in\nvarious visual tasks and solving the problem of ViT local information being easily damaged.\nThe difference between the self-attention mechanism and the attention mechanism is\nthat the queries and keys come from different sources, while the queries and keys of the\nself-attention mechanism come from the same set of elements. Zhu et al. [ZLWZ21] designed\na Transformer Prediction Head YOLOv5 (TPH-YOLOv5) model for tiny object detection in\ndrone images. The model uses the the Transformer to detect low-resolution feature maps,\nenhancing the network’s capability to extract different local information and achieving better\nperformance for high-density objects. However, using the Transformer module in multiple\nparts of the model resulted in a significant computational workload.\n3.PROPOSED ENHANCED FEATURE PYRAMID NETWORK\n3.1.Hybrid Attention Feature Pyramid Network Architecture. In the task of\ndetecting solder joint defects, some small defects are challenging to detect. Strengthening\nthe feature fusion ability of FPN can help improve the detection effect of small defects. To\nenhance the feature fusion ability of FPN, this study proposes a Hybrid Attention Feature\nPyramid Network (HA-FPN) shown in Figure 3(a). Adding a Hybrid Attention Mechanism\n(HAM) in the basic FPN enhances FPN’s ability to perceive contextual information. It\nalso expands its utilization of feature information and solves the problem of severe loss of\nlocation information. The HAM network structure is shown in Figure 3(b).\n3.2.Hybrid attention mechanism. The Hybrid Attention Mechanism (HAM) module is\nbased on the Transformer structure. Firstly, the input features are passed through a Depth\nWise Convolution (DWConv) residual block to achieve parameter sharing and enhance\nthe learning of local features. Next, Layer Normalization (LN) is used for normalization\nprocessing. The output is processed through two attention mechanism modules, Enhanced\nMulti Head Attention (EMSA) and CA. After processing, it is normalized through an LN\nlayer, and finally, the processing results are output through the MLP layer. The entire\nprocessing process is shown in Equation (3.1).\nX1=X+DW conv( X)\nX2= LN ( X1)\nX3=CA(X2) + EMSA ( X2) +X1\nY= MLP (LN ( X3)) +X3(3.1)\nIn the Equation (3.1), X represents input features, Y represents output features, X1,\nX2, and X3 are intermediate features. DWconv represents deep separable convolution, LN\nrepresents layer normalization, CA represents coordinate attention, and EMSA represents\nenhanced multi head self-attention. MLP is Multilayer Perceptron.\n(1)Enhanced Multi-head Self Attention8 L. ANG, SKNA. RAHIM, R. HAMZAH, R. AMINUDDIN, AND G. YOUSHENG\n(a)\n (b)\nFigure 3: Hybrid Attention Feature Network Architecture. (a)Hybrid Attention Feature\nNetwork (HAFPN), (b)Hybrid Attention Mechanism (HAM)\nA novel EMSA module, as shown in Figure 3 (b), has been proposed for obtaining\ncontextual information and global features, simultaneously using the CA mechanism to\ncapture accurate positional features and capture information between channels effectively.\nThen, the fusion of information features captured by EMSA and CA to enhance the feature\npyramid network’s feature fusion capability is executed. The design concept is based on the\nMSA mechanism in Transformer, as shown in Figure 4 (a).\nThe architecture of EMSA is shown in Figure 4 (b). The entire processing process of\nEMSA is shown in Equation (3.2).\nQ, K, V =FC(Xinput )\nQ′= Linear( Q)\nK′= Linear( K)\nV′= Linear( V)\nXm= SiLU\u0000\nFC\u0000\nQ′⊗K′\u0001\u0001\nXn= Tanh\u0010\nFC(Xm)/√\nd\u0011\nXoutput =FC\u0000\nXn⊗V′\u0001\n+Xinput(3.2)\nIn the Equation (3.2), Xinput represents input features, Xoutput represents output features,\nXmandXnrepresent intermediate features. Q, K and V represent the query matrix, key\nmatrix, and value matrix, respectively. Linear is a linear transformation operation, SiLU is\nthe Sigmoid Linear Unit activation function, FC represents Full Connection processing, and\nd is the scalar factor.YOLO ALGORITHM WITH HAFPN FOR SOLDER JOINT DEFECT DETECTION 9\n(a)\n (b)\nFigure 4: Hybrid Attention Feature Network Architecture. (a)Muti-Head Self Attention,\n(b)Enhanced Muti-Head Self Attention(EMSA)\nFirstly, Q, K, and V components are formed through a fully connected (FC) layer, and\nthen linear transformations are performed on each of the three components. Multiply the\ntransformed Q and K matrices, and then perform a series of nonlinear treatments on them.\nThen, a fully connected layer is used to input the Silu activation function, which is very\nsimilar to the model of signal transmission within neurons, thus more in line with some\nbiological implementation mechanisms and better simulating the information processing\nmechanisms of the human brain. After passing through a fully connected layer, the Tanh\nactivation function is used for processing, and the output result is a matrix multiplied with\nthe linearly transformed V component. Finally, an FC layer is fused with the original input\nfeatures to obtain the final output result. Compared to the vanilla MSA, the EMSA has\nmore nonlinear transformations, which can make the attention network have stronger context\nawareness ability, further enhancing the network’s utilization range of features, and making\nthe network have stronger nonlinear expression ability.\n(2)Coordinate attention\nThis study introduces the coordinate attention (CA) mechanism [HZF21] into HAM\nto enhance the fusion capability of FPN for location information. The CA mechanism can\neffectively enhance the association between different channels and improve the network’s\nperception ability for long-distance location information. The operation process of the CA\nmechanism is shown in Figure 5. For input H (Height of the input feature map) ×W (Weight\nof the input feature map) ×C (Channel of the input feature map), Firstly, global average10 L. ANG, SKNA. RAHIM, R. HAMZAH, R. AMINUDDIN, AND G. YOUSHENG\nFigure 5: Coordinate Attention Mechanism.\npooling is performed from both the height and width dimensions of the image to obtain\nfeature maps with sizes H ×1×C and 1 ×W×C; Next, concatenate the feature maps of two\nsizes and reduce the dimensionality from the channel dimension through shared convolution\nto obtain a feature map of size 1 ×(W + H) ×C/r. After passing through the nonlinear\nlayer, its nonlinear expression ability is improved. Subsequently, In order to increase the\ndimensionality, 1 ×1 convolution is utilized, restoring the feature maps from the width and\nheight dimensions to the A and B scales, and assigning weights through HardSigmaid. To\naccelerate the processing speed of the CA mechanism, HardSigmoid is used to replace the\noriginal Sigmoid activation function for weight assignment. HardSigmoid does not require\nexponentiation, so its calculation speed is faster than Sigmoid. Finally, The feature map’s\nsize is changed to H ×W×C.\n3.3.Improved Feature Fusion Network In YOLOv5. We use the HAFPN as a feature\nfusion module in YOLOv5, replacing the original PAFPN structure. The original feature\nfusion network architecture is shown in Figure 6(a). It comprises of Convolution + Batch\nNormalization + SiLu activation function (CBS), Cross Stage Partial (CSP) Bottleneck with\n3 convolutions (C3) and Spatial Pyramid Pooling Fast (SPPF). Compared to FPN, PAFPN\nhas better network accuracy, but its detection effect for some small defects in solder joints is\nnot good, and the network size is large and has many parameters. Our proposed method\nstrengthens the feature fusion ability of the FPN network, striving to improve recognition\naccuracy while ensuring detection speed. The original feature fusion network architecture is\nshown in Figure 6(b).YOLO ALGORITHM WITH HAFPN FOR SOLDER JOINT DEFECT DETECTION 11\nFigure 6: Original and Improved Feature Fusion Network in YOLOv5. (a) Original Feature\nFusion Network in YOLOv5, (b) Improved Feature Fusion Network in YOLOv5\n4.EXPERIMENT\n4.1.Experimental Environment. The hardware and software environment of this experi-\nment is shown in Table 1.\n4.2.Datasets. The solder joint defect dataset contained 3154 defective solder joint images\nobtained using a Couple-Charged Device (CCD) industrial camera. There are two types of\ndefects, namely ineffective and foot shifting. Among them, 1680 are ineffective defects and\n1474 are foot shifting defects. The dataset is randomly divided into training sets, validation\nsets, and test sets, with a division ratio of 80%, 10%, and 10%, for model training, validation,\nand testing. The method of preparing the dataset is adapted and modified by [YG22].12 L. ANG, SKNA. RAHIM, R. HAMZAH, R. AMINUDDIN, AND G. YOUSHENG\nTable 1: Experimental environment\nEnvironment Configuration/Version\nCPU Intel(R) Core ™i5-12400F 2.50GHz\nGPU NVIDIA RTX 3060\nVideo memory 12GB\nRAM 32 GB RAM\nPyTorch v1.13.1\nCUDA v11.3\ncudnn v8.0\nOperating System Windows 10 Pro\n4.3.Evaluation criterion. The evaluation indicators for this experiment are Precision as\ncalculated in formula (4.1), Recall as calculated in formula (4.2), Mean Average Precision\n(mAP) as calculated in formula (4.3), and Frames Per Second (FPS). In the formula, TP\nrefers to the number of correctly predicted positive samples, FP refers to the number of\nincorrectly predicted positive samples, and FN refers to the number of predicted negative\nsamples but actually positive samples. P represents Precision, and R represents Recall.\nPrecison =TP\nTP+FP(4.1)\nRecall =TP\nTP+FN(4.2)\nmAP =PN\ni=1APi\nNAP=Z1\n0P(R)dR (4.3)\n4.4.Experiments and analysis of results.\n(1) Comparative experiment\nTo verify the effect of hybrid attention proposed in this study, a heat map visualization\nwas used to compare the focusing ability of different attention mechanisms on defect areas,\nas shown in Figure 7. When not utilizing attention mechanisms, YOLOv5 has a weaker\nability to pay attention to solder joint defects. After adding several attention mechanisms, it\nshows some improvement. Among them, the attention mechanisms of SE and ECA have less\nimprovement in defect attention ability and even have a declining effect. CBAM and CA\nattention have an enhancing effect on defect attention. Transformer and Swin Transformer\nhave poor attention to shifting defects in which size is small. The hybrid attention proposed\nin this study significantly increases the coverage effect of the heat map at the defect location.\nIt has a more vital ability to focus on small defects. The location positioning is more\naccurate, proving that hybrid attention can combine contextual content to focus on more\npixels and also proving the effectiveness of hybrid attention.\nTo verify the superiority of the HAFPN algorithm, we compared the defect detection\nperformance of different FPN algorithms on the same dataset. The CSPDarknet53 was used\nconsistently as the feature extraction backbone network. The comparison feature fusion\nalgorithms are FPN, PAFPN, ASFF, BiFPN, and CFPNet. The results are shown in TableYOLO ALGORITHM WITH HAFPN FOR SOLDER JOINT DEFECT DETECTION 13\n(a) Original image\n (b) Without attention mechanism\n (c) Squeeze and Excitation\n(d) CBAM\n (e) Efficient Channel Attention\n (f) Coordinate Attention\n(g) Transformer\n (h) Swin Transformer\n (i) Hybrid Attention Mechanism\nFigure 7: Comparison of Heat Map Effects of Different Attention Mechanisms\n2. The detection indicators for all defects are higher than those of FPN, PAFPN, BiFPN,\nand CFPNet. The precision of Insufficient defects is slightly lower than ASFF. The overall\nprecision, recall, and mAP values of HAFPN are superior to other networks, with precision\nbeing 3.8%, 9.4%, 1.3%, 9.7%, 6.9% higher, recall being 0.5%, 4.8%, 0.7%, 1.5%, 1.2% higher,\nand mAP being 3%, 4.3%, 0.9%, 3.2%, and 3.4% higher, respectively.\nThis study used HAFPN to improve the YOLOv5 defect detection model and compares\nthe improved defect detection model with different detection models on the solder joint\ndefect dataset. The comparative models include one-stage detection models such as YOLOv4\n[DCL+21], YOLOv5 [G22], YOLOv7 [WBL23], and YOLOv8 [G23], improved YOLOv5\ndetection models such as STC-YOLOv5, TPH-YOLOv5, and two-stage detection models\nFaster R-CNN [RHGS15]. Table 3 records the experimental results. Compared with the\nYOLO series algorithms, our model achieves the best overall precision, recall, and mAP\nindicators. Regarding detection speed, although FPS is lower than the original YOLOv5\nmodel but higher than other models, its precision, recall, and mAP are 9.4%, 4.8%, and 4.3%\nhigher than YOLOv5. Compared to Faster R-CNN, the Recall value is lower, but the speed is\nthree times faster, and the proposed algorithm has effective real-time performance. Compared14 L. ANG, SKNA. RAHIM, R. HAMZAH, R. AMINUDDIN, AND G. YOUSHENG\nTable 2: Comparison of HAFPN with other feature pyramid network when using the same\nbackbone on solder joint defect dataset\nBackbone Precision/% Recall/% mAP/%\nInsufficient Shifting all Insufficient Shifting all Insufficient Shifting all\nFPN CSPDarknet53 91.4 82.5 87 98.5 82.6 90.6 97.7 79.3 88.5\nPAFPN CSPDarknet53 87.7 75.0 81.4 98.6 73.9 86.3 96.5 77.9 87.2\nASFF CSPDarknet53 91.6 87.5 89.5 99 81.3 90.4 98.6 82.6 90.6\nBiFPN CSPDarknet53 88.1 74.1 81.1 97.2 82 89.6 95.2 81.4 88.3\nCFPNet CSPDarknet53 88.6 79.2 83.9 97.2 82.6 89.9 93.1 83.1 88.1\nHAFPN(ours) CSPDarknet53 91.4 90.2 90.8 99.5 82.6 91.1 97.8 85.2 91.5\nTable 3: Comparative experiment on detection performance with different defect detection\nalgorithms\nPrecision/% Recall/% mAP/% FPS\nInsufficient Shifting all Insufficient Shifting all Insufficient Shifting all\nYOLOv4 82.3 70.1 76.2 87.4 59.8 73.6 90.6 72.2 81.4 123.5\nYOLOv5 87.7 75.0 81.4 98.6 73.9 86.3 96.5 77.9 87.2 163.9\nYOLOv7 87.2 56.2 71.7 99.5 78.3 89.1 95.6 67.9 81.7 140.5\nYOLOv8 87.8 68.5 78.2 98.2 60.9 79.7 96.6 71.7 84.2 113.6\nFaster R-CNN 87.3 82.0 84.7 98.7 91.5 95.1 90.1 75.3 82.7 50.6\nSTC-YOLOv5 [27] 90.8 77.9 84.4 99.2 76.7 88.0 96.5 80.9 88.7 137.3\nTPH-YOLOv5 [26] 91.1 85.7 88.4 99.5 78.3 88.9 97.2 84.5 90.9 128.2\nYOLOv5+HAFPN (ours) 91.4 90.2 90.8 99.5 82.6 91.1 97.8 85.2 91.5 159.8\nwith the improved YOLOv5 model STC-YOLOv5 and TPH-YOLOv5, the precision increased\nby 6.4%, 2.4%, recall increased by 3.1%, 2.2%, mAP increased by 2.8%, 0.6%, and FPS\nincreased by 22.5, 31.6, respectively.\nWe used the improved YOLOv5 network to visually compare the detection performance\nwith the original YOLOv5 network, as shown in Figure 8. Out of 12 pins, the first nine\nare defective. It can be found that the original YOLOv5 network has missed detection for\nshifting defects (the first two pins) for smaller insufficient defect targets. The improved\nnetwork detection capability has been enhanced, avoiding the occurrence of missed and false\ndetections. In Figure 8(b), all defects have been correctly detected, and better detection\nresults have been achieved.\n(2) Ablation Study\nTo verify the impact of attention module in the improved method on network detection\nperformance, ablation experiments were designed using CSPDarknet53 as the backbone.\nThe network detection effects of adding CA and EMSA in the two feature pyramids of FPN\nand PAFPN were compared, as shown in Table IV. When using FPN, adding EMSA to FPN\nresulted in an increase in precision, recall, and mAP by 3.3%, 0.2%, and 2.1%, respectively.\nAfter adding the hybrid attention mechanism of EMSA and CA, precision, recall, and\nmAP increased by 3.5%, 0.5%, and 3%, respectively. When using PAFPN, adding EMSA\nto PAFPN resulted in an increase in precision, recall, and mAP by 2.4%, 4%, and 1.7%,\nrespectively. After adding the hybrid attention mechanism of EMSA and CA, precision, recall,\nand mAP increased by 2.9%, 4.2%, and 3%, respectively. The hybrid attention mechanismYOLO ALGORITHM WITH HAFPN FOR SOLDER JOINT DEFECT DETECTION 15\n(a)\n(b)\nFigure 8: Comparison of defect detection visualization effects between YOLOv5 (a) and\nYOLOv5+HAFPN (b)\nTable 4: Results of ablation experiments on solder joint defect dataset\nCSPDarknet53 FPN PAFPN EMSA CA Precision/% Recall/% mAP/%\n✓ ✓ 87 90.6 88.5\n✓ ✓ ✓ 90.3 90.8 90.6\n✓ ✓ ✓ ✓ 90.8 91.1 91.5\n✓ ✓ 81.4 86.3 87.2\n✓ ✓ ✓ 83.8 90.3 88.9\n✓ ✓ ✓ ✓ 84.3 90.5 90.2\nmodule has a certain improvement impact on the feature fusion performance of both feature\npyramid networks, proving that the hybrid attention mechanism has stronger perception\nability for the contextual information of features and can promote the improvement of defect\ndetection performance.\n5.CONCLUSIONS\nIn order to improve the defect detection accuracy of SMT solder joints in industrial scenarios\nand reduce the issues of missed detection and false alarm rates of defective solder joints,\nan enhanced multi-head self-attention mechanism is proposed by deep learning for defect\ndetection method of SMT solder joint, which improve the network utilization range of features,\nand enable the network to have more robust nonlinear expression capabilities. We combine\nthe CA mechanism with the EMSA mechanism to construct a hybrid attention mechanism16 L. ANG, SKNA. RAHIM, R. HAMZAH, R. AMINUDDIN, AND G. YOUSHENG\nnetwork. The hybrid attention mechanism is employed to enhance FPN, improving its\ncapability to fuse features, and enhancing information transmission between network channels.\nThe enhanced FPN is applied to the YOLOv5 model, which improves YOLOv5’s detection\nability for solder joint defects, especially addressing the low detection rate issue for small-\nsized defects, while enhancing the generalization capability of the defect detection model.\nThe method designed in this article enhances the feature fusion capability of the network\nthrough an improved attention mechanism. The experimental results show that our method\nachieves a mAP of 91.5% on the solder joint defect dataset, which 4.3% more expensive\nthan the comparison model. Compared with the popular self attention improvement models\nSTC-YOLO and TPH-YOLOv5, the mAP is 2.8% and 0.6% higher, and the FPS index\nis 159.8, which is 22.5 and 31.6 higher than STC-YOLO and TPH-YOLOv5, respectively.\nThis FPS indicate shows that our model performs well in real-time and is beneficial for the\napplication of industrial scenarios. The next step will be to continue improving the network\nto make it more lightweight in model parameters, while further improving the detection\naccuracy of solder joint defects.",
      "metadata": {
        "filename": "YOLO algorithm with hybrid attention feature pyramid network for solder joint de.pdf",
        "hotspot_name": "PCBA_Production",
        "title": "YOLO algorithm with hybrid attention feature pyramid network for solder\n  joint defect detection",
        "published_date": "2024-01-02T14:04:42Z",
        "pdf_link": "http://arxiv.org/pdf/2401.01214v1",
        "query": "THT and SMD soldering PCBA production energy efficiency improvement quantitative efficiency gains"
      }
    },
    "Air plasma key parameters for electromagnetic wave propagation at and out of the": {
      "full_text": "Air plasma\n \nkey parameters for \nelectromagnetic\n \nwave propag\nat\nion\n \nat and out \nof thermal equilibrium:\n \napplications to electromagnetic compatibility\n \n \n \nP\n. \nAndre\n \n1\n, \nG\n. \nFaure\n \n1\n, \nA. Mahfouf\n \n1\n, \nand \nS\n. La\nlléchère\n \n2\n \n \n \n1 \nLAEPT, EA 4646\n \nUniversité Clermont Auvergne, \nUniversit\né\n \nBlaise Pascal\n, \nBP 10448\n, \nF\n-\n63000 Clermont\n-\nFerrand\n, \nFrance\n \n{pascal.andre}/{geraldine.faure}@univ\n-\nbpclermont.fr\n, \nmahfoufphysique@gmail.com\n \n \n2\n \nInstitut Pascal, UMR CNRS 6602\n \nUniversité Clermont Auvergne, Université Blaise Pascal, \nBP 10448\n, \nF\n-\n63000 Clermont\n-\nFerrand\n, \nFrance\n \nsebastien.lallechere@univ\n-\nbpclermont.fr\n \n \n \nAbstract \n─\n \nThis \narticle\n \naddresses the importance of \naccurate characterization of plasma parameters for \nelectromagnetic compatibility (EMC) purposes\n. Most of \nEMC issues involving plasma materials are \nobviously \nmulti\n-\nphysics problems (linking chemical, mechanical, \nthermal and electromagnetic wondering) with deep \ninteractions. One of the main objectives of this paper is \nto establish the theoretical effect of thermal non\n-\nequilibrium on electromagnetic wav\ne propagation. This \nwill be characterized throughout plasma key parameters \n(including complex permittivity). Numerical \nsimulations based upon Finite Integral Technique (FIT) \nwill demonstrate the EMC interest of this methodology \nfor shielding purposes and g\neneral air plasma.\n \n \nIndex Terms\n \n─\n \nPlasma modelling, dielectric \nparameters, thermal equilibrium, electromagnetic \npropagation, electromagnetic compatibility\n.\n \n \nI. INTRODUCTION\n \n \nIntentional or non\n-\nintentional plasma generations \nimply highly multi\n-\nphysics studi\nes involving chemistry, \nthermic, physics and of course electromagnetics to \nproperly characterize electromagnetic (EM) fields. \nPrevious studies [1\n-\n2] have demonstrated the benefit \nthat could be taken from microwave breakdowns to \nimprove shielding effectiven\ness (SE) of enclosures \nembedded with slots and equipment under test. Some \ncurrent electromagnetic compatibility (EMC) issues \nrequire an accurate assessment of materials EM \nproperties in various configurations: for instance \ndamaging of aeronautical systems \n(wires, antennas) due \nto lightning, spacecraft re\n-\nentry (radio frequency, RF, \nplasma generation). Physical properties of such systems \nneed some macroscopic data such as viscosity, \nelectrical conductivity, internal energy. Thus, in several \napplications as d\nischarges with liquid non\n-\nmetallic \nelectrodes, circuit breakers, arc tracking, RADAR \napplications, the electrons reach a temperature (\nT\ne\n) \nhigher than other chemical species (\nT\nh\n). In this \nframework, the interaction between charged particles \nplays a key role\n. Computational electromagnetics \nrequires efficient tools [3] \nin order to assess dielectric \npermittivity that depends greatly on the material and in \nthe case of plasma on its thermodynamic state. \nAlthough thermal equilibrium (i.e. \n\u0000\n=\n\u0000\n\u0000\n\u0000\n\u0000\n=\n1\n⁄\n) is \nassumed in \nmany test cases (decreasing computational \nneeds), it is often badly used. Indeed, authors [4] have \npreviously demonstrated that \nthermal non\n-\nequilibrium \ncan play a major influence on \nargon plasma\n \nproperties\n.\n \n \n \nFig. \n1\n. Air composition at atmospheric pressure at \nthermal equilibrium.\n \n \nFigures 1 and 2 clearly depict the high differences \nexisting between concentrations of heavy species and \nelectrons at and out of thermal equilibrium from\n \nGibbs \nfree energy minimisation [5]\n \nfor air plasma at \natmospheric pressure\n. This \ngeneralizes argon results \nwith classical air composition (i.e. 80%\n \nN\n2\n \nand \n20%\n \nO\n2\n).\n \nIt is to be noticed that ionisation appears at \n1\n0\n1\n8\n1\n0\n2\n0\n1\n0\n2\n2\n1\n0\n2\n4\n5\n0\n0\n0\n1\n0\n0\n0\n0\n1\n5\n0\n0\n0\nO\n-\nN\n2\nO\nN\nO\n2\nN\n+\n2\nN\nO\n+\nO\n+\nN\n+\ne\n-\nN\nO\nN\nO\nO\n2\nN\n2\nT\ne\nm\np\ne\nr\na\nt\nu\nr\ne\n \n(\nK\n)\nC\no\nn\nc\ne\nn\nt\nr\na\nt\ni\no\nn\n \n(\nm\n-\n3\n)lower heavy species temperature when the thermal non \nequilibrium \nratio \nθ\n \nincreases\n \nfor a given heavy species \ntemperature \nT\nh\n. Since \nEM properties highly depend on \nchemical concentrations, it involves potential huge \neffects on plasma behavioural modelling.\n \n \n \nFig. 2.\n \nAir composition at atmospheric pressure out of \nthermal \nequilibrium \n\u0000\n=\n\u0000\n\u0000\n\u0000\n\u0000\n=\n3\n⁄\n.\n \n \nThis \narticle\n \nis organized as follows: section II \nbriefly describes the theoretical methodology\n \nand\n \nplasma characteristic parameters are evaluated for \nseveral frequencies and air plasma in section; section I\nII\n \nis dedicated to the \nelectromagnetic impact of plasma \nmodelling on field propagation, and an EMC illustrative \nexample is proposed. The contribution ends with a \nconclusion and some prospects in section \nI\nV.\n \n \nII. \nPLASMA KEY PARAMETERS AT AND \nOUT OF THERMAL EQUILIBRIUM\n \n \nA. \nTheoret\nical model\n \n \nBy considering one electron inside a given \nelectromagnetic environment (depicted by electric field \nE\n), one can obtain from the Newton’s second law:\n \n \n\u0000\n\u0000\n\u0000\u0000\n\u0000\u0000\n=\n−\n\u0000\u0000\n−\n\u0000\n\t\n\u0000\n\u0000\n\u0000\n,\n \n \n(1)\n \n \nwhere \n−\n\u0000\n\t\n\u0000\n\u0000\n\u0000\n \nis a restoring force, \n\u0000\n \nis the velocity of \nelectron, and \n\u0000\n\u0000\n \nand \ne\n \nare respectively mass and \nelementary charge of electron. Assuming the electric \nfield as \n\u0000\n=\n\u0000\n\u0000\n\u0000\n\u0000\u0000\u0000\n \nand neglecting dipole creation \ninside plasma resolving (1), the celerity of electrons is \nobtained:\n \n \n\u0000\n=\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\u0000\u0000\n \n \n \n(2)\n \n \nIntroducing the drift velocity one can obtain when \n\u0000\n=\n0\n \nthe parameter equal to the collision frequency \n\u0000\n\u0000\n\u0000\n\t\nof electrons with the other particles inside the \nplasma. So the real current density can be written as:\n \n \n\u0000\n\u0000\n\u0000\n\u0000\n⃗\n=\n−\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n⃗\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\u0000\n\u0000\n\u0000\n\u0000\u0000\u0000\n \n \n(3)\n \n \nIntroducing effective current inside Ampere's law \nwe obtain:\n \n \n∇\n\u0000\n\u0000\n⃗\n×\nH\n\u0000\n\u0000\n⃗\n=\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n⃗\n\u0000\u0000\n+\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n⃗\n=\n\u0000\n\u0000\n\u0000\n\u0000\n⃗\n\u0000\u0000\n \n \n(4)\n \n \nThen the real permittivity is written as:\n \n \n\u0000\n=\n\u0000\n\u0000\n\u0000\n1\n−\n\t\n\u0000\n\u0000\n\u0000\n\u0000\n(\n\u0000\n\u0000\n\u0000\n\t\n\u0000\n\u0000\n\u0000\n)\n\u0000\n. \n \n \n(5)\n \n \nThis permittivity is available for isotropic and non\n-\nmagnetized plasma. \nWe can feature key parameters: \nplasma pulsation \n\u0000\n\u0000\n=\n2\n\u0000\n\u0000\n\u0000\n \nand \ncollision frequency \n\u0000\n\u0000\n\u0000\n\t\nof electrons, and \nthe thermal velocity of electrons. \nThis last parameter will be given in th\ne following, \njointly with plasma characteristic parameters (i.e. \n\u0000\n\u0000\n \nand \n\u0000\n\u0000\n\u0000\n)\n \ntaking into account in an original way the \nphysical properties of plasma material\n.\n \n \nB\n. \nPlasma characteristics and equivalent complex \npermittivity\n \n \nFig\nure\n \n3 shows frequency collisions of electrons \nand plasma frequency extracted from air composition \nand thermal assumptions\n \n(see Fig. 1\n-\n2).  From \nequation\n \n(5), we can deduce that the formulation \ndepends greatly on plasma state and wave frequency.\n \n \n \nFig. 3. Pla\nsma frequency and electrons collisions \nfrequency in air plasma at atmospheric pressure.\n \n \nAs a matter of fact, as we can see in Fig\nure\n \n3, the \nelectron plasma frequency is restricted for weaker \ntemperatures.\n \nAs a first approximation, plasma can be \nconsidered\n \nas a dielectri\nc material. In Fig\nures\n \n4\n-\n5\n, we \nhave plotted the real and imaginary part of the relative \n1\n0\n1\n8\n1\n0\n2\n0\n1\n0\n2\n2\n1\n0\n2\n4\n5\n0\n0\n0\n1\n0\n0\n0\n0\n1\n5\n0\n0\n0\nN\n2\n+\nN\n+\n+\nN\n+\ne\n-\nN\nO\nN\nO\n+\nO\n+\n+\n+\nN\n+\n+\n+\nO\n+\n+\nN\n2\nO\nN\nO\n2\nO\n+\n2\nO\n+\ne\n-\nN\nN\nO\nO\n2\nN\n2\nH\ne\na\nv\ny\n \nS\np\ne\nc\ni\ne\ns\n \nT\ne\nm\np\ne\nr\na\nt\nu\nr\ne\n \n(\nK\n)\nC\no\nn\nc\ne\nn\nt\nr\na\nt\ni\no\nn\n \n(\nm\n-\n3\n)\n0\n2\nx\n1\n0\n1\n2\n4\nx\n1\n0\n1\n2\n6\nx\n1\n0\n1\n2\n5\n0\n0\n0\n1\n0\n0\n0\n0\n1\n5\n0\n0\n0\n0\n0\n.\n5\nx\n1\n0\n1\n4\n1\n.\n0\nx\n1\n0\n1\n4\n1\n.\n5\nx\n1\n0\n1\n4\nP\nl\na\ns\nm\na\n \nf\nr\ne\nq\nu\ne\nn\nc\ny\n \n(\ns\n-\n1\n)\nE\nl\ne\nc\nt\nr\no\nn\n \np\nl\na\ns\nm\na\n \nf\nr\ne\nq\nu\ne\nn\nc\ny\n \n(\ns\n-\n1\n)\n\n=\n1\n\n=\n3\nH\ne\na\nv\ny\n \ns\np\ne\nc\ni\ne\ns\n \nt\ne\nm\np\ne\nr\na\nt\nu\nr\ne\n \n(\nK\n)\nP\nl\na\ns\nm\na\n \nf\nr\ne\nq\nu\ne\nn\nc\ny\n \n(\ns\n-\n1\n)\nE\nl\ne\nc\nt\nr\no\nn\n \nc\no\nl\nl\ni\ns\ni\no\nn\n \nf\nr\ne\nq\nu\ne\nn\nc\ny\n \n(\ns\n-\n1\n)permittivity for the air plasma at and out of thermal \nequilibrium.\n \nIt is to be not\ned\n \n(data not shown here)\n \nthat, \nfor the lower temperature the real relative permittivity \nis cl\nose\n \nto 1 and the imaginary part is close to zero. \nFig\nures\n \n4 and \n5\n \nshow \nreal components of dielectric \nconstant are lower than unit\n \n(\nT\nh\n \n= 10,000 K\n)\n.\n \n \n \nFig. 4.\n \nComplex \npermittivity \nof\n \nthe air plasma at\n \nthermal equilibrium\n \n(\n\n \n= 1):\n \nr\neal \n(blue) and imaginary \n(green)\n \nparts at \nT\nh\n \n= 10,000 K from 64 MHz to 10 GHz\n.\n \n \n \nFig. \n5\n.\n \nComplex \npermittivity \nof\n \nthe air plasma \nout of \nthermal equilibrium\n \n(\n\n \n= 3):\n \nr\neal \n(blue) and imaginary \n(green)\n \nparts \nat \nT\nh\n \n= \n10,000 K from 64 MHz to 10 GHz\n.\n \n \nI\nII\n. \nNUMERICAL RESULTS: PLASMA \nCHARACTERIZATION FOR \nSHIELDING \nEFFECTIVENESS (SE) APPL\nI\nCATIONS\n \n \nSome EM simulations were achieved using CST© \nMWS to assess EM field penetration inside air plasma \nat and without thermal equilibrium. Time solver and \ndispersive\n \nmodel based upon data from Fig\nure\n \n3 were \nused to quantify plasma shielding strength (i.e. E\n-\nfield \nmagnitude decreasing while penet\nrating plasma \nmaterial) up to 5\n \nGHz. For the sake of exhaustiveness, \nthe \nnext sections\n \nwill detail numerical simulations\n.\n \n \nA. \nTest case #1: assessment of canon\nical shielding \neffectiveness (SE)\n \nThe first test case is inspired by works in [\n6\n]. \nIndeed, the aim of this paper was to characterize the \ninfluenc\ne of plasma physical parameters \n(e.g.\n \nconcentrations and reactions between chemical \nspecies) on electromagnetic\n \nwave (EMW) propagation \nin to a slab. In this framework, the crucial part of the \nwork relies on the characterization of material \nthroughout models and plasma charact\neristics (i.e. \nplasma \nfrequency\n\t\n\u0000\n\u0000\n, and collision frequency\n\t\n\u0000\n\u0000\n\u0000\n) as \ndepicted in Figure\n \n3. Those characteristics are highly \ndependent to the \nequilibrium (thermal and chemical \ncomposition\n) of heavy species \nand electrons\n.\n \n \nThe aim\ns\n \nof this section \nare\n \nto demonstrate the \ndifference that may be expected from taking into \naccount (or not) potentia\nl thermal non\n-\nequilibrium\n \njointly with the relevance of using computational \nelectromagnetics tool (e.g. CST© with time domain \nsolver)\n. First of all, we put the focus on a canonical \ncase, and in order to prepare n\numerical experiments in \nsection\n \nI\nII\n.2 (test \ncase #2), we propose to model two \nkinds of plasmas (data given in Fig. 3) for \nT\nh\n \n=\n \n10\n,000\n \nK \nand \nT\nh\n \n=\n \n15,000\n \nK \nwith CST© MWS at \nthermal and non\n-\nthermal equilibrium.\n \n \n \nFig. \n6\n. Numerical setup for plane wave impinging on \nplasma slab (\n2\n \nx\n \n2\n \nx\n \n1cm\n3\n, infinitely\n \nextended in x\n-\n \nand \ny\n-\n \ndirections\n) using CST© MWS time solver and \nplasma material modelling (\ndispersive one\n).\n \n \nFig\nure\n \n6\n \ndepicts the numerical setup proposed for \nstraightforward characterization of the EM attenuation \nof waves throughout plasma illuminated by a normal\n-\nincidence plane wave. The plasma slab is a \n2\nx\n2\nx1 cm\n3\n \nvolume (Fig. \n6\n). The time simulation (CST© MWS, \ntime so\nlver) is maintained up to ensure\n \nat least that\n \nmore than 40\n \ndB of the maximum energy has vanished \nfrom the computational domain. The plasma dielectric \ndispersion relies on\n \npurely dispersive modelling \naccording to\n \ndata in\n \nFig\nures\n \n4\n-\n5\n.\n \n \nA huge number of pote\nntial EM applications of \nplasma layers exist in literature as expressed in the \nintroduction. Canonical characterization of plasma \nattenuation at atmospheric pressure is carried out in [\n7\n] \nwhereas spacecraft flight re\n-\nentry is studied in [\n6\n]. In \neach of the\n \ntwo previous cases, attenuation is defined in \na different manner. We will next consider the shielding \neffectiveness (SE) of the plasma (Fig. 9) as follows:\n \n \n\u0000\u0000\n=\n\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n\t\n\u0000\u0000\u0000\n\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n\t\n\u0000\u0000\u0000\n \n \n \n(6)\n \n \n\u0000\u0000\n\u0000\u0000\n=\n−\n20\n\u0000\u0000\u0000\n\u0000\n\u0000\u0000\n\u0000\n\u0000\u0000\u0000\n \n \n \n(7)\n \n \nwhere \n\u0000\n\u0000\u0000\n \nis the electric field located behind the infinite \nplasma slab\n \n(transmitted electromagnetic wave, EMW)\n, \nand \n\u0000\n\u0000\u0000\u0000\n \nis the incident EMW.\n \n \nIn the following and based upon relations (6\n-\n7\n), the \ntransmitted electric field (\nE\nin\n) is computed from CST© \ntime domain solver and dispersive medium given by \noriginal theoretical models from section II. The \nnumerical results are compared to the analytical \napproach from [6] where the transmission coefficient \nt\n \n(\nt\n=\nE\nin\n/\nE\nout\n \nin relation (7)) \nis obtained as follows:\n \n \n\u0000\n=\n\u0000\n√\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n√\n\u0000\n\u0000\n\u0000\u0000\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n(\n\u0000\n\u0000\n\u0000\n\u0000\n)\n\u0000\u0000\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n \n(8)\n \n \nwhere \n\u0000\n\u0000\n \nis the complex permittivity of plasma, \n\u0000\n\u0000\n \nis \nthe wave number in bulk medium (air), \n\u0000\n\u0000\n \nis the wave \nnumber in plasma (here with different characteristics in \nterms of temperature, thermal equilibrium…), and \nd\n \nis \nthe width of considered plasma slab.\n \n \n \nFig. \n7\n. Shielding effectiveness\n \n(attenuation in dB from \n64\n \nMHz to 5\n \nGHz)\n \na\nt\n \nthermal equilibr\nium\n \n\n=1\n \n(\nblue, \nT\nh\n \n= 10,000\n \nK; red, \nT\nh\n \n= 15,000\n \nK\n) \nand out of thermal \nequilibrium \n\n=3\n \n(\nblack, \nT\nh\n \n= 10,000\n \nK; green \nT\nh\n \n=\n \n15,000\n \nK\n)\n \nrelying on analytical formulation \n(dotted lines: reference [6]; dielectric permittivity from \nFig. 4\n \nand \n5) and CST© (plain lines)\n.\n \n \nFigure 7\n \nillustrates the impact of non\n-\nthermal \nequilibrium of air plasma at atmospheric pressure \nand \nat \nT\nh\n \n=\n \n10\n,000\n \nK\n \n/\n \nT\nh\n \n=\n \n15,000\n \nK \non material SE in \nfunction of frequency\n; similarly to Figures\n \n4\n-\n5, the \ndielectric properties of plasma are obtained at \nT\nh\n \n=\n \n15,000 K (data not shown here)\n \nwith original \nwork\ns based upon assessment of plasma characteristics \nfrom air plasma composition (\nFig. 1\n-\n2\n)\n.\n \nFirst, the \nresults are in accordance with literature \nones\n. \nIndeed, \nassuming similar characteristics of plasma (e.g. \npressure, heavy species distribution, width of plasma \nslab), \ntens dB \nof a\nttenuation are expected in [6\n-\n7\n]\n.\n \nObviously, plasma composition and collision frequency \n(Fig. 1 and 2) play \nmajor roles, their increase leading to \na proportional decrease of transmitted electric fields. \nThe maximum gap existing between SE at and out of \nthermal equilibrium is higher for \nT\nh\n \n=\n \n10,000 K than for \nT\nh\n \n=\n \n15,000 K. Indeed, the gap is comprised between \n3\n \ndB and 45\n \ndB considering heavy species temperature \nT\nh\n \n=\n \n15,000 K, whereas SE is between 5\n \ndB and 100\n \ndB \nhigher out of thermal equilibrium than at thermal \nequilibrium at temperature\n \nT\nh\n \n=\n \n10,000 K. Finally, it is \nnoted that the SE differences between \n\n=3 and \n\n=1\n \nincreases with frequency for each plasma temperature. \nFigure\n \n7 validates use of fully dispersive plasma model \ngiven by the original theoretical model proposed in this \nwork (section\n \nII). The next section will illustrate the \nimportance of a careful definiti\non of plasma \ncharacteristics (via complex permittivity and plasma \ncharacteristic frequencies) in EMC framework\n.\n \n \nB\n. \nTest case #2: cabinet shielding at and out of \nthermal equilibrium\n \n \n \nFig. \n8\n. Characterization of the influence of thermal \n(\n\n=1) or \nnon\n-\nthermal (\n\n=3) equilibrium (numerical \nsetup) on the SE of PEC cabinet (sectional view) \nsubject to plane wave illumination.\n \n \nRelying on previous results for canonical case (test \ncase\n \n#1),\n \nwe illustrate the influence of thermal or non\n-\nthermal equilibrium assumption throughout an EMC \nshielding example. \nThe \nnumerical \nconfig\nuration is \nillustrated\n \nin Fig\nure\n \n8\n: a perfectly condu\ncting (PEC) \nenclosure (6 x 6 x 3.96\n \ncm\n3\n) is considered jointly with \na \nsquare aperture (length=4\n \ncm) and 4 mm\n-\nwalls\n \n(and \n4mm\n-\nwidth of plasma). Figure\n \n8\n \nshows the direc\ntion of \nthe impinging plane wave (incident\n \nelectric field \nE\ny\n \n=\n \n260 kV/m\n).\n \nPlasma characteristics are based upon \nresults given in Figures\n \n4\n-\n5 (\nT\nh\n \n=\n \n10,000 K).\n \n \nFig\nure\n \n9\n \nrepresents the evolution of the \nSE of the \ncabinet\n \nin relation with frequency [\n0.064\n \nMHz\n; 5\n \nGHz]. \nThe averaged gap existing between plasma at thermal \n(red) and non\n-\nthermal (green) equilibrium is \nbetween\n \n9\n \ndB\n \nand 45\n \ndB.\n \nThis perfectly justifies\n \nthe EMC need \nfor\n \na particular care since plasma may reveal \nuseful in \nthat framework. It is to be noticed that, due to the \nproposed configuration (worst case regarding size of \nthe aperture and plane wave source), the shi\nelding \neffectiveness without plasma material quickly decrease \nwithin negative levels (i.e. field enhancement instead of \nshielding) \nfrom\n \n2.8\n \nGHz. Contrary to previous case, the \nplasma slab improves SE of the system (enclosure + \nplasma) up to 45\n \ndB\n \n(\n\n=1) and\n \n85\n \ndB (\n\n=3). It should \nalso be noticed that the system is subject to cavity \nresonances, \ndecreasing\n \nSE for instance at \nf\n=4.859\n \nGHz \n(\nresonance frequency \nin accordance with inner sizes of \nthe enclosure). Finally, due to dispersive effect, plasma \nslab closes \nthe cabinet and involve enhancement of \n4.859\n \nGHz\n-\nresonance frequency as depicted in \nFig\nure\n \n9.\n \n \n \nFig. \n9\n. \nSE of the cabinet\n \n(electric field measured at\n \nthe \ncenter of the \nenclosure, \nFig. \n8\n) with plasma at\n \n(red)\n \n/ \nout of\n \n(green)\n \nthermal equilibrium\n, and \nwithout plasma \n(blue\n, \nE\nw\n) including only the enclosure\n.\n \nResults are \ngiven by normalizing data following relation (7) with \nE\nout\n.\n \n \nIn order to illustrate the importance of taking into \naccount \ninner \nthermal characteristics of plasma \n(differences between electrons and heavy species \ntemperatures), \nit is proposed to focus on the influence \nof plasma by normalizing SE (Fig.\n \n9) obtained in the \ntwo cases (\n\n=1) and (\n\n=3) by results computed without \nplasma mat\nerial.\n \nFig\nure\n \n1\n0\n \nshow\ns\n \nthe plasma \nattenuation (based upon \nelectric field \nEy\n-\ncomponent\n \ncomputing\n) \nfollowing the \nrespective \ndB\n-\ndifferences\n \n20\n\u0000\u0000\u0000\n10\n(\n\u0000\n\u0000\n\u0000\n\u0000\n/\n\u0000\n\u0000\u0000\u0000\n)\n \na\nnd \n20\n\u0000\u0000\u0000\n10\n(\n\u0000\n\u0000\n\u0000\n\u0000\n/\n\u0000\n\u0000\u0000\u0000\n)\n \n(\nsee \nFig. 9). As expected the cabinet is involved for a \nnoticeable part in shi\nelding characteristics\n.\n \nFig\nure\n \n10 \ngives an overview of the dedicated effect of plasma \nmaterial\n \nin the proposed EMC configuration (Fig. 8)\n \nby \nnormalizing with test case involving only the enclosure\n. \nThis lays emphasis on the importance of considering \nthermal equilibrium or not from theoretical model to \nEMC application\n \nsince high gaps exist (from 10\n \ndB to \n40\n \ndB) over the whole frequency bandwidth.\n \nAs \naforementioned in Fig\nure\n \n9 and due to the \ncharacteristics of starting resonance frequency (i.e. the \npres\nence of the\n \nair\n \naperture\n, see Fig\nure\n \n8, involving \nboth the resonance mode vanishing and a huge \nreflection of impinging plane wave\n), the shielding \neffectiveness is considerably spoiled in ‘empty’ case \n(withou\nt plasma, see blue line in Fig.\n \n9). Contrary to \np\nrevious case, plasma plays dual role since it affects the \nlevels of fields penetrating in the cabinet but also closes \nit\n, enhancing first resona\nnce mode influence around \n4.859\n \nGHz\n \nas illustrated in Fig\nure\n \n9\n.\n \n \n \nFig. \n10\n. \nPlasma attenuation including the \neffect of the \nenclosure\n \n(normalization with E\n-\nfield given without \nplasma, \nE\nw\n)\n \nat (red) and out of (green) thermal \nequilibrium; results are proposed including dispersive \nplasma medium (plain lines\n, see relation (5) and Fig. 4 \nand 5\n) and Drude’s model (dotte\nd lines)\n \nbased upon \nplasma characteristics (Fig. 3)\n.\n \n \nSimilarly to Fig\nure\n \n9, Fig\nure\n \n10 illustrates \nprevious point and enriches the discussion by providing \ndata obtained with time simulations taking into account \nthe non\n-\nlinear nature of plasma characteristi\ncs via \nDrude’s modelling [3]\n.\n \nIndeed, in that case, the \ndefinition of plasma dielectric properties relies on \nintrinsic plasma characteristics (Fig. 3) jointly with a \nnon\n-\nlinear E\n-\nfield \nthreshold\n \n(\n“\nbreakdown\n”\n)\n \nmodel\nling \nas explained in the following\n. By varying the plasm\na \ndensity \nwithin a given field level\n, plasma attenuation is \ndecreased\n \nrelatively to purely dispersive medium (see \nfigures 7 and 9). I\nt is to be noted i\nn Fig\nure\n \n10\n \nthat\n \nweak \ngaps \nexist \nregarding thermal equilibrium \n(\n\n=1\n, red \ncurves\n)\n;\n \nhuge\nr\n \ndifferences\n \nare obtained\n \nout of thermal \nequilibrium (\n\n=3\n, green curves\n). \nT\nhe same trends\n \nbetween dispersive and Drude’s material\n \nare observed \nin Fig\nure\n \n10 for \n\n=1 (red) whereas up to 35\n \ndB\n-\ngaps are \ncomputed\n \nfor \n\n=3 (green). These variations mostly \ndepend on Drude’s m\nodel including: plasma \ncharacteristics (\ne.g. \nplasma frequency\n \n\u0000\n\u0000\n \nand\n \ncollision \nfrequency\n \n\u0000\n\u0000\n\u0000\n \nfrom developments\n \nin section II\n, \nhere see \nFig. 3 and temperature \nT\nh\n \n= 10,000 K), and electric \nfield breakdown level (here \nE\nbreak\n \n= 1 kV/m). As \naforementioned in [8], plasma induced by microwave\n \nmay efficiently offer EMC advantages by providing \ninteresting EM shielding properties. In this case, we \ndemonstrate the capability of time domain simulations \n(including Drude’s mod\nelling and breakdown level) to \nenrich purely dispersive approach. This also lays \nemphasis on the huge importance of properly defining \nplasma characteristics (plasma frequencies and/or \ncomplex permittivity) in EMC context, especially when \nthermal equilibriu\nm assumption is not satisfied. It \nshould be noticed that, for air plasma at atmospheric \npressure, purely dispersive plasma modelling is \nsufficient to accurately assess the EM shielding \nproperties of the material. In this framework, plasma \ncharacterization \nmay be useful to improve the \nassessment of EMC shielding.\n \n \nI\nV. \nCONCLUSION AND PROSPECTS\n \n \nThis contribution aims at demonstrating the \nimportance of modelling plasma behaviour in EMC \nframework. In this context, a particular care needs to be \ntaken in order to\n \nproperly define the impact of the \nphysical conditions assumed for the definition of \nplasma. Of course, it is well\n-\nknown that the \ncomposition, temperature, pressure of the material \n(plasma) is of great importance. The thermal \nequilibrium respectively betwe\nen the temperatures of \nelectrons and heavy species plays also a key role as \nillustrated by the different characteristics of plasma (i.e. \nplasma and collision frequencies) given at and out of \nthermal equilibrium. Obviously, this involves major \nchanges regar\nding the dielectric properties of the \nmaterial (complex permittivity; non\n-\nthermal \nequilibrium may lead to increase dielectric losses up to \na scaling factor of 6 in comparison with thermal \nequilibrium assumption)\n.\n \n \nIn this paper, we have shown how this orig\ninal \nplasma modelling enriches the understanding of \nshielding properties in EMC context. Thus, we have \nobserved comparable levels of electromagnetic \nattenuation than results found in literature at thermal \nequilibrium. On the contrary, non\n-\nthermal equilibri\num \nmay involve noticeable differences (here 40\n \ndB at \nmaximum). Using “Full\n-\nWave” simulation tool such as \nCST© jointly with the proposed theoretical plasma \nmodels enrich the physical understanding of wave \npropagation in complex media. Moreover, the \nassessme\nnt of EMC criteria (e.g. shielding \neffectiveness) is improved.\n \nFurther works are nowadays under consideration \nto enhance this study. Parametric and multi\n-\nphysics \nworks based upon these models may be useful for EMC \napplications and/or various electromagnet\nic issues \n(e.g.\n \nmaterial characterization, plasma, lightning, \ntransport, space re\n-\nentry, and communications). It \nshould also be useful to assess the effect of non\n-\nlinear \nfield behaviour due to plasma inclusion. Based upon \nproposed work, it should be notice\nd that plasma \nmaterial may be modelled throughout use of proposed \nplasma frequency and collision frequency (parallel to \ncomplex permittivity). This may lead to enrich time \ndomain model and illustrates threshold effects in EMC \ncontext (involving shielding o\nr field enhancement) and \noffers an extension to multi\n-\nphysics issues \n(e.g.\n \nelectromagnetic and thermal ones).\n \n ",
      "metadata": {
        "filename": "Air plasma key parameters for electromagnetic wave propagation at and out of the.pdf",
        "hotspot_name": "Steel_Punching_and_Bending",
        "title": "Air plasma key parameters for electromagnetic wave propagation at and\n  out of thermal equilibrium: applications to electromagnetic compatibility",
        "published_date": "2019-02-19T12:38:27Z",
        "pdf_link": "http://arxiv.org/pdf/1902.07026v1",
        "query": "punching and bending steel EMC shield material substitution studies resource reduction percentages"
      }
    },
    "Anisotropic behaviour law for sheets used in stamping_ A comparative study of st": {
      "full_text": " \n 1 \nJournal home page :http://www.sciencedirect.com/science/journal/16310721  \n Anisotropic behaviour law for sheets used in stam ping: A comparative study of steel and aluminium \nComptes Rendus Mecanique , Volume 331, Issue 1 , January 2003 , Pages 33-40  \nJean-Jacques Sinou and Bruno Macquaire \n \n  \nANISOTROPIC BEHAVIOUR LAW FOR SHEETS USED IN STAMPING - COMPARATIVE STUDY \nOF STEEL AND ALUMINIUM \n \nLOI DE COMPORTEMENT ANISOTROPE POUR TOLES UTILISEES EN EMBOUTISSAGE - \nETUDE COMPARATIVE DE L'ACIER ET L'ALUMINIUM \n  \nJean-Jacques SINOU\n+, Bruno MACQUAIRE* \n \n \nMECANIQUE, Mécanique des solides et des structures/ Mechanics of  solids and structures (M6). \n \n \n+Laboratoire de Tribologie et Dyna mique des Systèmes UMR CNRS 5513, \nEcole Centrale de Lyon, 36 avenue Guy de Collongue, 69134 Ecully, FRANCE. \n \n*RENAULT S.A, \nManager BIW Advanced Design & new Materials, \nTechnocentre Renault, \n1 avenue du golf, 78288 Guyancourt Cedex, FRANCE. \n \n \nAbstract  - For a car manufacturer, unweighting vehicles  is obvious. Replacing steel by aluminium moves \ntowards that goal. Unfortunately, aluminium’s stamping numerical simulation results are not yet as reliable \nas those of steel. Punch-strength and spring-back phenomena are not correctly described. This study on \naluminium validates the behaviour law Hill 48 quadratic  yield criterion with both isotropic and kinematic \nhardening. It is based on the yield surface and on associ ated experimental tests (uniaxial test, plane tensile \ntest, plane compression and tensile shearing). \nSolids and Structures / stamping of sheets / yield surface / mixed hardening . \n \nRésumé  - Réduire le poids des véhicules est l’un des pr incipaux objectifs des constructeurs automobiles. \nLe remplacement de l’acier par l’aluminium va dans  ce sens. Malheureusement, la simulation numérique \nde l’emboutissage des tôles d’aluminium n’a pas encore  atteint le niveau de fiabilité de l’acier : des \nproblèmes tels que l’effet poinçon ou le retour élasti que n’y sont pas encore correctement décrits. L’étude \nconsiste donc à valider la loi de comportement du critère quadratique HILL 48 avec écrouissage mixte \n(écrouissage isotrope et cinématique) à partir de la su rface de charge et des essais expérimentaux classiques \n(essais de traction simple, traction plane, compression plane et cisaillement).  \nSolides et structures / Emboutissage des tôles /  surface de charge / écrouissage mixte. \n \nVersion française abrégée   \n 2Actuellement, l’un des principaux objectifs des c onstructeurs automobiles est l’allégement des \nvéhicules. Or la tendance actuelle ne va pas dans ce se ns : équipement standard de plus en plus complet, \nconfort optimisé, assistance électronique et renforts  de structure omniprésents. Cela contribue à une \naugmentation du poids des véhicules. Se trouvant, de plus, confronté aux règlements imposant une \nréduction de la consommation des véhicules, le sect eur automobile se doit d’axer ses recherches sur \nl’allégement des structures. L’aluminium se positionne donc aujourd’hui comme un concurrent potentiel de l’acier : on peut espérer un gain de poids de 30-40%  par rapport à l’acier. La diversité des matériaux \nimplique que chacun d’entre eux soit employé de façon optimale. Ainsi, la mise en forme des matériaux \nmétalliques de l’automobile passe par la maîtrise de l’ emboutissage. En effet, parmi les procédés de mise \nen forme, l’emboutissage des tôles minces est l’un de s plus répandus dans l’industrie automobile. L’enjeu \ndes prochaines années est de concevoir et valid er des outils d’emboutissage grâce à la simulation \nnumérique, sans avoir recours aux essais de valida tion actuels qui nécessitent la construction d’outils \nd’essai coûteux. Cependant, la simulation numérique ne peut valider des calculs sur la faisabilité de pièces \nde carrosserie qu’à la condition que les paramètres caractérisant les matériaux soient les plus justes possible et reflètent au mieux la réalité. C’est pour cette raison que les codes de calcul nécessitent des modèles de comportement réalistes et des données fiables sur les matériaux. \nL’objectif de ce travail est donc de trouver et valider une loi de comportement pour les tôles d'acier XES \net d'aluminium 6016 à partir d’essais expérimentaux simples (essais de traction simple, traction plane, \ncompression plane et cisaillement). \n Dans un premier temps, nous recherchons à modé liser le comportement des matériaux étudiés \n(aluminium et acier); nous prenons le critère quadr atique  HILL 48 non centré, avec une loi d’écrouissage \nmixte, qui permet de tenir compte de “ l’histoire  du matériau ” provenant de son élaboration. Les \nexpressions théoriques des coefficients de Lankford associés r\n0 et r 90 , ainsi que les lois gouvernant \nl'évolution du centre de la surface de charge, caractéris ant l'écrouissage cinématique, et l'évolution de la \nvariable d'écrouissage isotrope sont établies. \nDans un second temps, nous recherchons, à partir d' essais expérimentaux, à identifier la surface de \ncharge décrite par le critère quadratique HILL 48 non cen tré. Les essais de cisaillement et de compression \nplane permettent d’avoir des points expérimentaux, pour  l’identification de la surface de charge, dans des \nrégions non exploitées par les essais de traction simple et plane et l'identification du comportement s'effectue alors par la caractérisation de la surface de charge (figure 1). De plus , afin d’identifier la loi \nd’écrouissage mixte, comportant la loi d’écrouissage ci nématique et la loi d’écrouissage isotrope (Lemaître \n& Chaboche [4]), nous déterminons, à partir d’essais expérimentaux, les surfaces de charges pour des prédéformations uniaxiales (sens long 0°) de 8% et 14% . Ainsi, à partir des évolutions des surfaces de \ncharges obtenues (figure 2 pour l’acier  et figure 3 pour l’aluminium), nous en déduisons les coefficients \ncaractéristiques des lois d’écrouissage mixte. Nous observons alors une bonne corrélation entre les divers \nessais expérimentaux et les surfaces de charges obte nues en appliquant le modèle quadratique HILL 48 non \ncentré (figure 2 et figure 3). De même, les comparai sons expérimentales et théoriques sur un essai de \ntraction simple (figure 4), dans le cas de l’aluminium, nous permettent de valider la loi de comportement HILL 48 quadratique non centrée avec écrouissage mixte et l’ identification des divers coefficients associés. \nDans le cas de l’aluminium (figure 3), nous remar quons que la surface de charge initiale est décentrée \net que cette dernière se translate progressivement et tend vers une position peu évolutive pour les grandes déformations : même si l’écrouissage cinématique rest e faible par rapport à l’écrouissage isotrope, omettre \nce dernier peut nous conduire à une identification a pproximative voir fausse du comportement des alliages \nd'aluminium. En revanche, le critère quadratique HILL 48 avec un écrouissage isot rope peut suffire pour \nidentifier correctement le comportement de l’acier (figure 2).  \n1 Introduction \nThe lightening of vehicles is obvious ly one of the many goals of cars manufacturers. However, this is \nnot the current trend, which is toward more and more complete standard equipment, optimum convenience,  \n 3electronic assistance, and omnipresent strengthening stru ctures. All these transformations contribute to the \nincreasing weight of the vehicles. The use of alumin ium in automotive/transport application is primarily \ndriven by its high strength to weight ratio characteristics.  This characteristic contributes to the efficiency in \nfuel consumption because of the reduction in weight. Aluminium is a serious challenge to steel : 30-40% \nreduction weight can be expected in using aluminium instead of steel. \nThe large range of available materials implies that th e use of each one has to be optimised. In this way, \nthe imposition of the metallic material of a car needs the stamping control. The stamping of thin aluminium \nsheets is one of the most widely used shaping pro cesses in the car industry. The validation of stamping \ntools by numerical simulation needs models having realistic behaviour and reliable data concerning \nmaterial. The goals are to find an “aluminium behavi our law” for numerical simulation of the stamping \nprocess and to validate the constitutive equations (plas ticity and hardening criterions) based on very simple \nexperimental tests (uniaxial tensile test, plane tensile  test, plane compression test and monotonic and cyclic \ntensile shearing tests). The behaviour identification is  carried out by characterisation of the yield surface \nand the determination of the elastic limit of each material.   \nThe tensile shearing and plane compression tests give additional information for the identification of the \nyield surface and for both the kinematic  and isotropic hardening. These experiments allowed us to obtain \nexperimental plots in areas not studied previously, as illustrated in figure 1. Next, we can check the validity \nof the quadratic or non-quadratic criterion. \nIn this study, we are considering aluminium 6016 (Alloy 6000 series (AlMgSi). Composition in % mass: \n0.9-1.5% Si., 0.4% max. Fe, 0.2% max. Cu, 0.2% ma x. Mn, 0.3-0.6% Mg, 0.1% max. Cr, 0.2% max. Zn, \n0.15% max. Ti, Al remaining,. Heat treatment T4 ) and steel XES (Composition in % mass: 0.08%C max., \n0.03% P max., 0.4% Mn max.). \n \n2 Theoretical approach \n2.1 Constitutive equations \nWhatever the mechanical tests and the associated lo ading area, the later is identified in relation to a \nbehaviour model. Such a model could be defined as  follow [1]: a linear isot ropic elastic behaviour \n(Young’s modulus, Poisson’s coefficient) and a plastic be haviour identified from a plastic criterion with an \nassociated flow rule (quadratic or not) identifie d by the initial yield surface and an hardening model \n(isotropic, kinematic or both) identified by the devel opment of the yield surface or the cyclic test. The \nproblem is to find the constitutive equation resulting from all the realized experimental tests and \ncorresponding to a possible physical description of  all the phenomena observed during the shape up-\nmaking and the use of materials. \nDuring the elaboration and the transformation of th e metals or semi-products, the steel sheets are \nflattened. This flattening diminishes their thickness and gives them particular physical qualities (skin-pass \nprocess : hardening passing on rolling mills which give s an elongation of 0.5 to 2.5 %). Under the same \nconditions, the aluminium sheets are subject to a plan age going from coils to plane sheets. These various \nprocesses concerning the shape up-making of the mate rials show a coupling between the plastic criterion \nand the hardening model (Macquaire [2]) as being a result of the “material story” (with the kinematic \nhardening which is very important at low strains). For metal sheets possessing or thotropy, Hill’s (1948) [3] \nyield criterion has received the most attention and favor. This quadratic non-centered criterion imposes a \nmixed hardening law (kinematic and isotropic). Moreover,  this criterion allows one to take into account the \n“material his story” resulting from its processing. So , we define the following model, called quadratic non \ncentered Hill’s (1948) yield criterion : \n \n222 2\n11 11 22 22 22 22 33 33 33 33 11 11 23 23\n22\n31 31 12 12H( X X ) F( X X ) G( X X ) 2L( X )1f( ,X ) R0\n22M( X ) 2N( X )σ − −σ + + σ − −σ + + σ − −σ + + σ −\nσ= −=\n+σ − +σ −⎛ ⎞\n⎜ ⎟⎜ ⎟⎝ ⎠ (1)  \n 4 \nwhere σ, X, R, are the strain tensor , the tensor con cerning the translation of the yield locus and the \nisotropic hardening variable, respectiv ely. F, G, H, L, M, N define material parameters characterizing the \nanisotropy. Moreover, the mixed hardening law (kinematic and isotropic [4]) can be expressed as follow:  \np\n0 dX C .d .X.d=ε − γ λ   and  Rs a t dR C (R R).dp= −     (2) \n \nwhere 0C,γ are the material parameters characterizing the kinematic hardening. C R, R sat are the material \nparameters characterizing the isotropic hardening. pdε, dλ and dp define the incremental strains, the \nconstant parameter and the equivalent plastic deforma tion, respectively. The equivalent plastic strain can \nbe expressed as follow (Lemaître & Chaboche [4])  \npp pp p p p p 222 2 2 p2\n33 22 11 33 22 11 23 31 12\n2F(Hd Gd ) G(Fd Hd ) H(Gd Fd ) d d ddp 2 2 2 2\nNLM (GH FG HF)ε− ε + ε− ε + ε−ε ε ε ε=+ + +\n++⎛ ⎞\n⎜ ⎟⎜ ⎟⎝ ⎠  (3) \n \n2.2 The mixed hardening law  \nWe note that yield functions provide information on the properties of metals such as the orientation \ndependence of plastic strain ratio, the uniaxial tensile yield stress and the principal direction of strain-rate \ntensor. But it is not sufficient to obtain all the parame ters characterizing the mixed hardening law. This is \nwhy we considered also the evolution of the Lankfor d coefficient, initial and subsequent yield surfaces \nduring complex loading path, to determine all the para meters of the mixed hardening law. It was  known \nthat the directional 0° and 90° plastic strain ratios are given by p p\n0 23rd d=εε  and p p\n90 13rd d=ε ε . The \nassociated plasticity condition imposes p\n1 1df . dε=∂ ∂σ λ , p\n2 2df . dε=∂ ∂σ λ  and p\n3 3df . dε= ∂∂ σ λ . By \nsubstitutions, we obtain : \n \n12 1 23\n0\n31 1 23H(X X ) F(X X )\nG( X X ) F(X X )r−− σ − −=\n−+− σ + −      and  22 1 3\n90\n13 223H( X ) 2X GXr\nG(X X ) F( X X )σ− + −=\n−− + σ − −    (4) \nThe objective is to obtain the four material  parameters of the mixed hardening law ( C0,γ,CR, Rsat). Here, \nwe considered the evolution of the yield locus X and the isotropic hardening parameter R. To determine the \nevolution of the yield locus, we use the Lemaître & Chaboche model [4]: \n \np pp\n00 1dX C .d .X.d C .d .X. (F, G, H, j).d=ε − γ λ =ε − γ ψ ε     (5) \n \nwhere 22 2\n2F( H( j 1) Gj) G(F H( j 1)) H(Gj F)(F,G,H, j) 2.\n(GH FG HF)−+ − + +++ −ψ=\n++      and p\n2\np\n1dj\ndε=\nε. \n \nSo, the  three equations governing the yield locus evolution can be deduced :  \n()p p\n11 0 0\n11CX1 e X e−γψε −γψε=− +\nγψ;()p p\n11 0 0\n22C. jX1 e X e−γψε −γψε=−+\nγψ; ()p p\n11 0 0\n33C. ( j 1 )X1 e X e−γψε −γψε −+=− +\nγψ (6) \nWe applied the same process for the determination of the evolution of the scalar R. We decide to note R as \n(Lemaître & Chaboche [4]) Rs a t dR C (R R).dp=−  (with () dp f R .d d=∂∂ − λ =λ ). Then, we obtain \np\nRs a t 1dR C (R R). (F, G, H, j).d=− ψ ε . The equation governing the evolution of the scalar R can be deduced :  \n  \n 5()p p\nR1 R1Cd Cd\nsat 0 RR 1 e R e−ψε − ψε=− +      ( 7 )  \n \nFinally, we could obtain the analytical expressions of the initial and subsequent yield surface, of the \nevolutions of the yield locus and the scalar R duri ng complex determined loading path. They allow the \nidentification of the material parameter for the obtention of the constitutive equation. \n3 Yield surface: experimental and theoretical results  \nThe first step is to obtain the initial yield locus. We need to identify the material parameters characterizing \nthe anisotropy, the initial yield locus (ijX for i,j=1 to 3) and the scalar  coefficient R. Because of the \nelaboration and transformation of metals (metal sheets possessing orthotropy, skin-pass…) and the \ndefinition of the Lemaître & Chaboche model, we only need four experimental te sts for the identification \nof the initial yield locus. We deci de to use the two uniaxial tensile tests (0° and 90°) and the two plane \ntensile tests (0° and 90°). The others tests (plane compression tests and linear and cyclic tensile shearing \ntests) are only used to validate definitively the in itial yield surface. By considering the quadratic non \ncentered Hill’s yield criterion (1), the associated pl asticity condition and the specific conditions for each \ntensile test (uniaxial 0° tensile test: 23 0 σ= σ= ; uniaxial 90° tensile test:13 0 σ=σ = ; plane strain 0° tensile \ntest: p\n2d0ε=  and 30σ= ; plane strain 90° tensile test:p\n1d0ε= and 30σ=), we obtain the expressions: \n \n() ()2 22 2\n32 32 2 3 2 3uniaxial\n11(GX HX ) GX HX 2 F H X (G F)X 2FX X 2R\nX\n2−++ + −+ + + − −\nσ= +⎡ ⎤⎣ ⎦    (8) \n \n()2 22 2\n13 13 1 3 1 3uniaxial\n22(HX FX ) HX FX (F H) 2X (G F)X 2GX X 2R\nX\nFH−++ + − + + + − −\nσ= +\n+⎡ ⎤⎣ ⎦    (9) \n \n() () ( )( )\n()2 22\n33 3plane\n1 122\n2X G HF/(F G) X G HF/(F G) 2 H (F H) X G F F (F H) 2R\nX\n2H ( FH )//\n/−++ + ++ − − + + −+ −\nσ= +\n−+⎡ ⎤⎣ ⎦ (10) \n \n()() ( )( )\n()2 22 2\n33 3plane\n2 222\n2X HG 2F X GH 2F 4 H F H X G F G 2R\nX\n2F H H/2 /2\n/2− + + + − − ++ +− −\nσ= +\n+−⎡ ⎤⎣ ⎦ (11) \nBy using equations (4), (8), (9), (10) and (11), we obtain the expression of the quadratic non centered \nHill’s (1948) yield surface. Figure 2 and Figure 3 pres ent the different results obtained for steel and \naluminium, respectively. One can observe a perfect correlation between the experimental tests and the yield surface. Hence, we note that the yield locus for al uminium does not remain centered, as illustrated in \nFigure 3 : this implies the need of kinematic law. \nIn order to obtain the constitutive law of steel and aluminium, it is necessary to have subsequent \nexperimental yield surfaces during complex loading. We carry out predeformations on steels (8% and 14% \nuniaxial deformations 0°), which were then cut to perf orm classical experimental tests for the yield surface \n(uniaxial tensile tests, plane tensile test, etc...). We decide to use two uniaxial tensile tests (0° and 90°),  \ntwo plane strain tensile tests (0° and 90°) to determin e the subsequent yield surface. The other data (plane \ncompression tests, a cyclic tensile shearing test and the evolutions of the Lankford coefficients) are only \nused to validate definitively the yield surfaces. Figure 2 and Figure 3 present the results obtained for steel \nand aluminium, respectively. One can observe a perfect  correlation between all the experimental tests and \nthe yield surface. Hence, we note that the evolu tion of the yield surface during the complex imposed \nloading path has a predominant isotropic form. Howeve r, in the case of aluminium, we observe that the  \n 6initial yield center is different from zero and we have a evolution of this yield center for the subsequent \nyield surfaces. \n4 Determination of the mixed hardening law \nIn this part, we consider the mixed nonlinear hardening law (Lemaître & Chaboche) defined previously. \nWe intend to identify the four coefficients of this law ( C0,γ,CR, Rsat). We previously determined the initial \nyield surfaces and the evolution of the yield surfaces during a complex loading path. Using the evolution of \nthe yield surfaces, it is easier to determine the evolution of the yield locus and the evolution of the scalar R. \nTherefore, we could use a lot of tests to identify the f our coefficients and to validate them definitively. At \nthe beginning, we introduce the evolution of the yield locus and the evolution of the scalar R. We obtain also the four coefficients of the mixed non-linear hardening law “Lemaître & Chaboche non-linear”. The \nvalues of the coefficients for aluminium and steel  are given in Table 1. Secondly, we have a good \nagreement between the experimental uniaxial 0° tensile test and the theoretical expression associated, as \nillustrated in figure 4.  \n5 Conclusion \nIn this study, the “Hill 48 quadratic yield criterion w ith both isotropic and kinematic hardening “. By \ncomparing both experimentally measured and calculated da ta based on this criterion, it is demonstrated that \nthis criterion leads to a good description of the phenomen a. The use of the characterisation of yield surface \nis necessary to have a good representation of the cons titutive equation because of the sensitivity of models \nto plastic strain ratios. In this paper, we explain th at the determination of the elastic limit of each material \nand the use of all mechanical tests (uniaxial tensile test, plane tensile test, plane compression test, linear and cyclic tensile shearing tests) are used to c onfirm the “Hill 48 quadratic yield criterion with both \nisotropic and kinematic hardening” behaviour law.  \n ",
      "metadata": {
        "filename": "Anisotropic behaviour law for sheets used in stamping_ A comparative study of st.pdf",
        "hotspot_name": "Steel_Punching_and_Bending",
        "title": "Anisotropic behaviour law for sheets used in stamping: A comparative\n  study of steel and aluminium",
        "published_date": "2008-01-19T07:48:14Z",
        "pdf_link": "http://arxiv.org/pdf/0801.3018v1",
        "query": "punching and bending steel EMC shield material substitution studies resource reduction percentages"
      }
    },
    "Bayesian Model Selection for Network Discrimination and Risk-informed Decision M": {
      "full_text": "Bayesian Model Selection for Network Discrimination and\nRisk-informed Decision Making in Material Flow Analysis\nJiankan Liao∗, Sidi Deng†, Xun Huan‡, and Daniel Cooper§\nJanuary 13, 2025\nAbstract\nMaterial flow analyses (MFAs) provide insight into supply chain level opportunities for resource\nefficiency. MFAs can be represented as networks with nodes that represent materials, processes,\nsectors or locations. MFA network structural uncertainty (i.e., the existence or absence of flows\nbetween nodes) is pervasive and can undermine the reliability of the flow predictions. This article\ninvestigates MFA network structure uncertainty by proposing candidate node-and-flow structures\nand using Bayesian model selection to identify the most suitable structures and Bayesian model\naveraging to quantify the parametric mass flow uncertainty. The results of this holistic approach to\nMFA uncertainty are used in conjunction with the input-output (I/O) method to make risk-informed\nresource efficiency recommendations. These techniques are demonstrated using a case study on the\nU.S. steel sector where 16 candidate structures are considered. Model selection highlights 2 networks\nas most probable based on data collected from the United States Geological Survey and the World\nSteel Association. Using the I/O method, we then show that the construction sector accounts for\nthe greatest mean share of domestic U.S. steel industry emissions while the automotive and steel\nproducts sectors have the highest mean emissions per unit of steel used in the end-use sectors. The\nuncertainty in the results is used to analyze which end-use sector should be the focus of demand\nreduction efforts under different appetites for risk. This article’s methods generate holistic and\ntransparent MFA uncertainty that account for structural uncertainty, enabling decisions whose\noutcomes are more robust to the uncertainty.\nKeywords: Input–Output analysis, Uncertainty, Decision support, Bayesian model averaging,\nBayesian inference, Bayes factor, Model evidence\n1 Introduction\nMaterial flow analyses (MFAs) are typically represented as directed graphs to track the flow of a\nresource (e.g., aluminum) along a supply chain [1]. As described by Cullen and Cooper [2], MFAs are\nessential for evaluating the potential environmental impacts of material (resource) efficiency because\nopportunities for such efficiency are dispersed across the supply chain and product life cycle. MFAs\nelucidate the connections between material production, yields in manufacturing processes, and end-user\ndemand for products. To accurately gauge the impact on emissions, encompassing both upstream and\ndownstream effects, MFAs are necessary since emission-intensive processes (such as clinker production\nin cement kilns) may be spatially and temporally distant from the intervention itself (e.g., improved\n∗jkliao@umich.edu, Ph.D. student, Mechanical Engineering, University of Michigan, Ann Arbor, MI 48109.\n†sidideng@umich.edu, Postdoctoral researcher, Mechanical Engineering, University of Michigan, Ann Arbor, MI 48109.\n‡xhuan@umich.edu, Associate Professor, Mechanical Engineering, University of Michigan, Ann Arbor, MI 48109.\nhttps://uq.engin.umich.edu\n§Corresponding author: drcooper@umich.edu, Associate Professor, Mechanical Engineering, University of Michigan,\nAnn Arbor, MI 48109.\n1arXiv:2501.05556v1  [stat.AP]  9 Jan 2025bridge maintenance for prolonged concrete lifespan). Through MFA, mitigation strategies from various\npoints within the material system can be compared on an equal footing.\nThe data used to construct an MFA are often sparse, noisy, and diverse; e.g., data pertaining\nto the mass flow between two processes might be expressed as a percentage of the sum of all the\nmass flows into a group of processes or alternatively in relation to flows elsewhere in the network [3]\n(see section S1 of the Supporting Information (SI) for more details on the different forms of MFA\ndata). Given the poor data quality typically used to construct an MFA, making environmentally\nmotivated decisions and policies without uncertainty and confidence measures may lead to reduced\nor even negative environmental benefits. For these reasons, it has in recent years been increasingly\nrecognized that it is important to quantify MFA uncertainty to enable informed decision- and policy-\nmaking [2,4,5].\nFor any MFA problem, the mass flow uncertainty is the combined effect of the parametric uncer-\ntainty (e.g., the uncertain allocation of flows through one node that are destined to another node)\ngiven a certain network structure (i.e., the nodes and connections between the nodes that define the\nstructure of the directed graph) and the network structure uncertainty itself; i.e., the presence or ab-\nsence of nodes and/or connections between nodes. Typical origins of parametric uncertainty include a\nlack of data on mass flows, mass flow measurement/data record error, and imputation; e.g., calculating\nthe iron ore produced in Minnesota based on nationwide statistics and estimates of Minnesota’s mar-\nket share. Network structure uncertainty may originate from the MFA practitioner’s lack of expertise\nregarding the material system in question or conflicting evidence on the correct sequence of processes\nin the supply chain. Network structure uncertainty may be exacerbated if the true MFA network\nstructure has changed in recent years and/or varies across regions.\nA popular tool for quantifying parametric uncertainty in MFA is the STAN open-source package [6].\nBy representing collected data inputs using probability density functions (PDFs), STAN reconciles\nthe collected data through least square fitting and uses error propagation to determine the mass flow\nuncertainty [7]. However, such PDFs are often not available; e.g., no error bars or PDFs are published\nalongside the U.N. Comtrade Database’s mass flow statistics [8]. Furthermore, the STAN package\ndoes not allow “multiple data records to be directly considered for an individual flow variable” [9].\nAlternatively, Bayesian inference, a general probabilistic approach to uncertainty quantification, has\nbeen effectively used to handle MFA parametric uncertainty [10–13]. Following Bayes’ rule, an initial\nprobability distribution (the “prior”) representing the starting uncertainty in MFA variables is updated\n(conditioned) based on observed material flow data, resulting in a new distribution (the “posterior”)\nthat reflects the refined uncertainty informed by the data (see, e.g., [14–18]). The Bayesian framework\nis powerful because it allows the incorporation of domain knowledge through the prior, integrates\ndata from multiple sources, and derives a justified level of uncertainty even when data is sparse or\nnoisy. Moreover, Bayes’ rule can be applied iteratively, allowing a sequential updating of the MFA\nuncertainty as new data is collected, with the posterior from one iteration acting as the prior for the\nnext.\nNetwork structure uncertainty is typically not acknowledged in MFAs and has not been rigorously\nstudied. While observation of a large discrepancy between collected and reconciled MFA data may\nsuggest a missing flow in the network [19], there is currently no systematic method that quantifies the\nnetwork structure uncertainty in MFA. However, network structure uncertainty is considered a form of\nmodel uncertainty, and model discrimination under a Bayesian framework has been explored elsewhere,\nincluding in related problems such as determining whether MFA parameters change over multiple\nyears [12] and in predicting future material demand [20]. Below, we first review the existing work\non MFA network structural uncertainty (Section 1.1), the existing application of Bayesian parameter\ninference in MFA (Section 1.2), and then the scope of this paper to extend the Bayesian approach in\nMFA to account for network structure uncertainty (Section 1.3).\n21.1 Previous work on network structure uncertainty in MFA\nExisting studies for uncertainty quantification in MFA are largely based on the assumption that the\nunderlying network structure is correct. However, even if not acknowledged, there is often network\nstructure uncertainty in MFAs. This is because MFAs often model complex supply chains for which\ndetailed knowledge on each aspect of the MFA is dispersed across stakeholders and likely unavailable\nto the MFA practitioner. Material flow data recorded in the literature and by statistical agencies\nmay also be simplified, mislabelled, or misinterpreted in a manner that suggests flows exist between\nnodes where there are none in reality and vice versa. Supply chain structures for nominally identically\nmaterials may also vary by region, introducing further uncertainty. For example, Klinglmair et al. [21]\nshow that the structure of the phosphorus material flow in Denmark is very different from that in\nAustria.\nTo the authors’ knowledge, no published research so far has focused on MFA network structure\nuncertainty. Research pertaining to the evaluation of network structure more generally includes Chat-\nterjee et al. ’s [22] study on supply chain resilience using graph-theoretic metrics and the concept of\n“window of vitality” from the ecology field aimed at balancing redundancy and efficiency. Elsewhere,\nSchwab and Rechberger [4] build on the network resilience theory developed by Ulanowicz et al. [23]\nand the notion of “information defect” defined by Schwab et al. [24]. They define an MFA system\ncomplexity metric as a function of the number of nodes and connections, as well as an MFA data\nsize and quality metric. By comparing these two metrics, Schwab and Rechberger derive a system\nproperty that provides an ordinal measure of the extent to which a system is known (0–100%). For\nexample, a simple MFA network with few nodes and connections and noiseless data collection on each\nmass flow would result in the MFA “system [being] known to an extent of [100%]” [4]. The system\nproperty defined by Schwab and Rechberger allows a quantitative measure of the state of knowledge\non different MFAs or the improving state of knowledge during the incremental development of a given\nMFA. However, neither Chatterjee et al. nor Schwab and Rechberger provide a measure of a given\nnetwork structure being the correct structure for a given system.\n1.2 Previous work on Bayesian inference in MFA\nBayesian parameter inference has previously been used to quantify mass flow uncertainty but assuming\nthe network structure, defined by the MFA practitioner, is correct. Gottschalk et al. [10] were the\nfirst to use Bayesian inference for this purpose, quantifying the uncertainty of nano-TiO 2particle\nreleases into the environment in Switzerland. They introduced the concept of transfer coefficients,\nalso known as allocation fractions (i.e., the fraction of the total flow through a node that is destined to\nflow to a given downstream node), to model an MFA network as a linear system using matrix algebra,\nautomatically guaranteeing mass balance as long as all transfer coefficients emanating from a node sum\nto 1. Instead of forming priors on the mass flows directly, they applied uni-variate prior distributions\non individual transfer coefficients using historical data. Later, Lupton and Allwood [11] adopted a\nmulti-variate Dirichlet distribution to model the prior distribution jointly on all transfer coefficients\nemanating from a given node. The use of Dirichlet distribution provides a flexible way to construct\nthe prior distribution on a node’s transfer coefficients that automatically sums to unity. Lupton and\nAllwood demonstrated their method by remapping the 2008 global steel flow from Cullen et al. [25]\nusing mainly uniform Dirichlet priors and assuming a constant noise level on all collected MFA data\nof±10%. More recently, Dong et al. [12] studied expert elicitation [26] and data noise learning for\nMFA using Bayesian inference. They demonstrated how informed multivariate MFA priors can be\nderived using expert elicitation and how the MFA data noise level can be inferred concurrently with\nthe MFA parameters by modeling the noise as a random variable. Elsewhere, Wang et al. [13] took a\ndifferent approach where instead of parameterizing the MFA using transfer coefficients, they assigned\n3priors on mass flows and process stocks directly, and imposed mass balance constraints through a\nviolation penalty in the likelihood function. They demonstrated their method on two case studies,\nan aluminum system in 2009 based on Liu et al. [27] and a zinc system from 1994 to 1998 based on\nGraedel et al. [28].\nSeveral studies have compared the performance of different Bayesian setups in MFA related prob-\nlems; e.g., determining the consistency of process yields across multiple years [12] and estimating\nincome and price elasticity of future material demand [20]. While Dong et al. [12] did not address\nnetwork structure uncertainty, they did utilize Bayes factor (a quantitative metric for model selection\nthat, as we will show, is useful for capturing network structure uncertainty) to justify using MFA\ndata from multiple years to enhance learning of MFA data noise parameters. Bhuwalka et al. [20]\nused Bayesian hierarchical regression to model copper demand in five regions and sectors as a function\nof price and income, and compared the model fitting results with those generated from an un-pooled\nmodel (individual models for individual regions and sectors) and a fully pooled model (one global model\nfor any region and sector). The result from the hierarchical model showed better uncertainty-reduction\nthan the un-pooled model, while capturing copper demand characteristics region- and sector-wise com-\npared with the fully-pooled model. Despite the advantages of applying a Bayesian framework to MFA,\nBornhoft et al. [29] pointed out the increased modeling effort as a potential drawback.\n1.3 Scope of this paper\nThe key contributions of this paper are 1) applying Bayesian model selection to quantify the network\nstructure uncertainty and calculate the probability of the candidate structures using collected MFA\ndata, 2) deriving individual mass flow PDFs that account for both MFA data noise and network\nstructure uncertainty, and 3) utilizing the mass flow uncertainty results for informed decision making\non demand reduction for decarbonization. Section 2 introduces a stylized MFA model example to\nguide the reader through the methodology. Section 3 demonstrates the method using a case study\non U.S. annual steel flow. Finally, Section 4 concludes with lessons learned from the case study and\npotential future work.\n2 Methodology\nWe first introduce the linear system to formulate the MFA network (Section 2.1). The Bayesian\nframework is then established to quantify the MFA parametric (Section 2.2) and network structure\nuncertainty (Section 2.3). The final mass flow uncertainty then encompasses both the parametric\nuncertainty under a given network structure, and the network structure uncertainty (Section 2.4).\nFinally, we introduce common decision-making metrics and how to use the quantified mass flow un-\ncertainty to guide decision making for resource efficiency (Section 2.5). Figure 1 gives an example of\nthe overall procedure and is referred to throughout this section.\n2.1 A mathematical representation of MFA\nAs described in previous work (e.g., [12]), an MFA can be illustrated using a directed graph, as depicted\nin Figure 1 (top). The nodes in the graph, numbered 1, 2, . . . , np, represent a total of npprocesses,\nproducts, or locations. Each directed edge between two nodes signifies the mass flow of material from\none process to another.\nAt the heart of MFA is the principle of mass conservation, which mandates that the total mass\nof material flowing into each node (total input) must equal the total mass of material flowing out of\neach node (total output). We represent the total input (or equivalently, total output) flow for node i\nasxi. The flow along an edge from node ito node jis then given by zij=ϕijxi, where ϕij∈[0,1]\n4Figure 1: Demonstration of the MFA model selection and risk-informed decarbonization decision making through\ndemand reduction procedure using a simple MFA model example. Dir(·) denotes the Dirichlet distributions\nwith the corresponding hyper-parameters.5represents the allocation fraction of node i’s total outflow directed towards node j(ϕij= 0 if there is\nno flow from node ito node j). Consequently,\nnpX\ni=1ϕijxi=xj andnpX\nj=1ϕij= 1. (1)\nWe suggest using the allocation fractions ( ϕij) as model parameters instead of the direct mass flow\nvalues. This is because, as explained by Gottschalk et al. [10], the allocation fractions provide a\nconvenient method of expressing and enforcing the mass balance relationships for the entire MFA in\nthe form of a linear system. For example, the mass balance equations for the MFA model 1 ( M1)\nillustrated in Figure 1 (top) can be formulated as:\n\n1 0 0 0 0 0 0 0 0\n−ϕ12 1 0 0 0 0 0 0 0\n0−ϕ23 1 0 0 0 0 0 0\n0 0 −ϕ341 0 0 0 0 0\n0 0 −ϕ350 1 0 0 −ϕ850\n0 0 0 0 0 1 0 0 0\n0 0 0 0 0 −ϕ67 1 0 0\n0−ϕ28 0 0 0 0 −ϕ78 1 0\n0 0 0 0 1 0 0 −ϕ891\n\n| {z }\nI−Φ⊤\nx1\nx2\nx3\nx4\nx5\nx6\nx7\nx8\nx9\n\n|{z}\nx=\nq1\n0\n0\n0\n0\nq6\n0\n0\n0\n\n|{z}\nq, (2)\nwhere Idenotes the np×npidentity matrix, Φ ∈Rnp×npis the adjacency matrix with entries being\nthe allocation fractions ϕij,x∈Rnpis the vector collecting all nodal mass flows, and q∈Rnpis the\nvector catching any external inflows qito the network (see Figure 1, top; e.g., aluminum imports in a\ncountry-level aluminum MFA).\nGiven Φ and q, the model prediction for all nodal mass flows can be solved as:\nx= (I−Φ⊤)−1q. (3)\nThe term ( I−Φ⊤)−1is also known as the Ghosh inverse [30], a supply-driven alternative to the more\ncommon demand-driven input/output (I/O) analysis [31]. From the values of x,Φ, q, other common\nMFA quantities of interest (QoIs) can be derived, such as mass flows for each connection ( zij=ϕijxi),\nand the sums, products, and ratios of mass flows. We represent these QoIs through a vector-valued\nfunction, G(Φ, q). These QoIs (the components of G) typically correspond to the same quantities\ncollected as MFA data (the components of y).\nWe further refine our notation to introduce θm={ϕij, qi|ϕij̸= const. , qi̸= const. under Mm}to\ncollectively describe the set of all uncertain model parameters (i.e., of existing connections and external\ninflows) under a given network structure Mm. For example, if the mass flow zijis not present in the\nnetwork structure M1but is in M2, then ϕijwill always be zero and not be included in θm=1, but ϕij\nwould be a non-trivial parameter and be part of θm=2. Hence, what θmentails is dependent on the\nnetwork structure Mm, and we emphasize this point with its subscript m. This notation allows us to\nwrite succinctly G(Φ, q;Mm) =G(θm;Mm).\n2.2 Parametric uncertainty under a fixed network structure\nThe parametric uncertainty, as represented by the parameter posterior distribution, under a fixed\nnetwork structure Mmcan be obtained using Bayes’ rule:\np(θm|y, M m) =p(y|θm, Mm)p(θm|Mm)\np(y|Mm), (4)\n6where p(θm|y, M m) is the posterior PDF, p(y|θm, Mm) is the PDF for the likelihood, p(θm|Mm) is\nthe prior PDF, and p(y|Mm) is the marginal likelihood (also known as model evidence). We assign\nmulti-variate Dirichlet prior distributions to the allocation fractions ϕij’s to automatically satisfy mass\nbalance without needing to introduce additional constraints, while using independent truncated nor-\nmal distributions with a non-negative lower bound for the mass flow input ( qi’s) priors. Different\napproaches can be adopted to assign hyper-parameters for the prior distributions (e.g., the concentra-\ntion parameters of the Dirichlet, the mean and variance of the truncated normal). For example, Dong\net al. [12] used expert elicitation, via online surveys, to define informed priors that help to reduce the\nvolume of data that must be collected to reach a desired reduction in uncertainty. Less taxing options\ninclude defining informed priors based on historical data or using non-informative priors [11].\nThe likelihood models the probability of obtaining the collected data, y, conditioned on the model\nparameters, θm, and network structure Mm; thus, it provides a probabilistic measure on the mismatch\nbetween the observations, y, and the model prediction of the corresponding QoIs, G(θm;Mm). One\noption is to form the likelihood through an additive noise data model [12]. In this article, we adopt a\nrelative noise model:\nyk=Gk(θm;Mm)(1 + ϵk), (5)\nwhere the subscript kindicates the kth component of the vector, and ϵk∼ N(0, σ2\nk) is also independent.\nSubsequently, we obtain\np(y|θm, Mm) =nyY\nk=1pϵk\u0012yk\nGk(θm;Mm)−1\u0013\n=nyY\nk=11√\n2πσkexp\n−\u0010\nyk\nGk(θm;Mm)−1\u00112\n2σ2\nk\n (6)\ndue to the independence of ϵk’s. Existing literature has explored two approaches to assigning values\nto the data noise standard deviation, σk. Lupton and Allwood [11] assigned a fixed value (e.g.,\nσk= 0.1, corresponding to a level of ±10% relative noise) while Dong et al. [12] modeled σkas an\nunknown parameter and inferred it from data. They showed that with multiple years worth of data,\nthe uncertainty in σkcan be significantly reduced; however, the computational cost to generate 10,000\nposterior samples greatly increases from 3 hours to 17 hours for a network of 67 nodes and 169 flows\ninformed by 95 data records, using a computer with Intel(R) CoreTM i7-11800H CPU, 2.30 GHz.\nWhen considering candidate network structures, an observation (data record) may pertain to a flow\nor node that is deemed non-existent in some of the structures. In this scenario, the practitioner may\neither exclude the data record (as we recommend) or establish specific likelihood models to incorporate\nthem (see S2).\n2.3 Network structure uncertainty\n2.3.1 Generating candidate network structures Mm\nTo quantify the network structural uncertainty, the first step is to generate candidate node-and-flow\nstructures for consideration. Here, we restrict the network structure uncertainty to the connectivity\nbetween nodes, excluding uncertainty on whether nodes exist. An intuitive approach is to generate a\npool of candidate network structures by considering every possible permutation of connections between\nthe nodes. However, such an exhaustive approach would lead to an exponential number of candidate\nstructures, 2n2\np, an infeasible consideration. Instead, we recommend using a combination of exploita-\ntion andexploration to create a sensible pool of candidate structures. Exploitation: A practitioner\nshould extract existing MFA network structures from the literature (if they exist) and/or enlist do-\nmain experts to suggest and critique candidate network structures. Exploration: A practitioner may\ninclude a number of semi-randomized “wild-guess” network structures to help increase diversity. Once\n7a total of nLconnections (targeted connections) are identified, where nLis the combined number of\nnetwork “mutations” from exploitation and the “wild-guesses”, a total of 2nLnetwork structures can\nbe formed to generate the candidate pool from the complete permutation of the nLconnections. We\nhypothesize that the combination of exploitation and exploration will enable an inclusive population\nof plausible candidate network structures, and allow us to find a better network structure than if using\nexploration or exploitation alone.\n2.3.2 Network structure prior p(Mm)\nOnce the network structure candidates are established, we can either assign equal prior probabilities\nto all the candidates considered (i.e., a non-informative prior), or craft an informative prior. For\nexample, an informative prior could be based on a complexity ranking to penalize more complex\nnetwork structures [32], aligning with the principle of Occam’s razor. Alternatively, like the prior\ndistributions for parametric uncertainty, expert elicitation can be applied, where a prior distribution is\naggregated from individual experts, under the assumption that the existence of individual connections\nare independent from each other:\np(Mm) =nLY\nl=1pdml\nexist,l(1−pexist,l)1−dml (7)\nwhere pexist,lis, from individual experts, the aggregated probability of the lth connection existing\nin the network structure, and dml∈ {0,1}is an indicator variable associated with the state of the\nconnection l(i.e., dml= 1 if the connection exists, 0 otherwise). Readers are directed to Dong et\nal.[12] for details on aggregation of expert elicited priors.\nFor the example model, we assign equal priors to both network structure candidates (Figure 1\nsecond row).\n2.3.3 Network structure posterior p(Mm|y)\nTwo common model selection methods are based on comparing the models’ Alkaike Information Cri-\nterion (AIC) [33] and Bayesian Information Criterion (BIC) [34]. AIC and BIC are easy to compute\nand apply; however, they do not account for any prior knowledge on the network structures and do\nnot provide a probabilistic measure for the network structure candidates. Instead, we follow the ideas\nof Bayesian model selection [35–37] and apply Bayes’ rule to the network structure Mm, to arrive at\nits posterior probability distribution conditioned on the observed data y:\np(Mm|y) =p(y|Mm)p(Mm)\np(y), (8)\nwhere p(Mm) is the network structure prior probability, p(y) is the network structure marginal like-\nlihood, and p(y|Mm) is the PDF for the “model likelihood”, which is the same term as the model\nevidence in the denominator of Equation (4). In contrast to AIC or BIC, network structure posterior\nprobability is more computationally expensive to evaluate due to the need to estimate p(y|Mm). How-\never, we still deem the Bayesian approach to model selection in Equation (8) favorable because, unlike\nAIC and BIC, it assigns all network structure candidates with a probability measure as justified by\nthe observational evidence y. As a result, the risk of losing an important true feature depicted by one\nof the those models is reduced, as a Bayesian averaged model (i.e., taking the posterior expectation\nfollowing p(Mm|y)) will be a weighted average from all candidate models (see Section 2.4). The model\nposterior does not always favor the most complex model, but instead strikes a balance between model\nsimplicity and its ability to fit the data. We demonstrate this notion in Figure 2, which shows a cubic\n8Figure 2: (a) Bayesian fitting of polynomial models to training data generated from a cubic function with noise\nϵ; solid lines show the mean fit and band shows one standard deviation from mean on each side. (b) Polynomial\nmodel posterior probabilities following Bayesian model selection in Equation (8), and the root mean squared\nerror based on the individual polynomial models.\nTable 1: Guidelines for interpreting how strongly the posterior ratio provide evidence for the preference of one\nnetwork structure over the other (based on guidelines from [38]).\nQuantitative result Evidence in favor of Mmbeing the better model is:\n0.0<log10PRmn≤0.5 Non-substantial\n0.5<log10PRmn≤1.0 Substantial\n1.0<log10PRmn≤1.5 Strong\n1.5<log10PRmn≤2.0 Very strong\n2.0<log10PRmn Decisive\nequation to be the more probable model for a series of noisy data (generated from a cubic function),\ndespite higher order models resulting in a better fit to the data.\nUsing Equation (8), we can also compare the relative probability of two models being the true\nnetwork structure by taking the posterior ratio (PR):\nPRmn=p(Mm|y)\np(Mn|y)=p(y|Mm)p(Mm)\np(y|Mn)p(Mn)=Bmnp(Mm)\np(Mn), (9)\nwhere Bmn, commonly known as the Bayes factor, is the “model likelihood ratio” and indicates the\nmodel Mmis B-times more likely to be the true network structure than the model Mn. Jeffrey [38]\nprovided guidelines for interpreting how strongly the Bayes factor indicates evidence for preferring one\nmodel over the other. Here, we apply the same guidelines to interpret the PR that also incorporates\nprior knowledge, see Table 1.\nIn our work, we adopt the sequential Monte Carlo (SMC) algorithm [39] implemented in PyMC3.\nSMC is a method that uses particles to characterize the parameter posterior p(θm|y, M m) in Equa-\ntion (4), but also provides an estimate of the marginal likelihood p(y|Mm) in its denominator as a\nby-product [40]. This subsequently allows us to compute the PR terms in Equation (9).\n92.4 MFA posterior-predictive uncertainty\nFor a given network structure Mm, once the its parameter posterior is obtained, we can compute the\nposterior-predictive distribution for any QoIs in an MFA (e.g., mass flows) via\np(G|y, M m) =Z\np(θm|y, M m)G(θm;Mm) dθm, (10)\nwhere Grepresents any MFA QoIs (including the connection mass flows zij’s and nodal mass flows\nxi’s). Then, we can calculate the Bayesian model averaged posterior-predictive that now incorporates\nboth parametric uncertainty and network structure uncertainty:\np(G|y) =EMm|y\u0002\nEθm|y,Mm[G(θm;Mm)]\u0003\n=nmX\nm=1p(Mm|y)p(G|y, M m), (11)\nwhere nmis the total (finite) number of candidate network structures. Essentially, p(G|y) results from\ntaking expectations over both the model posterior and the parameter posterior (i.e., averaging Gover\nthe possible models and parameters weighted by their posterior probabilities). Equation (11) can be\nused to derive a final Bayesian-averaged MFA model as shown for the example problem in Figure 1\n(bottom-left).\nTo estimate p(G|y), we approximate the integral for θmusing the posterior samples obtained\nfrom SMC. The p(Mm|y) terms can be obtained from Equation (8) where the p(y|Mm) terms are\nprovided by SMC as a by-product and the denominator can then be easily computed via p(y) =Pnm\nm=1p(y|Mm)p(Mm).\n2.5 Decision making for environmental sustainability under MFA uncertainty\nThe uncertainty of the mass flows in an MFA, calculated using Bayesian model averaging, can be\ntranslated into uncertainty in the environmental impacts (EI) of the associated system via:\nEIsystem =e⊤x, (12)\nwhere eis a vector of environmental impact intensities (e.g., kg.CO 2eq./kgthroughput ) for each node and\nxis the corresponding vector of nodal mass flows. Uncertainty regarding emay also be included at\nthis stage. When exploring options for improving the environmental sustainability of a supply chain,\nit is often helpful to attribute a system’s environmental impacts to the final demand (consumption)\nsectors that drive the supply chain. These sectors are typically represented as the terminal nodes\nin an MFA. If no additional terminal loss nodes are present then, using classic demand-driven I/O\nanalysis [41, 42], the uncertainty of the mass flows in an MFA can be translated into uncertainty of\nthe environmental impact intensity (EII, per unit of consumption) attributable to node i, via.\nEIIi=e⊤L·i (13)\nwhere L= (I−A)−1is the Leontief inverse with L·iindicating its ith column and Abeing the\ntechnical coefficient matrix that describes the input mass flow to a given node from every other node\nand calculated from the nodal mass flows and allocation fractions:\nAij=zij\nxj=ϕijxi\nxj. (14)\nMany readers will be familiar with Equation (13) which is why we present it here; however, we\nalso derive in S3 an equivalent method for calculating the system-wide emissions attributable to a\n10consumption node using the allocation fraction matrix Φ, avoiding the need to calculate the Leontief\ninverse and offering a computational advantage. Using the I/O method, Figure 1 (bottom right) shows\nthe PDFs representing the emission-intensity of the three consumption nodes in the example problem.\nThe EII of a consumption node is equal to the environmental impact savings if mass flows into the\nsame consumption node are decreased by one unit of mass (assuming a linear relationship between\nconsumption and impacts).\nIn supply chain MFAs, besides final demand sectors, there can also be a terminal loss node;\ne.g., from oxidation in liquid metal processing. As the generation of losses is not the motivation for\nthe supply chains’ existence, it is desirable to reallocate the environmental impacts attributable to\nthe terminal loss node to other terminal (consumption) nodes. This reassignment can be achieved\nby considering a perturbation in demand from a consumption node and calculating the effect on\nemissions attributable to the consumption and loss nodes when the nodal process yield loss fractions\nremain constant (as is likely in reality). First, the vector of terminal nodal demand ( F) can be\nrearranged into a vector containing the consumption demand ( Fcons) and a vector containing the loss\nnode throughput re-attributed to the loss generation processes ( Floss). Consider the example problem\nin Figure 1, if node 5 serves as a loss node rather than a consumption node, the loss vector Flossfor\nthis problem would have non-zero elements at Floss,3andFloss,8, and the consumption vector would\nhave a zero element at Fcons,5= 0. The new loss node throughput ( Floss,new ) from any changes to the\nbaseline consumption demand ( Fcons,base ) can be expressed as Equation (15), where Γ is a diagonal\nmatrix of nodal yield losses (0–1) (Diag {Γ}= Φ·lossextractable from the column of allocation fraction\nmatrix Φ corresponding to the loss node), and xnewis the vector of new nodal mass flows induced\nby the change in terminal nodal demand. The new vector of terminal nodal demand ( Fnew) can be\nexpressed as Equation (16): the addition of the new consumption demand ( Fcons,new ) and Floss,new .\nNote that the Fnewvector contains zero-demand at the terminal loss node element. Finally, using\nclassic I/O analysis, the new vector of nodal mass flows ( xnew) is expressed as Equation (17): the\nproduct of the unchanging Leontief matrix and Fnew. These relationships are summarized as:\nFloss,new = Γxnew, (15)\nFnew=Fcons,new +Floss,new , (16)\nxnew=LFnew. (17)\nBy solving Equations (15) to (17), the new nodal mass flows after a perturbation in consumption\ndemand, assuming the nodal yield loss fractions remain unchanged, is\nxnew= (I−LΓ)−1LFcons,new . (18)\nThe EII attributable to node i, rectified to reallocate the environmental impacts otherwise attributable\nto the loss node to consumption node i, is therefore the difference between the EI of the associated sys-\ntem before and after a change in consumption demand at node i, divided by the change in consumption\ndemand at node i:\nRectified EIIi=EIbase−EInew\nFcons,base ,i−Fcons,new ,i(19)\nHaving derived the emission intensity distributions for each consumption node, the next step is\nto determine which node should be prioritized for consumption reduction efforts. Decisions are often\nmade by weighing criteria such as the potential benefits, risks, and the certainty of the outcome [43].\nWhen the benefit of choosing a decision option is expressed as a PDF, then these criteria can be\ntranslated into quantitative metrics to aid decision-making. McPhail et al. [43] reviewed multiple\npopular decision-making metrics. By analyzing the benefit distribution associated with each option,\na decision can be made, for example, to maximize: i. the mean benefit (maximizing the expected\n11benefit); ii. the inverse of the coefficient of variation (maximizing the certainty of the benefit); iii.\nthe maximum value from the distribution (“maximax”: maximizing the best possible outcome); or,\niv. the minimum value from the distribution (“maximin”: maximizing the benefits even under the\nworst outcome). The choice of decision-making metric depends on the risk-tolerance of the decision-\nmaker. Note that as the tails of normal distributions extend to infinity, then it is often necessary to\nconvert the the maximax and maximin criteria into maximizing a high (e.g., 95th) and low (e.g., 5th)\npercentile criterion respectively. Applying these metrics to the example problem, it can be observed\nin Figure 1 (bottom) that consumption reduction efforts should focus on node 4 to maximize both\nthe expected emission reduction and the emission reduction under a best possible outcome; whereas,\nconsumption reduction efforts should focus on node 5 to maximize both the certainty of the expected\nemission reduction and the emission reduction under a worst possible outcome. In contrast, reducing\nthe consumption at node 9 is not the priority under any of the decision-making metrics analyzed.\n3 Case study on the U.S. steel flow\nWe demonstrate the use of Bayesian inference to incorporate network structure uncertainty in MFA\nthrough a case study on the U.S. annual flow of steel in 2012. This year is chosen for the sake of\nconsistency with earlier work on expert elicitation and data nose learning in MFA using Bayesian\ninference [12]; however, any year could have been used. The mass flow uncertainty results are used to\ninform a decarbonization strategy based on reducing demand for steel in different end-use sectors. All\ndata and code used in this case study are available online (see the SI).\n3.1 Constructing candidate network structures\nWe first extract a baseline network structure from Zhu et al. ’s [9] study on the 2014 U.S. steel flow.\nTheir MFA includes 270 metal flows connecting 55 nodes. Following the exploitation and exploration\nmethodology described in Section 2.3.1, we generate additional candidate network structures by iden-\ntifying four targeted connections from the map.\nExploitation : The baseline network structure was discussed with industry experts from Nucor\nand U.S. Steel and compared to steel MFAs for other years [12] and geographies [25]. This revealed\na targeted connection from post-consumer steel scrap to the blast furnace (BF, connection index 1).\nThis connection is absent in the baseline structure from Zhu et al. and questioned by the industry\nexperts; however, it is present in Dong et al. [12] and in the United States Geological Survey (USGS)\nsteel statistics [44].\nExploration : To help increase the diversity of the candidate network structure pool, we consider\nthe presence or absence of connections between other nodes: the flow between scrap and the basic\noxygen furnace (BOF) (connection index 2), between BOF continuously cast slab and the rod and bar\nmill (connection index 3), and between BOF continuously cast slab and the section mill (connection\nindex 4). These connections are highlighted in the Sankey diagram in Figure 4. The existence of a\nscrap flow into the BOF (index 2) is well-known and acts in this case study as a basic test of the\nmodel selection methodology. It is also well-known that BOF continuous casting in the U.S. is used\nfor high-quality, flat-product production (i.e., connected to the hot strip mill and plate mill); however,\nit is less certain whether there are connections to the rod and bar (index 3), and section mills (index\n4).\nA pool of 24= 16 candidate network structures are generated from the complete permutation\nof the 4 targeted connections. The network structures are described using a 4-digit binary code to\nindicate whether the indexed connection is present (1) or absent (0). For example, all 4 targeted\nconnections exist in the network structure, 1111; whereas, out of the 4 targeted connections, only the\nflow between scrap and the BOF (index 2) exists in the network structure, 0100.\n123.2 Constructing prior distributions for network structure and parameters\n3.2.1 Network structure prior\nA panel of three academic experts on steel industry sustainability from the University of Michigan was\ninterviewed to assess the probability of existence, based on their expert judgment, of each targeted\nconnection identified in Section 3.1. The panel consensus was a 10% probability for the flow from\npost-consumer steel scrap to the blast furnace (index: 1), a 95% probability for the flow from scrap\nto the BOF (index: 2), a 15% probability for the flow from BOF continuous casting to the rod and\nbar mill (index: 3), and a 15% probability for the flow from BOF continuous casting to the section\nmill (index: 4). The expert elicited prior probability for each of the 16 network structures was then\nderived based on Equation (7) and is shown in the bar chart in Figure 3.\n3.2.2 Parameter priors\nTo form informative parameter priors, results from expert elicitation are used for upstream allocation\nfractions ( ϕij’s) and external inputs ( qi’s), while non-informative priors are used for the downstream\nallocation fractions. Readers are directed to Dong et al. [12] for the details of the expert elicitation\nand prior aggregation method, with the resulting prior distributions provided in the S4.\n3.3 U.S. steel flow and emissions data collection\nSteel flow data were collected from the USGS [44–46], World Steel Association (WSA) [47] and Zhu\net al. [9]. A complete record of all collected MFA data is provided in S5. For the likelihood, a fixed\nrelative data noise level with σk= 10% is applied to contain computational cost, which has been\nshown to be a reasonable for this dataset based on the noise-learning results from Dong et al. [12].\nEstimated nodal emissions intensities for each U.S. process in the steel network are shown in S7.\nThis analysis focuses on domestic emissions and no impacts are assigned to import nodes.\n3.4 Case study results and discussion\nFor each network structure (containing approximately 180 parameters) it takes about 5 hours using\nan Intel(R) Core i7-9700K CPU, 3.60GHz to generate 10,000 posterior samples using the SMC imple-\nmentation of PyMC3. This translates to a total computational time of around 80 hours across the 16\ncandidate network structures. While significant, the computational time could be reduced by, for ex-\nample, using multi-core processors to run algorithms that can be parallelized or applying approximate\nBayesian techniques such as variational inference [12,48].\n3.4.1 Network structure and mass flow uncertainty\nThe posterior probability for each of the 16 candidate network structures is shown in Figure 3 along\nwith their pairwise comparisons using the posterior ratios. Figure 3 shows that network structure 0100\n(the original structure from Zhu et al. ) achieves the highest posterior probability at 88%, increased\nfrom a prior probability of 32%. Conversely, the probabilities for the network structures where a flow\nexists from continuously cast slab to either the rod and bar mill or section mill decrease from the prior to\nthe posterior. The pairwise comparison indicates that evidence supporting 0100 is “Decisive” against\nall other network structures, except 1100 which the evidence remains “Substantial”. Furthermore,\nthere is “Decisive” evidence against network structures where scrap flowing into the BOF is absent\n(e.g., 1011).\nFigure 4 presents the Bayesian model averaged prior and posterior mass flows as Sankey diagrams\nfor the U.S. steel flow map in 2012. The width and color of the lines indicate the size and uncertainty\n13Figure 3: Posterior probability and the pair-wise posterior ratio interpretation for U.S. steel flow network\nstructure candidates.\n14Figure 4: Bayesian model averaged (a) prior- and (b) posterior-predictive mass flows for the U.S. steel flow in\n2012. All numbers on the flows refer to the mean of the mass flow in units of million metric tons (Mt). The\nuncertainty percentages refer to the flow standard deviation as a percentage of the mean of the mass flow. All\nmass flows refer to steel except for the iron ore flows that include the non-iron mass (e.g., oxygen and gangue).\n15of the flow, with a darker blue indicating a lower uncertainty level. As an averaged model, the network\nstructure in Figure 4 includes all targeted connections (i.e., 1111). However, given the high posterior\nprobabilities of network structures 0100 and 1100, the mass flows in the posterior Sankey diagram\nin Figure 4.b largely reflect the mass flows in the posterior Sankey diagrams for these two structures\nwith the fourteen other structures having only minor contributions. The prior and posterior Sankey\ndiagrams for each of the 16 individual candidate network structures can be found in S6.\n3.4.2 Informed decision making for decarbonization via demand reduction\nThe nodal emission intensities ( e, from S7) and the nodal mass flows ( x, from the posterior Bayesian\nmodel averaged MFA) are used in Equation (12) to calculate the total domestic emissions attributable\nto the 2012 U.S. steel system: a mean of 153 Mt.CO 2eq.and a standard deviation of 7.1 Mt.CO 2eq..\nFigure 5 shows the attributable emissions and emissions intensities for the U.S. consumption sectors\nplus export, calculated using Equation (19). Figure 5 shows that the automotive and steel product\nconsumption sectors have the highest mean emission intensities. This is because these sectors use\nsignificant quantities of high-quality sheet metal, much of which is produced using the emission-\nintensive BF-BOF primary steelmaking route due to sheet metal’s very low tolerance to copper which\nis abundant is post-consumer scrap [49, 50]. Export has the lowest mean emission-intensity as this\nsector is dominated by export of iron ore and post-consumer scrap that has yet to undergo emission-\nintensive processing into steel semi-finished products.\nFigure 5: Domestic emissions and emission intensities attributable to U.S. consumption sectors plus export.\nUsing the decision making criteria introduced in Section 2.5, Table 2 shows a prioritization for re-\nducing steel demand across the consumption sectors (plus export) dependent on the decision maker’s\nappetite for risk. Table 2 shows that under circumstances in which the decision maker wishes to max-\nimize the savings under the worst outcome (“Maximize low savings”), then demand reduction efforts\nshould focus on the automotive sector. Furthermore, if the decision maker is wishing to maximize the\ncertainty of the expected savings, then demand reduction efforts should focus on the “Other” sector.\nFinally, if the decision maker is wishing to either maximize the expected savings or savings under\nthe best outcome (“Maximize high savings”), then demand reduction efforts should focus on the steel\nproducts sector. This result is reflected in the distributions shown in Figure 5, where the right-hand\ntail of the steel products distribution extends further than for the other sectors, indicating higher\nemission savings per unit of reduced consumption under the best outcome. Limitations to using the\nemissions intensity per unit of consumption to prioritize consumption reduction efforts include that\nthe I/O analyses used to derive the emissions intensities are based on linear models that assume a\nconstant, fixed ratio of inputs are used to produce a sector’s output [42]. Focusing on emissions in-\ntensity per unit of consumption also ignores the overall scope for change; e.g., while the construction\n16Table 2: Decision criteria for decarbonization (“savings”) through demand reduction at U.S. steel consumption\nsectors.\nsector is not the most emissions intensive consumption sector per unit consumed, it accounts for more\nemissions (and steel produced) than any other single sector (see Figure 5); therefore, the overall scope\nfor reducing demand may be largest in the construction sector. Relatedly, focusing on emissions in-\ntensity does not account for the difficulty of implementing demand reduction efforts across different\nconsumption sectors, even though this would be a crucial factor in the decision-making process.\n4 Conclusions and future work\nThe Bayesian framework provides a systematic and mathematically rigorous method for incorporating\nnetwork structure uncertainty into MFA uncertainty results. Comparing the posterior ratios of differ-\nent network structures allows a practitioner to determine the level of evidence in favor of one model\nstructure versus another, and Bayesian model averaging allows the practitioner to gain insights from\nall the candidate models. The value of rigorous uncertainty quantification is to enable more informed\ndecision making. The holistic MFA mass flow uncertainty results generated by Bayesian model aver-\naging can be readily combined with the I/O method to help prioritize consumption reduction efforts.\nThrough a case study on the 2012 U.S. steel flow, we demonstrate the expanded Bayesian framework\nfor MFA and its utility in allowing more informed decision-making: the automotive industry is iden-\ntified as a priority for demand reduction efforts that maximize the expected emissions savings. The\ncode for this case study has been made available (via the SI) to help readers apply these methods.\nIntegrating network structure uncertainty into the uncertainty quantification in MFA may necessi-\ntate the collection of additional MFA data to reduce mass flow uncertainty to an acceptable level. This\ncould be problematic as data collection is often a bottleneck in constructing comprehensive MFAs.\nFuture work could address this challenge by combining the Bayesian framework with optimal experi-\nmental design [51–56] to prioritize data collection that most effectively reduces both network structure\nand mass flow uncertainty.\n5 Acknowledgments\nThe authors would like to thank all the steel experts interviewed for this study. This material is based\nupon work supported by the National Science Foundation under Grant No. #2040013.\n176 Supporting information\nThis Supporting Information (SI) document includes data, literature reviews and Sankey diagrams for\nindividual candidate network structures helpful to understanding the main article as well as links to\nPython Scripts and collected MFA data used to conduct the case study.\nPlease go to this link ( http://remade.engin.umich.edu/MFA_NSF.htm ) for downloads of the fol-\nlowing:\n•presentation of the underlying data used to construct the Sankey diagrams in the main paper\n(Figure 4) in a numerical, tabular format;\n•a Python script for performing both the Bayesian inference for both the parametric and network\nstructure uncertainty using specified prior PDFs and collected MFA data; and\n•a Python script for performing MFA Bayesian model averaging and decision-making using the\nrectified input/output (I/O) analysis.\n6.1 Different forms of data available for conducting MFAs\nTable 3: Typical data forms available for conducting MFAs. Adapted from Kopec et al. [3]. Data is generally\nsparse, e.g., it is uncommon to have more than 1,000 data records for constructing a static supply chain MFA.\nData type Example from U.S. steel flow MFA\nStated existence of a\nnode or a flow between\n2 nodesUSGS [57] reports a 24 Mt flow of pig iron between the blast furnace\n(BF) and basic oxygen furnace (BOF), revealing the existence of both\nnodes and the presence of a flow between them.\nFlow between 2 nodes USGS [57] reports 24 Mt flow between the “BF” node and “BOF” node.\nSums of flows USGS [58] reports the sum of all continuous casting product flows (slabs,\nbillets and blooms) at 88 Mt in 2016.\nPercentages of sums AISI [59] reports construction sector taking up 40% of total steel de-\nmands.\nPercentages to a desti-\nnationOmar [60] reports the average process yield when making an irregular\nsheet metal car side body panel as 38%.\nPercentages from an ori-\nginUSGS [58] reports 70% of the continuous casting products being slabs.\nAdditional linear rela-\ntionsWSA [61] reports oxidation losses from direct reduction and the blast\nfurnace are equal\nSequential multiplica-\ntionsMilford et al. [62] report blanking and stamping process yields sepa-\nrately, that can be multiplied together to get an overall sheet metal\nfabrication yield.\n6.2 Alternative method for likelihood modeling of targeted connections\nWhen considering candidate network structures, an observation (data record) may pertain to a flow\nor node that is deemed non-existent in some of the structures. In this scenario, the practitioner may\neither exclude the data record (as we recommend) or establish specific likelihood models to incorporate\nthem.\nThe inclusion of data records on the targeted connections that are missing in certain candidate\nnetwork structures requires special treatment. As these flows are not present in the network structure,\n18the observations should have no impact on the parametric posteriors from the inference regardless of\nthe value of the observation:\np(yk|θm;Mm) =Ckfor θm∈supp( θm), (20)\nwhere Ckis a constant regardless of the value of yk, and kis the index for any observation on the\nmissing connections. The choice of the constant value Cwould impact the marginal likelihood (see\nEquation (4) in the manuscript) and therefore the fairness of Bayesian model selection. For a proper\nuniform distribution model, a compact support for the likelihood is required, and should be consistent\nacross all candidate network structures. As a result, instead of a normal distribution for the parametric\nlikelihood model (see Equation (6)), a truncated normal with properly selected upper and lower bounds\nbuandblshould be applied. As a result, the likelihood can be modified as follows:\np(y|θm;Mm) =\n\nny−nLY\nl=1p(yl|θm;Mm)nLY\nk=1Ck=ny−nLY\nl=1p(yl|θm;Mm)nLY\nk=11\nbu,k−bl,k,forθm∈supp( θm)\n0, else,\n(21)\nwhere nyis the total number of observations and nLis the number of data records on the targeted\nconnections.\n6.3 Alternative method for a supply-driven Input/Output analysis\nThe alternative supply-driven I/O method to calculate the environmental impacts (EI) of the associ-\nated system avoids the calculation of the Leontief inverse [31], which provides computational benefits\nover the classic I/O method. For any system, an “emission-balance” can be established at a given\nnode iwhere the total emissions attributable to the output nodal mass flows is equal to the sum of\ntotal emissions attributable to the input nodal mass flows and any emissions produced/captured at\nthe process, which can be either due to emission released at the process or additional supplies at the\nnode:\nEIi=npX\nj=1EIj·ϕji+e0,i·xi+eq,i·qi, (22)\nwhere ϕjiis the allocation fraction, e0is a vector of emission-intensities produced/captured at each\nnode, and eqis a vector of emission-intensity associated with the material inflow q. Subsequently,\nEquation (22) can be assembled into a matrix form:\nEI = ( I−Φ⊤)−1(EI0+ EI q), (23)\nwhere EI 0and EI qare column vectors of the emissions produced/captured at each node and the\nembodied emissions of the material inflow q, respectively, with elements EI 0,i=e0,ixiand EI q,i=eq,iqi.\nTherefore, the environmental impact intensity (EII) at node iis:\nEIIi=EIi\nxi. (24)\n6.4 Parameter priors for the case study\nIn this section, we include the details of the informative priors ( ϕandq) used for the case study\non the 2012 U.S. steel flow. The informative priors for this case study are obtained from expert\nelicitation through interviews with domain experts. Readers are directed to Dong et al. [12] for\nthe details of the methodology to conduct expert elicitation and prior aggregation from multiple\n19experts. The elicitation process conducted is under the assumption of the network structure with\nall 4 targeted connections existent. As informative priors are applied to both the flows originating\nfrom scrap node and continuous cast slab node, adjustment is required to apply these informative\npriors to candidate network structures where one or more targeted connections are not present in the\nmodel. We delete the hyper-parameter(s) of the Dirichlet priors for allocation fractions corresponding\nto the targeted connection(s) if they do not exist in the candidate network structure, while keeping\nthe rest of the hyper-parameters fixed. For example, the informative prior distribution used for the\nallocation fractions originating from continuous cast slab to hot strip mill, plate mill, rod and bar mill\nand section mill when all four connections are present is ϕ∼Dir(11.46,2.11,2.82,1.81). In the case\nwhere the connection to the rod and bar mill does not exist, the revised informative prior for this set\nof allocation fraction will be ϕ∼Dir(11.46,2.11,1.81).\nFor the exact values of the hyper-parameters used for informative priors, please see the inference\ncode from the link below ( http://remade.engin.umich.edu/MFA_NSF.htm ).\n6.5 Case study: U.S. steel flow MFA collected data\nTable 4: MFA data from 2012.\nDescription Type Value (Mt) Source\nImport to Iron Ore Consumption External Input 5.16 1\nIron Ore Production External Input 54.7 1\nIron Ore Production to Export Flow 11.2 1\nIron Ore Consumption to Blast Furnace Flow 46.3 1\nBlast Furnace to Pig Iron Flow 32.1 3\nImport to DRI Consumption External Input 2.47 2\nDRI to Export Flow 0.01 2\nDRI Consumption to Blast Furnace Flow 0.049 2\nDRI Consumption to Basic Oxygen\nFurnaceFlow 1.91 2\nDRI Consumption to Electric Arc Fur-\nnaceFlow 1.62 2\nDRI Consumption to Cupola Furnace Flow 0.01 2\nDRI Consumption to Other Flow 0.01 2\nImport to Pig Iron Consumption External Input 4.27 2\nPig Iron to Export Flow 0.021 2\nPig Iron to Basic Oxygen Furnace Flow 31.5 2\nPig Iron to Electric Arc Furnace Flow 5.79 2\nPig Iron to Cupola Furnace Flow 0.057 2\nPig Iron to Other Flow 0.046 2\nImport to Scrap Consumption External Input 3.72 2\nPurchased Scrap to Scrap Collected External Input 70.98 2\nScrap Collected to Export Flow 21.4 2\nScrap Consumption to Electric Arc\nFurnaceFlow 50.9 2\nScrap Consumption to Cupola Furnace Flow 1.11 2\nScrap Consumption to Other Flow 0.167 2\nBOF CC to Continuous Casting Flow 36.281 4\n20HSM Yield to Hot Rolled Sheet Flow 19.544 3\nCRM Yield to Cold Rolled Sheet Flow 11.079 3\nPlate Mill to Plates Flow 9.12 3\nRBM Yield to Reinforcing Bars Flow 5.65 3\nRBM Yield to Bars Flow 6.7 3\nRBM Yield to Wire and Wire Rods Flow 2.784 3\nRBM Yield to Light Section Flow 2.13 3\nSMYield to Heavy Section Flow 5.03 3\nSMYield to Rail and Rail Accessories Flow 1.009 3\nPMYield to Export Flow 0.817 3\nTin Mill to Tin Mill Products Flow 2.009 3\nGalvanized Plant to Galvanized Sheet Flow 16.749 3\nPipe Welding Plant to Pipe and Tubing Flow 2.165 3\nSeamless Tube Plant to Pipe and Tub-\ningFlow 2.162 3\nElectric Arc Furnace to Billet Ratio 0.333 5\nElectric Arc Furnace to Bloom Ratio 0.157 5\nElectric Arc Furnace to Ingot Casting Ratio 0.02 5\nCold Rolled Sheet to Automotive Ratio 0.25 5\nCold Rolled Sheet to Machinery Ratio 0.079 5\nCold Rolled Sheet to Steel Products Ratio 0.313 5\nCold Rolled Sheet to Export Ratio 0.112 5\nGalvanized Sheet to Construction Ratio 0.19 5\nGalvanized Sheet to Automotive Ratio 0.42 5\nGalvanized Sheet to Export Ratio 0.15 5\nHot Rolled Sheet to Construction Ratio 0.59 5\nHot Rolled Sheet to Automotive Ratio 0.133 5\nHot Rolled Sheet to Machinery Ratio 0.108 5\nHot Rolled Sheet to Energy Ratio 0.01 5\nHot Rolled Sheet to Steel Products Ratio 0.0027 5\nHot Rolled Sheet to Export Ratio 0.065 5\nPipe and Tubing to Construction Ratio 0.227 5\nPipe and Tubing to Automotive Ratio 0.08 5\nPipe and Tubing to Machinery Ratio 0.04 5\nPipe and Tubing to Energy Ratio 0.55 5\nPipe and Tubing to Export Ratio 0.065 5\nPlates to Construction Ratio 0.0408 5\nPlates to Automotive Ratio 0.01 5\nPlates to Machinery Ratio 0.5187 5\nPlates to Energy Ratio 0.067 5\nPlates to Export Ratio 0.231 5\nBars to Construction Ratio 0.152 5\nBars to Automotive Ratio 0.311 5\nBars to Machinery Ratio 0.238 5\nBars to Energy Ratio 0.046 5\nBars to Export Ratio 0.131 5\n21Reinforcing Bars to Construction Ratio 0.925 5\nReinforcing Bars to Export Ratio 0.039 5\nTin Mill Products to Automotive Ratio 0.006 5\nTin Mill Products to Steel Products Ratio 0.685 5\nTin Mill Products to Export Ratio 0.067 5\nWire and Wire Rods to Construction Ratio 0.388 5\nWire and Wire Rods to Automotive Ratio 0.285 5\nWire and Wire Rods to Machinery Ratio 0.1 5\nWire and Wire Rods to Energy Ratio 0.049 5\nWire and Wire Rods to Export Ratio 0.094 5\nRail and Rail Accessories to Construc-\ntionRatio 0.779 5\nRail and Rail Accessories to Machinery Ratio 0.047 5\nRail and Rail Accessories to Export Ratio 0.141 5\nLight Section to Construction Ratio 0.86 5\nLight Section to Automotive Ratio 0.026 5\nLight Section to Export Ratio 0.057 5\nHeavy Section to Construction Ratio 0.877 5\nHeavy Section to Export Ratio 0.092 5\nSteel Product Casting to Construction Ratio 0.259 5\nSteel Product Casting to Automotive Ratio 0.385 5\nSteel Product Casting to Machinery Ratio 0.259 5\nSteel Product Casting to Export Ratio 0.111 5\nIron Product Casting to Construction Ratio 0.311 5\nIron Product Casting to Automotive Ratio 0.552 5\nIron Product Casting to Machinery Ratio 0.066 5\nIron Product Casting to Export Ratio 0.07 5\nReference in Table 3\n1USGS. 2012. Iron Ore. Minerals Yearbook. https://www.usgs.gov/centers/\nnational-minerals-information-center/iron-ore-statistics-and-information\n2USGS. 2012. Iron and Steel Scrap. Minerals Yearbook. https://www.usgs.gov/centers/\nnational-minerals-information-center/iron-and-steel-scrap-statistics-and-information\n3USGS. 2012. Iron and Steel. Minerals Yearbook. https://www.usgs.gov/centers/\nnational-minerals-information-center/iron-and-steel-statistics-and-information\n4WorldSteel. 2017. Steel Statistical Yearbook 2017. https://worldsteel.org/steel-by-topic/\nstatistics/steel-statistical-yearbook/\n5Yongxian Zhu, Kyle Syndergaard, and Daniel R. Cooper. Environmental Science\n& Technology 2019 53 (19) 11260-11268. DOI: 10.1021/acs.est.9b01016\n6.6 Sankey diagram for all 16 candidate network structures\nFigures below show the prior and posterior mass flows as Sankey diagrams of all 16 candidate network\nstructures for the 2012 U.S. steel flow.\n22Figure 6: Bayesian (a) prior- and (b) posterior-predictive mass flows of network structure 0000 for the U.S.\nsteel flow in 2012. All numbers on the flows refer to the mean of the mass flow in units of million metric tons\n(Mt). The uncertainty percentages refer to the flow standard deviation as a percentage of the mean of the mass\nflow. All mass flows refer to steel except for the iron ore flows that include the non-iron mass (e.g., oxygen and\ngangue).\n23Figure 7: Bayesian (a) prior- and (b) posterior-predictive mass flows of network structure 0001 for the U.S.\nsteel flow in 2012. All numbers on the flows refer to the mean of the mass flow in units of million metric tons\n(Mt). The uncertainty percentages refer to the flow standard deviation as a percentage of the mean of the mass\nflow. All mass flows refer to steel except for the iron ore flows that include the non-iron mass (e.g., oxygen and\ngangue).\n24Figure 8: Bayesian (a) prior- and (b) posterior-predictive mass flows of network structure 0010 for the U.S.\nsteel flow in 2012. All numbers on the flows refer to the mean of the mass flow in units of million metric tons\n(Mt). The uncertainty percentages refer to the flow standard deviation as a percentage of the mean of the mass\nflow. All mass flows refer to steel except for the iron ore flows that include the non-iron mass (e.g., oxygen and\ngangue).\n25Figure 9: Bayesian (a) prior- and (b) posterior-predictive mass flows of network structure 0011 for the U.S.\nsteel flow in 2012. All numbers on the flows refer to the mean of the mass flow in units of million metric tons\n(Mt). The uncertainty percentages refer to the flow standard deviation as a percentage of the mean of the mass\nflow. All mass flows refer to steel except for the iron ore flows that include the non-iron mass (e.g., oxygen and\ngangue).\n26Figure 10: Bayesian (a) prior- and (b) posterior-predictive mass flows of network structure 0100 for the U.S.\nsteel flow in 2012. All numbers on the flows refer to the mean of the mass flow in units of million metric tons\n(Mt). The uncertainty percentages refer to the flow standard deviation as a percentage of the mean of the mass\nflow. All mass flows refer to steel except for the iron ore flows that include the non-iron mass (e.g., oxygen and\ngangue).\n27Figure 11: Bayesian (a) prior- and (b) posterior-predictive mass flows of network structure 0101 for the U.S.\nsteel flow in 2012. All numbers on the flows refer to the mean of the mass flow in units of million metric tons\n(Mt). The uncertainty percentages refer to the flow standard deviation as a percentage of the mean of the mass\nflow. All mass flows refer to steel except for the iron ore flows that include the non-iron mass (e.g., oxygen and\ngangue).\n28Figure 12: Bayesian (a) prior- and (b) posterior-predictive mass flows of network structure 0110 for the U.S.\nsteel flow in 2012. All numbers on the flows refer to the mean of the mass flow in units of million metric tons\n(Mt). The uncertainty percentages refer to the flow standard deviation as a percentage of the mean of the mass\nflow. All mass flows refer to steel except for the iron ore flows that include the non-iron mass (e.g., oxygen and\ngangue).\n29Figure 13: Bayesian (a) prior- and (b) posterior-predictive mass flows of network structure 0111 for the U.S.\nsteel flow in 2012. All numbers on the flows refer to the mean of the mass flow in units of million metric tons\n(Mt). The uncertainty percentages refer to the flow standard deviation as a percentage of the mean of the mass\nflow. All mass flows refer to steel except for the iron ore flows that include the non-iron mass (e.g., oxygen and\ngangue).\n30Figure 14: Bayesian (a) prior- and (b) posterior-predictive mass flows of network structure 1000 for the U.S.\nsteel flow in 2012. All numbers on the flows refer to the mean of the mass flow in units of million metric tons\n(Mt). The uncertainty percentages refer to the flow standard deviation as a percentage of the mean of the mass\nflow. All mass flows refer to steel except for the iron ore flows that include the non-iron mass (e.g., oxygen and\ngangue).\n31Figure 15: Bayesian (a) prior- and (b) posterior-predictive mass flows of network structure 1001 for the U.S.\nsteel flow in 2012. All numbers on the flows refer to the mean of the mass flow in units of million metric tons\n(Mt). The uncertainty percentages refer to the flow standard deviation as a percentage of the mean of the mass\nflow. All mass flows refer to steel except for the iron ore flows that include the non-iron mass (e.g., oxygen and\ngangue).\n32Figure 16: Bayesian (a) prior- and (b) posterior-predictive mass flows of network structure 1010 for the U.S.\nsteel flow in 2012. All numbers on the flows refer to the mean of the mass flow in units of million metric tons\n(Mt). The uncertainty percentages refer to the flow standard deviation as a percentage of the mean of the mass\nflow. All mass flows refer to steel except for the iron ore flows that include the non-iron mass (e.g., oxygen and\ngangue).\n33Figure 17: Bayesian (a) prior- and (b) posterior-predictive mass flows of network structure 1011 for the U.S.\nsteel flow in 2012. All numbers on the flows refer to the mean of the mass flow in units of million metric tons\n(Mt). The uncertainty percentages refer to the flow standard deviation as a percentage of the mean of the mass\nflow. All mass flows refer to steel except for the iron ore flows that include the non-iron mass (e.g., oxygen and\ngangue).\n34Figure 18: Bayesian (a) prior- and (b) posterior-predictive mass flows of network structure 1100 for the U.S.\nsteel flow in 2012. All numbers on the flows refer to the mean of the mass flow in units of million metric tons\n(Mt). The uncertainty percentages refer to the flow standard deviation as a percentage of the mean of the mass\nflow. All mass flows refer to steel except for the iron ore flows that include the non-iron mass (e.g., oxygen and\ngangue).\n35Figure 19: Bayesian (a) prior- and (b) posterior-predictive mass flows of network structure 1101 for the U.S.\nsteel flow in 2012. All numbers on the flows refer to the mean of the mass flow in units of million metric tons\n(Mt). The uncertainty percentages refer to the flow standard deviation as a percentage of the mean of the mass\nflow. All mass flows refer to steel except for the iron ore flows that include the non-iron mass (e.g., oxygen and\ngangue).\n36Figure 20: Bayesian (a) prior- and (b) posterior-predictive mass flows of network structure 1110 for the U.S.\nsteel flow in 2012. All numbers on the flows refer to the mean of the mass flow in units of million metric tons\n(Mt). The uncertainty percentages refer to the flow standard deviation as a percentage of the mean of the mass\nflow. All mass flows refer to steel except for the iron ore flows that include the non-iron mass (e.g., oxygen and\ngangue).\n37Figure 21: Bayesian (a) prior- and (b) posterior-predictive mass flows of network structure 1111 for the U.S.\nsteel flow in 2012. All numbers on the flows refer to the mean of the mass flow in units of million metric tons\n(Mt). The uncertainty percentages refer to the flow standard deviation as a percentage of the mean of the mass\nflow. All mass flows refer to steel except for the iron ore flows that include the non-iron mass (e.g., oxygen and\ngangue).\n386.7 Nodal emission intensities\nTable 5 presents the estimated nodal emission intensities (kg.CO 2eq./kg.mat in) for the U.S. steel\nsector. These emission intensities were provided by Dr. Mohammad Heidari [63]. We are grateful to\nhim for this contribution. This analysis focuses on domestic emissions; i.e., emissions released within\nthe U.S. Therefore, any import product (e.g., Import DRI) is attributed a nodal emission intensity\nfactor of 0 kg.CO 2eq./kg.mat in. Some of the nodes in Table 5 are compiler nodes. These nodes are\nfor visualization and calculation purposes. They do not represent actual processes and therefore have\nan emission intensity of 0.\nTable 5: Node emission intensity as per unit of material into the process.\nNode nameEmission intensity\n[kg.CO 2/kg.mat in]Note\nIron ore production 0.11Domestic production of iron\nore\nIron ore consumption 0Compiler node aggregating\nimported iron ore and domes-\ntic iron ore not exported\nImport iron ore 0Focus of the analysis is domes-\ntic emission: imports assigned\n0 emission intensities\nDRI production 0.67\nDRI 0Compiler node describing\nDRI produced domestically\nImport DRI 0Focus of the analysis is domes-\ntic emission: imports assigned\n0 emission intensities\nDRI consumption 0Compiler node aggregating\nimported DRI and domesti-\ncally produced DRI not ex-\nported\nBlast furnace 1.50\nImport pig iron 0Focus of the analysis is domes-\ntic emission: imports assigned\n0 emission intensities\nPig iron 0Compiler node describing pig\niron produced domestically\nfrom blast furnace\nPig iron consumption 0Compiler node aggregating\nimported pig iron and domes-\ntically pig iron not exported\nPurchased scrap 0.04Post-consumer scrap collected\ndomestically\nScrap collected 0Compiler node aggregating all\npost-consumer scrap collected\ndomestically\n39Import scrap 0Focus of the analysis is domes-\ntic emission: imports assigned\n0 emission intensities\nScrap consumption 0Node aggregating post-\nindustrial process scraps and\ndomestically collected post\nconsumer scrap not exported\nBasic oxygen furnace 0.13\nElectric Arc furnace 0.29\nEAF yield 0Compiler node aggregating all\nproducts from electric arc fur-\nnace\nCupola furnace 0.13 Zhu et al. [64]\nOther casting 0.13Emission intensity modeled\nthe same as continuous cast-\ning\nOCyield 0Compiler node aggregating all\nproducts from other casting\nprocess\nOCloss 0Compiler node aggregating\nrun-around prep and loss for\nother casting process\nContinuous casting - slabs 0.13Continuous casting process\nproducing slabs\nCCyield 0Compiler node aggregating all\nproducts from continuous cast\nslabs\nCCloss 0Compiler node aggregating\nrun-around prep and loss for\ncontinuous cast slab\nContinuous casting - billets 0.13Continuous casting process\nproducing billets\nBTyield 0Compiler node aggregating all\nproducts from continuous cast\nbillets\nBTloss 0Compiler node aggregating\nrun-around prep and loss for\ncontinuous cast billets\nContinuous casting - blooms 0.13Continuous casting process\nproducing blooms\nBMyield 0Compiler node aggregating all\nproducts from continuous cast\nblooms\nBMloss 0Compiler node aggregating\nrun-around prep and loss for\ncontinuous cast blooms\n40Ingot casting 0.13Emission intensity modeled\nthe same as continuous cast-\ning\nICyield 0Compiler node aggregating all\nproducts from ingot casting\nprocess\nICloss 0Compiler node aggregating\nrun-around prep and loss for\ningot casting process\nIngot import 0Focus of the analysis is domes-\ntic emission: imports assigned\n0 emission intensities\nPrimary mill 0.13Emission intensity modeled\nthe same as continuous cast-\ning\nPMYield 0Compiler node aggregating all\nproducts from primary mill\nHot strip mill 0.18An emission intensity of 0.25\nkg.co 2eq./kg.mat in [63] mod-\neled for the combined hot and\ncold rolling process; Milford et\nal.[62] reports a 72–28 emis-\nsion intensity split between\nhot and cold rolling process\nHSM Yield 0Compiler node aggregating all\nproducts from hot strip mill\nPlate mill 0.18Emission intensity modeled\nthe same as hot rolling process\nRod and bar mill 0.18Emission intensity modeled\nthe same as hot rolling process\nRBM Yield 0Compiler node aggregating all\nproducts from rod and bar\nmill\nSection mill 0.18Emission intensity modeled\nthe same as hot rolling process\ninto i-beam, profiled rolling\nprocess\nSMYield 0Compiler node aggregating all\nproducts from section mill\nCold rolling mill 0.07An emission intensity of 0.25\nkg.co 2eq./kg.mat in [63] mod-\neled for the combined hot and\ncold rolling process; Milford et\nal.[62] reports a 72–28 emis-\nsion intensity split between\nhot and cold rolling process\n41CRM Yield 0Compiler node aggregating all\nproducts from cold rolling mill\nGalvanizing plant 0.19Galvanizing plant taking\nsheet rolls and coating with\nzinc\nTin mill 0.08Emission intensity for tin\nmills adapted from galvaniz-\ning process, with the minimal\nenergy to melt tin 40% lower\nthan that of zinc\nPipe and tubing 0.18Emission intensity modeled\nthe same as hot rolling process\nBars 0Cutting process with negligi-\nble emission intensities\nCold rolled sheet 0.32Manufacturing process of\nstamping and assembly of\nsteel sheets\nGalvanized sheet 0.32Manufacturing process of\nstamping and assembly of\nsteel sheets\nHot rolled sheet 0.32Manufacturing process of\nstamping and assembly of\nsteel sheets\nIron product casting 0Machining process with neg-\nligible emission intensities at\ncritical interfaces of interme-\ndiate products\nLight section 0Cutting process with negligi-\nble emission intensities\nPipe welding plant 0.02GREET model [65] reports\n32.6 kg.CO 2eq.per passenger\nvehicle with an average vehi-\ncle weight of 1443 kg\nPlates 0Cutting process with negligi-\nble emission intensities\nSeamless tube plant 0.18Emission intensity modeled\nthe same as hot rolling process\nReinforcing bars 0Cutting process with negligi-\nble emission intensities\nRails and rail accessories 0Cutting process with negligi-\nble emission intensities\nHeavy section 0Cutting process with negligi-\nble emission intensities\nTin mill products 0.32Manufacturing process of\nstamping and assembly of\nsteel sheets\n42Wire and wire rods 0.01Forming process to manufac-\nture wire, fasteners and tools\nSteel product casting 0Machining process with neg-\nligible emission intensities at\ncritical interfaces of interme-\ndiate products\nIntermediate product import 0Focus of the analysis is domes-\ntic emission: imports assigned\n0 emission intensities",
      "metadata": {
        "filename": "Bayesian Model Selection for Network Discrimination and Risk-informed Decision M.pdf",
        "hotspot_name": "Steel_Punching_and_Bending",
        "title": "Bayesian Model Selection for Network Discrimination and Risk-informed\n  Decision Making in Material Flow Analysis",
        "published_date": "2025-01-09T20:03:18Z",
        "pdf_link": "http://arxiv.org/pdf/2501.05556v1",
        "query": "punching and bending steel EMC shield material substitution studies resource reduction percentages"
      }
    },
    "Drawing of the wire of low-carbon steel_ plasticity resource_ optimal reduction": {
      "full_text": "1 \n DRAWING OF THE WIRE OF LOW -CARBON STEEL : PLASTICITY \nRESOURCE,  OPTIMAL REDUCTION,  STRUCTURE, PROPERTIES.  \n \nA. Zavdoveev1,2, Ya.  Beygelzimer1, Е. Pashinskaya1, V. Grishaev1, А. Мaksakova1. \n \n1 Donetsk institute for physics and engineering NAS of Ukraine.  \n2 Paton Electr ic Welding  Institute NAS of Ukraine.   \n \nAbstract  \n \nThe work considers the effect of deformation on the exhaustion of the plast icity \nresource of steel 45 after the drawing deformation. The results of theoretical and \nexperimental studies of damage accumulation are listed. A possibility of employment \nof a scanning electron microscope to observe submicropores is demonstrated.   \n \nIntro duction  \n \nThe problem  on nucleation , growth  and propagation  of cracks  seems  to have  no \ndirect  relation  to the drawing  of well-deformed  materials , namely , steel and iron. The \nsupposed p robable formation of cracks in the course of processing seems to be  \nrejec ted due to the fact that the processing enhances the properties of a steel wire (an \nincrease in the number of inflections, kinkings, necking -down). But a number of the \nwire properties definitely indicate to the possible emergence of micro - or \nsubmicrocrack s affecting the plastic properties and some operation characteristics.   \nCold-worked state of the carbon steel and iron  is associated with a peculiarity \nthat is not usually considered in respect of the mechanical properties of steel.  The \npoint in questio n is a density decrease in the course of cold deformation of steel. It is \nknown that dislocations and vacancies increase the volume of the co ld-worked \nmaterial  [1, 2], but the simplest calculation indicates that even the ultimate density of \ndislocations a nd vacations cannot increase the volume by more than   0.1%, whereas \nthe registered values are of 0.5 -0.6 and even 1.0% of the material [3].  \nIn the course of nucleation and evolution of micro -discontinuities under plastic \ndeformation, simultaneous healing of the defects occurs with the intensity controlled \nby the hydrostatic pressure and the temperature increase. Energetically unstable \nmicro -discontinuities generated under metal working  can be healed by recovery \nannealing, contrary to the energetically stab le ones. Proper planning of deformation \nprovides annealing before the micro discontinuities become energetically stable. The \ntheory of deformation of metals allows accounting for this fact.  \nNow the most wide -spread approach is based on the assumption abou t the \nexistence of some state of the material that can be quantitative described by the \ndamage. The fracture occurs when the damage reaches the maximum.  \nThe present paper contains the calculation and the experiments for the \nevaluation of the damage accumul ation in the course of cold drawing of steel 45.  \n 2 \n  \nEXHAUSTION OF THE PLASTICITY RESOURCE OF METALS UNDER \nDRAWING  \n \nAll kinds of the defects are generated and accumulated in the material in the \ncourse of plastic deformation. To describe this phenomenon, the po stulate was \nsuggested and developed in [11 -23] that there exists a macro -object called damage \nthat is the quantitative measure of the microfracture of the metal under deformation.   \nThe last phenomenon precedes the macrofracture immediately.  \nAccording to t he principle of macroscopic definability by Ilyushin [24], the \nmagnitude of the damage is supposed to be univocally defined by the loading \nprocess, being presented in the form of a functional of the process.  \nThe criterion of the macroscopic fracture is wri tten as a condition when a \ncertain degree of the damage achieves the critical value.   \nThe works reporting this approach to the modeling of the fracture differ in the \nchoice of the object treated as the \"damage\" (a scalar as in [11 -15, 19 -23], or a tensor \nas in [20 -22]), in the form of the functional expression and the measure of the \ndamage entering the criterion of the fracture.  \nNow the metal working researchers mostly use the criteria based on the scalar \nmeasure of the damage [11 -19, 20 -24]. The point is that the use of tensor variables \nmust be based on more perfect theories of plasticity that have not been developed for \nnow (there are no sufficient theoretical and experimental reasons for this kind of \ntheories). The main idea of the approach realized in t he listed works is as follows.  \nThe measure of the damage is a scalar variable  called cleavage in the basic \nwork by V.L. Kholmogorov  [11] and softening in further publications on the topic. It \nis commonly supposed that an increase in the softening is pro portional to the \nincrement of the shear strain  \ndΛ : \n \ndΛα=dε\n ,    (1) \n \nwhere  is the factor determining the intensity of accumulation and evolution of \nmicrocracks.  \nIt is assumed that the formation  of a microscopic crack occurs at the moment \nwhen the softening reaches the critical value кр. The degree of the shear strain \napplied to the representative volume of the metal before the critical softening was \nachieved is called plasticity and labeled by р. Supposing   = const when the metal \nis deformed, it follows from (1) that кр=р. Dividing (1) by the last relation yields  \ndψ=dΛ\nΛp\n ,    (2) \nwhere \ndψ=dε\nεkp , and \nψ  is called the degree of exhaustion of the plasticity \nresourse . \nIt results from (2) that  3 \n \ny=∫\n0tHdt\nLp,     (3) \nwhere \nHdt=dL , \nHis the intensity of the velocity of the shear strain . \nThe condition of the deformation without fracture is  \n \ny=∫\n0tHdt\nLp<1\n,    (4) \nand the fracture condition is  \nψ=∫\n0tHdt\nΛp=1\n.     (5)  \nThe results of the plasticity tests under the proportional load with sufficient \naccuracy can be presented by parametric dependences describing the relation of the \nlimit shear strain р to the parameters of the stressed state \ns\nT  ( is the hydrostatic \nstress,  is the intensity of the tangent stresses);  is the Nadai -Lode parameter; as \nwell as the intensity of the velocity of the shear strain H and the temperature : \n \nΛp=Λp(σ\nT;μσ;H;θ)\n    (6) \n \nIn [11 -24], the dependences (6) are listed for a number of metals and alloys.  \nRelation (3) allow s evaluation of the degree of the exhaustion of the plasticity \nresource when the temperature, the stressed state and the deformed one are known (in \nother words, , \ns\nT,  and H along the trajectory of the particle motion in t he \ndeformed metal are in disposal), in the case of the known parameter dependence of \nthe metal plasticity (6). The analysis of the results of calculations permits making a \nconclusion about possible fracture of the metal during the process and fixation of t he \nareas with the highest fracture probability in the billet.  \nWe think that the most important application of the theory of deformability in \nmetal working is not only the forecast of the fracture moment according to criterion \n(5), but also the estimation of the damage of the metal by the evaluation of  \nψ . This \napproach allows predicting of the parameters of the quality of the billets and articles \nrelated to the damage and proper selection of the modes of recovery anneals. F or this \npurpose, the theory of deformability develops conceptions of the anneal effect on the \ndegree of the exhaustion of the plasticity resource [13,20 -24].  \nIt was mentioned above that the microdiscontinuities can be stable or unstable \nfrom the energetic viewpoint. The first -type discontinuities can be healed in the \ncourse of the recovery anneals, contrary to the second -type ones. This fact means that \nthere is a critical parameter \nψ  that controls the efficiency of anneals. At \nψ<ψ  , the \nmicrodiscontinuities can be totally healed and the undamaged metal structure can be \nrecovered  by the anneal; at \nψ>ψ , only partial elimination of the damage is pos sible. \nThe magnitu de of \nψ  depends on the deformed metal, the concrete data are reported \nin [20 -24]. The characteristic range is \nψ=0.2÷0.4 .  4 \n According to [20 -24], there is the second critical value \nψ  associated with \nsharp decrease of the recovery o f the plasticity resourse . When the unhealed \nmicropores appear above \nψ , unhealed microcracks arise above \nψ . The \ncharacteristic range is \nψ=0.5÷0.7 .  \nCriterion (4) agrees to the experiment under a simple load only (or a load close \nto the simple one). The processes of a complex load require more perfect criteria \nbecause the axis of the principal strain veloci ties are rotated through substantial \nangles with respect to the metal particles  (see [16 -19,20,23]). They account for the \nexperimental fact of enhanced metal plasticity after sign -alternating deformation.  \nKinetic equation (1) does not consider the process es of microdiscontinuity \nhealing. The only indirect reference is expression of the ultimate plasticity (6). This \ndemerit results in restriction of the area of adequacy of the fracture model and a \nnumber of necessary calibration experiments. This fact was s everal times pointed out \nby V.L.Kholmogorov. In a fundamental work [11], he made the first steps to explicit \naccount of the healing of microdiscontinuities, but the work had not been finished.  \nRecently the Ural school of metal working proceeded with this r esearch area. \nIn [23], a generalization of the kinetic equations of the phenomenological theory of \nfracture has been made in the case when the plastic deformation is combined with \nthermal treatment of a metal. In [23], an attempt was made to formulate the problem \nof deformation an the fracture of the metal in the course of metal working with \naccount of the effect of fracture on the stress -strain state of the metal. In this work, \nthe approach to the study of the fracture under metal working described above i s \naligned with another approach developed recently. The necessity of the search for \nnew approaches is determined by two reasons, at least. The first reason is that the \nmagnitude of the damage can not be measured directly. It is evaluated only in the \ncase o f the macroscopic fracture of the material.  The second reason is that the \ncriteria  [15 -23] do not described the fracture associated with the strain localization.   \nThe solution of the mentioned problems is possible within the frameworks of \nthe fracture m odeling reported in  [4, 12, 24] only . The point is that the magnitude of \nthe relative porosity is assumed to be a measure of microfracture, and the related \nkinetic equation can be written on the basis of one or another consideration. As \nshown above, this  approach is well substantiated because the fracture is associated \nwith the formation of microdiscontinuities determining the value of the porosity [24].  \nContrary to the damage, the porosity can be directly measured, and the first \nabovementioned problem ca n be solved.  \nThe works performed within the frameworks of this approach differ in the \nmethods of derivation of the kinetic equations for the porosity.  \nWe believe that the studies where the kinetic equations are derived from the \nbasic relations for a defo rmed material [12, 20 -24], are of the highest interest. The \nfact is that the  localization of the plastic strain and the related fracture can be studied \nwithin the frames of the research.  So, the second problem is solved.   5 \n The described approach is very promising, being sufficiently agreed with the \nphysical conceptions of the fracture under metal working.   \nWe shall apply the simplest variant of the theory of deformability to \ncalculation of the plasticity resource exhaustion when a wire of steel 45 is draw n.   \nIn Fig .1 the plasticity diagram of the steel  [25] is presented.  \n \n \n-0.4 -0.2 0.0 1.0 1.5 p \n/T \n \nFig.1  – Plasticity diagram of steel 45 (see [25])  \n \n \nIn the same paper, a dependence approximating the experimental curve  was \npresented:  \nΛp=exp(−1.11σ\nT)\n     (7) \nIn the course of drawing, the exponent \nσT  is measured along the deformation \nzone. According to [38], in the first approximation one can accept that  \n \n1.15 ln 1 1.730ffctgαt m+( =σT\n     (8) \nwhere m is the friction coefficient,  is the half -angle of the die cone, f0 is the \nsectional area of the wire before drawing, f is the current sectional area of the \ndeformation zone.  \nRelation (8) shows that \nσT  under drawing  (without back -tension) ranges  from \n(σT)0=−1.15\n at the inlet of the deformation zone (\nf=f0  ) to \n 1.15 ln 1 1.73\n10\n1 ffctgαt m+( =σT\n at the outlet (\nf=f1 , where \nf1  is the sectional area of \nthe wire after drawing).  \nIn the first approximation [38], the increment of shear deformation after \ndrawing can be determined by  \n \ndΛ=−df\nf\n    (9) 6 \n If we substitute  (7) -(9) in (3), we g et the following expression  of the \nexhaustion of the plasticity resource after a pass through the die:  \n \nψ=0.252λ1.92(1+m⋅ctgα)−1\n1+m⋅ctgα\n,     (10) \n \nwhere \nλ=f0\nf1  is the elongation ratio of the die.  \nWhen passing to single drafti ngs \nφ  and accounting of \nλ=1\n1−φ , we get a \nformula describing the exhaustion of the plasticity resource after the drawing of a \nsteel wire  \nctgα m+φ=ψctgαt m+(\n\n\n\n\n\n1111\n0.2521 1.92\n     (11) \n \n where \nλ=f0\nf1  is the elongation ratio of the die.  \nFormula (11) can be applied to the calculation of the exhaustion of the plasticity \nresource after the sequential drawing.  \nThe employment of the formula allows estimation of the value of single \ndraftings when the recovery of the damaged metal structure due to anneal is still \npossible. In other words, there appears a possibility of rational control of the anneal.  \nThe simplest model we have developed allows testing of the effect of \ndisintegration of deformation on the exhaustion of the plasticity resourse . Suppose it \nis necessary to obtain some total drafting \nφΣ  per two passes. We study the relation \nof the total exhaustion of the plasticity resource\nψΣ  and the pass distribution of the \ndrafting. The value of \nψΣ  is determined by the formula valid for the monotonic \ndeformation:  \n \nψΣ=ψ1+ψ2\n    (12) \n \nwhere \nψ1  and \nψ2  are the exhaustion of the plasticity resource for the first and \nthe second passes, respectively.  \nIt can be easily shown   \nφ2=φΣ−φ1\n1−φ1\n      (13) \nwhere \nφ1  and \nφ2  is the drafting for the first and the second passes, \nrespectively.  \nIn Fig .2 , the relation of  \nψΣ  and  \nφ1  is illustrated.  \n 7 \n \n0 0.057 0.11 0.17 0.23 0.29 0.34 0.40.350.40.450.50.55\n1()\n1 \nFig.2  – Relation of the  total exhaustion of the plasticity resource  and the drafting \nduring the first pass (\nφΣ =0.4, =80, m=0.05), calculated by (11).  \n \nFig.2  shows that redistribution of the strain over the passes can substantially \nreduce the exha ustion of the plasticity resour ce.  At \nφΣ =0.4, the most preferable \nvariant of the drafting redistribution is \nφ1 =0.23 and \nφ2 =0.22. In this case, almost \nthe whole damage accumulated by the metal can be healed by the recovery anneal  \n(\nψΣ= 0.35=ψ ), contrary to the single drafting of 40%  (\nψΣ= 0.52>ψ =0.35).  \nIn Fig .3, single drafting dependence of the exhaustion of the pla sticity resource  \nof steel 45 is illustrated, being calculated by (11).  \n \n0.1 0.18 0.26 0.34 0.420.05870.430.81.181.551.92\n11()\n12()\n21()\n22()\n\n \nFig.3.  Single drafting dependence of the exhaustion of the plasticity resource of steel \n45:  = 50 , m=0.05 ( 11);  = 50 , m=0.10 ( 21);  = 80 , m=0.05 ( 12);  = 80 , \nm=0.10 (22);  \n \nFig.3  shows that variant 12 is the most preferable with respect to plasticity   (  \n= 80 , m=0.05), and variant 21 is the least preferable (  = 50 , m=0.10). By using (11), \nwe can evaluate single draftings when the recovery of the damaged str ucture by 8 \n anneal stays possible. The critical value of steel 45 is  \nψ =0.35. It follows from (11) \nthat this value is associated with the maximum drafting  33% (in the case of the most \npreferable variant 12).  \nAn enhancement of the metal plasticity due to disintegration of the metal is a \nknown fact. The example mentioned above can not be considered to be a \ndemonstration of the abilities of the theory of deformability. This is rather a short test \nillustrating validity of the th eory conclusions in the case. It becomes really useful \nwhen dealing with multioperational processes of metal working including \nintermediate anneals  (for instance, multi -pass drawing). A number of possible \nvariants of the technology arise here and the most  rational method should be selected \nwith respect to a number of the parameters. In these cases, adequate rheological \nmodels of the deformed materials should be employed as well as powerful methods \nof calculation of the stress -strain state and modern varian ts of the theory of \ndeformability. At the same time, the technique should be developed with the use of \nthe effective approaches o f the optimum design: dynamic programming, methods of \nmulticriteria optimization, sensitivity analysis etc. (see [12, 38, 39]).   \nThis method allows forecasting  the exhaustion of the plasticity resource in the \ncourse of drawing and putting the achievements of the theory of deformabilty into \npractice when designing the drawing routes that can provide high quality of the wire \nwith re spect to the plasticity parameters.  \nThe experimental verification of the obtained formules have been finished now. \nIn the course of the experiment, the value of the accumulated strain reached \nψ = \n0,75. The formation of micr opores and microcracks was registered in the metal \nstructure  during the drawing of the steel wire.  \n \nEXPERIMENTAL PROCEDURE  \nA number of works demonstrate the formation of micropores in the course of \ndrawing [1,2]. The method of small -angle X -ray scattering  allowed indirect \nregistration of nanopores of 20..30 nm in size after ECAP processing [3], i. е. under \nuniform compression that seems supposed to be preventing the generation of \nmicrodiscontinuities. However the evolution of the metal structure in the cour se of \nSPD is that the strong reduction  of the grains is accompanied by a substantial \nmisorientatiion of the grain boundaries (over 150), the boundaries are transformed \ninto high -angle ones. As a result, the boundaries become amorphous,  weakened and  \nincoherent; their volume fraction increases and becomes comparable with the volume \nfracture of the very grains. This fact facilitates the rotation of the grains  and modifies \nthe mechanism of the plastic deformation, terminating the strengthening of the \nmateri al. Probably, the incoherence of high -angle boundaries is provided by the \naccumulation of nanopores within them. Even the presence of microdiscontinuities of \nsmall size affects the mechanical properties of the materials. Thus, the direct \nobservation of nan oporosity becomes very actual because of possible verification of \nthe conceptions about the reconstruction of the metal structure under SPD.  \nTraditional testing of the structure by an electron microscsope implies \nprocessing of the surface by smoothing and polishing. As a result, the pores become 9 \n invisible, being closed with the processing products. There is a known method of the \nstudy of the fracture surface formed by a brittle crack that is aimed at detection of the \npores of micron size [4]. Before the fra cture, the sample is cooled in liquid nitrogen \nup to the state of coldbrittleness. After that, a starting microcrack is induced and \nwedged up to the fracture .  \nWe have suggested another simpler method of the sample fracture. A cooled \nsample is fractured by  the three -point bend (Fig .4). A concentrator in the form of a \nsharp cu t is preliminary applied to the surface. A turning workstation or a planning \nmachine is used to obtain a sample of the round or rectangular section, respectively. \nThe labels are made in  order to place the sample on the end supports. The labels are \nlocated symmetrically about the concentrator and the distance to the concentrator is  \nno more than 2,5 heights of the sample section. The load is applied to the centre in \nthe plane of the conce ntrator. At these conditions, the crack propagates strictly in the \nplane perpendicular to the axis of the sample [5,6].  \n \n \nFig. 4  — Three -point scheme of the fracture  \n \nDepending on the aim of the experiments, the samples were made of differen t \nmaterials of varied cross -section form. First of all, it was necessary to be sure that  an \nelectron microscope allows registration of pores of hundreds and tens of nanometers \non the surface of the brittle fracture. For this purpose, construction steel 45  was used \nafter thermal treatment up to the state of the lowest plasticity (quenching only). The \nsample (Fig . 5) was of 14 mm in diameter, with a ring concentrator of 2 mm in depth. \nThe length of the load levers was  30 mm. The fracture was performed by an  impact   \nby the three -point scheme.  \n 10 \n \n \nFig. 5  – The sample before and after the fracture  \n \nCold deformation of steel 45 was performed by multi -pass drawing in order to \nachieve the strain about unity. The drawing was made at a drawing bench along the \nroute (8 -7.6-6-5.6-5-4). The analysis of the hardness density anbd the structure was \nmade with using the electronic scales Shimadzu 200, the durometer В5, the \nmicroscope Axiovert 40 МАТ . The fracture surface was analysed by the scanning \nelectron microscope JS M-6490LV (JEOL, Japan) in the SEI mode.  \n \nRESULTS AND DISCUSSION.  \n \nThe fracture surface was analysed by the scanning electron microscope JSM -\n6490LV (JEOL, Japan) in the SEI mode. The images of the surface structure of steel \n45 at different magnifications ( Fig.6).  \n \n \nFig. 6  – Microphotos of the fracture of steel 45  \n 11 \n The mode of the microscope work allows to see the fracture surface due to  the \nsecondary electron emission of the surface details. The inclined convex and concave \nareas of the surface are regi stered as bright and dark areas, respectively. We see a \nnumber of small bright convexities with the corresponding concavities of the same \nsize that can be treated as dimples. These objects are complementary, i. е. the \nconvexities of the first fracture surfa ce correspond to the concavities of the secodnd \npart of the fractured sample.   \nAt the same time, dark spots of  0,1 m and lower are found, that are looking \nlike extended recesses, not dimples. We can not register the corresponding extended \nconvexities be cause they would be unavoidably broken in the course of the fracture. \nDark spots of this type are usually interpreted as pores, including not round ones  \n[10].  \nFinally, small nanoobjects can appear to be some non -metallic inclusions. To be \ncertain, it is sufficient to test the chemical composition of a spot. The data of the \nSEM analysis demonstrate that the basic element of the tested areas of the surface is \niron  (over 90%). This fact permits to interprete dark areas of hundreds and tens \nnanometers in siz e as pores.  \nThus, the pores of submicro - and nanosize can be registered on the surface of a \nbrittle fracture. Naturally, the porosity can be described as a surface porosity, i. е. the \nratio of the dark spots treated as pores to the tested surface area. Ext rapolation to the \nvolume porosity provides no additional information.  \nThe analysis of microstructures was performed with using the metallographic \nmicroscope Axiovert 40 MAT. The microstructure of the sample before drawing is \nferrite -perlite structure with the perlite content about 50 %. In the initial sample, the \nferrire grain size was about 20 m. After the deformation, the redistribution of the \nperlite component was registered as well as the emergence of granular perlite. The \nperlite component has also be en redistributed and reduced. Besides, the transient \nareas between the lamellar and granular perlite have appeared.  \nThe analysis of the microstructure tests has shown that the drawing results in  \nwork hardening, i. е the reduction of the structure component s. In particular,  the \ninitial average ferrite grain size was about 20 m, contrary to 10 m after the \ndeformation (Fig . 7).  \n \n \n  \nа) b) \nFig. 7  — Microstructure of steel 45: а) the initial annealed steel, 8000С 1 h., b) \nafter the drawing, е=0.98.  12 \n  \nBesid es, an increase in strain results in increase in the strength of the samples \ndue to work hardening.  \nThe dependences of these parameters ane presented in Figs. 8 and 9.  \nThe figures illustrate strong correlation between the density and the hardness of \nthe pr ocessed material that is strain -dependent. In particular, an increase in strain is \naccompanied by a hardness increase and a density reduction. An analogous behavior \nis observed in the relation of the hardness, the density  and the exhaustion of the \nplastic ity resourse .  \n \n \nFig. 8 – Dependence of density vs. plasticity resource.  \n \n-0,1 0,0 0,1 0,2 0,3 0,4 0,5 0,6 0,7 0,87,787,797,807,817,827,837,84\n  Density, g/cm3\nPlastisity resource\n0,0 0,2 0,4 0,6 0,8 1,0140160180200220240260\n HV\n плотность\nдеформHV\n7,787,797,807,817,827,837,84плотность13 \n Fig. 9 – Dependence of hardness and density vs. deformation.  \n \nThe density decrement was about 1% during the whole deformation course. The \ndecompaction of the material can be explai ned by the formation of pores and \nmicrocracks. In this context, to confirm the existence of micropores and cracks, \nbrittle f racture of steel was performed b efore and after the deformation. It is \nexplicitly seen in Figs.  10 and 11 , that the fracture is brit tle. The original sample has  \nbeen broken by the mechanism of brittle fracture, so the micropores and the cracks \nare almost absent. The deformed sample  demonstrates multiple formation of pores \nand cracks.  \nThus, it has been demonstrated in the work that m icrodefects are generated \nwithin the metal under drawing. The results are the density reduction and the \nenhancement of the plastitity resourse . \nIt has been shown theoretically and experimentally that the exhaustion of the \nplasticity resource in the course of  drawing is related to the accumulation of pores. \nThe coalescence of the pores results in formation of the cracks and fracture of the \nmaterial.  \n \n \n \n \n \n \n \n \nFig. 10 – Fractograms  of fracture  surface : Initial – right , cold drawing  е=0.98 – left. \n \n14 \n  \nFig. 11 - Fractograms  of fracture  surface : Initial – right , cold drawing  е=0.98 – left. \n \n \nCONCLUSIONS  \nIn the present work, we have made an attempt to attract the attention of the \nresearchers to the modern abilities of the metal working theory with re spect to the \nforecast of the pl astic characteristics of the metal. As an illustration, the simplest \nvariant of the theory of deformability was considered. When analyzing  the process of \ndrawing theoretically and experimentally, the magnitudes of the ultimate single \ndraftings and the exhaustion of the plasticity resource after the single and multi ple \ndrawing  were evaluated. It has been shown that first the processes of the damage \nhealing prevail after the single drafting, i. е. they are energetically unstable. When the \nstrain becomes high, accumulation of the damage starts. The existing armoury  allows \nemployment of the theory of deformability to design the drawing routes that can \nprovide high quality of the wire with respect to plasticity.  \nThe theoretical analysis of structure imperfection calculated by the method  of \nestimation of the exhaustion  of the plasticity resource shows that the increase in strain \n(i.е. the reduction of the diameter of the original billet)  is associated with an \naccumulation of the structure imperfection that is expressed as an increase in the \nplasticity resourse . Additiona l studies performed by hydrosatic weighing have also \nshown that the structure imperfection is accumulated as the strain increases.  \nThe pores of submicro - and nanosize can be observed on the brittle fracture \nsurface by a scanning electron microscope.  \n \n \n \n15 \n REFERENCE . \n \n1. Lomer W.M / - Phil.Mag., Ser. 8, 1957, 2, 20, 1053  \n2. Seeger A., Haasen P. - Phil.Mag., Ser. 8, 1958 3, 29, 470  \n3. Gridnev VN, Gavrilyuk VG, Meshkov YY - MiTOM, 1971, 1, 21  \n4. Meshkov YY On the change in volume during plastic deformation of metals. \nThe physical nature of plastic deformation. Ser. \"Metal Physics\", vyp.13, str.89 -98. \n5. Gridnev VN, Gavrilyuk VG, Meshkov YY Study the density of deformed \nmetals and alloys. Ser. \" Metal Physics\", vyp.13, str.89 -98. \n6. Betehin VI, Kadomtsev AG, Sklenicka V., Saxl I. ultrakristallicheskih \nnanoporous aluminum and alloys thereof. Solid State Physics. t.49, vol. 10, 2007 \ns.1787 -1790.  \n7. Smirnov SV, Levit VI, richly LL, VL Kolmogorov, Sha limova LV \nInvestigation of damage to wire drawing of low carbon steel. Metals. №2, 1987, p. \n144-149. \n8. Grishaev Vladimir Vysotsky EN The trajectory of a crack in bending the rod. \nProbl. strength. №6, 1989, s.52 -56. \n9. VV Grishaev Investigation of crack pa th in the rod during bending. Abstract. \ndis. ... Cand. Sci. Sciences. Kharkov, 1993, 21, p.  \n10. L. Engel, G. Klingele scanning electron microscope. Directory. M. - \nMetallurgy, 1986, 232 p.  \n11. VL Kolmogorov Stress, strain, razrusheniya. M .: Metallurgy, 19 70.-230c.  \n12. VA Ogorodnikov Evaluation of deformability of metals in processing \ndavleniem. - Kiev Vishcha School, 1983. -175c.  \n13. Bogatov AA, Mizhiritsky OI, Smirnov SV Resource of plasticity in the \nprocessing of metals by pressure. -M .: Me¬tallurgiya, 19 84.-150c.  \n14. Del GD Technological mehanika. M .: Engineering, 1978. -174c.  \n15. Krasnovskii SM, Makushok EM, Shchukin VJ Failure of metals in plastic \ndeformirovanii. -Minsk: Science and Technology, 1983. -173c.  \n16. Del GD Plasticity deformed metal // Fi¬zika and technique of high -\ndavleniy. - 1983. ¬Vyp.11. -C.28 -32. \n17. VM Mikhalevich Model ultimate strains during hot deformation // Math. \nUSSR Academy of Sciences. Metally. - 1991. -¬N5. -C. 89 -95. \n18. VM Mikhalevich Cyclic plasticity at hot deformation // Problems . \nprochnosti. -1994. -№6.-S.10-17. \n19. VL Kolmogorov, Migachev BA Predicting fracture during hot -metal plastic \ndeformation // Math. AN CCCP. Metally. - 1991. -¬N3. -C. 124 -128. \n20. BA Migachev Defining the parameters of the phenomenological model of \nlimit defor mations // Math. Russian Academy of Sciences. Metally. - 1997. -¬N2. -C. \n109-113. \n21. Bogatov AA Features rheological behavior and fracture of metal at \nmonotonic and alternating strain / Plastic deformation of steels and alloys. Coll. \nScien. works. Ed. prof. A.V.Zinoveva. M .: MISIS. -1996 -S.90-98. 16 \n 22. VL Kolmogorov Some theoretical problems of metal forming and possible \nsolutions / Plastic deformation of steels and alloys. Coll. Scien. works. Ed. prof. \nA.V.Zinoveva. M .: MISIS. -1996 -S.140 -151. \n23. Theory of pl astic deformation of metals / E.P.Unksov, U.Dzhonson, \nV.L.Kolmogorov etc .: Ed. E.P.Unksova, A.G.Ovchinnikova. M .: Mechanical \nEngineering, 1983. -598 with.  \n24. AA Ilyushin Plasticity. Fundamentals of general mathematical teorii. - M .: \nAN SSSR, 1963. -270c.  \n25. Guryanov GN, SV Smirnov Effect of surface hydrogenation and plating wire \nductility 70. / In. Metal Forming. Interuniversity collection of scientific papers. \nSverdlovsk: UPI, 1983. -S.28-31. ",
      "metadata": {
        "filename": "Drawing of the wire of low-carbon steel_ plasticity resource_ optimal reduction.pdf",
        "hotspot_name": "Steel_Punching_and_Bending",
        "title": "Drawing of the wire of low-carbon steel: plasticity resource, optimal\n  reduction, structure, properties",
        "published_date": "2014-11-29T21:42:44Z",
        "pdf_link": "http://arxiv.org/pdf/1412.0157v1",
        "query": "punching and bending steel EMC shield material substitution studies resource reduction percentages"
      }
    },
    "Evidence of strong electron correlation effects and magnetic topological excitat": {
      "full_text": "1 \n Evidence  of strong  electron  correlation  effects  and magnetic  topological  \nexcitation  in low carbon  steel   \n \n P. C. Mahato1, Suprotim  Saha1, D. Banik2, K. Mondal2, Shyam  Kumar  Choudhary 3, \nAshish  Garg4, S. S. Banerjee1,+ \n1Department  of Physics,  Indian  Institu te of Technology  Kanpur,  Kanpur,  Uttar  Pradesh  \n208016,  India  \n2Department  of Material  Science  and Engineering , Indian  Institute  of Technology  Kanpur,  \nKanpur,  Uttar  Pradesh  208016,  India  \n3Graphene  Center,  Tata Steel  Limited,  Jamshedpur,  Jharkhand  831007,  India. \n4 Department  of Sustainable  Energy  Engineering,  Kotak  School  of Sustainability,  Indian  \nInstitute  of Technology  Kanpur,  Kanpur,  Uttar  Pradesh  208016,  India  \n \n \n \nCorresponding author ’s email ID:  \n+: satyajit@iitk.ac.in  \n \n \nAbstract  \n \nPresent  study explores ho w thermal  treatments  and strain affect the magnetic response of two \nplain -carbon steels , with 0.05 wt% and 0.7 wt% carbon. Electron -backscattered diffraction and \nhigh-frequency magnetic susceptibility ( 𝜒) measurements in 0.05% C steel reveal that \nannealing increases 𝜒 by enlarging grains , while quenching reduces it by decreasing grain size. \nWe also study the 0.7% C steel, to delineate effects of quench -induced strain and possible \ncarbon -rich (Fe 3C) phase  admixture  in 0.05% C steel whic h could affect  its magnetic response. \nIn 0.7%C steel, uniaxial tensile strain enhances 𝜒 via altered magnetic anisotropy, avoiding the \nreduction seen in quenched 0.05%C steel. Micro -magnetic model ling and magnetic force \nmicroscopy identify magnetic topological structures (MTS) in 0.05% C steel, especially with \nquench ing. Low -temperature transport measurement suggest s strong electron correlations \ndrive Kondo effect  in 0.05%C steel and MTS is an emergent feature of interplay wit h local \nstrain . Thus, steel exhibits strong electron correlations and novel magnetic excitations , making \nit a promising quantum material platform for developing new d evice  applications.  \n \n \n 2 \n 1. Introduction  \nMagnetization dynamics in magnetic materials are widely studied. Weiss' molecular field \ntheory introduced magnetic domains determined by interplay of magneto -crystalline \nanisotropy and exchange energy [1-3]. Such idealized models are further complicated by the \ndefects like dislocations and voids in real magnetic systems [4-6]. Studies show how domain \nwall (DW) interact with defects sites which act as pinning  centres [7-10], hinderin g DW motion \nunder external magnetic field. Such irregular DW pinning and unpinning  are manifested as \ndiscontinuous steps in magnetization in Barkhausen noise studies  [11-13]. Therefore, a  \nmaterial's magnetic proper ties often reflect its imperfections —whether chemical, structural, or \nstrain -induced [14]. Recent research themes attempt at discovering new quantum materials \nexhibiting a correlation between  microstructur e, strong  electronic correlations and strain \neffects driving unique magnetic properties [5, 15]  and creat ing exotic magnetic excitations  like \nmagnetic vortices [16], skyrmions  [17] and other s [18-22]. Magnetic properties are also \nsensitive to thermal and stain induced magneto -mechanical effects [23-27]. It is well known  \nthat strain -induced changes impact dislocation density, DW pinning, and magnetizatio n \nhysteresis [28]. Although many quantum materials exist, the potential of steel to host exotic  \nand tunable magnetic ground states [29] remains under -explored by physicists, despite its status \nas a high -performance material [30] with well-established property and fine-tuning techniques.  \nSteel, an alloy of iron (Fe) and carbon (C), along with elements like Mn, P, Cr and Si, is vital \nto modern engineering due to its strength, durability, and adaptability. Its microstructure, \ngoverned by carbon content and processing tem perature, comprises phases such as ferrite (soft, \nductile BCC phase with low carbon), austenite (tough FCC phase with higher carbon,  usually  \nstable at high temperatures), and cementite , Fe3C (hard, brittle phase with 6.67 wt% carbon ) \n[31]. This is an active area of research even recently [32] – indicating the richness of steel’s \nphase diagram in terms of crystallography, microstructures and magnetic properties. Tailoring \nthese phases via heat treatment and alloying optimizes steel for varied applications.   \nWhile studies often focus on steel’s magnetic properties —like coercivity, saturation \nmagnetization, and Barkhausen noise —for non -destructive evaluation  [23, 33 -42]; our analysis \nof two carbon steels (0.05 wt% and 0.7 wt%) reveals that thermal treatment and strain \nsignificantly influence their magnetic behavio ur. In quenched low -carbon (0.05%) steel, \nsusceptibility drops due to the emergence of magnetic topological structures associated with 3 \n correlated electronic states and Kondo localization. T hese findings underscore steel’s potential \nfor quantum materials research.  \n2. Experimental Results and Discussions  \n2.1. Effects of thermal treatment on l ow carbon (0.05%) steel  \nWe investigate two steel types: one with very low carbon content (0.05%, labe lled as S0.05%) \nand another with higher carbon content (0.7%, label led S0.7%), close to the eutectoid   \ncomposition  i.e., 0.8% . Our study primarily focuses on the effects of thermal treatment on the \nmagnetic behavio ur of S0.05% cold-rolled steel . We use S0.7% steel for comparison , in order to \nexamine the impact of pure strain variation on magnetic properties . The t ables S1 and S2 \n(supplementary information, SI-1) show the chemical composition  of both samples . The 5 mm \n× 5 mm × 0.8 mm samples, take n from the as received  (AR)  cold-rolled S0.05% steel sheets, are \ndesignated SAR0.05%. The SAR0.05% is subjected to : (a) furnace -annealing (FA) after Austenitization \nat 1000°C, holding for 2 min  followed by furnace cooling to room temperature ove r ~20 h  \n(SFA0.05%); and (b) rapid water -quenching  (WQ) after complete Austenitization for the same \ntemperature and time (SWQ0.05%)  (Fig. 1(a)).  Figure 1(a) shows the Time -temperature -\ntransformation (TTT) and continuous cooling transformation (CCT)  diagrams  for S0.05% steel \n[43] along  with our heat treatment regimes . The crystallographic and morphological properties \nof SAR0.05% ,SFA0.05% and SWQ0.05% samples were studied using  Electron backscattered diffraction \n(EBSD ) and X -ray Diffraction (XRD) techniques (details of metallographic sample -\npreparation in SI-2 and experimental details of XRD and EBSD in SI-3). XRD spectra  is similar \nfor SAR0.05% ,SFA0.05% and SWQ0.05% samples  (Fig. 1( b)) implying dominant  ferrite content in all 3 \nsamples . Since the carbon content is very low  (0.05%) , there is hardly any shift or splitting in \nthe (110) ferrite -plane (2𝜃 ~ 52.3° for Co -Kα) [44] even after quenching  (Fig. 1(b)) . Moreover, \nthe polygonal shapes of the microstructure after quenching (see Fig. 1(e) and  microstructural \nimages in Fig. S2(c) and (f)) indicate pr esence of strained ferrite.  The Inverse pole figure ( IPF) \nmaps obtained from EBSD in figs. 1(c)-(e) show the polycrystalline grain orientation s in the \nSAR0.05% ,SFA0.05% and SWQ0.05% samples , respectively . In  SFA0.05%, we see annealing in creases grain size \nw.r.t SAR0.05%steel (compare Figs. 1(c) and (d)). In SWQ0.05%, quenching reduces grain size \nsignificantly and introduces a wider variation in grain orientation (Fig. 1( e)). A compari son of  \nthe average grain sizes determined from optical and scanning electron micrograph ( SEM ) \nimages , show  that (see table S 4, SI-3) the grains of  SFA0.05% are ~  5 times larger compared to 4 \n SAR0.05%, while grains in SWQ0.05% samples are ~ 1/10 of that of SAR0.05% sample . Generally, fine r grains \nenhance toughness, hardness, and shock resistance compared to coarser -grained steel [45, 46] . \nThis agrees with our Vickers hardness test result  which show s an almost 100% increase in \nhardness of SWQ0.05% compared to SAR0.05% steel and a drop in hardness for SFA0.05% steel (see table S 5 \nin SI -4).  \nFIG 1. (a) Time -temperature -transformation (TTT) diagram and continuous cooling transformation (CCT) curves \nfor two different thermal processes, furna ce annealing and water -quenching performed on a 0.05% carbon steel \n(S0.05%). Various phases e.g., Austenite, Ferrite and Cementite ( Fe₃C ) represented by their initials A, F, and ‘Cem’ \nrespectively. A S is the Austenite transformation temperature. (b) X Ray Diffraction 2 𝜃 scan on 0.05% C steel \n(S0.05%):  as-received ( SAR0.05%), furnace -annealed ( SFA0.05%), and water quenched (SWQ0.05%) steel samples with Co K α \nradiation. (c -e) IPF maps obtained from EBSD of SAR0.05% (c), SFA0.05% (d) and SWQ0.05% (e) steel samples. Note the \ncolor map in Fig. 1(e) denotes the three cubic crystallographic axes. (cf. text for details) (f) -(g) Real part of ac \nsusceptibility  (χAC) vs magnetic field ( H) obtained from TDR -based measurement of SFA0.05% (f) and SWQ0.05% (g) steel \nsamples compared to SAR0.05% steel sample of equal dimensions measured at a fixed bias voltage 0.615 V (see Fig. \nS4(d) for details) . \n \nWe investigate the high -frequency ( 𝑓~72 MHz) AC susceptibility ( 𝜒𝐴𝐶) response to fo cus on \nmagneti zation dynamics while avoiding the slow domain wall regime (kHz to sub -kHz) \nprevalent in steel due to grain boundary effects. Operating at high frequencies also minimizes \n1/𝑓 noise and helps determine the sample's response to an oscillating magnetic field.  We \ndetermine the 𝜒𝐴𝐶 response of the SAR0.05%, SFA0.05% and  SWQ0.05% steel samples using a Tunnel Diode \nResonator ( TDR )-based  susceptometer  [47, 48] . We previously demonstrated [49] that a \ncompact TDR -based sus ceptometer enables real -time monitoring of high -frequency 𝜒𝐴𝐶 \n20 40 60 8040060080010001200Co Ka  (l= 0.179 nm )Intensity (arb. units)\n2q (deg.)\n0 100 200 3000.150.160.170.180.19\n142.0~\nACAC\nχAC (arb. units)\nH (Oe)\n(c)     .  %\n0 100 200 3000.110.120.130.14χAC (arb. units)\nH (Oe)\n117.0~\nACAC\n\n(b)\n10010110210310410502004006008001000 T (°C)\nt (s)QuenchingA\nA + F\nAnnealingF + CemAS\nS0.05%\n[101] [001][111](d)     .  %\n(e)     .  %(a) (f)\n(g)\n    .  %\n    .  %    .  %    .  %\n    .  %\n    .  %    .  %5 \n responses in steel, providing a practical tool for assessing structural integrity and predicting \npotential damage in steel -based infrastructures.  Briefly, TDR is a self -resonant LC tank ci rcuit \nwhich measures the 𝜒𝐴𝐶 of a material from the shift in circuit ’s resonant frequency \n𝑓0( ~  12𝜋 √𝐿𝐶) ⁄ , 𝜒𝐴𝐶 ∝∆𝑓𝑚\n𝑓0  where ∆𝑓𝑚 is the shift in frequency because of the  change in L \nof the tank circuit by the materi al (𝑓0 ~ 72 MHz for our TDR circuit, for details  see SI -5). \nFigures 1(f)-(g) shows the TDR based 𝜒𝐴𝐶 response measured as a function of magnetic field \n(H) for the SFA0.05% and SWQ0.05% compared to the SAR0.05% sample . In the H range s tudied (< 300 Oe) \nthere is no significant variation in the 𝜒𝐴𝐶 value. Therefore,  any variations in 𝜒𝐴𝐶 cannot be \nattributed to any differences in the thermomagnetic history of steel. Figure 1(f) shows that the  \n𝜒𝐴𝐶 of SFA0.05% is highe r than SAR0.05% by ~ 11.7% . However, the 𝜒𝐴𝐶 for SWQ0.05%shows an opposite \ntrend  viz., χAC of SWQ0.05%sample is lower than that of SAR0.05% by ~ 14.2 % (Fig. 1( g)) (in SI-6 we \nshow  the DC magnetization response of these samples , and in table S4 , SI-3 a comparison with \nthe AC response ). In SWQ0.05%samples, rapid cooling induce s thermal contraction mismatches, \ndislocation generation, grain boundary stresses, and residual stresses, all of which may \ncollectively influence the magnet ic properties  of steel . Furthermore,  in our samples  any \npresence of cementite (Fe₃C) phase  (albeit in very low concentrations, Fig. 1(a)) , which has \ndistinct magnetic properties compared to the Ferrite phase, could also play a role in determining \nthe magnetic properties of steel. In view of these issue s, we compare our mag netization results \non thermally treated S0.05% steel with a S0.7% steel (rich in Fe₃C ) exposed to varying levels of  \nuniaxial tensile  strain .  \n2.2. Studying pure strain effects on high C (0.7%) steel  \nFor pure strain effect  study , the S0.7% steel sa mple  isn’t expose d to any thermal  treatment s. We \napply a  uniaxial tensile -load at constant strain rate of 0.001s⁻¹ to the S0.7% work -piece for stress \n(σ) - strain (ε) measurements (work -piece dimensions and relevant ASTM standards are \ndetailed in SI -7). The σ -ε curve for S0.7% is shown in Fig. 2(a). Based on the σ -ε plot, four \ndifferent S0.7% strain ed samples were prepared: sample STS10.7% was strained within the elastic \nlimit , up to TS 1 (Fig. 2(a)), sample STS20.7% strained slightly above elastic regime  up to TS 2, \nsample STS30.7% strained deep into the plastic regi me near Ultimate Tensile Strength (UTS)  up to \nTS3, and sample STS40.7% strained beyond breaking stress till TS 4 (see the vertical drop beyond \nUTS) . After strai ning, sections were cut from each sample’s central gauge length, with \ndimensions 6 mm × 2 mm × 1.9 mm. SEM micrographs of strained samples STS10.7% and STS40.7% 6 \n (see Fig. 2(b) and (c); additional images in Fig. S7 (a)-(d), SI-7) show Ferrite g rains (F in Fig. \n2(b)) and lamellar pearlite colonies (P in Fig. 2(b)), characterized by alternating layers of \nFerrite and Cementite  (Fe₃C ) [31]. Tensile straining (strain -axis in dicated by the horizontal \ndouble arrow in Fig. 2(b) and (c)) causes Fe₃C  lamellas to stretch along strain direction [50, \n51], thereby reducing interlamellar spacing ( Lₛ) from ~230 nm (Fig. 2(b) ) to ~180 nm (Fig. \n2(c)) [50, 52] . Remarkably, the microscopic strain distribution in S0.7% (to be discussed later) \nand its microstructure closely resembles that of SWQ0.05% (Fig. S7(i), SI -7). The strain -induced \nmagnetic behavi our of S0.7% steel is influenced by two competing factors: while the stretching  \nFIG 2. (a) Engineering stress ( σ) vs engineering strain ( ε) curve of a 0.7% carbon steel sample S0.7%. Red circles \nin the σ- ε plane labelled TS 1, TS 2, and TS 3 denotes points up to which different S0.7% steel samples have been \ntensile -strained. The fourth sample has been strained beyond  Ultimate Tensile Strength (UTS) till fractured, see \nvertical drop (labelled TS 4). (b-c) SEM micrographs of 0.7% carbon (S0.7%) strained steel samples, strained within \nelastic limit up to TS 1, sample STS10.7% (b) and strained beyond UTS till fractur e up to TS 4, sample  STS40.7% (c). Ferrite \ngrains and pearlite colonies are marked by F and P respectively in Fig. 2(b). Note the strain axis is represented by \nhorizontal double -arrow in (b) and (c). (d -e) IPF maps obtained from EBSD of 0.7% carbon (S0.7%) strained steel \nsamples, strained up to TS 1, sample STS10.7%(d), and strained beyond UTS till fracture up to TS 4, sample STS40.7% (e) \n(cf. text and Fig. 2 (a) for details). Note the color map in (d) denoting three cubic crystallographi c axes. (f) Mean \nMisorientation angle ( ⟨KAM⟩) obtained from IPF maps plotted as a function of true strain %. (inset) Probability \ndistribution of misorientation angles in two maximally strained samples, STS30.7% and STS40.7%(refer to Fig. 2(a) and \ntext) with lognormal fits.  \n \nof fer romagnetic Fe₃C  lamellas enhances magnetization, the build -up of interfacial strain \ncounteracts this effect  [53]. To understand the effect of microscopic strain  (and its imprint on \nmagnetism) , we performed EBSD. IPF maps for samples STS10.7% and STS40.7% are sh own in Figs. \n2(d)-(e)) (see rest  in Fig. S 7(e)-(h), SI-7). All strained S0.7% samples show a mosaic of grains \n(e)\n strained up to TS4strained up to TS1 (d)\n5 10 15 200.150.200.250.300.35⟨ KAM ⟩ (deg.)\nTrue strain (%)sample S0.7%\n0.0 0.5 1.0 1.50.00.51.0  S0.7%\nTS4\n Lognormal fit\n S0.7%\nTS3\n Lognormal fitProbability density KAM (deg)\n(f)\n(c)\n(b)F\nP\nstrained up to TS1\nstrained up to TS4\n0 5 10 15 2002004006008001000 Engg. stress σ (MPa)\nEngg. strain % ( ε)strain rate = 0.001 s-1sample S0.7%TS1TS2TS3UTS\nTS4 sample fractured(a)7 \n with random orientations similar to  the quenched sample  SWQ0.05% (Fig. 1( e)). The IPF maps for \nsamples STS10.7%, STS20.7%, STS30.7% and STS40.7% appear quite similar (Fig. S 7, SI-7), suggesting grain size \nisn’t significantly modified  as a result of straining.  \n \nElastic strain usually results in changes in unit cell parameter(s). When the material is strained \nbeyond the elastic limit, it undergoes permanent deformation and the distortions in the crystal \nlattice are relieved by the formation of dislocations [54, 55] . Plastic strain can be quantified by \nanalysing  local crystallog raphic misorientation using a pixel -based analysis of IPF maps \n(details in SI -7). We calculate the Kernel Average Misorientation (KAM) as KAM =  \n 1\n4∑𝜃 (𝑃,𝑃𝑖)4\n𝑖=1 , where 𝜃(𝑃,𝑃𝑖) is the misorientation between point P in a pixel and its four \nnearest neighbours  on a square grid  [55]. A tolerance angle of 5° was set to exclude larger \nmisorientations between neighbouring  grains . The inset in Fig. 2(f) shows log -normal \nprobability distribution of KAM for two maximal ly strained samples , STS30.7% and STS40.7%. Fig. 2(f) \nplots the mean KAM (<KAM>), derived from the log -normal fit s for misorienta tion \ndistribution of four  strained sample , versus true macroscopic strain ( 𝑒 = 𝑙𝑛(1 + 𝜀)), \nrevealing a clear increase in <KAM> with strain and supporting the effectiveness of KAM -\nbased quantification of microscopic plastic strain. Figure  S7(i) in SI -7 shows the KAM \ndistributio n similarities between STS30.7% and SWQ0.05%samples , which justifies our approach to use a \nhigh C S0.7% sample to assess the role of pure -strain effects on magnetism . \n \nNext, w e measure TDR -based 𝜒𝐴𝐶 for strained  samples  STS10.7%, STS20.7%, STS30.7% and STS40.7% .The STS10.7% \nsample closely resembles an unstrained sample, as strain effects are largely relaxed, and its \nmagnetization response is comparable to that of an unstrained S0.7% steel sample . Fig. 3(a) \nshows that 𝜒𝐴𝐶 increases with increasing strain  and remains independent of H variation in the \nlow-H regime (H is applied || to ε direction). This increase in 𝜒𝐴𝐶 with strain is attributed to \nlocal lattice distortions (from KAM analysis) rather than changes  in grain size  (compare Fig. \nS7(e)-(h)). Strain anisotropy, induced by the magneto -elastic coupling  in steel changes  the \nmaterial’s net magnetic anisotropy [56, 57] . Fig. 3(b)(i) illustrates an unstrained sample (ε  = 0) \nwhere magnetic anisotropy (MA) aligns differently from the direction of  H. Corresponding \nM(H) curve for ε = 0 case is shown in Fig. 3(b)(iii)  (black curve). When strain is applied (ε ≠ \n0), Fig. 3(b)(ii), strain -induced anisotropy reorients MA by angl e  closer to the H direction , \nwhich is same as strain -direction in our experiments  (recall the stretching of ferromagnetic  \nFe₃C  lamellas towards strain -axis, Fig. 2(b) -(c)). This,  therefore , result s in a steeper M(H) curve 8 \n (red curve in Fig. 3(b)(iii)) thereby exhibiting enhanc ed 𝜒 for ε ≠ 0 . Figure 3(c) confirms this, \nwhere our measured dc M(H) curves for the strained STS10.7%, STS20.7% and STS40.7% samples show \nenhanc ing trend of  𝜒 with increas ing strain. Thus, strain modifies the net MA, increasing the 𝜒 \nof our strained S0.7% sample. This finding suggests that pure magneto -elastic strain effects or \npresence of any Fe3C phase in the 0.05% carbon steel sample alone cannot explain the decrease \nin 𝜒 observed in SWQ0.05% sample  (Fig. 1( g)). It may be worth commenting here on the increase \nin 𝜒 in our SFA0.05% sample  compared to  SAR0.05%. The SAR0.05%sample has dislocations due the process  \nFIG 3. (a) χAC vs H for 0.7% carbon steel samples ( S0.7%) with varying tensil e-strain levels, STS10.7% , STS20.7% , STS30.7% \nand STS40.7%, (see Fig. 2(a)). (b) Enhancement of susceptibility as a result of uniaxial tensile -strain (𝜺), unstrained \ncase (i) and reorientation of anisotropy by angle , solid yellow arrow in (ii) as a result of applied strain 𝜺. Note \nthat H is applied along strain -direction. (c) M(H) curves for S0.7% steel samples with varying tensile -strain levels, \nSTS10.7% , STS20.7%  and STS40.7% as shown in Fig. 2(a)).  Note that H is applied along strain -direction in the experiments \n(cf. text for details).  \n \nof cold rolling, which generate pinning sites  obstructing DW motion while magnetizing the \nsample [2, 26, 58] . Slow  annealin g enhances crystallinity by enlarging grains and reducing \nH\n   H\nMA = (unstrained)M\nH =     \n (      )\n (i)\n(ii)(b)\n0.0 0.4 0.8 1.2050100150200\n   S0.7%\nTS1\n   S0.7%\nTS2\n   S0.7%\nTS4M (emu/g)\nH (kOe)\n(c)(a)\n0 50 100 150 200 250 3000.0850.0900.0950.1000.105χAC (arb. units)\nH (Oe)S0.7%\nTS4\nS0.7%\nTS3\nS0.7%\nTS2\nS0.7%\nTS1\n(iii)9 \n dislocation density relative to the cold -rolled SAR0.05%sample, and facilitates easy domain growth , \nresult ing in higher 𝜒 for SFA0.05%. Additionally , the SAR0.05%sample  exhibits a preferential  alignment \nof grains along the [111] direction (Fig. 1( c)), which is Ferrite ’s hard ax is of magnetization  \n[26]. Annealing aligns grains more uniformly, thereby boosting 𝜒. We explore below the \ndecrease d 𝜒 feature in SWQ0.05%. \n \n2.3. Evidence of magnetic topological s tructures in low C steel  \n \nWe simula te grain size effects on steel’s magnetic properties using MuMax3 [59], which solves \nthe Landau -Lifshitz -Gilbert equation to  micro -magnetically  model the evolution of magnetic \nmoment s. To reduce computational load, we scale down the problem size, including grain sizes, \nby ~103. A Voronoi tessellation algorithm generates grains of varying sizes for polycrystalline \nsamples (see SI -8 for details). In our simulation , we modelled t wo low c arbon steel samples \n(1024 nm × 1024 nm × 40 nm) with mean grain sizes of 40 nm and 10 nm , to represent the \nSFA0.05% and SWQ0.05% samples respectively (Figs. 4(a) and 4(b)). The grain colo urs in Figs. 4(a) and \n(b) represent  the varying MA direction of each grain. In our model, the MA of each grain is  \n-20 -10 0 10 20-200-1000100200  S0.05%\nFAexpt. data\n 40 nm grains simu\nH (kOe)M (emu/g)\n-1.0-0.50.00.51.0M / MS \n0 1 2 3 40.00.40.81.2M / MS\nH (kOe) 10 nm grains\n 40 nm grains(a)\n(c)\n(d)(b)10 \n FIG 4. (a -b)  Simulated microstructures of steel with varying sizes of grains, 40 nm (a) and 10 nm (b) with \nmagnetic anisotropy values in the range ( 0.10 ± 0.01) MJ/m3 distributed randomly (denoted by d ifferent colors) \nin each grain (cf. text for details). (c) Comparison of experimentally obtained M(H) data for 0.05% carbon steel \nfurnace -annealed sample ( SFA0.05%) with simulated M(H) result for 40 nm grain -sized steel sample i.e., Fig. 4(a). \n(d) Comp arison of simulated M(H) results in the low field  regime for 40 nm and 10 nm grain -sized steel samples.  \n \n \nrandomly chose n in the range  (0.10 ± 0.01) MJ/m³  (compare cubic anisotropy of pure iron , K1 \n~ 0.1 MJ/m³ [60]). SI-8 gives information about additional parameters,  e.g., exchange stiffness \n(A) and saturation magnetization ( MS) values used.  Fig. 4(c) shows  that the simulated M(H) \ncurve for the 40 nm grain -sized  steel (i.e., Fig.4(a))  aligns well with  our experimental ly \nmeasure d M(H) for the SFA0.05% sample , validating our  micro -magnetic  model . The simulated \nlow-field M(H) behaviour  of 40 nm and 10 nm grain samples in Fig. 4(d) shows that the \n'annealed' sample ( i.e., 40 nm grain) has a higher 𝜒 than the 'quenched' sample (10 nm grains), \nwith the susceptibility ratio consistent with our experimental observations  from VSM and TDR.  \nFig. 5(a) -(c) shows simulated field evolution of magnetic domains in the ‘quenched’ ( i.e., 10 \nnm grain) sample (see  Fig. S9, SI-8 for the complete set and Fig. S8 for that of  40 nm, or \n‘annealed’ sample). In these images, colo urs represent magnetization direction  (note, in the \ntop-right corner the color wheel , at each point of which, magnetization direction is along th e \ntangent to the circle) , with white and black indicating moment s along or opposite to H \nrespectively . As H increases, magnetic moments align progressively with H, approaching \nsaturation near 4.5 kOe (image appears whiter, Fig. S 8 and S 9). A closer examina tion of  Figs. \n5(a)-(c) reveals intriguing  local  features viz., persistent blackish  spots at DW intersections  \naround which we see  a swirling of moments  (encircled in Fig. 5(a) -(c)). Figures \n5(a)→5(b)→5(c) shows that as H increases, the spot  density  decrease s (also see Fig. S 9). With \nthe vanishing of the spots , the domains expand  (see the expansion of green -yellowish and light \nblue regions in Figs. 5(b)→5(c)). These  spots do not move at low H and DW are pinned at these \nsites. We also imaged the magnetic lands cape with magnetic force microscopy ( MFM ) (see \nexperimental details in SI -9). The zero -field, dual-pass-phase image of the demagnetized state \nof SAR0.05%, SFA0.05%and SWQ0.05% steel samples  (Fig. 5(d) -(f)) show  dark spots (magnetic features) of \nlateral size ~50 nm . These spots are  comparable  in size  to the magnetic exchange length -scale  \nof steel, lex = √2𝐴\n𝜇0𝑀𝑠2⁄  ~ 20 nm, where A ~ 20 pJ/m  and 𝑀𝑆= 4   kA/m.  We find maximum \nspot density in SWQ0.05% (Fig. 5(f)), lower in  SFA0.05%steel (Fig. 5(e)) and least in SAR0.05%(Fig. 5 (d)). \nWe also imaged the surface topography of the s teel samples with atomic force microscopy 11 \n (AFM) using  a non -magnetic tip (details given in SI -9). AFM  imag es (Fig. 5(g) -(i)) show \nuniform topographic features  (r.m.s surface roug hness of ~ 4 nm ) and, more importantly, the \nabsence of any features similar to  the dark spots  observed in MFM. Therefore, we  rule out the \npossibility of convolution of topographical surface features (hillocks/pits) for seeing the spots \nin MFM and thereby confirm their magnetic origin . \nOur simulations (Figs.5(a) -(c)) reveal that dar k localized spots, forming at DW intersections , \ndisplay a swirling, non -collinear arrangement of magnetic moment  with moment flipping at \n \nFIG 5. (a -c) Evolution of the simulated magnetic landscape ( 𝑀⃗⃗  (x, y)) as a function of field applied perpendicular \nto the plane ( Hz). Magnetization direction is given by the color wheel in north -east corner, magnetization at each \npoint on the color -wheel is tangential to the circle. White and black color denote magnetization along -field-\ndirection and antiparallel -to-field-direction, respectively. Note the localized black spots (circled around in (a) -(c)) \nat the intersections of the domains. (d -f) Dual -pass-phase images in demagnetized state obtained by MFM showing \nmagnetic landscape in 0.05% carbon steel, as -received SAR0.05% (d), furnace -annealed, SFA0.05% (e), and water -\nquenched, SWQ0.05% (f) steel samples. Note the adjoining color bars in each image denoting change in phase in \ndegree. (cf. text for details) (g -i) AFM maps showing surface topography on 0.05% carbon steel, as -received, \nSAR0.05% (g), furnace -annealed, SFA0.05% (h), and water -quenched SWQ0.05% (i) steel samples. Note the color bar adjacent \nto (i) denoting surface roughness in nm for (g -i). (j) Resistivity ( ρ) vs temperature  (T) for water -quenched sample \n(SWQ0.05%) showing Kondo behavior fitted with Hamann function (red curve) and Bloch -Gruneisen fit at high \ntemperature, T > 80 K (green curve) (cf. text for details).  \n \n(g) (i) (h)\n(e) (d)(f)y\nxHz\n0.5 kOe 3.0 kOe\n 2.5 kOe (c) (a)\n0 50 100 150 2000.050.100.150.20\n Expt. data\n Bloch Gruneisen fit\n Hamann fitρ (μΩ.m )\nT (K)Model Hammanfit (User)\nEquationA + q*x^2 + p*x^5 + B*(1-((ln(\nx/T_K))/((ln(x/T_K))^2 +s*(s+1\n)*9.86)^0.5))\nPlot Resistivity\nA -9.17895E-8 ± 3.23135E-9\nq 6.13698E-11 ± 1.11658E-12\np -6.25226E-17 ± 1.33506E-18\nB 6.13028E-8 ± 1.22548E-9\nT_K 46.18813 ± 0.05139\ns 6.38303E-4 ± 4.50216E-5\nReduced Chi-Sqr 4.33498E−18\nR-Square (COD) 0.98047\nAdj. R-Square 0.98024Warming mode\n(j)\n(b) 1.0 kOe\n    .  %12 \n the centr e (a quasi -singularity  featu re), resembling magnetic topological structures (MTS) like \nskyrmions or vortices [61]. Micro -magnetic energy calculations predict such swirli ng \nstructures at the surface of soft magnetic materials (i.e., low anisotropy,  so that DW thickness \n~ √𝐴/𝐾 becomes  very small  and formation of swirls are favored ) and estimate  a cut -off of 3𝑙𝑒𝑥 \nas radius of the swirl  [62, 63] . The diameter of the dark spots in MFM are in agreement with \nthis estimate . MFM observations show the presence of MTS in all low carbon samples, though \nthey are denser in thermally quenched samples . It is t hese MTS which pin DWs and restrict \ntheir expansion with increasing H, consequently reducing the χ of quenched steel , as observed \nexperimentally . Smaller grains (10 nm) retain MTS up to higher fields (~3.5 kOe) than larger \ngrains (40 nm) ( compare Fig. S8 and S9,  SI-8). Since MTS form at DW intersections with \nvaried moment directions, they are more likely in quenched steel, with its small grains and \nrandom magnetic anisotropy distribution . We believe that thermal quenching produces \nsignificant local strain s in the SWQ0.05%which  generate local lattice distortio ns and randomize local \nMA, thereby favouring formation of swirls.  \n2.4. Kondo localization  in quenched low C steel  \nFigure 5(j) presents four-probe resistivity ( ρ) vs. temperature ( T) behaviour  of SWQ0.05% steel \n(details in SI -10). For T > 80 K, 𝜌(𝑇) exhibits metallic behavior, described by the equation \n𝜌(𝑇) = 𝜌0  + 𝑞𝑇² + 𝑝𝑇⁵, where 𝜌0 = 0.0947 μΩ·m  (residual resistivity) , p = 0.56 \nμΩ·m·K⁻5 (electron –phonon interactions), and q ~ 10⁻³ μΩ·m·K⁻2 (electron –electron \ninteractions). Below 65 K, 𝜌(𝑇) drops sharply, reaching a minimum at ~50 K, then rises again \nbefore saturating below 40 K. Th is minimum  representing Kondo localization [64], is model led \nusing the Hamann function [65-67] (red curve), 𝜌(𝑇)=𝜌 +𝑞𝑇2+𝜌𝐾 {1−𝑙𝑛(𝑇\n𝑇𝐾)\n√ln2(𝑇\n𝑇𝐾)+𝑠(𝑠+1)𝜋2} \nwith a localization temperature 𝑇𝐾 = 46.18  (± 0.05)  K and 𝑠 = 0.001  (see SI -10 for details). \nThe fit confirms  Kondo screening of Mn moments (0.17 wt%, see table S1, SI-1) by Fe d -\norbital conduction electrons. We see the above effect also in  SAR0.05%and SFA0.05% samples though \nthe effect is most pronounced in SWQ0.05%. These findings reveal th at low carbon steel hosts strong \nelectron correlation effects . In tandem with local strain effects producing local lattice \ndistortions and their effect on electron correlation s at these locations, the MTS features is likely \nto be an emergent feature . Creat ion of large density of MTS as in SWQ0.05% is identified from the \ndrop detected in 𝜒. 13 \n 3. Summary and Conclusions  \nOur study reveals that morphology, thermal treatments, and strain significantly affect the \nmagnetic properties of low -carbon steel. Therm al annealing enlarges grains and enhances 𝜒𝐴𝐶, \nwhile quenching reduces both. Strain and presence of additional Fe 3C phase increases 𝜒𝐴𝐶 by \naltering magnetic anisotropy but does not explain its drop. Micromagnetic modeling and MFM \nidentify magne tic topological struc tures (MTS) in quenched steel as  domain wall pinning sites, \ncausing the 𝜒𝐴𝐶 reduction. Low -temperature transport links MTS formation to interplay of \nstrain and strong electron correlation  effects in steel . This work positions ste el as a potential \nquantum material and calls for further exploration of its MTS state, such as skyrmions or \nmagnetic vortices.  \n \nAcknowledgements  \nThe authors acknowledge Centre for Nanoscience, IIT Kanpur for providing the MFM facility. \nSSB acknowledges Department of Science and Technology (DST) -SERB SUPRA  program, \nthe DST -AMT p rogram of the Government of In dia, and IIT Kanpur  for funding support. \nSuprotim Saha acknowledges the Prime Mini ster’s Research Fellows (PMRF) s cheme of the \nMinistry of Human Resource  Develo pment, Government of India, for funding suppor t. PCM \nthankfully acknowledges Mr. Sounavo Ghosh and Dr. Apala Banerjee for fruitful discussions \non electronic circuitry and simulations. PCM also thanks Mr. Bireshwar Roy , Mr. Prayas \nSharma  and Dr. Anki t Kumar  for their involvements at different stages of the work.  \n ",
      "metadata": {
        "filename": "Evidence of strong electron correlation effects and magnetic topological excitat.pdf",
        "hotspot_name": "Steel_Punching_and_Bending",
        "title": "Evidence of strong electron correlation effects and magnetic topological\n  excitation in low carbon steel",
        "published_date": "2025-03-11T18:53:56Z",
        "pdf_link": "http://arxiv.org/pdf/2503.08822v1",
        "query": "punching and bending steel EMC shield material substitution studies resource reduction percentages"
      }
    },
    "Highly flexible electromagnetic interference shielding films based on ultrathin": {
      "full_text": " 1 Highly flexible electromagnetic interference shielding films based on ultrathin \nNi/Ag composites on paper substrates  \nXiangli Liuc, Ziheng Yeabc, Ling Zhangabc, Pengdong Fengabc, Jian Shaoabc, Mao \nZhongabc, Zheng Chen*d, Lijie Cic, Peng Heb, Hongjun Jiabc, Jun Weiabc, Mingyu Li*abc \nand Weiwei Zhao*abc \n \naFlexible Printed Electronics Technology Center, Harbin Institute of Technology, \nShenzhen 518055, People’s Republic of China. Email: wzhao@hit.edu.cn  \nbState Key Laboratory of Advanced Welding & Joining, Harbin  Institute of Technology, \nShenzhen 518055, People’s Republic of China. Email: myli@hit.edu.cn  \ncThe School of Materials Science and Engineering, Harbin Institute of Technology, \nShenzhen 518055, People’s Republic of China  \ndSchool of Material Science and Engi neering, China University of Mining and \nTechnology, Xuzhou, Jiangsu 221116, P.R. China. Email: chenzheng1218@163.com  \nAbstract  \nHighly flexible electromagnetic interference (EMI) shielding material with \nexcellent shielding performance is of great significanc e to practical applications in next -\ngeneration flexible devices. However, most EMI materials suffer from insufficient \nflexibility and complicated preparation methods. In this study, we propose a new \nscheme to fabricate a magnetic Ni particle/Ag matrix comp osite ultrathin film on a \npaper surface. For a ~2 µ m thick film on paper, the EMI shielding effectiveness (SE) \nwas found to be 46.2 dB at 8.1 GHz after bending 200,000 times over a radius of ~2 \nmm. The sheet resistance (R□) remained lower than 2.30 Ω after  bending 200,000 times. \nContrary to the change in R□, the EMI SE of the film generally increased as the weight \nratio of Ag to Ni increased, in accordance with the principle that EMI SE is positively \nrelated with an increase in electrical conductivity. Desi rable EMI shielding ability,  2 ultrahigh flexibility, and simple processing provide this material with excellent \napplication prospects.  \nKeywords:  Electromagnetic  interference shielding ; Ag/Ni blend;  High flexibility; \nComposite structure   3 1. Introduction  \nFlexible electronics have rapidly developed in recent years as an important \ndirection for the future development of electronics and are of great significance to \nmankind. In traditional non -flexible electronic devices, solving the electromagnetic \ninterference ( EMI) problem, which interferes with delicate electronic equipment and \nposes environmental problems (namely long -term threats to human health), is essential \nin ensuring normal device operations [ 1-7]. EMI shielding is similarly critical in flexible \nelectron ic devices [1, 8]. For shielding materials, electrical conductivity is needed to \nreflect radiation by the interaction between charge carriers and electromagnetic fields \n[1]. In the past few years, extensive effort has been devoted to fabricating EMI shield ing \nmaterial with high flexibility for applications in flexible devices; strategies for \neffective ly combin g conductive fillers with flexible polymer matrix have become a \ncommon theme [1, 8 -19]. One of common  preparation  methods is to mechanica lly \nmixing th ese materials, requiring a significant amount of conductive fillers to achieve \ndesirable EMI shielding performance and can compromise flexibility. Another is to mix \nthe conductive filler into the polymer material first, and then coat it on the surface of t he \nflexible substrate to form a shielding material with a layered structure. However, this \nmethod not only needs to deal with the dispersion of the conductive material, how to \nimprove the adhesion between layers is also a problem  In addition to these commo n \nmethods , researchers have focused on new fabrication methods to promote shielding \nperformance and flexibility simultaneously [1, 3, 20, 21].  \nMetal and carbon materials are commonly used as conductive fillers for the \npreparation of flexible EMI shielding materials [ 21-24]. Among metals, silver is most \ncommonly used because of its high conductivity, corrosion resistance, and mature  4 production technology. In recent years, graphene ( sometimes combined with metal) has \nbeen widely used in this area due to its o utstanding comprehensive physical properties. \nFor example, Zongping Chen et al. [1] prepared a graphene/polymer foam composite, \nwhere graphene sheets were fabricated by chemical vapor deposition of methane on Ni \nfoam first, and then the Ni foam was etched away after coating a thin layer of \npolydimethylsiloxane (PDMS) on the graphene surface. The method provided this \nmaterial (~0.8 wt% graphene, ~1 mm thick) with approximately 20.0 dB shielding \neffectiveness (SE) in the X band frequency, and its EMI SE decre ased slightly after \nbending 10,000 times over a radius of 2.5 mm. Besides conductive fillers, magnetic \nentities are often used to improve the EMI SE of shielding materials due to the  \nmaterial’s magnetic dipoles interacting with the radiation [2, 3, 25]. Ya nhu Zhan et al. \n[3] obtained a flexible NR/Fe 3O4@rGO composite, showing an EMI SE of 42.4 dB at \n9.0 GHz (10 phr rGO, 1.8 mm thick) and an SE exhibiting only 3.5% loss after bending \nto a 60°  angle 2000 times. These studies effectively combined flexibility w ith shielding \nproperties; however, the degree of flexibility for many flexible electronic devices \nremains insufficient, and these shielding materials are still too thick.  \nIn this paper, a new scheme is proposed to fabricate a highly flexible EMI \nshielding material with a unique composite structure by depositing and sintering Ag and \nAg/Ni blends onto the paper surface. Studies have confirmed that much greater strain \ncan be sustained by a metal film that is well bonded to substrates compared with a film \nthat is not [26, 27]. Thus, the shielding film exhibits outstanding flexibility under highly \nconductive conditions owing to the contribution of the paper surface. Its EMI SE only \ndeclined 2.2 dB at 8.1 GHz with an increase of 1.20 Ω in R□ after undergoing 200,0 00 \nbending tests. The Ag/Ni blends were made from two types of Ni (Ni particles with  5 diameters of 20 -100 nm and an average diameter of 60 nm, and Ni particles with \ndiameters of 350 nm -1.16 µ m and an average diameter of 560 nm). The weight ratios of \nAg to N i were 1:2, 1:1, 2:1, 4:1, and 6:1, respectively. When the weight ratio of Ag to \nNi (60 nm) reached 6:1, the Ag/Ni blend sintering film demonstrated an EMI SE larger \nthan 46.7 dB in the 8 -12 GHz frequency range and a small R□ of 0.78 Ω, while the \nthickness  of the metal layer was only ~2 µ m. Such a small thickness of the metal layer, \ncoupled with the unique composite structure and appropriate combination of metal and \npaper, provides this shielding material ultrahigh flexibility and excellent shielding \nperfor mance that fully satisfies application demands in many flexible electronics.  \n2. Experimental  \n2.1 Materials  \nAg (50 -70 nm, 99.99%), Ni (20 -100 nm, 99.9%), Ni (0.34 -1.12 µ m, 99.8%), \nAgNO 3 (AR, 99.8%), polyvinylpyrrolidone (PVP) (Macklin, K29 -32), hydrazine \n(Macklin, 50%), and deionized water were used in experiments.  \n2.2 Preparation of Ag/Ni blend  \nAg/Ni blends were prepared by chemical reduction. First, 0.68 g of AgNO 3 and \n0.68 g of PVP were added to 720 mL of deionized water in a beaker wrapped by in \naluminum  foil, and then stirred until all solvent dissolved. Then, the nano -nickel powder \nwas added to the solvent followed by sonicating sonication of the solvent for one1 min. \nAfter adding 18 mL of hydrazine hydrate, the solvent was stirred until it became nearl y \ntransparent. The precipitate was then washed and centrifugated several times. An Ag/Ni \nblend was obtained after drying.  \n2.3 Preparation of EMI shielding metal/paper film   6 Ag/Ni blend or Ag (60 nm [representing 50 -70 nm]) particles were first mixed with \nethylene glycol at a weight ratio of 1:4. After sonicating and stirring for two min, the \nmixture was quickly coated on high -temperature resistant paper (a polyimide film \ncoated with TiO 2 coating, ~100 µ m thick totally) by dropper to prevent silver from \ndepos iting in liquids. Then, the paper was placed in a drying oven at 140 ° C for 1 h \nfollowed by natural cooling in air. Some particles on the paper surface were not \nconnected as a whole layer. These particles were wiped away slightly by  tissue paper, \nand EMI s hielding films were obtained.  \n2.4 Characterization  \nX-ray diffraction (XRD) analysis of shielding film was obtained by D/max 2500pc. \nThe morphology and structure of Ag/Ni blend sintering films were examined using \nscanning electron microscopy (SEM; Hitachi S 4700, 15 kV) and transmission electron \nmicroscopy (TEM; FEI Talos F200X). The thickness of the metal layer of the film was \nmeasured using a profilometer (Vecco Dektak 150). The R□ was measured with a \nmultifunction digital four -probe tester (ST -2258C). EMI SE was measured using a \nvector network analyzer (Keysight, E5063A). Magnetic properties were measured by a \nmagnetic property measurement system (Quantum Designed MPMS3).  \n3. Results and discussion  \nFig. 1a shows the structure of the Ag/Ni blend sintering fil m in which Ag was \ntightly connected to the paper as an integral layer with Ni particles dispersed throughout. \nOur previous work has proved that the presence of TiO 2 in the substrate facilitates the \nsintering of Ag and the formation of high -strength interfa ces between the Ag coating \nand the substrate  [28]. The film sintering of the Ag/Ni blend with a continuous Ag layer \nwas conductive, whereas the film sintering from pure Ni was not. Fig . 1b and 1c present  7 XRD analysis results of the substrate, pure Ag sinte ring films, and Ag/Ni blend \nsintering films. Except for the substrate peak, the film sintered from pure Ag only \nexhibited a diffraction peak of Ag (PDF#04 -0783), while films sintered from Ag/Ni \nblends only demonstrated peaks of Ag (PDF#04 -0783) and Ni (PDF #04-0850), \ndemonstrating that no other substance was generated after sintering. Fig. 1d shows that \nthe thickness of the metal layer of film, in which the weight ratio of Ag to Ni (60 nm) \nwas 1:2, was ~2.49 µ m. The total thickness of shielding film is ~100 µm, much smaller \nthan many graphene composite shielding film [1, 3, 22]. This figure also indicates that \nthe surface of the shielding film was not flat, confirmed by the SEM image of the film \ncross -section. From the SEM image , it was noted that the Ag/Ni b lend has formed a \nmetal layer tightly connected to the paper after sintering. Data were processed by \nignoring the two peaks near the step and then calculating the average values at the high \nand low points, respectively. The height difference is regarded as  the thickness of the \nmetal layer. Fig. 1e depicts the average thicknesses of the metal layer of several \nshielding films; all values were between 2 µ m and 2.4 µ m. Each given thickness in Fig. \n1e is the average of 10 values measured in different areas. The TEM and HAADF \nimages demonstrate that magnetic Ni particles were dispersed in the continuous Ag \nlayer as shown in Fig. 1a. This desirable structure provides this material with magnetic \nproperties along with good electrical conductivity.   8 \n  9 Figure 1 a  Schema tic of the Ag/Ni blend sintering film. XRD analysis of shielding \nfilms made from b Ni (60 nm) particles and c Ni (560 nm) particles. d Thickness of the \nmetal layer measured by profilometer; inset is the SEM image of the film cross -section. \ne Respective ave rage thickness of the metal layer of shielding films. f TEM image of the \nAg/Ni blend sintering film. g HAADF image of the Ag/Ni blend sintering film . \nExcellent flexibility under highly conductive conditions is an indispensable aspect \nof performance require d for shielding materials in flexible electronics applications. To \nevaluate flexibility, several shielding films were bent to test the change in R□. For each \nshielding film, 0.016 g of the Ag/Ni blend or Ag particles were used, and the metal \nlayer was slig htly larger than 1 cm× 1 cm. All R□ were measured in the bending area. \nFig. 2a and 2b show the change in R□ for the shielding films, indicating that all films \nexhibited outstanding flexibility. The R□ of these shielding films rose slightly during \n70,000 be nding tests, and the variation tendency became stronger thereafter. The R□ \nalso declined with an increase in Ag content and approximate reduction in Ni content. \nNevertheless, the reduction in R□ was inconspicuous as the content of Ag increased \nwhile the we ight ratio of Ag to Ni reached a certain value (1:1 for films made from Ni \n[60 nm] particles or 2:1 for films made from Ni [560 nm] particles); thus, no \nconsiderable effects were caused by the presence of a relatively small amount of Ni on \nthe sintering of  the Ag/Ni blend.   10 \n \nFigure 2  R□ changes in the bending area of shielding films made from a Ni (60 nm) \nparticles and b Ni (560 nm) particles after bending. The left inset shows the start state \nand end state of a bending circle; the right inset shows the min imum radius (~2  mm) of \na bending circle.  \nOwing to the high electrical conductivity of Ag, small thickness of the metal layer, \nand appropriate combination of the metal and substrate, the R□ of the film made from \npure Ag increased gradually to only ~5.40 Ω a fter bending 50,000 times and to ~13.00 \nΩ after bending 100,000 times. Notably, when the weight ratios of Ag to Ni (60 nm) \nwere 1:1, 2:1, 4:1, and 6:1, the Ag/Ni blend sintering films demonstrated similar R□ of \nless than roughly 5.00 Ω after bending 60,000  times and less than 9.00 Ω after bending \n100,000 times. Given the unique composite structure and flexible paper, these Ag/Ni \nblend sintering films were highly flexible and conductive. However, when the weight \nratio of Ag to Ni (60 nm) reached 1:2, the sin tering of the Ag/Ni blend was greatly \naffected by the high amount of Ni. Thus, the R□ of the corresponding shielding film \nwas largest but no higher than 28.00 Ω after bending 70,000 times and 38.00 Ω after \nbending 100,000 times, among all shielding films s hown in Fig. 2a.   11 Similar to the electrical performance of shielding films made from Ni (60 nm) \nparticles, the Ag/Ni blend sintering films at Ag:Ni (560 nm) weight ratios of 2:1, 4:1, \nand 6:1 showed low R□, less than 6.00 Ω, after bending 70,000 times, and 9.00 Ω after \nbending 100,000 times. Yet when the weight ratios of Ag to Ni (560 nm) were 1:2 and \n1:1, the corresponding films had higher R□ compared with other films made from Ni \n(560 nm) particles. The R□ of the 1:2 ratio was less than 50.00 Ω after bendi ng 70,000 \ntimes and 68.00 Ω after bending 100,000 times, whereas that of the 1:1 ratio was less \nthan 19.00 Ω after bending 70,000 times and 26.00 Ω after bending 100,000 times.  \nUsing shielding material to block electromagnetic waves is a common and \neffecti ve way to solve the EMI pollution problem. Generally, reflection (SE R), \nabsorption (SE A), multiple reflection (SE M), and transmission are generated while \nelectromagnetic waves encounter shielding material; the sum of the first three factors is \ncalculated a s the total EMI SE (SE T). The SE A, SE R, and SE T can be calculated by \ncombining the power coefficients of reflection (R), absorption (A), and transmission (T). \nThe corresponding expressions are as follows:  \nM A R T SE SE SE 10logT SE ++=−=\n                                  (1) \nR 10SE 10log (1 R)=− −\n                                             (2) \nR)] [T/(1 10log SE 10 A − −=\n                                          (3) \nThe SE M is generally ignored when SE T is larger than 15.0 dB.  \nFig. 3a and 3b present the EMI SE and R□ of  several shielding films measured in \nthe 8 -12 GHz frequency range. For each shielding film, 0.144 g of Ag or the Ag/Ni \nblend were used; the metal layer was 3 cm×  3 cm. The sample holder of the vector \nnetwork analyzer with shielding film is shown in Fig.. S 1. Electrical conductivity of a \nshielding material is highly significant to EMI SE. The substrate barely shielded the  12 electromagnetic wave due to its non -conductivity, resulting in an EMI SE of \napproximately 0 dB. Conversely, the pure Ag sintering film was  highly conductive with \nan EMI SE greater than 36.0 dB within 8 -12 GHz (the maximum was 43.0 dB at 8 GHz), \nimplying it can block more than 99.97% of electromagnetic waves. The R□ of some \npure Ag sintering films was smaller than 1.84 Ω; The film with an R□ of 1.84 Ω was \nselected for comparison with other Ag/Ni blend sintering films. When the weight ratio \nof Ag to Ni (60 nm) was 2:1, the EMI SE of the Ag/Ni blend sintering film was only 1.0 \ndB smaller than that of the pure Ag sintering film, on average, withi n 8-12 GHz; the R□ \nof the Ag/Ni blend sintering film was 0.82 Ω larger than that of the pure Ag film. \nHowever, when the weight ratio of Ag to Ni (60 nm) was 6:1, a difference in R□ of only \n0.02 Ω emerged between the pure Ag sintering film and the Ag/Ni ble nd sintering film; \nthe EMI SE of the latter was 4.7 dB larger on average than that of the former at 8 -12 \nGHz, proving that the presence of magnetic Ni particles can enhance EMI shielding \nperformance of the films. The 53.5 dB high EMI SE of the Ag/Ni blend sintering film is \nmuch larger than most of flexible metal -based sponge and carbon -based (carbon \nnanotubes, graphene) shielding composites, although its thickness is much smaller. An \nExamples are the silver nanowire wrapped carbon core -shell hybrid sponge ( Ag@C) \nwith 37.9 dB EMI SE at 1 mm, and the graphene/PDMS foam composite with 20 dB \nEMI SE at 1 mm [1, 29]. Generally, it is difficult to form a connected conductive \nnetwork in insulative polymer matrices with low conductive fillers. Thus, a high filler \ncontent and large thickness are necessary to achieve desirable shielding ability. The \nexamples mentioned are well equipped with shielding performance compared with other \nsame -kind material but still poor in shielding ability compared with the experimental \nshielding film with a continuous silver layer and magnetic Ni particles. In addition,  13 fabricating these two materials is challenging; the former needs chemical vapor \ndeposition and the latter needs annealing at 1000 ° C in an argon atmosphere.   14 \n  15 Figure 3  EMI S E of shielding films made from a Ni (60 nm) particles and b Ni (560 \nnm) particles measured in the 8 -12 GHz frequency range. Comparison of SE T, SE R, and \nSEA of films made from c Ni (60 nm) particles and  d Ni (560 nm) particles at 8 GHz. \nMagnetic properties of metal layer of films made from e Ni (60 nm) particles and f Ni \n(560 nm) particles. Inset images are amplified views of magnetization vs. magnetic field. \ng EMI SE of shielding films measured in the 13 -18 GHz frequency range.  \nCombining Fig. 3a and 3b, the  film made from Ni (560 nm) particles showed lower \nR□ and higher EMI SE compared with that made from Ni (60 nm) particles when the \ntwo films shared the same weight ratio of Ag to Ni. For example, at an Ag to Ni weight \nratio of 6:1, the EMI SE of the film m ade from Ni (560 nm) particles was 53.5 dB at 8 \nGHz, whereas that of the film made from Ni (60 nm) particles was 46.6 dB at 8 GHz. \nAs mentioned earlier, the R□ of Ag/Ni blend sintering films increased as the weight \nratio of Ni to Ag increased. Unsurprising ly, the shielding film with a 2:1 weight ratio of \nAg to Ni (560 nm) had a R□ of 1.56 Ω and an EMI SE of 50.9 dB at 8 GHz, whereas the \nfilm with a 1:2 weight ratio of Ag to Ni (560 nm) exhibited 8.13 Ω and 37.7 dB at 8 \nGHz, respectively.  \nThe thickness of a shielding material significantly influences its EMI SE (Note 1 in \nAppendix). To determine the shielding performance of a material more realistically, it is \nreasonable to use the SSE/t as an evaluation criterion, wherein the specific EMI \nshielding effective ness (SSE) is divided by the thickness of the shielding material to \nyield a normalized value. Surprisingly, when the weight ratio of Ag to Ni (560 nm) was \n6:1, the metal layer of the Ag/Ni blend sintering film had a high SSE/t of 65,224 dB cm2 \ng-1, much la rger than that of copper foil (7,812 dB cm2 g-1), aluminum foil (30,555 dB \ncm2 g-1), and a Ti 3C2Tx MXene film (30,830 dB cm2 g-1) [2, 29].   16 Shielding of an electromagnetic wave primarily originates from reflection and \nabsorption mechanisms; reflection resul ts from a mismatch between the absorber and air, \nwhereas absorption occurs from ohmic loss, polarization loss, and magnetic loss [3, 25]. \nTo further analyze the shielding mechanism in EMI SE results, SE R and SE A were \ncalculated. Fig. 3c and 3d show the SE R, SE A, and SE T of several shielding films. All \nfilms exhibited similar SE R values but different SE A and SE T values at 8 GHz. Clearly, \nSEA and SE T each increased with a decline in R□, while SE A remained generally stable. \nOwing to the contribution of magneti c Ni particles, the Ag/Ni blend sintering film with \na weight ratio of Ag to Ni (60 nm) of 2:1 showed a larger SE A than the pure Ag \nsintering film; however, the R□ of these two films was approximately the same. For any \nshielding film in Fig. 3 c or 3d, SE A was much larger than SE R, implying that absorption \ncontributed more to EMI SE compared to reflection. Considering the damage to the \nreflected portion of electromagnetic waves on the surroundings, this absorption -\ndominant EMI shielding material is preferred in areas that require EMI shielding and \ngenerate electromagnetic radiation [1].  \nFig. 3e and 3f illustrate magnetic properties of the shielding films. Similar to the \nchange in R□, the remnant magnetization (Mr) and saturation magnetization (Ms) of \nmetal lay ers made from the same Ni particles increased with an increase in Ni content. \nFor instance, the R□, Mr, and Ms of the metal layer with a weight ratio of Ag to Ni (60 \nnm) of 6:1 were 1.82 Ω, 0.25 emu g-1, and 3.01 emu g-1, respectively, whereas those of \nthe metal layer with a weight ratio of Ag to Ni (60 nm) of 2:1 were 2.66 Ω, 0.73 emu g-1, \nand 10.25 emu g-1. Interestingly, a coercivity (Hc) as high as 94.0 Oe was found in films \nmade from Ni (60 nm) particles, but the Hc of films made from Ni (560 nm) parti cles \nwith Ag:Ni weight ratios of 1:2, 2:1, and 6:1 were 97.4 Oe, 34.1 Oe, and 34.1 Oe,  17 respectively. This difference is primarily due to different Ni particle sizes [30]. \nAlthough distinct magnetic properties were identified in the films made from Ni (60 a nd \n560 nm), the differences in R□ primarily distinguished the EMI SE of these films.  \nGenerally, the problem of EMI SE declining in the high -frequency band due to \nstronger penetration of high -frequency electromagnetic waves poses challenges to many \nshieldin g materials [31]. Similar results also occurred in these experiments. To further \nevaluate changes in the shielding effect at different frequencies, our shielding films \nwere measured at 8 -12 and 13 -18 GHz frequency ranges. Fig. G displays the EMI SE of \nshielding films in the 13 -18 GHz frequency range. For the film with an Ag:Ni (560 nm) \nweight ratio of 6:1, the EMI SE was larger than 45.8 dB in the 13 -18 GHz frequency \nrange. With the aid of magnetic Ni (60 nm) particles, the EMI SE of the Ag/Ni blend \nsinteri ng film with a weight ratio of Ag to Ni of 6:1 was ~5.0 dB higher than that of the \npure Ag sintering film. The shielding performance of these films generally decreased \nbut remained satisfactory for applications in many shielding fields as the frequency \nincreased.  \nStable shielding performance under mechanical deformation is urgently needed for \nshielding material applications in flexible electronics. Fig. 4a depicts changes in R□ \nafter repeated bending. The metal layer of each film measured ~3 cm× 3 cm. The in itial \nR□ measured in the bending area was lower than the average R□ of the shielding film. \nAs a main factor affecting EMI SE, the R□ of the pure Ag sintering film increased by \n2.96 Ω, whereas that of the Ag/Ni blend sintering film only increased by 1.20 Ω.  Fig. 4b \nshows the change in EMI SE after repeated bending. Encouragingly, after bending \n200,000 times, the EMI SE of the pure Ag sintering film and Ag/Ni blend sintering film \nonly decreased by 11.7% and 4.5% at 8.1 GHz, respectively. Owing to the  18 reinforc ement of Ni particles, the Ag/Ni blend sintering film with a composite structure \nexhibited more stable shielding and electrical performance than the pure Ag sintering \nfilm [32]. These films exhibit much better flexibility than popular flexible graphene \ncomposites. The representative composites, as mentioned before, are a \ngraphene/polymer foam composite and a NR/Fe 3O4@rGO composite [1, 3]. EMI SE of \nthe former only showed a slight decrease after 10,000 times bending to a radius of 2.5 \nmm while the latter cou ld withstand 2000 times bending at an angle of 60° . However, \nhigher degree of flexibility is needed for practical applications in flexible electronics. \nHigh loading of conductive fillers, weak connection of conductive networks, large \nthickness of shielding  material, and poor binding between fillers and matrix still \nchallenge many traditional flexible shielding materials [1, 3, 33] . For the studied \nshielding film, a desirable EMI SE (above 46.7 dB within 8 -12 GHz), low density (1.39 \ncm3 g-1), small low thick ness (thickness of the metal layer was ~2 µ m and that of the \ntotal film was ~100 µ m, although the paper can be made much thinner than 100 µ m), \nexcellent flexibility (capable of undergoing 200,000 bending tests), and simple \nprocessing (sintering Ag/Ni blend  on papers at 140 ℃) of this shielding film fully \nsatisfy the needs of commercial applications.  \n  19 Figure 4 a R□ change in bending area of shielding films after bending. b EMI SE \nchange of shielding films at 8.1 GHz after bending (bending radius: ~2 mm).  \n4. Conclusions  \nIn summary, we developed a flexible EMI shielding film with a unique composite \nstructure using a simple fabrication method that directly sinters Ag/Ni blend onto paper. \nThe ~2 -µm-thick metal layer endowed the shielding material with a high EMI  SE of \nmore than 46.7 dB within the 8 -12 GHz frequency range and a small R□ of 0.78 Ω. As \nthe content of Ag declined, R□ increased, leading to a reduction in EMI SE. After \nbending to a radius of ~2 mm 200,000 times, the shielding film with an Ag:Ni (60 nm)  \nweight ratio of 6:1 only exhibited a 4.5% loss in EMI SE and an increase of 1.20 Ω in \nR□. Such stable shielding performance under mechanical deformation combined with \nsimple processing and small thickness provide this material strong application potential  \nin next -generation flexible electronics.  \nConflicts of interest  \nThere are no conflicts to declare.  \nAcknowledgements  \nWe thank Guojian Cao and Guohua Fan for the TEM measurements and Sixia Hu \nin Southern University of Science and Technology for the measureme nts on magnetic \nproperties. This work was supported  by Shenzhen Science and Technology Program \n(Grant No. KQTD 20170809110344233, Grant No. JCYJ20170811160129498 ) and \nBureau of Industry and Information Technology of Shenzhen through the Graphene \nManufactur ing Innovation Center (201901161514 ). Z.C. acknowledges the Natural  20 Science Foundation of China (No.51771226). X.L. acknowledges the Natural Science \nFoundation of China (No. 11672090).  \nAuthor contribution  \nW.Z. designed the experiments. X.L., Z.Y. L.Z. made the paper -based devices with \nthe help of W.Z., P.H. , J.W. . X.L., Z.Y., P.F., J.S., M.Z. did the measurements on \nflexibility properties with the help of W.Z., H.J.  and M.L.. Z.Y. performed \nmeasurements of the electromagnetic interference shielding  with the help of L.C. . Z.C. \nmade the Ni nano particles. Z.Y. , M.L. and W.Z. wrote the Manuscript. All the authors \nparticipated in the analysis of the data and the preparation of the final manuscript.   21 Notes and references  \n1 Chen Z P, Xu C , Ma C Q, Ren W C, Cheng  HM (2013) Lightweight and flexible \ngraphene foam composites for high -performance electromagnetic interference shielding. \nAdv Mater 25:1296 -1300  \n2 Shahzad F, Alhabeb M, Hatter CB, Anasori B, Man Hong S, Koo CM , Gogotsi Y \n(2016) Electromagnetic interference shiel ding with 2D transition metal carbides \n(MXenes). Science 353:1137 -1140  \n3 Zhan YH, Wang J, Zhang KY et al (2018) Fabrication of a flexible electromagnetic \ninterference shielding Fe 3O4@reduced graphene oxide/natural rubber composite with \nsegregated network. Chem Eng J 344:184 -193 \n4 Yan DX , Pang H , Li B, Vajtai R , Xu L , Ren PG , Wang JH , Li ZM  (2015) Structured \nreduced graphene oxide/ polymer composites for ultra-efficient electromagnetic \ninterference shielding . Adv Funct Mater 25:559 -566 \n5 Yousefi N, Sun XY, Li n XY et al (2014) H ighly aligned graphene/ polymer \nnanocomposites with excellent dielectric properties for high-performance \nelectromagnetic interference shielding . Adv Mater 26:5480 -5487  \n6 Zhang Y, Huang Y, Zhang TF, Chang HC, Xiao PS, Chen HH, Huang ZY, Ch en YS \n(2015) Broadband and tunable high-performance microwave absorption of an ultralight \nand highly compressible graphene foam. Adv Mater 27:2049 -2053  \n7 Frey AH (1998)  Headaches from cellular telephones: are they real and what are the \nimplications? Enviro n Health Perspect 106:101 -103 \n8 Yang YL, Gupta MC, Dudley KL (2005) Lawrence RW, Novel carbon nanotube -\npolystyrene foam composites for electromagnetic interference shielding. Nano Lett \n5:2131 -2134   22 9 Chung DDL (2001) Electromagnetic interference shielding e ffectiveness of carbon \nmaterials. Carbon 39:279 -285 \n10 Geetha S, Satheesh Kumar KK, Rao CRK, Vijayan M, Trivedi DC (2009) EMI \nshielding: Methods and materials —A review. J Appl Polym Sci 112:2073 -2086  \n11 Zhang HB, Yan Q, Zheng WG, He ZX; Yu ZZ (2011) Tough graphene -polymer \nmicrocellular foams for electromagnetic interference shielding. ACS Appl Mater Inter \n3:918 -924 \n12. Eswaraiah V, Sankaranarayanan V, Ramaprabhu S (2011) Functionalized graphene -\nPVDF foam composites for EMI shielding . Mater Eng 296:894 -898 \n13 Fletcher A, Gupta MC, Dudley KL, Vedeler E (2010) Elastomer foam \nnanocomposites for electromagnetic dissipation and shielding applications. Compos Sci \nTechnol 70:953 -958 \n14 Thomassin JM, Pagnoulle C, Bednarz L, Huynen I, Jerome R, Detrembleur C (2008) \nFoams of polycaprolactone/MWNT nanocomposites for efficient EMI reduction. J \nMater Chem 18:792 -796 \n15 Li N, Huang Y, Du F et al (2006) Electromagnetic interference (EMI) shielding of \nsingle -walled carbon nanotube epoxy composites. Nano Lett 6:1141 -1145  \n16 Li ang JJ, Wang Y, Huang Y et al (2009), Electromagnetic interference shielding of \ngraphene/epoxy composites. Carbon 47:922 -925 \n17 Liu ZF, Bai G, Huang Y, Ma YF, Du F, Li FF, Guo TY, Chen YS (2007) Reflection \nand absorption contributions to the electromagneti c interference shielding of single -\nwalled carbon nanotube/polyurethane composites. Carbon 45:821 -827  23 18 Wang LL, Tay BK, See KY, Sun Z, Tan LK, Lua D (2009) Electromagnetic \ninterference shielding effectiveness of carbon -based materials prepared by screen \nprinting. Carbon 47:1905 -1910  \n19 Yang Y, Gupta MC, Dudley KL, Lawrence RW (2005) Conductive carbon \nnanofiber -polymer foam structures. Adv Mater 17:1999 -2003  \n20 Kwon S, Ma RJ, Kim U, Choi HR, Baik S (2014) Flexible electromagnetic \ninterference shields made o f silver flakes, carbon nanotubes and nitrile butadiene rubber. \nCarbon 68:118 -124 \n21 Liu J , Zhang HB , Sun R H, Liu, Y F, Liu Z S, Zhou A G, Yu ZZ  (2017) Hydrophobic, \nFlexible, and lightweight MXene foams for high -performance electromagnetic -\ninterference shield ing. Adv Mater 29:1702367  \n22 Ghosh S, Ganguly S, Das P, Das T, Bose M, Singha N, Das A, Das N (2019) \nFabrication of reduced graphene oxide/silver nanoparticles decorated  conductive cotton \nfabric for high performing electromagnetic interference shielding an d antibacterial \napplication. Fibers and Polymers 20:1161 -1171  \n23 Wen BY, Wang XJ, Zhang Y (2019) Ultrathin and anisotropic polyvinyl butyral/Ni -\ngraphite/short -cut carbon  fibre film with high electromagnetic shielding performance.  \nComposites Science and Tec hnology 169:127 -134 \n24 Zeng SP, Li XP, Li MJ (2019) Flexible PVDF/CNTs/Ni@CNTs composite films \npossessing excellent electromagnetic interference shielding and mechanical properties \nunder heat treatment. Carbon 155:34 -43 \n25 Sharif F, Arjmand M, Moud AA, Sun dararaj U, Roberts EPL (2017) Segregated \nhybrid poly(methyl methacrylate)/graphene/magnetite nanocomposites for \nelectromagnetic interference shielding. ACS Appl Mater Interfaces 9:14171 -14179   24 26 Li T, Suo Z (2007) Ductility of thin metal films on polymer s ubstrates modulated by \ninterfacial adhesion. International Journal of Solids and Structures 44:1696 -1705  \n27 Xiang Y, Li T, Suo Z, Vlassak JJ (2005) High ductility of a metal film adherent on a \npolymer substrate. Appl Phys Lett 87:161910  \n28 Zhang L, Feng PD , Xie SP et al (2019) Low -temperature sintering of silver  \nnanoparticles on paper by surface  modification. Nanotechnology 30:505303  \n29 Wan YJ , Zhu PL , Yu SH , Sun R , Wong CP  (2018)  Liao WH , Anticorrosive, \nultralight, and flexible carbon -wrapped metallic nano wire hybrid sponges for highly \nefficient electromagnetic interference shielding. Small 14:1800534  \n30 D’addato S, Gragnaniello L, Valeri S, Rota A, Di Bona A, Spizzo F, Panozaqi T, \nSchifano SF (2010) Morphology and magnetic properties of size -selected Ni \nnanoparticle films. J Appl Phys 107:104318  \n31 Oh H, Dao V, Choi H (2018) Electromagnetic shielding effectiveness of a thin silver \nlayer deposited onto PET film via atmospheric pressure plasma reduction. Appl Surf Sci \n435:7 -15 \n32 Ramakrishnan N (1996) An anal ytical study on strengthening of particulate \nreinforced metal matrix composites. Acta Mater 44:69 -77 \n33 Das NC, Liu YY, Yang KK, Peng WQ (2009) Maiti S, Wang H, Single -walled \ncarbon nanotube/poly(methyl methacrylate) composites for electromagnetic interfer ence \nshielding. Polym Eng Sci 49:1627 -1634  ",
      "metadata": {
        "filename": "Highly flexible electromagnetic interference shielding films based on ultrathin.pdf",
        "hotspot_name": "Steel_Punching_and_Bending",
        "title": "Highly flexible electromagnetic interference shielding films based on\n  ultrathin Ni/Ag composites on paper substrates",
        "published_date": "2020-05-11T06:09:53Z",
        "pdf_link": "http://arxiv.org/pdf/2005.04875v1",
        "query": "punching and bending steel EMC shield material substitution studies resource reduction percentages"
      }
    },
    "Intelligent data collection for network discrimination in material flow analysis": {
      "full_text": "Intelligent data collection for network discrimination in material flow\nanalysis using Bayesian optimal experimental design\nJiankan Liao∗, Xun Huan†, and Daniel Cooper‡\nApril 21, 2025\nAbstract\nMaterial flow analyses (MFAs) are powerful tools for highlighting resource efficiency oppor-\ntunities in supply chains. MFAs are often represented as directed graphs, with nodes denoting\nprocesses and edges representing mass flows. However, network structure uncertainty–uncertainty\nin the presence or absence of flows between nodes–is common and can compromise flow predic-\ntions. While collection of more MFA data can reduce network structure uncertainty, an intelligent\ndata acquisition strategy is crucial to optimize the resources (person-hours and money spent on\ncollecting and purchasing data) invested in constructing an MFA. In this study, we apply Bayesian\noptimal experimental design (BOED), based on the Kullback–Leibler divergence, to efficiently tar-\nget high-utility MFA data—data that minimizes network structure uncertainty. We introduce a\nnew method with reduced bias for estimating expected utility, demonstrating its superior accuracy\nover traditional approaches. We illustrate these advances with a case study on the U.S. steel sector\nMFA, where the expected utility of collecting specific single pieces of steel mass flow data aligns\nwith the actual reduction in network structure uncertainty achieved by collecting said data from\nthe United States Geological Survey and the World Steel Association. The results highlight that\nthe optimal MFA data to collect depends on the total amount of data being gathered, making it\nsensitive to the scale of the data collection effort. Overall, our methods support intelligent data\nacquisition strategies, accelerating uncertainty reduction in MFAs and enhancing their utility for\nimpact quantification and informed decision-making.\nKeywords: Material Flow Analysis, Network structure, Bayesian Inference, Model Discrimi-\nnation, Bayesian Optimal Experimental Design, Kullback-Leibler Divergence\n1 Introduction\nMaterial flow analysis (MFA) is a fundamental tool in industrial ecology research for tracing the\nmovement and transformation of resources across a supply chain. The result of an MFA is often\nrepresented as a directed graph, where nodes represent processes, materials, or locations, and edges\nindicate the mass flows of material. As noted by Cullen and Cooper [1], MFAs are crucial for assessing\nthe potential environmental impacts of resource efficiency, as opportunities for improving efficiency\nare spread across the supply chain.\nA key challenge in constructing an MFA is the quantity and quality of available data [2–4], as MFA\ndata are often:\n∗jkliao@umich.edu, Ph.D. student, Mechanical Engineering, University of Michigan, Ann Arbor, MI 48109.\n†xhuan@umich.edu, Associate Professor, Mechanical Engineering, University of Michigan, Ann Arbor, MI 48109.\nhttps://uq.engin.umich.edu\n‡Corresponding author: drcooper@umich.edu, Associate Professor, Mechanical Engineering, University of Michigan,\nAnn Arbor, MI 48109.\n1arXiv:2504.13382v1  [stat.AP]  18 Apr 20251.sparse , leading to data gaps or questionable imputation from other time periods, geographies,\nor processes/supply chains;\n2.noisy , due to measurement, recording, or interpretation error; and\n3.ill-defined , regarding the specific section of the supply chain to which they pertain.\nData sparsity often requires sourcing information from multiple places, further exacerbating issues\nwith ill-defined boundaries and increasing the time and cost of constructing the MFA [4]. These data\nchallenges introduce significant epistemic uncertainty into the final MFA results due to uncertainty not\nonly on individual mass flows between processes but also the structure of the supply chain network;\ni.e., the nodes and edges that define the directed graph. Uncertainty quantification (UQ) of final MFA\nresults is gaining recognition as a critical component for ensuring study transparency and supporting\ninformed decision-making based on MFA [1, 2, 4]. Ideally, however, the MFA practitioner should\nhave tools for not only quantifying uncertainty but also targeting data collection that can most\nefficiently reduce the uncertainty so as to allow greater confidence in the resulting metrics (e.g., recycled\ncontent) or decisions (e.g., allocation of research and development funding) informed by MFA.\n1.1 Review: Quantifying uncertainty in MFA\nConsider a mass flow, z, between two processes. We represent the uncertainty of this flow with a\nprobability density function (PDF), p(z). The overall uncertainty indicated by p(z) can be decomposed\ninto an MFA parametric uncertainty component (via p(z|Mm) under a fixed network structure Mm\nwhere mindexes the nMdifferent candidate network structures being considered) and a network\nstructure uncertainty component (via p(Mm)) [5]:\np(z) =nMX\nm=1p(z|Mm)p(Mm). (1)\nBelow, we review the literature on quantifying both the parametric and network structure uncertainty.\nMFA parametric uncertainty Addressing uncertainty in MFA results has gained increasing at-\ntention in recent years [6], leading to the development of various methods to quantify it. These\nmethods can be grouped under three banners: 1) forward error propagation [7–9], 2) Bayesian infer-\nence [5,10–13], and 3) determining the fit of the mass-balanced results to the data collected [2,14].\nIn forward error propagation, as implemented in the widely-used STAN open-source MFA package\n[8], collected MFA data are represented by PDFs, with at most one PDF assigned to each MFA\nparameter. The final mass flow uncertainty is then determined by applying mass balance constraints\nat each node [7,9,15,16].\nBayesian inference updates prior beliefs about MFA parameter values into posterior distributions\nby incorporating additional collected data, thereby reducing uncertainty. This updating process follows\nthe rules of conditional probability [17–21] and is particularly effective for handling sparse and noisy\ndata. It allows the incorporation of expert domain knowledge through priors and facilitates the\nintegration of multiple data sources. Several studies have applied Bayesian inference for UQ in MFA.\nGottschalk et al. [10] first used this approach to quantify the uncertainty of nano-TiO2 particle releases\ninto the environment in Switzerland. They introduced transfer coefficients (allocation fractions) to\nmodel an MFA network as a linear system using matrix algebra. Later, Lupton and Allwood [11]\nemployed a multivariate Dirichlet distribution to model the transfer coefficients emanating from a\nnode, ensuring mass balance. They demonstrated their approach using global steel data for 2008.\nDong et al. [12] incorporated expert elicitation and data noise learning, deriving informed Dirichlet\n2priors and modeling noise as a random variable. Wang et al. [13] took a different approach, instead\nassigning priors directly on mass flows and imposing mass balance constraints via a penalty in the\nlikelihood function, which they applied to case studies on aluminum and zinc systems.\nBoth the forward error propagation and Bayesian inference methods require MFA data to be\nmodeled using PDFs. However, many data sources do not provide the necessary information to directly\nderive PDFs—for example, the U.N. Comtrade database’s trade statistics [22] are not published with\naccompanying error bars or PDFs alongside the mass flow values. Alternatively, uncertainty in MFA\ncan be indicated using (non)linear least squares optimization to enforce mass balance across the system,\nwhere the residuals between collected and reconciled data then serve as an indicator of how well the\nbalanced MFA mass flow results align with the original data [2,14]. However, the size of these residuals\nis at best indicative of uncertainty and should not be interpreted as a precise measure of mass flow\nuncertainty.\nMFA network structure uncertainty As discussed by Liao et al. [5], network structure uncer-\ntainty can arise from an MFA analyst’s lack of subject matter expertise, outdated representations of\nthe supply chain, or reliance on data from different regions that may not be applicable to the region\nof interest [23]. Despite the ubiquitous presence of network structure uncertainty in MFA, most of the\nliterature on MFA uncertainty has focused on quantifying mass flows (parametric) uncertainty under\na fixed network structure, without considering structural uncertainty.\nA few studies have explored the effects of MFA network structure. Schwab and Rechberger defined\nan MFA system complexity metric to describe how well the supply chain is known (0–100%) depending\non the uncertainty of the data collected and the number of nodes and edges in the network [4].\nChatterjee et al. evaluated MFA network structures using common graph-theoretic metrics as well as\nthe concept of a “window of vitality” from the ecology field to assess the trade-off between supply chain\nresiliency and efficiency [24]. Elsewhere, Anspach et al. ’s work touched on MFA network structure\nuncertainty, noting that significant discrepancies between reconciled (mass-balanced) MFA results and\nthe original data may indicate network structure errors [9]. However, to the authors’ knowledge, only\nLiao et al. conducted formal UQ of MFA network structures [5]. They used Bayes’ rule to derive the\nprobability of different network structure candidates being the true underlying network structure as\nsupported by the collected MFA data, demonstrating the technique with a case study on the U.S. steel\nsector.\n1.2 Review: Collecting MFA data\nThe literature on MFA data collection focuses on acknowledging and addressing key challenges such\nas data sparsity, noise, and ill-defined parameters. As Graedel [3] expressed, these challenges have\ntransformed the MFA practitioner into “. . . part detective, part archivist, part extractor of informa-\ntion from experts, and part bold estimator.” The sparsity of available data often means that MFA\npractitioners cannot rely solely on measurements from scientific instruments, with Brunner and Rech-\nberger [25] categorizing data collection methods into direct (e.g., through measurements) and indirect\n(e.g., inferred from expert estimates). While recognizing the challenges of comprehensive data collec-\ntion, Khlifa et al. [26] emphasized the importance of collecting data on as many inflows, outflows, and\nintermediate flows as possible, while highlighting the significant effort required to filter irrelevant data\nand standardize the remaining information.\nAlthough MFA data challenges typically require discussion and analysis of uncertainty to prop-\nerly contextualize the results [3], there has been limited publication on formal methods to reduce\nthis uncertainty. One approach to reducing MFA uncertainty is through the collection of more data.\nTraditionally, data collection prioritization in MFA has been driven by intuition, with heuristics such\nas collecting all available data within resource constraints, or focusing on large mass flows that lack\n3previous MFA estimates. While these approaches may often be adequate, there is an opportunity to\nenhance data acquisition based on rigorous probability and statistical frameworks. A more sophisti-\ncated, targeted approach could reduce the resources (person-hours and costs) needed to generate an\nMFA with sufficient certainty, ultimately enabling more informed and confident decision-making.\nUnder the Bayesian framework, data collection can be optimized using Bayesian optimal experi-\nmental design (BOED) [27–32]. BOED formulates data collection as an optimization problem, where\nthe best design option maximizes the expected utility—a measure of the experiment’s value. Lind-\nley [33] introduced mutual information between model parameters and observed data (equivalently,\nthe expected information gain in the parameters) as the expected utility, quantifying the reduction\nin uncertainty from prior to posterior. Liao et al. [34] recently applied BOED to MFA, identifying\nhigh-utility data sources to reduce parametric uncertainty under a fixed network structure. However,\ntheir study did not address the network structure uncertainty, which also contributes to the overall\nmass flow uncertainty (Equation (1)), and is the focus of this article.\nExtending BOED to optimize data collection for reducing network structure uncertainty is a logical\nnext step. Outside industrial ecology, BOED has been applied to model discrimination (e.g., [35–40]),\nwhich is related to network structure discrimination in MFA. However, few of these studies explicitly\nestimated the expected utility based on mutual information. McGree et al. [37] and Drovandi et al. [38]\nused sequential Monte Carlo to characterize parameter posteriors and subsequently evaluate the model\nevidence (i.e., marginal likelihood). Aggarwal et al. [39] demonstrated BOED for model discrimination\non a simple material science example where it was possible to extract the expected utility analytically.\nIn this work, we present general numerical estimators of mutual information for model discrimination\nusing nested Monte Carlo that can accommodate more complex, nonlinear models.\n1.3 Scope of this paper\nIn light of the review, this study aims to define and demonstrate an intelligent, targeted MFA data\ncollection strategy for the rapid reduction of network structure uncertainty. The key contributions\nof this work are: 1) applying BOED to optimize MFA data collection for reducing network structure\nuncertainty; 2) deriving and evaluating three numerical estimators using nested Monte Carlo sam-\npling and solving the data collection optimization problem without the characterization of parameter\nposteriors; and 3) demonstrating the new framework in a case study on the U.S. steel sector. We\ndemonstrate that the framework can effectively target the collection of single or multiple data points.\nThe paper is structured as follows. Section 2 introduces the MFA problem formulation, the BOED\nframework, and the derivation of three numerical estimators for the model discrimination mutual\ninformation. A simple MFA example is provided to facilitate understanding. Section 3 applies the\nframework in a case study on the 2012 U.S. steel industry. Section 4 discusses the trade-offs be-\ntween collecting single versus multiple data points, evaluates estimator performance, and outlines the\napproach’s limitations.\n2 Methodology\nThe goal of intelligent data collection through BOED is to identify the data sources with the greatest\npotential to reduce uncertainty. To quantify this “potential”, we first introduce the Bayesian UQ\nframework. Let Mmrepresent the mth model (e.g., an MFA network structure) from a set of nM\nmodels, θmdenote the parameters of that model (e.g., MFA transfer coefficients), ythe collected data,\nandξthe data collection option choice (i.e., design). Following Bayes’ theorem, once yis obtained\nfrom ξ, the uncertainty in θmandMmcan be updated from their prior distributions to their posterior\n4distributions:\n(Parameter Bayes’ rule) p(θm|y, M m, ξ) =p(y|θm, Mm, ξ)p(θm|Mm)\np(y|Mm, ξ), (2)\n(Model Bayes’ rule) p(Mm|y, ξ) =p(y|Mm, ξ)p(Mm)\np(y, ξ). (3)\nWe refer to Equation (2) as the parameter Bayes’ rule, where p(θm|y, M m, ξ) is the parameter pos-\nterior, p(θm|Mm) is the parameter prior (we assume prior does not depend on the pending design\nξ),p(y|θm, Mm, ξ) is the parameter likelihood, and p(y|Mm, ξ) is the model evidence (i.e., marginal\nlikelihood). Similarly, we call Equation (3) the model Bayes’ rule, where p(Mm|y, ξ) is the model\nposterior, p(Mm) is the model prior (similarly independent of ξ), and p(y|Mm, ξ) (the model evidence)\nalso appears in both equations. Notably, model evidence serves as the likelihood in Equation (3) and\nas the denominator to normalize the posterior distribution in Equation (2).\nWith these terms established, we now define the general expected utility Ufor collecting data from\na source (i.e., design) ξ:\nU(ξ) =Ey,θm,m|ξ[u(ξ, y, θ m, Mm)], (4)\nwhere u(ξ, y, θ m, Mm) is a utility function that quantifies the value of data collection under design\nchoice ξ, given that the model is Mm, the parameters are θm, and the observation data is y. Since y,\nθm, and Mmare unknown at the time of designing the experiment, we take an expectation over their\njoint distribution. Note that depending on the data collection options, ξmay represent a single piece\nof data from a single source, or multiple pieces of data from the same or multiple sources.\nFollowing an information-theoretic formulation, we define our utility function uas the Kullback–\nLeibler (KL) divergence from the model (network structure) prior to its posterior. The KL divergence\nquantifies the change in distribution, where a larger value indicates a greater update in belief, and\nthus a more informative experiment. With this choice, the expected utility Uis given by:\nU(ξ) =Ey|ξ\u0002\nDKL(pMm|y,ξ||pMm)\u0003\n=Z\np(y|ξ)nMX\nm=1p(Mm|y, ξ) logp(Mm|y, ξ)\np(Mm)dy, (5)\nwhich is equal to the mutual information between the network structure Mmand the observed data\ny.\nFinally, the BOED problem is to find the optimal data collection option ξ∗that maximizes the\nexpected utility:\nξ∗= arg max\nξ∈ΞU(ξ), (6)\nwhere Ξ is the set of all feasible design choices (MFA data collection options).\nSolving the BOED problem requires evaluating and optimizing the expected utility U. However, U\ngenerally lacks a closed-form solution and must be estimated numerically, requiring the computation\nof MFA flows under various network structures and parameter settings. To establish this procedure,\nwe first represent a MFA network as a linear system in Section 2.1. We then show how to quantify\nMFA parametric uncertainty under a fixed network structure in Section 2.2 and how to quantify the\nMFA network structure uncertainty in Section 2.3. Finally, in Section 2.4, we derive three different\nMonte Carlo sampling methods to numerically estimate the BOED expected utility in Equation (5).\nFigure 1 illustrates the overall BOED framework with a simple MFA example, providing a visual aid\nto the methodology.\n5Figure 1: BOED procedure for the 9-node example MFA model. Note: Dir(·) represents a Dirichlet distribution.\nUnderlying data are available at data repository: https://doi.org/10.7302/k35m-xz34\n62.1 Modeling the MFA as a linear system\nWe depict the MFA network as a directed graph where different processes, products and locations are\nrepresented as nodes (indexed 1 ,2, . . . , n p) and mass flows between nodes are represented as directed\nedges. Conservation of mass requires that at each node, the total input mass of the material flows equals\nthe total output mass of the material flows, assuming stock accumulation or depletion is negligible.\nWe use xito denote the total input (equivalently, total output) flow for node i. The mass flow from\nnode itojis then expressed as zij=ϕijxi, where ϕij∈[0,1] is the allocation fraction of node i’s total\noutflow into node j(ϕij= 0 if no flow is present from node itoj). Therefore,\nnpX\ni=1ϕijxi+qj=xj andnpX\nj=1ϕij= 1. (7)\nApplying allocation fractions ( ϕij) as model parameters instead of the direct mass flow values provides\na convenient method of expressing and assembling the mass balance relationships for the entire MFA\ninto a linear system, as explained by Gottchalk et al. [10]. For example, the mass balance equations\nfor the MFA model 1 ( M1) illustrated in Figure 1 (top) can be written as:\n\n1 0 0 0 0 0 0 0 0\n−ϕ12 1 0 0 0 0 0 0 0\n0−ϕ23 1 0 0 0 0 0 0\n0 0 −ϕ341 0 0 0 0 0\n0 0 −ϕ350 1 0 0 −ϕ850\n0 0 0 0 0 1 0 0 0\n0 0 0 0 0 −ϕ67 1 0 0\n0−ϕ28 0 0 0 0 −ϕ78 1 0\n0 0 0 0 1 0 0 −ϕ891\n\n| {z }\nI−Φ⊤\nx1\nx2\nx3\nx4\nx5\nx6\nx7\nx8\nx9\n\n|{z}\nx=\nq1\n0\n0\n0\n0\nq6\n0\n0\n0\n\n|{z}\nq, (8)\nwhere Iis the np×npidentity matrix and Φ ∈Rnp×npis the adjacency matrix with entries representing\nthe allocation fractions ϕij.x∈Rnpandq∈Rnpare column vectors representing all nodal mass flows\nand external inflows to the network (see Figure 1, top), respectively. The material inflows ( qi) are\nfrom outside the network; e.g., aluminum imports in a country-level aluminum MFA.\nWith Φ and qestablished, all nodal mass flows xcan be solved via:\nx= (I−Φ⊤)−1q. (9)\nThe transformation of the allocation fraction matrix ( I−Φ)−1is also known as the Ghosh inverse [41],\nserving as a supply-driven alternative to the more common demand-driven input/output (I/O) analy-\nsis [42]. Other common MFA quantities of interest (QoIs) can be derived from the values for x,Φ and\nq; e.g., mass flows for each connection, zij=ϕijxi. We represent these QoIs through a vector-valued\nfunction, G(Φ, q, ξ;Mm), which typically corresponds to the same quantities as those in the collected\nMFA data, y.\nFor each network structure Mm, we designate θm={ϕij, qi|ϕij̸= const. , qi̸= const. under Mm}to\ndescribe the set of all uncertain model parameters (i.e., of existing connections and external inflows)\nunder said network structure. The predicted QoIs can then be written as G(θm, ξ;Mm). As an\nexample, in Figure 1 (top), mass flow z73is not present in the network structure M1but is in M2,\nthen ϕ73is a trivial parameter always equal to zero and therefore not included in θm=1, but would be\na non-trivial parameter and therefore part of θm=2. Hence, the composition of θmvaries depending\non the network structure Mm, and the subscript mconcisely captures this distinction.\n72.2 Parametric uncertainty under a fixed network structure\nWe now outline our specific prior and likelihood setup in the parameter Bayes’ rule (Equation (2))\nfor capturing parametric uncertainty. To ensure mass balance constraints are automatically satisfied,\nwe assign Dirichlet priors to the allocation fractions ϕ. For mass flow inputs q, we use truncated\nnormal priors with a non-negative lower bound. Prior hyper-parameters can be determined through\nexpert elicitation [12], leveraging domain expertise to reduce the subsequent volume of data that must\nbe collected to reach a desired reduction in uncertainty. Alternatively, they can be specified using\nhistorical data or by adopting non-informative priors [11] when domain knowledge is limited.\nThe likelihood quantifies the probability of observing data y, given the model parameters θm,\nnetwork structure Mm, and the data collection option (design) ξ. It provides a probabilistic measure\nof the mismatch between the observation yand the model predicted QoI, G(θm, ξ;Mm). One approach\nis to form the likelihood through an additive noise data model [12]. However, in this study, we adopt\na relative noise model to better capture proportional noise in the data:\nyk=Gk(θm, ξ;Mm)(1 + ϵk), (10)\nwhere the subscript kindicates the kth component of the vector, and ϵk∼ N(0, σ2\nk) is an independent\nrelative noise term. Subsequently, we obtain\np(y|θm, Mm) =nyY\nk=1pϵk\u0012yk\nGk(θm, ξ;Mm)−1\u0013\f\f\f\fd\ndyk\u0012yk\nGk(θm, ξ;Mm)−1\u0013\f\f\f\f\n=nyY\nk=11√\n2πσkexp\"\n−1\n2σ2\nk\u0012yk\nGk(θm, ξ;Mm)−1\u00132#\n1\nGk(θm, ξ;Mm), (11)\nas a result of the independent ϵk’s. We can assign a fixed value to the data standard deviation (e.g.,\nσk= 0.1, corresponding to a level of ±10% relative noise). Alternatively, the noise term can be\nmodeled dynamically using different approaches. One option is to treat it as an inferable parameter\nwithin the model parameter set θm[12]. Another approach is to manually adjust the noise value and\nrepeatedly solve the inference problem until the Bayesian posterior predictive aligns well with the\ncollected data [13]. Both such approaches, however, would require substantially higher computational\ncost for solving the overall Bayesian inference problem.\n2.3 Network structure uncertainty\nWe now outline our specific prior and likelihood setup in the model Bayes’ rule (Equation (3)) for cap-\nturing network structure uncertainty. First, we need to generate a pool of nMcandidate node-and-flow\nnetwork structures. We restrict this uncertainty to the existence of connections between nodes. These\ncandidate network structures may already be known to the MFA analyst. Otherwise, as detailed by\nLiao et al. [5], a combination of exploitation andexploration can be adopted to populate the candidate\npool: identifying a series of uncertain connections by comparing network structures used in existing\nMFAs on the same or similar subjects or consulting domain experts (exploitation), and generating\nsemi-randomized “wild-guesses” about the existence or absence of certain connections to enhance the\ndiversity of the candidate pool (exploration). Once a total number of nLcritical uncertain connec-\ntions has been identified, a complete permutation of these connections yields 2nLpossible candidate\nnetwork structures. After establishing the candidate network structures, a model prior distribution\nis assigned for the network structures, p(Mm). Liao et al. [5] adopted an informative prior based\non expert elicitation of the probability that individual uncertain connections exist. Alternatively, an\nuninformed uniform prior can be used, setting p(Mm) =1\nnMfor all m.\n8The model likelihood, p(y|Mm, ξ), is in fact the model evidence (denominator) in the parameter\nBayes’ rule (Equation (2)). It can be explicitly expressed as an integral over the parameter likelihood:\np(y|Mm, ξ) =Z\np(y|θm, Mm, ξ)p(θm|Mm) dθm. (12)\n2.4 Mutual information estimators for Bayesian model selection in MFA\nWe now derive three numerical estimators for the mutual information expected utility in Equation (5),\nfollowing a nested Monte Carlo structure that avoids the need for explicit evaluation or sampling from\nthe parameter posteriors. The key difference between these estimators is their sampling processes in\nthe outer loop. To reflect these differences, we refer to the three estimators as: 1) data-model joint\nMC, 2)model enumeration , and 3) data marginal MC .\nEstimator 1: data-model joint MC We begin by applying Bayes’ rule to Equation (5) and then\nuse a standard Monte Carlo approximation:\nU(ξ) =Z\np(y|ξ)nMX\nm=1p(Mm|y, ξ) logp(Mm|y, ξ)\np(Mm)dy\n=ZnMX\nm=1p(Mm)p(y|Mm, ξ) logp(y|Mm, ξ)\np(y|ξ)dy\n≈1\nNoutNoutX\nℓ1=1h\nlogp(y(ℓ1)|Mm(ℓ1), ξ)−logp(y(ℓ1)|ξ)i\n, (13)\nwhere Noutis the number of Monte Carlo samples, and samples Mm(ℓ1)∼p(Mm) are drawn from\nthe model prior and y(ℓ1)∼p(y|Mm(ℓ1), ξ) from the model evidence (by first drawing a temporary\nparameter sample θ′∼p(θ) followed by y(ℓ1)∼p(y|θ′, Mm(ℓ1), ξ)). However, the two log-density terms\nin Equation (13) are not available analytically. The first term, as defined in Equation (12), can be\napproximated using an inner-loop Monte Carlo:\nlogp(y(ℓ1)|Mm(ℓ1), ξ) = log\u0012Z\np(y(ℓ1)|θm(ℓ1), Mm(ℓ1), ξ)p(θm(ℓ1)|Mm(ℓ1)) dθm(ℓ1)\u0013\n≈log\n1\nNin,1Nin,1X\nℓ2=1p(y(ℓ1)|θ(ℓ2)\nm(ℓ1), Mm(ℓ1), ξ)\n, (14)\nwhere Nin,1is the number of Monte Carlo samples, θm(ℓ1)represents the parameter random vector\ncorresponding to the model whose index has been sampled to be m(ℓ1)from the outer loop, and\nθ(ℓ2)\nm(ℓ1)denotes the ℓ2-th sample of that parameter random vector drawn from its prior distribution\np(θm(ℓ1)|Mm(ℓ1)). The second term can be similarly approximated using an inner-loop Monte Carlo:\nlogp(y(ℓ1)|ξ) = log nMX\nm=1Z\np(y(ℓ1)|θm, Mm, ξ)p(θm|Mm)p(Mm) dθm!\n≈log\nnMX\nm=1\n1\nNin,2Nin,2X\nℓ3=1p(y(ℓ1)|θ(ℓ3)\nm, Mm, ξ)\np(Mm)\n, (15)\nwhere Nin,2is the number of Monte Carlo samples, and samples θ(ℓ3)\nm∼p(θm|Mm) are drawn from its\nprior distribution. Note that the θmandMmterms are “internal” variables of marginalization and\n9do not depend on the outer-loop ℓ1sample. Only the θmmarginalization is approximated by Monte\nCarlo, while that for Mmis summed exactly over all nMmodels (this can also be replaced by a Monte\nCarlo). Additionally, the two inner loops may, but do not have to, have the same number sample\nsizes, Nin,1andNin,2. Finally, combining Equations (14) and (15) into Equation (13), we obtain the\noverall estimator 1, bU1:\nU(ξ)≈bU1(ξ) :=1\nNoutNoutX\nℓ1=1\nlog\n1\nNin,1Nin,1X\nℓ2=1p(y(ℓ1)|θ(ℓ2)\nm(ℓ1), Mm(ℓ1), ξ)\n\n−log\nnMX\nm=1\n1\nNin,2Nin,2X\nℓ3=1p(y(ℓ1)|θ(ℓ3)\nm, Mm, ξ)\np(Mm)\n\n. (16)\nEstimator 2: model enumeration The second estimator differs from bU1by exhaustively looping\nover all nMcandidate network structures in the outer loop instead of randomly sampling. This\neliminates the additional variation from Monte Carlo but at the cost of needing to complete looping\nover all network structures. Equation (13) then becomes:\nU(ξ) =Z\np(y|ξ)nMX\nm=1p(Mm|y, ξ) logp(Mm|y, ξ)\np(Mm)dy\n=nMX\nm=1Z\np(Mm)p(y|Mm, ξ) logp(y|Mm, ξ)\np(y|ξ)dy\n≈nMX\nm=11\nNoutNoutX\nℓ1=1p(Mm)h\nlogp(y(ℓ1)|Mm, ξ)−logp(y(ℓ1)|ξ)i\n, (17)\nwhere y(ℓ1)∼p(y|Mm, ξ); note that Mmis no longer a sample, but iterated over the index min the\nsummation. Following a similar derivation as estimator 1, we arrive at the overall estimator 2, bU2:\nU(ξ)≈bU2(ξ) :=nMX\nm1=1p(Mm1)1\nNoutNoutX\nℓ1=1\nlog\n1\nNin,1Nin,1X\nℓ2=1p(y(ℓ1)|θ(ℓ2)\nm1, Mm1, ξ)\n\n−log\nnMX\nm2=1\n1\nNin,2Nin,2X\nℓ3=1p(y(ℓ1)|θ(ℓ3)\nm2, Mm2, ξ)\np(Mm2)\n\n.(18)\nEstimator 3: data marginal MC The third estimator is formed by interpreting Equation (5)\nas varying KL divergence realizations (of the model prior to posterior) under different possible data\nobservations:\nU(ξ) =Z\np(y|ξ)nMX\nm=1p(Mm|y, ξ) logp(Mm|y, ξ)\np(Mm)dy\n=Z\np(y|ξ)nMX\nm=1p(y|Mm, ξ)p(Mm)\np(y|ξ)logp(y|Mm, ξ)\np(y|ξ)dy\n≈1\nNoutNoutX\nℓ1=1nMX\nm=1p(y(ℓ1)|Mm, ξ)p(Mm)\np(y(ℓ1)|ξ)h\nlogp(y(ℓ1)|Mm, ξ)−logp(y(ℓ1)|ξ)i\n, (19)\n10where y(ℓ1)∼p(y|ξ) are samples drawn from the marginal distribution for y(by first drawing a\ntemporary model sample Mm′∼p(Mm), a temporary parameter sample θ′∼p(θ), and then y(ℓ1)∼\np(y|θ′, Mm′, ξ)). The two terms, p(y(ℓ1)|Mm, ξ) and p(y(ℓ1)|ξ), appear twice in Equation (19)—once\noutside the logarithm and once inside—and need to be approximated separately using, e.g., the inner-\nloop Monte Carlo formulas from Equations (14) and (15). Substituting them in, we arrive at the\noverall estimator 3, bU3:\nU(ξ)≈bU3(ξ) :=1\nNoutNoutX\nℓ1=1nMX\nm1=1\n\nh\n1\nNin,1PNin,1\nℓ2=1p(y(ℓ1)|θ(ℓ2)\nm1, Mm1, ξ)i\np(Mm1)\nPnM\nm2=1h\n1\nNin,2PNin,2\nℓ3=1p(y(ℓ1)|θ(ℓ3)\nm2, Mm2, ξ)i\np(Mm2)\n\nlog\n1\nNin,1Nin,1X\nℓ2=1p(y(ℓ1)|θ(ℓ2)\nm1, Mm1, ξ)\n\n−log\nnMX\nm2=1\n1\nNin,2Nin,2X\nℓ3=1p(y(ℓ1)|θ(ℓ3)\nm2, Mm2, ξ)\np(Mm2)\n\n\n\n.(20)\nTo reduce the computational cost of the nested Monte Carlo estimators, we implement sam-\nple reuse techniques, which also helps prevent near-zero model evidence estimates at small sample\nsizes [28]. This approach involves generating a batch of prior samples and pre-computing their\ncorresponding G(θm, ξ;Mm) for all nMcandidate network structures. These pre-computed values\nare then reused for both the outer and inner loops (e.g., for bU1, setting θ′=θ(ℓ2)\nm(ℓ1)=θ(ℓ3)\nmand\nNout=Nin,1=Nin,2). While sample reuse may introduce some bias in the estimators, the effect is\nvery small [43]. Notably, sample reuse significantly reduces the computational complexity in terms of\nunique likelihood evaluations (i.e., unique MFA solves) at each ξ. Specifically, the cost is reduced from\nO(Nout(Nin,1+nMNin,2)) toO(nMNout) forbU1, from O(nMNout(Nin,1+nMNin,2)) toO(nMNout)\nforbU2, and from O(nMNout(Nin,1+nMNin,2)) toO(nMNout) forbU3.\nThe simple MFA example in Figure 1 illustrates the BOED results using bU2. Among the four\navailable downstream mass flow data collection options, BOED identifies measuring the mass flow on\nz89as the optimal choice for reducing network structure uncertainty. This result aligns with intuition,\nas the existence/absence of the mass flow z73would cause a larger percentage fluctuation on node 8,\nwhich has a smaller nodal mass flow than node 3. In addition, mass flow z89receives a larger allocation\nout of the flows originating from node 8. Consequently, collecting mass flow data on z89is more likely\nto reveal the underlying network structure than the other data collection options.\n3 Case study on the U.S. steel flow\nWe demonstrate the BOED framework for reducing MFA network structure uncertainty through a\ncase study on the U.S. steel flow in 2012. We select this year to maintain consistency with previous\nwork on uncertainty quantification and reduction in MFA using Bayesian inference [5,12,34], though\nthe framework is applicable to any year. All data and code for this case study are available online (see\nthe SI).\n3.1 Candidate network structures and prior distributions\nWe revisit the 16 candidate network structures for the U.S. steel sector defined by Liao et al. [5]. They\nextracted a baseline network structure from Zhu et al. [14], which includes 270 metal flows (edges)\nconnecting 55 nodes. Based on the exploitation approach (discussions with industry experts) and\nthe exploration approach (semi-randomized guesses), Liao et al. identified four edges whose presence\n11or absence in the network was uncertain: (1) post-consumer steel scrap to the blast furnace (BF,\nconnection index 1); (2) scrap to the basic oxygen furnace (BOF, connection index 2); (3) the BOF\ncontinuously cast slab (CC) to the rod and bar mill (RBM, connection index 3); and (4) the BOF\nCC to the section mill (connection index 4). These edges are highlighted in the Sankey diagram in\nFigure 2. The 16 candidate network structures were generated from the complete permutation of the\n4 questionable flows (24= 16). Per Liao et al. , the network structures are described using a 4-digit\nbinary code to indicate whether the indexed connection is present (1) or absent (0). For example,\nnone of the 4 targeted connections exist in the network structure, 0000; whereas, only the flow between\nscrap and the blast furnace (index 1) exists in the network structure, 1000. A uniform prior is used\nto model the network structure uncertainty before any data is collected, with the probability for each\nnetwork structure equal to1\n16.\nFor each candidate network structure, we use the same parameter priors as Liao et al. They formed\ninformative parameter priors for upstream allocation fractions ( ϕij’s) and external inputs ( qi’s) using\nexpert elicitation, and used non-informative priors for the downstream allocation fractions. These\nprior distributions are provided in S1 and readers are directed to Dong et al. [12] for the methodology\nto conduct expert elicitation. Figure 2 shows the Sankey diagram representation of the U.S. steel\nsector MFA using the parameter priors and a network structure of 1111.\n3.2 Targeting MFA data collection\nA total of 33 candidates have been identified for data collection. Each candidate is an absolute\nmass flow value published by the United States Geological Survey (USGS) [44–46] or the World Steel\nAssociation (WSA) [47]. The 33 candidates for mass flow data collection are labeled with “#” numbers\nin Figure 2, with the immediate upstream and downstream nodes defined in Table S1 of the SI. Per\nthe work of Lupton and Allwood [11] and Dong et al. [12] on steel flow analyses, all candidates for\ndata collection are assumed to exhibit a relative noise (standard deviation of a normal distribution) of\n10% of the nominal value. Using the BOED framework (see Section 2.4), we target the collection of\ndata that reduces the network structure uncertainty when collecting a single piece of data and when\ncollecting multiple pieces of data.\n3.3 Case study results\n3.3.1 Expected utility results for single-piece data record collection\nWe use all three mutual information estimators proposed in Section 2.4 to solve the BOED problem\nfor the case study on the U.S. steel flow with 16 candidate network structures (each containing ap-\nproximately 180 parameters). Using the sample reuse technique outlined in Section 2.4, we allocate\n160,000 unique MFA solves to each mass flow data collection option, ξ, across all three methods. This\nconfiguration sets parameter sample sizes to 10,000 for each estimator: 160,000 unique evaluations\ndivided by the 16 candidate network structures. It took 30 seconds to compute bU1(data-model joint\nMC estimator) and bU3(data marginal MC estimator) and 10 minutes for bU2(model enumeration\nestimator) using an Intel(R) Core i7-9700K CPU, 3.60GHz.\nFigure 3 presents the expected utility for all 33 mass flow data collection designs, evaluated using\nall three estimators. The figure shows that collecting data on mass flow #19 (the output of the BOF\nto continuous casting) has the highest expected utility, regardless of the estimator used. The second\nmost valuable piece of data is expected to be mass flow #16, the flow of scrap into the electric arc\nfurnace (EAF). Upon examining Figure 2, it is evident that collecting data on mass flows #16 and\n#19 are likely valuable for reducing network structure uncertainty because they share nodes with\nmultiple uncertain flows: mass flow #16 shares nodes with connection indices 1 and 2, while mass\nflow #19 shares nodes with indices 2, 3, and 4. In contrast, collecting data on many other mass flows\n12Figure 2: Bayesian prior-predictive mass flows for the U.S. steel flow in 2012. The uncertainty percentages refer\nto the flow standard deviation as a percentage of the mean of the mass flow. All mass flows refer to steel except\nfor the iron ore flows that include the non-iron mass (e.g., oxygen and gangue). Candidate MFA mass flows\nfor data collection are marked in the “#” flows with numbering (see Table S1 for details) Underlying data are\navailable at data repository: https://doi.org/10.7302/k35m-xz34\n13Figure 3: Expected utility of collecting a single data record on U.S. steel sector mass flows to reduce MFA\nnetwork structure uncertainty. The expected utility is evaluated using the three numerical estimators derived\nin Section 2.4. Underlying data are available at data repository: https://doi.org/10.7302/k35m-xz34\n14is expected to have negligible utility in reducing uncertainty. For example, collecting data on mass\nflows #5-9, which represent relatively minor upstream flows directing Direct Reduced Iron (DRI) to\ndifferent furnaces, is expected to contribute little to reducing uncertainty. While collecting data on\nother mass flows, such as #3 and #29, is still expected to be useful for reducing network structure\nuncertainty, it is not as expected to be as effective as collecting data on mass flows #16 and #19.\nReferring to Figure 2, mass flows #3 and #29 each share a node with only one uncertain connection,\nlimiting their impact on uncertainty reduction compared to flows #16 and #19.\nTo test the alignment between the expected utility results and the actual reduction in network\nstructure uncertainty achieved by data collection, we collect data on mass flows #3, #16, #19, and\n#29 from the USGS [44–46] and WSA [47] (see Table S1). We then update the network structure\nuncertainty using Equation (3). Figure 4 presents the network structure prior and posterior distribu-\ntions, along with the KL divergence from prior to posterior after collecting data on the selected mass\nflows. A higher KL divergence indicates a greater change from the prior and posterior, reflecting a\nlarger reduction in uncertainty. The results show alignment between the order of the expected utility\nvalues for these flows (from highest to lowest: #19, #16, #3, #29), as shown in Figure 3, and the\nactual utility achieved by collecting the data, as demonstrated in Figure 4.\n3.3.2 Expected utility result for multi-piece data record collection\nThe updated MFA uncertainty from collecting a single-piece of MFA data could be the starting point\nfor a new round of BOED and collection of a second data point. An alternative to this one-at-a-time\napproach to data collection is to use BOED to target the collection of batches of data. To demonstrate\nthis, the BOED framework was used to target optimal collection of two-pieces of MFA data, where we\nexplore all the possible combinations to collect two pieces of data (33 ×17 = 561 total combinations).\nThe same settings for the estimators are chosen for this analysis where we allocate a total of 160,000\nunique MFA solves to each mass flow data collection option with sample reuse.\nFigure 5 presents the expected utility for all data collection combinations and is symmetric along\nthe diagonal line, y=x. The highest expected utility is achieved by collecting data on both mass flow\n#11 (from Pig iron consumption to BOF) and mass flow #19 (output of BOF to continuous casting).\nOther high-utility combinations include mass flows #3 and #19, #16 and #19, and #19 and #20. It\nis also important to note that collecting independent data on mass flow #19 twice is expected to yield\na high utility.\nTo assess the alignment between the expected utility of collecting two data points and the actual\nreduction in network structure uncertainty, we collected the relevant data from the USGS [44–46] and\nWSA [47] (see Table S2). Figure 6 illustrates the prior and posterior distributions of the network\nstructure, along with the KL divergence between them, after incorporating the data on the selected\nmass flows. This data was used to update the network structure uncertainty through the procedure\noutlined in Equation (3).\n3.3.3 Error associated with the numerical estimators\nTo identify which numerical estimator of expected utility minimizes error for a given computational\ncost, we calculate the standard deviation and root mean square error (RMSE) from 100 evaluations of\nexpected utility for separately collecting data on mass flows #16 and #19 (single-piece data collection).\nThese evaluations use the same settings as for the case study. The RMSE metric requires knowledge of\nthe exact expected utility value. Since this cannot be obtained analytically, we instead use a reference\nvalue that is estimated by a high-quality bU2with 100,000 samples, with sample reuse.\nFigure 7 presents the error (standard deviation and RMSE) of the numerical estimators, with the\nground truth for the RMSE calculation approximated using bU2, as it is theoretically the least biased\n15Figure 4: Prior and posterior probability for all 16 candidate network structures with data collection on selected\nmass flow candidates. Underlying data are available at data repository: https://doi.org/10.7302/k35m-xz34\n16Figure 5: Expected utility of multi-piece data collection - collecting two mass flow data records at the same\ntime. The expected utility is evaluated using Estimator 2): model enumeration approach, derived in Section 2.4.\nUnderlying data are available at data repository: https://doi.org/10.7302/k35m-xz34\n17Figure 6: Prior and posterior probability for all 16 candidate network structures with selected data collection on\nmultiple mass flows. Underlying data are available at data repository: https://doi.org/10.7302/k35m-xz34\n18Figure 7: Standard deviation and RMSE from 100 trials of numerical evaluations on expected utility through\nall 3 estimators with true value generated with Estimator 2): model enumeration approach. Underlying data\nare available at data repository: https://doi.org/10.7302/k35m-xz34\nmethod (see Section 2.4). The figure shows that bU2yields the lowest error, exhibiting the smallest\nstandard deviation and RMSE. As demonstrated in S4, this result holds true even when a different\nhigh-quality numerical estimator with 100,000 samples is used to estimate the true expected utility\nvalue.\n4 Discussion\nIn this section, we discuss the implications of shifting from targeting collection of single-pieces of data\nto batches of data (Section 4.1), the performance of the numerical estimators (Section 4.2), and the\nlimitations of the BOED methodology as applied to MFA (Section 4.3).\n4.1 Single-piece versus multi-piece data collection design\nThe case study results show that the hightest expected utility for two-piece data collection (Figure 5,\nmass flows #11 and #19) is not simply the combination of the highest and second-highest expected\nutilities for single-piece data collection (Figure 3, mass flows #16 and #19). This is because the two-\npiece design problem solved by the BOED framework reveals the synergy of collecting multiple data\nsimultaneously on network structure uncertainty. Intuitively, this can be interpreted as the BOED\nframework favoring data collection from different subregions of the supply chain. For example, the\ncombination of mass flow #11 and 19 is more likely to reveal key information about the existence of\nboth connection index 1 (mass flow from scrap to BF) and connection index 2 (mass flow from scrap to\nBOF) while the combination of mass flow #16 and 19 is more likely to only provide greater confidence\non the existence of targeted connection index 2 as both mass flows #16 and #19 share nodes with\nconnection index 2 only.\nComparing the network structure posterior probability results for single-piece (see Figure 4) and\ntwo-piece (see Figure 6) data collection, it is interesting to note that the actual posterior results for\nsingle-piece data collection align well with the expected utility outcomes indicated by the BOED\nframework. However, the posterior results for two-piece data collection reveal a discrepancy in the\n19order of actual utility achieved, with the highest-to-lowest sequence being #16 and #19, #19 and\n#20, #3 and #19, and #11 and #19, which does not match the expected utility order (highest to\nlowest: #11 and #19, #3 and #19, #16 and #20, #19 and #20). Despite this misalignment, the\ncredibility of the method is not undermined, as expected utility for a given data collection option is\ninherently an expectation over all possible data values. The credibility of the method stems from its\nrigorous adherence to mathematical principles (via the laws of probability), rather than any direct\ncomparison to empirical results.\nThe targeted collection of two pieces of MFA data between rounds of BOED can be easily extended\nto accommodate the collection of any batch size of MFA data records (see Equation (5)). The batch\nsize and the number of BOED rounds are choices for the MFA analyst, as they determine the level\nof uncertainty in the results that is acceptable, as well as the level of certainty that can be achieved\nwithin the constraints of the MFA project’s resources.\n4.2 Performance of the numerical estimators\nFigure 7 shows that, strictly speaking, the second numerical estimator ( model enumeration estimator)\nis the most accurate of the three estimators. An MFA analyst would therefore be well justified in using\nthis estimator to evaluate the expected utility of candidates for data collection. However, all three\nestimators exhibit only small errors, with the standard deviation and RMSE of all estimators being\nless than 5% of the nominal expected utility. The absolute value of the bias (i.e.,p\nRMSE2−σ2)\nfor all three estimators is less than 1 ×10−3. In the case study, the choice of numerical estimator\nhad minimal impact on the expected utility results, with Figure 3 showing that both the absolute\nexpected utility and the order of expected utility from collecting different data records were nearly\nidentical. Therefore, while the bU2is the most accurate numerical estimator, all three estimators are\nviable options for the BOED framework.\nOther considerations, beyond accuracy, when choosing a numerical estimator include computa-\ntional time. The extended time associated with evaluating bU2in the case study (10 minutes, compared\nto 30 seconds for evaluating bU1andbU3) is due to the necessity to conduct more likelihood evaluations\nin the bU2method (see Section 2.4). In the case study, sample reuse is applied to improve estimator\nefficiency. However, in cases where sample reuse is not applied, then both bU2&bU3can be hindered\nby a smaller sample size, given a fixed computational budget. Therefore, an MFA analyst would also\nbe justified in using Estimator 1 given the improvement in evaluation time for the relatively small loss\nin accuracy.\n4.3 Limitations\nAt first glance, the BOED framework introduced in this paper may seem computationally expensive\ndue to the nested Monte Carlo structures in the mutual information estimators. However, it is actually\nhighly efficient, requiring only 30 seconds to 10 minutes on a standard laptop to obtain the results of\nthe case study investigated.\nOne limitation of the BOED framework in this case study is that it does not account for the varying\ndifficulty and cost associated with collecting data from different sources. A metric reflecting the cost\nthat an MFA analyst is willing to incur per unit of expected information gained (e.g., USD per nat or\nbit of information) could be incorporated into the optimization problem in Equation (6).\nAnother limitation is that the proposed framework is designed to optimally reduce MFA network\nstructure uncertainty but does not explicitly aim to reduce MFA mass flow uncertainty in Equation (1),\nwhich is often the primary objective in MFA. Addressing this would require advancements in goal-\noriented BOED [48, 49], capable of targeting uncertainty reduction for more broadly defined QoIs,\nincluding specific mass flows in an MFA.\n205 Conclusions and future work\nIn this study, we have applied a Bayesian optimal experimental design (BOED) framework for targeted\ncollection of MFA data to reduce network structure uncertainty. Since data collection is often a\nbottleneck in MFA, this work aims to formalize targeted data collection using rigorous probabilistic\nmeasures, helping to reduce the time and cost associated with MFA data collection while maximizing\nthe certainty in the final results.\nWe developed three numerical estimators to solve the data collection optimization problem and\ndemonstrated through a case study that all three methods are viable options for an MFA analyst,\nwith slight trade-offs between speed and accuracy. The case study on the U.S. steel sector highlighted\nthe difference between targeting the collection of single data points versus batches of data, with the\noptimal choice of data to collect depending on the batch size.\nExtensions of this work could involve applying the BOED framework to additional theoretical and\nreal-world MFA case studies to derive next-generation heuristics for targeted data collection. For\ninstance, the case study results suggest the benefits of collecting data on large mass flows that share\ncommon nodes with at least one uncertain edge in the network. Another potential extension of this\nwork is the development of goal-oriented BOED methods that target data collection to reduce MFA\nmass flow uncertainty, accounting for both parametric uncertainty under a fixed network structure\nand network structure uncertainty.\nThe BOED framework developed here for reducing MFA network structure uncertainty, and by\nLiao et al. [34] for reducing MFA parametric uncertainty, could be adapted for use in other areas of\nindustrial ecology. For example, it could inform targeted data collection, measurements, or experiments\naimed at reducing parameter uncertainty in gate-to-gate life cycle assessment models of manufacturing\nprocesses. Additionally, it could be applied to reduce uncertainty in the model form of environmental\nprocess models; e.g., helping discrimination between modeling the per-unit output impacts of an\nindustrial process as having exponential, independent, or logistic dependencies on output volume.\n6 Acknowledgments\nThis material is based upon work supported by the National Science Foundation under Grant No.\n#2040013. The authors declare that they have no conflicts of interest.\n7 Sample Data Availability Statements\nThe data that support the findings of this study are openly available in University of Michigan Deep\nBlue Repositories at https://doi.org/10.7302/k35m-xz34 .\n8 Supporting Information\nThis Supporting Information (SI) document includes data, literature reviews and Sankey diagrams for\nindividual candidate network structures helpful to understanding the main article as well as links to\nPython Scripts and collected MFA data used to conduct the case study.\nPlease go to this link ( https://doi.org/10.7302/k35m-xz34 ):\n•presentation of the underlying data used to construct the Sankey diagrams in the main paper\n(Figure 2) in a numerical, tabular format;\n•a Python script for performing BOED to evaluate the expected utility for candidate on mass\nflow data collection; and\n21•a Python script for performing Bayesian model inference on network structure uncertainty using\nspecified prior PDFs and collected MFA data.\n8.1 Parameter priors for the case study\nIn this section, we include the details of the informative priors ( ϕandq) used for the case study\non the 2012 U.S. steel flow. The informative priors for this case study are obtained from expert\nelicitation through interviews with domain experts. Readers are directed to Dong et al. [12] for\nthe details of the methodology to conduct expert elicitation and prior aggregation from multiple\nexperts. The elicitation process conducted is under the assumption of the network structure with\nall 4 targeted connections existent. As informative priors are applied to both the flows originating\nfrom scrap node and continuous cast slab node, adjustment is required to apply these informative\npriors to candidate network structures where one or more targeted connections are not present in the\nmodel. We delete the hyper-parameter(s) of the Dirichlet priors for allocation fractions corresponding\nto the targeted connection(s) if they do not exist in the candidate network structure, while keeping\nthe rest of the hyper-parameters fixed. For example, the informative prior distribution used for the\nallocation fractions originating from continuous cast slab to hot strip mill, plate mill, rod and bar mill\nand section mill when all four connections are present is ϕ∼Dir(11.46,2.11,2.82,1.81). In the case\nwhere the connection to the rod and bar mill does not exist, the revised informative prior for this set\nof allocation fraction will be ϕ∼Dir(11.46,2.11,1.81).\nFor the exact values of the hyper-parameters used for informative priors, please see the inference\ncode from link.\n8.2 Case study: U.S. steel flow MFA collected data\nTable 1: MFA data from 2012.\nObs # Upstream node Downstream node Value (Mt) Source\n1 Iron Ore Production Export 11.2 1\n2 Iron Ore Consumption Blast Furnace 46.3 1\n3 Blast Furnace Pig Iron 32.1 3\n4 DRI Export 0.01 2\n5 DRI Consumption Blast Furnace 0.049 2\n6 DRI Consumption Basic Oxygen Furnace 1.91 2\n7 DRI Consumption Electric Arc Furnace 1.62 2\n8 DRI Consumption Cupola Furnace 0.01 2\n9 DRI Consumption Other Casting 0.01 2\n10 Pig Iron Export 0.021 2\n11 Pig Iron Consumption Basic Oxygen Furnace 31.5 2\n12 Pig Iron Consumption Electric Arc Furnace 5.79 2\n13 Pig Iron Consumption Cupola Furnace 0.057 2\n14 Pig Iron Consumption Other Casting 0.046 2\n15 Scrap Collected Export 21.4 2\n16 Scrap Consumption Electric Arc Furnace 50.9 2\n17 Scrap Consumption Cupola Furnace 1.11 2\n18 Scrap Consumption Other Casting 0.167 2\n19 Basic Oxygen Furnace Continuous Casting Slabs 36.281 4\n20 Electric Arc Furnace EAF Yield 52.414 4\n2221 Pipe Welding Plant Pipe and Tubing 2.165 3\n22 Seamless Tube Plant Pipe and Tubing 2.162 3\n23 HSM Yield Hot Rolled Sheet 19.544 3\n24 CRM Yield Cold Rolled Sheet 11.079 3\n25 Tin Mill Tin Mill Products 2.009 3\n26 Galvanized Plant Galvanized Sheet 16.749 3\n27 Plate Mill Plates 9.12 3\n28 RBM Yield Reinforcing Bars 5.65 3\n29 RBM Yield Bars 6.7 3\n30 RBM Yield Wire and Wire Rods 2.784 3\n31 RBM Yield Light Section 2.13 3\n32 SM Yield Heavy Section 5.03 3\n33 SM Yield Rail and Rail Accessories 1.009 3\nReference in Table 1\n1USGS. 2012. Iron Ore. Minerals Yearbook. https://www.usgs.gov/centers/\nnational-minerals-information-center/iron-ore-statistics-and-information\n2USGS. 2012. Iron and Steel Scrap. Minerals Yearbook. https://www.usgs.gov/centers/\nnational-minerals-information-center/iron-and-steel-scrap-statistics-and-information\n3USGS. 2012. Iron and Steel. Minerals Yearbook. https://www.usgs.gov/centers/\nnational-minerals-information-center/iron-and-steel-statistics-and-information\n4WorldSteel. 2017. Steel Statistical Yearbook 2017. https://worldsteel.org/steel-by-topic/\nstatistics/steel-statistical-yearbook/\n8.3 Description on nodes representing the steel flow\nTable 2 presents the description of all nodes featured in the steel flow case study. Some of the nodes\nin Table 2 are compiler nodes. These nodes are for visualization and calculation purposes. They do\nnot represent actual processes.\nTable 2: Description of nodes featured in the steel flow case study\nNode name Note\nIron ore production Domestic production of iron ore\nIron ore consumptionCompiler node aggregating imported iron ore and do-\nmestic iron ore not exported\nImport iron ore\nDRI production\nDRI Compiler node describing DRI produced domestically\nImport DRIFocus of the analysis is domestic emission: imports\nassigned 0 emission intensities\n23DRI consumptionCompiler node aggregating imported DRI and domes-\ntically produced DRI not exported\nBlast furnace\nImport pig iron\nPig ironCompiler node describing pig iron produced domesti-\ncally from blast furnace\nPig iron consumptionCompiler node aggregating imported pig iron and do-\nmestically pig iron not exported\nPurchased scrap Post-consumer scrap collected domestically\nScrap collectedCompiler node aggregating all post-consumer scrap\ncollected domestically\nImport scrap\nScrap consumptionNode aggregating post-industrial process scraps and\ndomestically collected post consumer scrap not ex-\nported\nBasic oxygen furnace\nElectric Arc furnace\nEAF yieldCompiler node aggregating all products from electric\narc furnace\nCupola furnace\nOther casting\nOCyieldCompiler node aggregating all products from other\ncasting process\nOClossCompiler node aggregating run-around prep and loss\nfor other casting process\nContinuous casting - slabs\nCCyieldCompiler node aggregating all products from continu-\nous cast slabs\nCClossCompiler node aggregating run-around prep and loss\nfor continuous cast slab\nContinuous casting - billets\nBTyieldCompiler node aggregating all products from continu-\nous cast billets\nBTlossCompiler node aggregating run-around prep and loss\nfor continuous cast billets\nContinuous casting - blooms\nBMyieldCompiler node aggregating all products from continu-\nous cast blooms\nBMlossCompiler node aggregating run-around prep and loss\nfor continuous cast blooms\nIngot casting\nICyieldCompiler node aggregating all products from ingot\ncasting process\nIClossCompiler node aggregating run-around prep and loss\nfor ingot casting process\nIngot import\nPrimary mill\n24PMYieldCompiler node aggregating all products from primary\nmill\nHot strip mill\nHSM YieldCompiler node aggregating all products from hot strip\nmill\nPlate mill\nRod and bar mill\nRBM YieldCompiler node aggregating all products from rod and\nbar mill\nSection mill Profiled rolling process\nSMYieldCompiler node aggregating all products from section\nmill\nCold rolling mill\nCRM YieldCompiler node aggregating all products from cold\nrolling mill\nGalvanizing plantGalvanizing plant taking sheet rolls and coating with\nzinc\nTin millGalvanizing plant taking sheet rolls and coating with\ntin\nPipe and tubing\nBars Cutting process\nCold rolled sheetManufacturing process of stamping and assembly of\nsteel sheets\nGalvanized sheetManufacturing process of stamping and assembly of\nsteel sheets\nHot rolled sheetManufacturing process of stamping and assembly of\nsteel sheets\nIron product castingMachining process at critical interfaces of intermediate\nproducts\nLight section Cutting process\nPipe welding plant\nPlates Cutting process\nSeamless tube plant\nReinforcing bars Cutting process\nRails and rail accessories Cutting process\nHeavy section Cutting process\nTin mill productsManufacturing process of stamping and assembly of\nsteel sheets\nWire and wire rodsForming process to manufacture wire, fasteners and\ntools\nSteel product castingMachining process at critical interfaces of intermediate\nproducts\nIntermediate product import\n258.4 Benchmark with results generated using Estimator 3: data marginal MC\nFigure 8 shows the benchmark results for all three estimators with the true value generated using\nEstimator 3: data marginal MC with a sample size of 100,000 for both the inner and outer loops.\nFigure 8: Standard deviation and RMSE from 100 trials of numerical evaluations on expected utility through\nall 3 estimators with true value generated with Estimator 3: data marginal MC",
      "metadata": {
        "filename": "Intelligent data collection for network discrimination in material flow analysis.pdf",
        "hotspot_name": "Steel_Punching_and_Bending",
        "title": "Intelligent data collection for network discrimination in material flow\n  analysis using Bayesian optimal experimental design",
        "published_date": "2025-04-18T00:04:12Z",
        "pdf_link": "http://arxiv.org/pdf/2504.13382v1",
        "query": "punching and bending steel EMC shield material substitution studies resource reduction percentages"
      }
    },
    "Neutron Activation Background in the NvDEx Experiment": {
      "full_text": "Neutron Activation Background in the\nNνDEx Experiment\nQianming Wang1, Zeyu Huang1, Pengchong Hu1and Emilio\nCiuffoli2∗\n1) Lanzhou University, Lanzhou 730000, China\n2) Institute of Modern Physics, NanChangLu 509, Lanzhou 730000, China\nAbstract\nAn extremely low-background environment is a crucial require-\nment for any neutrinoless double beta decay experiment. Neutrons\nare very difficult to stop, because they can pass through the shields\nand activate nuclei in the detector, even inside the fiducial volume\nitself. Using Geant4 simulations we have studied the neutron back-\nground for N νDEx-100 and the most efficient way to reduce it. Using\na 60 cm thick external HDPE shield the neutron background can be\nreduced down to 0 .24±0.06 events/year, lower than the background\nrate due to natural radioactivity (0.42 events/year), which was used as\na benchmark for these calculations. The amount of shielding material\nneeded can be significantly reduced by placing HDPE in the empty\nspace between the lead shield and the steel vessel; in this way, it is\nsufficient to add 20 cm external HDPE shield to reduce the neutron\nbackground down to 0 .15±0.05 events/year.\n1 Introduction\nOne of the most important problems in physics is related to the fundamental\nnature of neutrinos, namely whether they are Dirac or Majorana particles.\nMany experiments, currently under design or already running, will at-\ntempt to answer this question by looking for neutrinoless double beta decay:\nthis lepton number-violating process is the ”smoking gun” that would defini-\ntively prove the presence of a Majorana mass term for neutrinos. In a ββ\ndecay, two neutrons are simultaneously transformed into protons, emitting\ntwo electrons and two antineutrinos. If they are Majorana particles, the\ntwo antineutrinos can annihilate each other, and all the energy of the decay\nwould be carried out by the electrons, creating a bump at the end of the β\nspectrum.\n∗emilio@impcas.ac.cn\n1arXiv:2307.12785v1  [physics.ins-det]  24 Jul 2023The No Neutrino Double-beta-decay Experiment (N νDEx) is one of the\nexperiments that will look for this kind of process [1]. It will search for\n0νββ events using a high-pressure gas time projection chamber (TPC) filled\nwith SeF 6, using82Se as a source of ββdecays. N νDEx will be placed at\nChina Jinping Underground Laboratory (CJPL) which has the deepest rock\nshield in the world (2400 m rock overburden) and can provide an incredibly\nlow-background environment. N νDEx-100, which is currently under design\n[2], will employ 100 kg of SeF 6; during the first phase natural Se will be used\n(82Se abundance: 8.7%), later on, enriched Se (82Se abundance >90%) will be\nemployed. Due to the high electronegativity of SeF 6, free electrons in the gas\nwill be captured almost immediately, which means that the negative particles\ndrifting towards the high voltage plate will be ions and electron avalanche\nmultiplication will not be possible. A new kind of sensor, Topemtal-S [3, 4],\nhas been developed and will be used in the detector, allowing it to reconstruct\nwith high precision the energy of the events. The other main advantage of\nNνDEx is that the high Q-value of82Se (2.995 MeV) will place the Region\nof Interest (ROI) above most of the environmental background.\nThe main challenge that neutrinoless double beta decay experiments must\nface is that the 0 ν2βdecay rate is proportional to m2\nββ, where mββis the ef-\nfective Majorana mass. However, since neutrino masses are incredibly small,\nthis kind of process would be strongly suppressed. For this reason, an ex-\ntremely low-background environment is a crucial requirement for the success\nof any neutrinoless double beta decay experiment [5]: the detector should\nbe carefully shielded in order to suppress the environmental background and\ngreat care should be taken to keep the contamination of the materials of the\ndetector itself as low as possible. Moreover, a careful study of the background\nbudget is required.\nThere are several possible sources of background. αandβparticles cre-\nated from radioactive decays can be stopped easily, so unless they are pro-\nduced in the fiducial volume, they will not constitute a problem; γ’s, on the\nother hand, are more difficult to stop. They will be the main source of back-\nground for N νDEx-100: it cannot be completely eliminated, because traces\nof radioactive elements will be present in the materials of the detector itself,\nhowever using a 20 cm lead shield it will be possible to suppress significantly\ntheγflux coming from the experimental hall, reducing the background due\nto natural radioactivity down to 0.42 events in ROI/year1[2] or, equivalently,\n1Please notice that in the computation of this source background, topological cuts have\nnot been taken into account, they should be able to reduce even further the background\nrate (for example in the NEXT experiment they can reduce the γbackground by one order\nof magnitude [6]). For consistency, topological cuts will not be taken into account in this\nwork as well, which means that the background rate here reported can be reduced even\n21.4×10−4events/(kg ·keV·y). Since adding additional shield will not reduce\nfurther this kind of background, such a value will be used in the current work\nas a benchmark.\nNeutrons are more difficult to stop even than γ’s since they only interact\nweakly; they can be created from natural fission, ( α,n) processes and from\ncosmogenic muons. They can activate nuclei in the detector, even inside\nthe fiducial volume itself, providing background. When they interact with\na nucleus, they can produce γ’s via (n, γ) and (n,n’ γ) reactions, or create\nunstable isotopes, for example via the reaction:\nAN+n→A+1N (1)\nγ’s could be dangerous if produced anywhere in the detector, while the elec-\ntrons emitted via βdecays of activated nuclei could be a source of background\nonly if the activation happened directly in the fiducial volume, since they are\nmuch easier to stop.\nCosmic rays are also related to several sources of background. When ma-\nterials are manufactured on the ground, the cosmic rays will create unstable\nisotopes; most of them will decay almost immediately, however some of them\nwill have relatively long half-lives, which means that, without any kind of\ncool-down period, they could still provide a non-negligible source of radioac-\ntivity even after the start of the experiment [2]. Cosmogenic muons can travel\ndeep underground. Interacting with the rocks surrounding the experimental\nhall, they can produce high-energy neutrons via spallation, moreover in prin-\nciple they could even hit directly the fiducial volume. However at CJPL, due\nto the large rock overburden, the muon flux will be drastically suppressed,\nwhich means that this source of background can be neglected.\nIn this paper we will focus on the study of the neutron background. It\nis structured as follows: in Sec. 2 we will present the simulations set-up, in\nSec. 3 we will discuss which isotopes can be created in the fiducial volume\nand how they can contribute to the neutron background, while in Sec. 4\nwe will examine the background due to neutron-induced γ’s. In Sec. 5 we\nwill discuss if and how using enriched Se can change the neutron-induced\nbackground and in Sec. 6 we will summarize the results.\n2 Simulation Set-Up\nThe neutron background was studied through Geant4 simulations [7, 8, 9],\nusing the Shielding Physics Lists, which allows the high precision treatment\nfurther.\n3of low-energy neutrons; the G4NDL dataset was used for the low-energy\nneutron cross sections.\nIn an underground laboratory environment, neutrons can be produced\nfrom natural fission, ( α,n) reactions or from the interactions of cosmogenic\nmuons with the rock surrounding the experimental hall. The latter could,\nin principle, be considerably harder to stop, since those spallation neutrons\ncan reach significantly higher energies (up to a few GeV), however deep\nunderground cosmogenic muons are strongly suppressed and the spallation\nneutron flux is several orders of magnitude smaller: the total neutron flux at\nCJPL is (2 .69±1.02)×10−5cm−2s−1[10], while the neutron flux from cosmo-\ngenic muons is estimated to be 8 .37×10−11cm−2s−1[11]. In our simulations\nneutrons were created near the surface of the detector, assuming an isotropic\ndistribution and with an energy spectrum equal to the one measured in CJPL\nand reported in [10], the total normalization factor was computed taking into\naccount the total neutron flux.\nFigure 1: Neutron energy distribution used in simulations\nFigure 2: Preliminary design of N νDEx detector\nA picture of the preliminary design of the N νDEx detector station can\nbe seen in Fig. 2, see Ref. [2] for more details. In our simulations, we used\nthe simplified design shown in the top panel of Fig. 3; it contains the lead\ncastle shield (gray), Stainless Steel Vessel (SSV), the holders to support it\n4and the pipes that will be connected to the outside of the shields (these\nthree elements are in dark grey), as well as the Inner Copper Shield (ICS,\norange) and the field cage (which was approximated as a POM shell inside\nthe ICS, indicated in green in the picture) and the SeF 6gas (blue). In the\nfisrt phase N νDEx will use natural Se (abundance of82Se: 8.7%), while in\nthe second phase enriched Se will be used (82Se abundance >90%). We will\nuse natural Se in our simulations; we will also show in Sec. 5 that an increase\nin abundance of82Se will only decrease the background rate. The lead shield\nwill stop the vast majority of the environmental γ’s, however in order to\nstop neutrons, low-Z materials are preferred. For this reason, high-density\npolyethylene (HDPE) shielding will be considered: it is formed by a chain of\nC2H4, the hydrogen atoms contained in it are very effective in slowing down\nand absorbing fast neutrons. Since it is relatively cheap and it is possible to\nobtain very low radioactive contamination levels, it is ideal for the task.\nWe considered two different configurations for the positioning of the\nHDPE shield, which are shown in the bottom panels of Fig. 3. The most\nstraightforward solution is to place it outside the lead shield, this configura-\ntion will be indicated as ”external”. Another possibility is to fill the empty\nspace between the SSV and the lead shield with HDPE, except for the regions\noccupied by the holder or the pipes (and, in case this is not sufficient, to add\nan external shield as well); this configuration will be indicated as ”full” or\n”full filler”.\n(a) No Shield\n(b) External\n (c) Full\nFigure 3: Different configurations considered for the HDPE shield (top panel:\nno shield).\n53 Neutron-Induced βDecays\nNeutrons can be captured by nuclei in the detector, creating unstable iso-\ntopes, that can emit αandβparticles during their decay.\nIf these particles are produced in the shields or in the supporting struc-\ntures of the detector, it would not constitute a problem, since they will easily\nbe stopped; however, if the nucleus activated is already inside the fiducial vol-\nume, its decay could be mistaken as a ββevent. In the N νDEx experiment,\nour working gas is SeF 6. The expected energy resolution is 1% FWHM, and\nthe ROI is between 2.98 MeV and 3.01 MeV [2]; the only isotopes that could\nbe a source of background are20F,16N,19O and83Se, which can be created\nvia the reactions\n19F+n→20F\n19F+n→19O+p\n19F+n→16N+α\n82Se+n→83Se (2)\nOther unstable isotopes, such as79Se,81Se, etc..., can be created as well, but\nthey can be ignored since their Q-value is lower than the ROI.\nIn the decay of all these isotopes a single βwill be emitted, except for\n83Se, which will have a decay chain:\n83Se− →83Br+e−+ ¯ν83Br− →83Kr+e−+ ¯ν (3)\nhowever the Q-value of83Br decay is 0.977 MeV, lower than the ROI, more-\nover its half-life is 2.4 h, which means we do not have to worry about pile-up\nbackground2.\nIn Tab. 1 are reported the Q-values and PROI,i.e. the fraction of β\nemitted with energy within the ROI, for all the relevant isotopes.\nIn Fig. 4 you can see the contribution of each isotope to the total back-\nground rate using external shield. The main contribution to the background\ncomes from20F, which has the highest PROI, 9.1×10−3. It should be pointed\nout that, after the decay, there could be γ’s emitted as well (for example,\nin the decay of20F, a 1.6 MeV γwill be emitted), however those can be\nsafely neglected in this particular case. First of all, the γ’s will travel con-\nsiderably more in the detector before interacting (if they do interact at all),\n2pile-up background could happen if two separate background events, both with en-\nergies <ROI, happen very close one to each other: they could be mistaken for a single\nevent, and the sum of their energy could be within the ROI. See Ref. [2] for an estimation\nof pile-up background in N νDEx.\n6Table 1: Q-value and PROIfor the relevant isotopes.\nIsotopes Q-Value (MeV) PROI\n20F 7.02 [12] 9 .1×10−3\n16N 10.04 [13] 6 .3×10−3\n19O 4.82 [14] 4 .6×10−3\n83Se 3.67 [15] 2 .4×10−5\nF20 Se83\nO19 N16\nTot\n10 20 30 40L(cm)10-510-40.0010.0100.100110Events/Year\nFigure 4: Contribution of each isotope to the background rate using external\nshield, as a function of the thickness\nwhich means it will be easy to distinguish the tracks caused by the γ’s emit-\nted during a radioactive decay from the ones due to the βemitted in the\nsame decay, so pile-up background is not a concern. Moreover, even if some\nhigh-energy γ’s can be emitted during these decays, these events are quite\nrare and their contribution to the total background rate will be negligible\ncompared to the γ’s produced via (n, γ) and (n,n’ γ) reactions (that can be\ncreated in the fiducial volume as well).\nWe can also notice that the contribution of83Se to the total background\nrate is negligible, since its PROIis only 2 .4×10−5, almost three orders of\nmagnitude lower than20F’s.PROIfor16N and19O are comparable to20F’s,\nbut their production rates are considerably lower since these reactions have\nan energy threshold, namely the neutron energy must be higher than 0.5 and\n3.5 MeV in order to create16N and19O, respectively. In Fig. 5 it is reported\nthe total background rate due to βdecays inside the fiducial volume for all\nthe configurations considered; to facilitate the comparison, in the x-axis it is\nreported the total amount of HDPE used, not the thickness of the external\nshield.\n7External\nFull\n5 10 15 20HDPE(ton)10-510-40.0010.0100.100110Events/YearFigure 5: Background rate due to βdecays inside the fiducial volume the\nthree configurations reported in Fig. 3. To facilitate the comparison, in the\nx-axes is reported the amount of HDPE used; the first data point corresponds\nto no external shielding at all, while in the others the thickness of the HDPE\nexternal shield is increased by 10 cm each step\n4 Neutron-Induced γ’s\nEnvironmental neutrons can create γ’s while interacting with the material of\nthe detector, via (n, γ) or (n, n’ γ) reactions. For example, if a hydrogen atom\nabsorbs a thermal neutron, it will emit a characteristic 2.2 MeV γ. Since this\nenergy is lower than the ROI, these γ’s cannot provide background, however\nall the other elements present in the materials of the detector will emit higher-\nenergy γ’s: their energies can go up to 5 MeV if a neutron is captured by a\nC atom (present in HDPE), 7-9 MeV if the target is Cu, Pb or Fe.\nIn Fig. 6 it is reported the energy deposited in the detector by neutron-\ninduced γ’s, in the absence of HDPE shielding. It is possible to see several\npeaks due to neutron capture: the 2.2 MeV peak due to hydrogen capture\nis clearly visible, as well as the peaks at 159, 279 and 878 keV due to63Cu\ncapture. It is also possible to see peaks at 239 and 1202 keV, due to neutron\ncapture by76Se and at 613 keV due to77Se [16]. The peak at 511 keV is not\ndue to neutron capture, but to electron-positron annihilation.\nIn Fig. 7 it is reported the background rate due to neutron-induced γ’s;\nfor comparison, the background rates due to neutron-induced βdecay and\nto natural radioactivity are reported as well. It is possible to see that, in all\nconfigurations, the γcontribution is dominant. If no HDPE shield is used,\nthere would be 1492 ±93 events in ROI/year due to neutron-induced γ’s,\nwhile the background due to βdecay in the fiducial volume would be only\n38.7±1.6 events/year. Moreover, even if HDPE is placed between the lead\nshield and the SSV, the neutron-induced background would still be larger\nthan the one due to natural radioactivity, which means that an external\n8Figure 6: Energy deposited in the detector by neutron-induced γ’s, if no\nHDPE shielding is added. Between red dashed lines it is indicated the ROI\nshield will always be needed. The presence of a filler between the lead shield\nand the SSV, however, will reduce significantly the amount of HDPE needed.\nIf only an external shield is present, it must be at least 60 cm thick, which\nwould require around 40 tons of HDPE. On the other hand, using a filler, a\n20 cm external shield will be sufficient to reduce the neutron background to\nsubdominant levels; this would correspond to 19 tons of HDPE in total.\n5 Enriched Se\nAfter the first phase, where natural Se will be employed, N νDEx will use\nenriched Se, where the abundance of82Se will be increased up to 90%. As it\ncan be seen in Tab. 2,82Se has the lowest absorption cross-section for thermal\nneutrons. In particular,76Se and77Se, have absorption cross-sections which\nare three orders of magnitude higher. This means that, if the abundance\nof82Se increases, fewer neutrons will be absorbed by Se isotopes, which will\nhave two consequences: on one hand, the production rate of other isotopes,\nin particular20F, would increase; on the other hand, there would be fewer\nγ’s emitted due to neutron capture in the fiducial volume.\nIn Fig. 8 (left panel) it is reported the background rate using natural\nSe or pure82Se3(only external shield was considered). We can see that,\n3Since the exact isotopic composition of enriched Se is not known yet, we have used\n9External-γ Full-γ\nExternal-β Full-β\nNatural Radioactivity\n10 20 30 40 50 60HDPE(ton)10-40.011100Events/YearFigure 7: Background due to neutron-induced γ’s and βdecays. For com-\nparison, the background due to natural radioactivity is indicated as well, as\nestimated in [2]\nas expected, the neutron-induced γbackground is lower if the gas contains\nonly82Se. Such a difference is of the order of 50% if there is no shielding\nat all (Fig. 8, right panel), it is more difficult to give a precise estimation if\nthe neutron background is strongly suppressed, due to the large statistical\nerrors4. The background due to neutron-induced βdecays is higher if82Se\nis used, but since such a contribution is strongly subdominant, the total\nbackground is nonetheless lower.\n6 Conclusions\nIn this paper we used Geant4 simulations to study the neutron background\nfor the N νDEx experiment.\nThe main source of background are neutron-induced γ’s. Neutrons could\nalso activate nuclei directly in the fiducial volume and induce βdecays, how-\never this source of background is strongly subdominant.\nThe material we considered for shielding is HDPE: due to its high hy-\npure82Se in our simulations, as the most extreme case: the actual background rate for\nenriched Se would lie in between those results and the ones obtained with natural Se\n4it should be pointed out that a more precise estimation of the background rate would\nbe not useful for the purpose of this paper since we are interested only in an upper limit\non the neutron background\n10Table 2: Thermal neutron absorption cross section for Se isotopes (cross\nsections taken from [17])\nIsotope Abundance (%) σn(barn)\n74Se 0.89 51.8\n76Se 9.4 85.\n77Se 7.6 42.\n78Se 23.7 0.43\n80Se 49.6 0.61\n82Se 8.7 0.044\nNatSe-γ Se82-γ\nNatSe-β Se82-β\n10 20 30 40 50 60HDPE(ton)10-40.011100Events/YearExternal\nγ\nβ\n10 20 30 40 50 60HDPE(ton)1234NatSe/Se82External\nFigure 8: Left panel: background rate (external shield) using natural Se or\n82Se. Right panel: ratio between the background rates\ndrogen content, it is very effective in slowing down and stopping neutrons.\nIf only an external HDPE shield is used, it must be 60 cm thick for the\nneutron background to be reduced down to 0 .24±0.06, lower than the natu-\nral radioactivity background (0.42 events/year); the total amount of HDPE\nneeded in this configuration would be of the order of 40 ton.\nWe found that the amount of HDPE needed can be significantly decreased\nif an HDPE is placed between the lead shield and the SSV. A certain amount\nof external shield is always needed, however the requirements on the thickness\nof such a shield can be significantly lowered. Using an HDPE filler, only a\n20 cm thick external shield will be required to reduce the neutron-induced\nbackground down to 0 .15±0.05 events/year, which corresponds to 19 tons\nof HDPE.\nWe have also shown that, if enriched Se is used, the neutron background\nrate will be lower. Indeed, if the abundance of82Se increases, the total\nneutron absorption cross-section of Se decreases. On one hand, this means\nthat the production rate of other isotopes, in particular20F, will increase,\nhowever this source of background is strongly subdominant. On the other\nhand, fewer neutron-induced γ’s will be created in the fiducial volume: since\n11this is the main source of neutron background, the total background rate will\ndecrease.\nAcknowledgement\nThis project is supported by the National Key Research and Development\nProgram of China 2021YFA1601300, From-0-to-1 Original Innovation Pro-\ngram of Chinese Academy of Sciences ZDBS-LY-SLH014, and International\nPartner Program of Chinese Academy of Sciences GJHZ2067. We wish to\nthank Hao Qiu, Qiang Hu and Fengyi Zhao for the useful discussions and\nsuggestions.",
      "metadata": {
        "filename": "Neutron Activation Background in the NvDEx Experiment.pdf",
        "hotspot_name": "Steel_Punching_and_Bending",
        "title": "Neutron Activation Background in the NvDEx Experiment",
        "published_date": "2023-07-24T13:33:15Z",
        "pdf_link": "http://arxiv.org/pdf/2307.12785v1",
        "query": "punching and bending steel EMC shield material substitution studies resource reduction percentages"
      }
    },
    "Radiation Shielding Properties of _Nd_0.6_Sr_0.4_Mn_1-y_Ni_y_O_3_ Substitute wit": {
      "full_text": " \n Radiation Shielding Properties of Nd 0.6Sr0.4Mn 1−yNiyO3 Substitute  with \nDifferent Concentrations of Nickle  \n \nM. Kh. Hamada, M. H. A. Mharebb,c,*, Y. S. Alajeramid,e, M. I. Sayyedf, Gameel Salehg                                          \nY. Maswadehh, Kh.  A. Z iqa \naPhysics Department, King Fahd University of Petroleum and Minerals, Dhahran 31261, \nSaudi Arabia  \nbDepartment of Physics, College of Science, Imam  Abdulrahman Bin Faisal University, P.O. \nBox 1982,  Dammam 31441, Saudi Arabia  \ncBasic and Applied Scientific  Research Center, Imam  Abdulrahman Bin Faisal University, \nP.O. Box 1982,  Dammam 31441, Saudi Arabia  \ndPhysics and Astronomy, Science Faculty, Ohio University, USA  \neMedical Imaging Department, Applied Medical Sciences Faculty, Al Azhar University -Gaza \nfDepar tment of Physics, Faculty of Science, University  of Tabuk, Tabuk, Saudi Arabia  \ngDepartment of Biomedical Engineering, College of Engineering, Imam Abdulrahman Bin \nFaisal University, P.O. Box 1982, Dammam, Saudi Arabia  \nhDepartment of Physics and Science of Advanced Materials Program, Central Michigan \nUniversity, Mt. Pleasant, MI 48859, United States  \n*Corresponding author:  Dr. M . H. A. Mhareb (mhsabumhareb@iau.edu.sa)  \n \nAbstract  \nIn this work, we investigate the effect of Ni concentration on several shielding p roperties of  \nNd0.6Sr0.4Mn 1−yNiyO3 (0.00≤y≤0.20) perovskite ceramic for possible use as radiation shielding \nmaterials. X -ray diffraction (XRD) analysis revealed that these ceramics have the orthorhombic \nstructure with group space Pnma over a wide range of N i-substitutions. Moreover, the analysis \nshowed a nearly linear decrease in the lattice parameters and the unit cell volume (V) causing a \ngradual increase in the packing density with increasing Ni concentration. The shielding features \nfor photons, neutrons,  and protons of all ceramic samples were assessed.  The mass attenuation \ncoefficient (MAC) was computed at 0.1, 0.6, 1.25, 5 and 15 MeV by utilizing (MCNP) (version  \n 5.0); the results were compared with the corresponding values obtained using Phy -X and XCOM  \nprogram. The results obtained showed slight enhancement with increasing Ni contents. The \nsubstitution of Ni leads to progressive enhancement in effective removal cross -section of fast \nneutron (∑ R) values. Whereas the values of Mass Stopping Power (MSP) an d projected range for \nthe protons showed a gradual reduction with increasing Ni concentration. These findings suggest \nthat the current ceramic samples can be useful as radiation shielding materials . \nKeywords:  Ceramic; Radiation shielding ; MCNP; Proton; Neu tron \n1- Introduction  \nFor example, reducing the exposure time, increasing the distance between radiation sources, and \nusing proper and suitable shielding between the individuals and radiation source. High atomic \nnumber material (high density) such as Lead (Pb)  is commonly used as a shield to protect from \nionizing radiation such as gamma and X -ray. However,  toxicity and health hazards for many \nmaterials (lead for example) necessitated the use of other materials such as glasses and ceramics \nfor shielding applicat ions (Gaikwad et al., 2018a; Bagheri et al., 2018; Al -Hadeethi and Sayyed, \n2020; Junior et al., 2017). One of the key factors for choosing proper shielding materials is the \ngamma rays attenuation coefficients which largely depend on the density of these ma terials. The \nattenuation coefficient has been measured for several materials such as glass, concrete, granite, \npolymer and ceramic (Najam et al., 2016; Al -Hadeethi et al., 2020a; Al -Hadeethi et al., 2020b; \nAkkurt et al., 2005; El -Khayatt et.al., 2010; El -Khayatt and Akkurt 2013; Akkurt and Elkhayat A \n2013; Kaky et al, 2020; Sayyed et al., 2020; Alya Abdsalam et al., 2020).  \nCeramics have been extensively used in several medical and industrial applications as they are \nstable at high temperature, high resistan ce to oxidation, low thermal expansion, and low \npermittivity (Zhang et al., 1998). Ceramics  are non -organic compounds  with several attractive \nproperties such as durability, hardness, toughness, modulus, lack of porosity, and \nenvironmentally friendly (non -toxic). Because of these properties, the ceramic composites are \nreceiving increasing interests to be used in the radiation shielding  field. \nAmritphale et al., (2007a) used an aluminum industry waste (known as red mud or RMSM) as a \nshielding material. RMSM a nd barium compounds have been formed by phosphate bonding \nusing the ceramic processing route. The X -ray attenuation characteristics of the proposed \nbauxite -red radiopaque material exhibited much less “half -value thickness (HVT)” compared to  \n the conventiona l concrete and lead, for the various tube potentials (100 -250 KV) of X -ray \nphotons (Amritphale et al., 2007a). Moreover, celsian ceramics prepared from fly ash, from the \ncombustion of pulverized coal, have been developed as X -ray radiation shielding mater ials \n(Amritphale et al., 2007b). The fly ash was mixed with a barium compound to form a new fly \nash radiopaque material (FARM) using the ceramic processing route and phosphate bonding. \nRMSM and FARM prepared ceramic samples showed the distinctive propertie s and these \nsamples can be used properly for shielding in radiographic imaging rooms to protect against X -\nray. New ceramic composites based on boron carbide (B 4C), were evaluated as a radiation \nshielding material for pulsed neutron scattering instrumentati on (M. Celli et al., 2006).  Other \nresearchers have studied the gamma -ray buildup factor of ceramic/ceramic hosts such as \nmagnesium diboride (MgB 2), titanium carbide (TiC), boron nitride (BN), silicon carbide (SiC), \nferrite (Fe 3O4). The exposure buildup fa ctor for apatite, zirconolite and other ceramic hosts was \ninvestigated by T. Singh et al., 2013; V.P. Singh et al., 2018. The effective properties of \nshielding for another type of ceramic, mullite -barite ceramic, were studied by Ripin et al., 2018. \nRadiati on tests showed that the best shielding ability of ceramics where the radiation attenuation \nbetween 99.11% and 97.42% was high at the tube potential 70 kV. Jawad et al., 2019 introduced \ndifferent glazed and unglazed ceramics as shielding materials and meas ured their linear \nattenuation coefficient using two radiation sources:  60Co, and 137Cs. It was concluded that the \nglazed ceramics exhibited better attenuation of gamma radiation than the unglazed. Glazed \nceramics may show reduced porosity and enhanced dens ity making them more effective in \nradiation shielding. Akman et al. 2019 studied the shielding properties of various types of \nceramics (silicide, boride and oxide ceramics) using transmission geometry at different energies \nvalues. The study showed that the  lowest and highest values of the mean free path (MFP) and \nhalf-values layer (HVL) were discerned for titanium dioxide (TiO2) and magnesium silicide \n(Mg 2Si), respectively.  \nIn this work, a series of the new perovskite ceramic Nd0.6Sr0.4Mn 1−yNiyO3 were prepared in order \nto investigate the shielding properties. We also investigate the effects of Ni -substitution on \nvarious shielding properties . X-ray diffraction measurements have been used to obtain various \nphysical properties. The calculated  parameters are based on the mass attenuation coefficient \nextracted from  Monte Carlo N -Particle Transport Code System (MCNP) (version 5.0) , XCOM, \nand Phy -X program.    \n 2- Material and methods   \n \nPerovskite ceramics of nominal composition Nd0.6Sr0.4Mn 1−yNiyO3 with y = 0, 0.1, 0.15 and 0.2 \nand referred to as C1, C2, C3 and C4 respectively in this paper. The s olid-state reaction \ntechnique  has been used to prepare the stoichiometric ceramic samples  (A.R. West . 2014) . High \npurity (4N to 5N) oxides of Neodymium oxide ( Nd2O3), strontium carbonate  (SrCO 3), \nmanganese oxide ( MnO 2), and nickel oxide ( NiO) were mixed and ground  into powder in an \nagate  mortar and pestle for about 30 minutes. The resulting powder was pressed into pellets and \nsintered at 1000 °C for 24 hours , and then at 1300  °C for  another 10 hours  (M. Kh. Hamad et al., \n2018; M.  Kh. Hamad et al. 2019). Table 1 shows  the composition ratio for the ceramic samples.  \nThe phase purity and the lattice parameters were calculated from  x-ray diffraction patterns .  \nBruker x-ray diffractometer  with Cu Kα (λ=1.54056 Å) has been used to obtain the XRD -\npatterns at  room temperature . The structure s were  examined  between 20o to 80o using \nFULLPROF software (R.A. Young 1995) . The electronic balance (Melter Toledo) with a \nsensitivity of 10-4 g was utiliz ed to measure  the density (ρ) based on Archimedes’ principle  using  \ntoluene as a buoyant liquid .  \nFrom the density data, the molar volume (Vm) can be computed  using : \n                                                                  𝑉𝑚=𝑀\n𝜌                                                                   (1) \nHere,  M is the average molecular weight for ceramic samples. On the other hand , there are some \nsignificant structural parameters such as packing density (V t). This parameter  was deduced from \nmolar volume data and calculated using  the following equations:  \n \n                                                                       𝑉𝑡=∑𝑉𝑖 𝑥𝑖\n𝑉𝑚𝑖                                                             (2) \n \nWhere V i and xi represent the packing factor, and the mole fraction of the oxides  respectively . Vi \ncan be obtained  by Pauling’s ionic radii (L. Pauling 1 940). While  the Poisson's ratio (σ) values \ncan be calculated  from  the Vt data by utilizing the following equation:   \n     \n𝜎=0.5−(1\n7.2𝑉𝑡)                                                             (3) \n  \n In this paper , Monte Carlo N -Particle Transpor t Code System (MCNP) (version 5.0) was \nconducted to estimate the MAC  of the new ly prepared ceramics. Previous studies have reported \nthe efficiency of MCNP to evaluate radiation shielding parameters (M.G. Dong  et al. 2017;  H.O. \nTekin and U. Kara 2016; K.A. Mahmoud 2019) . The simulation was conducted in a sphere of \n100 cm radius filled with dry air (ρ = 1.205 x 10-3 g cm-1). A point source with a mono -energetic \nbeam emission was assumed as the r adiation source, and exposed perpendicular to the front \nsurface of ceramic sam ples (in the Y -axis direction). To reduce possibility of background \nradiation, lead shielding collimators were assumed to be around the ionization chamber and \nceramic sample as shown in Fig. 1. The irradiation process was conducted with and without \nceramic  samples to estimate the value of I/I o. The simulation was performed based on the amount  \nof Ni and repeated three times for each sample (C1 -C4). The radiation source was proposed as a \npoint source and the required commands such as energy (ERG), type of rad iation (PAR), position \n(POS) and direction (DIR) were fully defined accordingly. The molecular weights of each \nconstituent  in the ceramic samples were obtained from the XCOM database program.  \nIn the current simulation, the required data for simulation pro cesses derived from the evaluated \nnuclear data file (ENDF), advanced computational technology initiative (ACTI), evaluated \nnuclear data library (ENDL), evaluated photon data library (EPDL), activation library (ACTL) \nand evaluations from the nuclear physics  (T–16) group at Los Alamos. The energy detection in \nthe NaI detector was determined by estimating the mesh tally (F4). The number of histories was \n), and geometry was simplified to reduce errors and variance of variance 7increases (NPS > 10\n(VOV). The simu lation was monitored by utilizing the CUT -OFF option to eliminate irradiation \nof low energy (< 1 keV) .  \nThe obtained results of MCNP -5 code were compared with  the results of XCOM  (Berger and \nHubbel (1987 ) and Phy -X (Sakar, et al. 2020) . In addition, severa l shielding properties such as \nMAC, mean free path (MFP), half-value layer (HVL), electron density, effective atomic number \n(Zeff), energy absorption build -up factor (EABF) , exposure build -up factor  (EBF) have been \ninvestigated. MSP and projected range for  the protons were evaluated of all ceramic samples. \nWhile ∑R was evaluated of all prepared ceramic samples . The shielding features were computed  \nby utilizing  several formula s listed in former  work  (Mhareb et al., 2019).   \n \n3- RESULTS AND ANALYSIS   \n Fig. 2 presents  XRD  patterns of all prepared ceramic samples. Refinemen t analysis of the \ncrystalline structure shows that all samples are consistent with the standard orthorhombic pattern \nwith Pnma (62) space group. However, there is a presence of a minor cubic perovskite phase in \nall samples. The cubic phase is commonly foun d in high -temperature treatment of these samples \n(M. Kh. Hamad et al. 2019) . The peak s are shifted to higher values of  2θ with increasing the Ni \nconcentration  in the ceramic  (inset of Fig. 2). This shift demonstrate s a steady  decrease  in the \nlattice parameter s (Table 2). The refined lattice parameters for all samples are shown in Table 2. \nThe results show  a nearly  linear decrease  in the parameters (a, b, and c), and the unit cell volume \n(V) with increasing  Ni concentration. Moreover, this leads to a gradual increase in the packing \ndensity  (Vt) of the ceramics with increasing Ni concentration.  \nSome  physical and structural properties such as molar volume (V m), packing density (V t), and \nPoisson's ratio (σ)  were calculated . These parameters are listed in Table 3. The packing density \nvalues show a significant increment with substitute the Ni at the Mn sites. This increase is in line \nwith the gradual reduction in the molar volume valu es. This increment in ceramic density can be \nattributed to substitute light element (Mn) by  heavy element  (Ni). Table 3 also lists the Poisson's \nratio which is a measure of the rigidity of the prepared ceramic samples . The Poisson’s ratio \nvaries between 0 -0.5 for most materials .  When the Poisson’s ratio value was above 0.3, the \nmaterials can be classified by low cross -linking density. While the materials with high cross -\nlinking density have Poisson's ratio values below 0.3 (V.V. Gowda et al., 2005). The va lues of \nPoisson's ratio for the ceramic samples are above  0.4, indicating low cross -linking for the current \nceramic samples. It is observed  slight variation in the Poisson's ratio values confirms the rigidity \nfor ceramic samples with substitute the Ni inst ead of Mn.  \nThe intensities of photons (both I and I 0) that pass through the ceramics have been enlisted in \nTable 4. These values helped us to simulate the MAC  values  for the investigated C1 -C4 samples \nat 0.1, 0.6, 1.25, 5 and 15 MeV. In Table 5, we summar ized the MAC  values for the selected \nceramics obtained by three methods namely MCNP5 code, Phy -X, and XCOM computer \nprograms. Moreover, the MAC  (both MCNP -5 and XCOM findings) for the present ceramics is \nplotted in three -dimensional  figure as a function of  the photon energy and the Ni-concentration \nas shown in Fig. 3. This is important to validate the simulated input file and to test the accuracy \nin the simulated µ/ρ . The data presented in  Table 5 indicate a qualitative agreement within \nacceptable variance between various results obtain ed by  MCNP5 code and data obtained by Phy- \n X and XCOM. The deviation between MCNP5  and XCOM as given in Table 5 varied between \n1.43-5.16% for C1, 1.21 -3.51% for C2, 0.49 -3.82% for C3 and 0.31 -2.84% for C4. As we can \nsee, the deviation is for all samples and  at any energy is less than 6% and this gives the \nconfidence in the present result.  For example, for C1 and at 0.1 MeV, the MAC  value obtained \nby the simulation method is 1.0191 cm2/g, while the corresponding value obtained by Phy -X \nsoftware is 0.9312 cm2/g and that evaluated by XCOM is 0.9447 cm2/g. \nIn order to understand the effect of the energy and the content of Ni on the radiation attenuation \nfeatures for the prepared ceramics, we plotted the XCOM results  in the energy range of 0.015 to \n15 MeV  in Fig.  4. This figure shows that the MAC  values for C1, C2, C3 and C4 ceramics \ndecrease with the increasing the energy. This is mainly due to several photon interac tion \nprocesses existing  at different energy zones (M.I. Sayyed  et al., 2020).  The results in Fig. 4 \nshowed a significant reduction in MAC  values at low energies.  For C1 (as an example), the \nMAC  changes from  about 50  to 1.5 cm2/g between these energies.  This is due to the \nphotoelectric effect which is  dominant at low energy. The chances of occurrence of this mode of \ninteraction change with the energy as E-3.5. Also, this mode of interaction changes  with the \natomic number as Z4-5, therefore the  expected maximum values of MAC  occur red in this zone. \nThe maximum MAC occurs at 0.015  MeV and equals to 49.890, 5 0.460, 50.747 and 51.034  \ncm2/g for C1, C2, C3 and C4 respectively . In the second energy zone namely between 0.15 and 1 \nMeV , the Compton scattering is energetically possible . Due to this process, MAC  changes \nslightly between 0.570  and 0.070  MeV  (for C1) . Also, due to this process, we can see that the \ncomposition of the ceramics is not affect ing the µ/ρ  values.  For example, at 0.2 MeV, the MAC  \nvalues for C1, C2, C3 and C4 respectively are  0.3137, 0.3148, 0.3153 and 0.3158 cm2/g \nrespectively . For the photon energy larger  than 1 MeV, the pair production becomes very \nimportant and accordingly the MAC becomes almost constant with energy.  This is in agreement \nwith the recent findings reported by different groups (M.I. Sayyed  et al., 2019;  M. Kurudirek  et \nal., 2018) . \nThe HVL for the C1, C2, C3 and C4 ceramics is plotted in Fig.  5. Evidently, for all te sted \nceramics, the HVL increases as the energy increases.  The increase  is dramatically steep for \nenergies bellow about 1 MeV  followed by gradual increases , reaching maximum value near 10 \nMeV . The minimum HVL is reported at 15 MeV and equals to 0.0027, 0.00 25, 0.0023 and \n0.0022 cm for C1, C2, C3 and C4 respectively. The discontinues in the HVL at 0.05 MeV  is  \n attributed to the K -absorption edge of Nd (Z=60).  At 0.1 MeV, the HVL values for the tested \nceramics are 0.0884, 0.0846, 0.0777 and 0.0726 cm. While, at  1 MeV the HVL increases to \n2.2022, 2.1148, 1.9474 and 1.8214 cm for C1 -C4. This implies that it is important to increase the \nthickness of the ceramics in case of utilizing the ceramics in applications  required high photon \nenergies. Also, Fig.  5 shows that  the HVL of these ceramics  changes  in order as C1>C2>C3>C4.  \nThis is related to the amount of Ni in the ceramics. The replacement of Mn by Ni causes an \nincrease in the density (see Table 3), therefore the HVL decreases with the addition of Ni.  The \nHVL curve s reveal that an improvement in the radiation attenuation features for the tested \nceramics with the addition of Ni.   \nThe MFP for the C1 -C4 ceramics is plotted as a function of the energy in Fig. 6 -A, while in Fig. \n6-B we plotted the MFP as a function of t he density at some selected energies namely 0.1, 0.6, \n1.25, 5 and 15 MeV. The minimum MFP for the four tested ceramics was noticed for \nE<0.1MeV. It lies within the range of 0.0031 -0.0039 cm at 15 keV and 0.1047 -0.1276 cm at 0.1 \nMeV. For E>0.1 MeV, the MFP increases rapidly as can be noticed from Fig. 6 -A. This means \nthat the MFP has the same HVL trend and it is in line with the results reported for some \nmaterials such as glasses (Ozge Kilicoglu and H.O. Tekin, 2020). Beyond 1 MeV, the MFP \nshows an energy -dependent behavior. This suggests that as the energy increases, the probability \nof radiation interaction with the C1, C2, C3, and C4 ceramics decreases, and hence more photons \ncan penetrate the tested ceramics. From Fig. 6 -B, increasing the density of the te sted ceramics \nleads to a decrease in the MFP. This result suggests that increasing the Ni content in the ceramics \n(which affects the density) can reduce the thickness required to shield the radiation to a specific \nvalue. For example, the density changes fr om 5.182 g/cm3 (for C1) to 6.258 g/cm3 (for C4) and \nthe MFP values of these two samples are 2.294 and 1.896 cm (this is at 0.6 MeV). At 5 MeV, the \nMFP of both ceramics is 5.707 cm (for C1) and 4.707 cm (for C4).  These results are in line with \nthe results f or other ceramics such as silicide, boride and oxide ceramics (Akman et al, 2019) . \n The gamma radiation shielding feature for the tested ceramics is investigated in term of the \neffective atomic number  (Zeff). In Fig.  7 we plotted the Z eff for the ceramics under study  as a \nfunction of photon energy and the content of Ni.  This figure shows that the Z eff for all test \nceramics decreases with increasing the energy (except at 0.05 MeV).  Several theoretical and  \nexperimental works have reported  the same  evidences f or the dependency of Z eff upon the energy \nlike rocks (Obaid, Shamsan S., et al., 2018) , and glasses (D. K. Gaikwad,  et al., 2018) . Also,  \n from Fig . 7 we can conclude  that the lowest and highest values of Z eff are corresponding to C1 \nand C4 respectively. As C4 contains t he maximum amount of Ni (y=0.2 ), this explains the \nhighest  Zeff for this ceramic.  This emphasizes our findings in the previous curves that the \naddition of Ni improves the attenuation ability for the tested ceramics and C4 can effectively \nabsor b more photons than C1, C2 and C3. The Z eff values at 0.015 MeV varied between 45.90 \nand 46.01 , at 0.05 MeV varied between 53.86 and 53.88 (and these are the maximum Z eff values \nreported for these ceramics), while at 1 MeV the Z eff takes the following valu es: 20.23, 20.42, \n20.51 and 20.61 for C1, C2, C3 and C4 respectively. For these ceramics,  the Z eff reaches the \nminimum values at 1.5 MeV and equals to 19.99, 20.81, 20.27 and 20.37. For E>1.5 MeV we \ncan see that the Z eff increases with increases the energy  due to the domination of pair production. \nFor instance, the Z eff for C1 changes from 20.24 to 29.31 between 2 and 15 MeV and from 20.62 \nto 29.77 for C4.  Additionally, Fig.  8 presents the relation between the Z eff and the effective \nelectron density . It is evident that the Z eff changes linearly with the effective electron density \nwhich means that both quantities have similar behavior with the energy.   \nThe variation  of EBF and EABF with the energy  for C4 (as an example) at some penetration \ndepths  has been  plotted in Fig . 9 (a for EBF and b for EABF).  C1, C2 and C3 have similar shape \ngiven in this figure. Obviously,  the value  of both parameters  for all penetration depths increases  \nwith the increase  of the energy  up to 0.05 M eV and sudden  jumps occur at  this ene rgy and these \ncan be explained by the k edge absorption of Nd as we mentioned in the HVL curves.  Thereafter,  \nthe EBF and EABF increase and reached the maximum at about 1 MeV, then again decrease in \nslight rate up to around 8 MeV and then both parameters in crease quickly especially  at 40 mfp. \nThe present trend in these parameters can be demonstrated according to three important mode of \nradiation interaction with the matter as discussed in detail  by Manjunatha and Rudraswamy  \n(H.C. Manjunatha et al., 2012) . It is important to mention that at fo r E>8 MeV and for 30 and 40 \nmfp, both EBF and EBAF have relatively high values. For example at 10  MeV, the EBF is \n290.19 ( at 40 mfp), while the EABF is 201.59.  Increasing the penetration depth of the tested \nceramics causi ng an increase of the thickness which in turn results in increasing the scattering \nevents in the ceramic samples  and this is the reason for the large EB F and E ABF values.  \nIn addition , the variation of ∑R with the content  of Ni is represented graphically in Fig.  10. The \nresults show that C1 and C4 have the lowest and largest values of ∑ R respectively. The  addition \nof Ni leads to enhancement in the neutron shielding ability since the ∑ R increases with the   \n addition of Ni content.  The ∑ R values for the tested ceramics are 0.109, 0.113, 0.122 and 0.130 \ncm-1. Recently, Kaçal et al. (Mustafa R. Kacal, et al., 2018)  prepared different ceramics and \ncalculated ∑ R for these ceramics  and they  reported the following values for ∑R: peridot  (0.0983 \ncm-1), aluminum nitride  (0.1152  cm-1), yttrium oxide  (0.0881  cm-1), ruby (0.1248  cm-1), silicon \nnitride (0.1225 cm-1) and magnesium silicate  (0.1145 cm-1). From these results , we can see that \nour tested ceramics have better n eutron shieling properties than peridot  and yttrium oxide, while \nC4 has better neutron shielding ability than ruby silicon nitride and magnesium silicate .  \nThe Mass Stopping Power (MSP) elucidates the rate of energy loss from incident particle \nthrough the medium. This parameter is very important to show the amount of energy created in a \nspecific area per 1 gram in the prepared ceramics. In the current study, we used the SRIM code \nwith the ESTAR database offered by the National Institute of Standard and Tech nology (NIST)  \n(Ziegler et al., 2008; SRIM.org).  Fig. 11 exhibits the calculated MSP of the prepared ceramics at \na different range of proton (H) energies (0.01 -10 MeV). In all prepared ceramics, the MSPs \nincreased with increased increase kinetic energy of t he proton. In addition, it is clear that the \nMSPs are in an opposite relation with the conc entrations of Ni, where C4 (0.2 of Ni) has the \nlowest MSP.  \nFig. 12 shows the calculated projected range of the prepared ceramics in case of proton \nirradiation. This parameter is significant to determine the effective shielding material by express \nhow far the proton can penetrate and at what depth can rest. This thickness will increase with \nincreasing energy of the incident proton. The current ceramics exhibit ver y promising thickness \nbased on the tested kinetic energy (0.01 -10 MeV), particularly for C4.  \n4- Conclusion  \nBy using a conventional solid -state reaction, four samples of Nd 0.6Sr0.4Mn 1−yNiyO3 \n(0.00≤y≤0.20) perovskite were fabricated to evaluate the shielding properties for photons, \nneutrons, and protons. The XRD results affirm the crystallinity nature and the major phase is \northorhombic structure for current ceramic samples.  The values  of packing density, MAC and ∑ R \nexhibit significant increment with addition Ni to the ceramic system . Moreover, the addition of \nNi inversely affected the MSP and the projected range for the protons. The obtained results \nshowed good shielding properties for  photons, neutrons, and protons. This improvement in  \n shielding features indicates the ability to use prepared ceramics samples as good shielding \nmaterials.  \n \nAcknowledgment  \nWe acknowledge the support provided by Deanship of scientific research, King Fahd U niversity \nof Petroleum &  Minerals, Kingdom of Saudi Arabia.  \n ",
      "metadata": {
        "filename": "Radiation Shielding Properties of _Nd_0.6_Sr_0.4_Mn_1-y_Ni_y_O_3_ Substitute wit.pdf",
        "hotspot_name": "Steel_Punching_and_Bending",
        "title": "Radiation Shielding Properties of $Nd_{0.6}Sr_{0.4}Mn_{1-y}Ni_{y}O_{3}$\n  Substitute with Different Concentrations of Nickle",
        "published_date": "2020-04-22T04:46:27Z",
        "pdf_link": "http://arxiv.org/pdf/2006.02525v1",
        "query": "punching and bending steel EMC shield material substitution studies resource reduction percentages"
      }
    },
    "Simulation of conventional cold-formed steel sections formed from Advanced High": {
      "full_text": "  Proceedings of the Annual Stability Conference Structural Stability Research Council San Antonio, Texas, March 21-24, 2017     Simulation of conventional cold-formed steel sections formed from Advanced High Strength Steel (AHSS)  Hamid Foroughi1 and Benjamin W. Schafer2  Abstract The objective of this paper is to explore the potential impact of the use of advanced high strength steel (AHSS) to form traditional cold-formed steel structural members. In this study, shell finite element models are constructed, and geometric and material nonlinear collapse analysis performed, on simulated lipped channel cross-section cold-formed steel members roll-formed from AHSS. AHSS sheet is currently being used in automotive applications with thickness ranging from 0.35 to 0.8 mm (0.0138 to 0.0315 in.) and yield strengths from 350 to 1250 MPa (51 to 181 ksi). However, AHSS has not yet been employed in cold-formed steel construction. To assess the impact of the adoption of AHSS on cold-formed steel member strength a group of forty standard structural lipped channel cross-sections are chosen from the Steel Framing Industry Association product list and simulated with AHSS material properties. The stress-strain models used in this study are based on AHSS in production, including dual-phase and martensitic steels. The simulations consider compression with work on bending about the major axis in progress. Three different bracing conditions are employed so that the impact of local, distortional, and global buckling, including interactions can be explored. Due to the higher yield stresses of AHSS the potential for interaction and mode switching is anticipated to be greater in these members compared with conventional mild steels. The simulations provide a direct means to assess the increase in strength created by the application of AHSS, while also allowing for future exploration of the increase in buckling mode interaction, imperfection sensitivity, and strain demands inherent in the larger capacities. The work is intended to be an initial step in a longer-term effort to foster innovation in the application of new steels in cold-formed steel construction.                                                      1 Graduate Research Assistant, Dept. of Civil Engineering, Johns Hopkins University, hforoug1@jhu.edu  2 Professor, Dept. of Civil Engineering, Johns Hopkins University, schafer@jhu.edu  \n 2  1. Introduction Cold-formed steel (CFS) structural members use cold bent sheet steel to provide efficient structural shapes that are noncombustible and highly structurally efficient. Use of cold-formed steel members continues to grow for both architectural and structural applications in building construction. Due to the manufacturing process thicknesses are naturally limited and ultimate strength of practical shapes is thus necessarily limited to a relatively modest value. Advanced High Strength Steels (AHSS) have been developed for the automotive industry over the last 20 years (Keeler and Kimchi 2014). AHSS sheet has yield stresses as high as 1250 MPa and are able to maintain large ultimate tensile elongations (>10%) even for these high yield stresses. As a result AHSS can be readily formed/manufactured and supply material yield values that are significantly in excess of current applications in CFS building construction. With this new strength comes new potential for design, particularly in mid-rise applications for CFS structural members.  Existing design specifications, such as the Direct Strength Method (DSM) in AISI S100-12, provide a potential design framework for CFS members formed from AHSS, but the validity of the provided rules has not been substantiated for higher strength steels. AISI S100-12 strength predictions include local-global (L-G) interaction, but based on experimental results at the time (Schafer 2002, Schafer 2008) excluded local-distortional (L-D), distortional-global (D-G), and local-distortional-global (L-D-G) interaction. More recent experimental research has shown that for higher strength (non AHSS) steels, such as G550 (Fy=550 MPa (80 ksi)), L-D interaction should be included (Yap and Hancock 2008, 2011). The interaction becomes more pronounced because of the additional local post-buckling demands, and because higher strength sections use thinner sheet steel requiring additional intermediate stiffeners, further complicating the response. Lead by Prof. Camotim at TU-Lisbon, Yap and Hancocks’s findings motivated new activity in buckling mode interaction for members. They began by using shell FE simulations and demonstrated conditions where L-D interaction may be significant (Dinis et al. 2011, Camotim and Dinis 2011, Silvestre et al. 2012). They then collaborated on a small test series on G550 columns to physically demonstrate L-D interaction and its related strength erosion (Young et al. 2013). Finally, additional simulations on simple and complex cross-sections and proposed design recommendations were provided (Dinis et al. 2014, Dinis and Camotim 2015, Martins et al. 2015, 2016). Work remains though as the AHSS steel grades have unique material response not captured in the studies on low ductility high strength G550 steel – and agreed upon solutions for handling these interactions are still needed.  Shell finite element (FE) models have proven to be a robust tool for exploring the strength of thin-walled cold-formed steel members (Schafer et al. 2010, Foroughi et al. 2014). Therefore, they are employed here to examine the impact of AHSS material properties on CFS member strength. This brief paper first summarizes the numerical modeling conducted, including boundary conditions, element choice, material model, and imperfections. Next the paper provides a summary of the strength results in comparison with the AISI-S100 strength predictions employing DSM. This summary is followed by brief comments on the results and a discussion of future work. This study is an initial step towards a longer-term study on the application of AHSS in CFS buildings and the development of improved design guidance.    3 2. Numerical modeling Shell finite element models of cold-formed steel members are developed and analyzed in ABAQUS. The based details of the developed models are provided in this section.  A total of six types of AHSS including both dual-phase and martensitic steels are considered here, as shown in Figure 1. In addition, a mild steel with yield stress 207 Mpa (30 Ksi) has also been selected for baseline comparison. To aid in comparisons across the materials the elastic modulus has been set to 203,500 MPa (29500 ksi) and the Poisson’s ratio to 0.3 for all materials. Note, for application in ABAQUS the engineering stress-strain is converted to true stress-strain.   \n Figure 1. Engineering stress-strain curves for different type of AHSS (Keeler and Kimchi (2014))      Forty standard structural lipped channel cross-sections from the Steel Framing Industry Association (SFIA) product manual are selected for study as detailed in Table 1. The nomenclature, e.g. 250S162-33 provides the nominal wed depth in inches 250=2.5 in. (63.5mm), the flange width in inches 162=1.62 in. (41.1mm), and the material designation thickness in mils 33 mils = 0.033 in. (0.84 mm) – note designation thickness is not equal to design or minimum delivered thickness. Round corners (r = 2t per SFIA) are considered in the models. The members are meshed using the quadratic shell element S9R5 in ABAQUS with a mesh density consistent with Figure 2. The length of the studied members is three times the critical length for distortional buckling, as determined by CUFSM with simply-supported end boundary conditions.  \n             (a) local b.c.                    (b) distortional b.c.          (c) global b.c.                 (d) local b.c. along the length Figure 2. Depiction of cross-section boundary conditions (b.c.) to establish different models  \n 4 To help separate the buckling response of the members three cross-section boundary conditions are explored as depicted in Figure 2. The local model (Figure 2a) allows only local buckling in the cross-section. The distortional model (Figure 2b) allows local and/or distortional buckling. The global model (Figure 2c) allows local, and/or distortional, and/or global buckling. Thus, the local model isolates L; the distortional model allows L, D, and L-D; while the global model allows all possibilities. The end boundary conditions in the model are locally pinned. All translation degrees of freedom are tied to a reference node at the shear center. The reference node is pinned and restricted from torsion at one end and restricted from in-plane translation at the other end. All actions on the model are applied at the reference node.  Table 1. List of cold-formed steel cross sections for parametric study Designation Thickness 33 43 54 68 97 250S162 ü ü ü ü  250S137   ü   350S162 ü ü ü   362S137    ü  362S162    ü  362S200   ü   400S137   ü   400S162    ü  400S200 ü ü ü   550S162 ü ü ü   600S137  ü ü ü  600S162    ü ü 600S200 ü ü ü  ü 800S137   ü ü  800S162    ü  800S200 ü     800S250  ü ü   1000S162  ü    1000S200     ü 1200S162   ü   1200S200     ü 1200S250   ü   1200S162    ü   A surrogate finite strip model in CUFSM is used to generate geometric imperfections. A linear combination of the first local, distortional, and global buckling mode are used as the geometric imperfection distribution. The imperfection magnitude employs the 50% probability of exceedance values from Zeinoddini and Schafer (2012), as given in Table 2.  Table 2. Selected 50% imperfection exceedance values  CFD Local (δ/t) Distortional (δ/t) Global Bowl (L/δ) Camber (L/δ) Twist (Deg/m) 50% 0.31 0.75 2909 4010 0.30  Residual stresses and strains are not considered in the models at this time.    5 3. Simulation results and discussion  For each studied cross-section the member is compressed until a peak strength (PFE) is achieved. Although complete response is of interest for future work, here only PFE is considered. For the local buckling boundary condition (Figure 2a), across the 40 studied members, and the 5 AHSS material grades and one mild steel grade, the normalized PFE as a function of local slenderness is provided in Figure 3 and compared with the DSM strength prediction for local buckling alone (PnLo). PnLo is determined from the DSM expressions in AISI S100-12 with the global strength set equal to the squash load of the column (Py). The results indicate good, but slightly conservative agreement for the DSM prediction in isolated local buckling. For isolated local buckling the models do not indicate a significant need for change in strength prediction to incorporate AHSS.  \n Figure 3. Comparison between DSM and AHSS member results under local boundary conditions of Figure 2a   For the distortional buckling boundary condition (Figure 2b) across the studied members the strength results are provided in Figure 4. If DSM predicts isolated local buckling controls (PnLo<PnD) then the PFE results are plotted against the DSM local curve, as in Figure 4a. If distortional buckling is predicted to occur, per existing DSM rules (PnD<PnLo) then the PFE are plotted against the distortional curve, as in Figure 4b. The results indicate that (1) distortional buckling is predicted to be far more prevalent in the AHSS materials with higher Fy and (2) the DSM distortional curve is generally a good predictor of strength, but greater scatter exists in comparison with local buckling. The DSM design method as provided in AISI S100-12 and examined here does not consider L-D interaction. For the studied cross-sections, boundary conditions, member length, and material grades the interaction appears weak. (Note, PcrL/PcrD ranges from 0.3 to 1.2, and 11 of the 40 studied members have 0.9<PcrL/PcrD<1.1).  \n Figure 4. Comparison between DSM and AHSS member results under distortional boundary conditions of Figure 2b for specimens predicted as (a) local buckling controlled, and (b) distortional buckling controlled.   6 For the global buckling boundary condition (Figure 2b) across the studied members the strength results are provided in Figure 5. For this boundary condition L, D, G or any combination thereof can theoretically occur. DSM, per AISI S100-12 considers the strength as the minimum of L-G (PnL) and D (PnD). If DSM predicts L-G controls (PnL<PnD) then the PFE results are plotted against the DSM local curve, as in Figure 5a. If distortional buckling is predicted to occur, per existing DSM rules (PnD<PnL) then PFE are plotted against the distortional curve, as in Figure 5b. The results indicate (1) distortional buckling rarely is predicted to control for longer unbraced members, (2) significant scatter exists in the local-global strength prediction when visualized as a function of local slenderness alone.  \n Figure 5. Comparison between DSM and AHSS member results under global “free” boundary conditions of Figure 2c for specimens predicted as (a) local-global buckling controlled, and (b) distortional buckling controlled.   To examine this second point regarding visualization of the predictions as a function of local slenderness, the PFE results that are predicted to be controlled by L-G interaction, are instead plotted as a function of global slenderness in Figure 6a. The results indicate the expected overall trend against global slenderness and the necessity to consider L-G interaction as the strength falls below the global strength prediction. In DSM L-G interaction is a function of the local slenderness (lL=(Pne/PcrL)0.5) to see how this impacts the column strength the reduced global strength for different values of lL are provided. The selected values of lL include the mean +/- a standard deviation for the studied values of lL as depicted in Figure 5a and in the histogram of lL itself in Figure 6b. From this we can conclude that the large scatter observed in Figure 5a is somewhat misleading as it is normalized to Pne, which is often a small capacity. The overall agreement with the L-G data is better shown against global slenderness as in Figure 6a. Nonetheless the long column results suggest that further refinement may be needed. In particular, D-G interaction is ignored, to what extent might this be influencing the scatter in the results? The results on columns which may fail in only L or D (Figure 4) indicate a large number of high yield stress AHSS sections fail in D – why would it be expected that essentially all of these would fail in L-G interaction for longer columns. Consistent with previous research findings of others, current DSM provisions may need to be revisited for D-G interaction, particularly for very high strength steel shapes, as is common with AHSS.   7  Figure 6. (a) Predicted strength as a function of global slenderness for Figure 5a specimens including reduced curves depending on local slenderness and (b) distribution of local slenderness in studied members  Significant work remains, even for the member-based FE collapse studies pursued herein. Pre-peak stiffness, post-peak response, local strain demands, impact of residual stresses and strains, impact of imperfection magnitude and distribution, examination of different loading actions including combined actions, detailed examination of existing and new options for considering interaction buckling in the strength prediction, reliability studies, and consideration of other cross-sections to name a few. Nonetheless, this initial study indicates that cold-formed steel members formed from AHSS do not radically depart from expected strengths, but additional studies may needed to fine tune existing design specifications when very high strength AHSS members are employed.   4. Conclusions  An initial examination of existing design strength predictions for cold-formed steel compression members formed from advanced high strength steel (AHSS) grades indicates that modest changes in the strength prediction methods will likely be needed if AHSS is adopted for steel construction. For members failing in isolated local or distortional buckling existing strength predictions from the Direct Strength Method in AISI S100-12 appear adequate. For longer members local-global interaction does not appear sufficient to accurately capture the strength and distortional-global interaction deserves further investigation. The finding is limited to the context of the study, which considers 5 AHSS grades, 40 commercially available cold-formed steel lipped channel cross-sections, at one member length, in compression, across a variety of cross-section boundary conditions. The results are arrived at through geometric and material nonlinear collapse shell finite element simulations performed in ABAQUS. Additional work is needed to explore additional loading actions, examine the strain demands inherent in the new sections, consider additional manufacturing impacts such as residual stresses and strains, and delve more deeply into buckling mode interaction issues for these members.    Acknowledgements The authors would like to thank the American Iron and Steel Institute for partial support of this work. All findings and recommendations are those of the authors and do not necessarily reflect the views of the sponsor.    \n0.5 1 1.5 2 2.5λL051015202530354045Frequency 8  References  Schafer, B.W. (2002). “Local, Distortional, and Euler Buckling in Thin-walled Columns.” Journal of Structural Engineering, 128 (3) 289-299.   Schafer, B.W. (2008). “Review: The Direct Strength Method of cold-formed steel member design.” Journal of Constructional Steel Research, 64 (7) 766-778.  Foroughi, H., et al. (2014). “Analysis and design of thin metallic shell structural members-current practice and future research needs.” Proc. of Annual Stability Conference Structural Stability Research Council, Toronto, Canada.  Yap, D. C. Y., Hancock. G.J. (2008). “Experimental Study of Complex High-Strength Cold-Formed Cross-Shaped Steel Section.” Journal of Structural Engineering, 134 (8) 1322-1333.   Yap, D. C. Y., Hancock. G.J. (2011). “Experimental Study of High-Strength Cold-Formed Stiffened-Web C-Sections in Compression.” Journal of Structural Engineering, 137 (2) 162-172.   Dinis, P. B., D. Camotim, E. M. Batista, and E. Santos. (2011). “Local / Distortional / Global Mode Coupling in Fixed Lipped Channel Columns: Behaviour and Strength.” Advanced Steel Construction 7 (1 SPEC. ISSUE): 113-130.  Camotim, D. and P. B. Dinis. (2011). “Coupled Instabilities with Distortional Buckling in Cold-Formed Steel Lipped Channel Columns.” Thin-Walled Structures, 49 (5) 562-575.   Silvestre, N., D. Camotim, and P. B. Dinis. (2012). “Post-Buckling Behaviour and Direct Strength Design of Lipped Channel Columns Experiencing local/distortional Interaction.” Journal of Constructional Steel Research, 73 12-30.   Young, B., N. Silvestre, and D. Camotim. (2013). “Cold-Formed Steel Lipped Channel Columns Influenced by Local-Distortional Interaction: Strength and DSM Design.” Journal of Structural Engineering, 139 (6) 1059-1074.   Dinis, P. B., B. Young, and D. Camotim. (2014). “Strength, Interactive Failure and Design of Web-Stiffened Lipped Channel Columns Exhibiting Distortional Buckling.” Thin-Walled Structures, 81 195-209.   Dinis, P. B., D. Camotim, E. M. Batista, and E. Santos. (2011). “Local / Distortional / Global Mode Coupling in Fixed Lipped Channel Columns: Behaviour and Strength.” Advanced Steel Construction 7 (1 SPEC. ISSUE): 113-130.  Martins, A. D., P. B. Dinis, and D. Camotim. (2016). “On the Influence of Local-Distortional Interaction in the Behaviour and Design of Cold-Formed Steel Web-Stiffened Lipped Channel Columns.” Thin-Walled Structures, 101 181-204.   9 Martins, A. D., P. B. Dinis, D. Camotim, and P. Providência. (2015). “On the Relevance of Local-Distortional Interaction Effects in the Behaviour and Design of Cold-Formed Steel Columns. ” Computers and Structures, 160 57-89.   Schafer, B.W., Li, Z., Moen, C.D. (2010). “Computational modeling of cold-formed steel.” Thin-walled Structures, 48 (10-11) 752-762.   Zeinoddini, V. M., and B. W. Schafer. (2012) \"Simulation of geometric imperfections in cold-formed steel members using spectral representation approach.” Thin-Walled Structures, 60 105-117.  ",
      "metadata": {
        "filename": "Simulation of conventional cold-formed steel sections formed from Advanced High.pdf",
        "hotspot_name": "Steel_Punching_and_Bending",
        "title": "Simulation of conventional cold-formed steel sections formed from\n  Advanced High Strength Steel (AHSS)",
        "published_date": "2017-12-21T16:02:50Z",
        "pdf_link": "http://arxiv.org/pdf/1712.08037v1",
        "query": "punching and bending steel EMC shield material substitution studies resource reduction percentages"
      }
    }
  },
  "query_mapping": {
    "search_queries": {
      "PBT Injection Molding": "injection molding PBT housing energy efficiency optimization cycle time reduction",
      "Aluminium Die Casting": "die casting aluminium radiator energy consumption reduction process optimization techniques",
      "PCBA Production": "THT and SMD soldering PCBA production energy efficiency improvement quantitative efficiency gains",
      "Steel Punching and Bending": "punching and bending steel EMC shield material substitution studies resource reduction percentages"
    },
    "downloaded_papers": {
      "PBT Injection Molding": [
        {
          "title": "DRL-Based Injection Molding Process Parameter Optimization for Adaptive\n  and Profitable Production",
          "hotspot_name": "PBT Injection Molding",
          "pdf_link": "http://arxiv.org/pdf/2505.10988v1",
          "published_date": "2025-05-16T08:35:31Z",
          "query": "injection molding PBT housing energy efficiency optimization cycle time reduction",
          "filename": "DRL-Based Injection Molding Process Parameter Optimization for Adaptive and Prof.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=injection%20molding%20pbt%20housing%20energy%20efficiency%20optimization%20cycle%20time%20reduction&start=0&max_results=10"
        },
        {
          "title": "Multi-Objective Population Based Training",
          "hotspot_name": "PBT Injection Molding",
          "pdf_link": "http://arxiv.org/pdf/2306.01436v1",
          "published_date": "2023-06-02T10:54:24Z",
          "query": "injection molding PBT housing energy efficiency optimization cycle time reduction",
          "filename": "Multi-Objective Population Based Training.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=injection%20molding%20pbt%20housing%20energy%20efficiency%20optimization%20cycle%20time%20reduction&start=0&max_results=10"
        },
        {
          "title": "Simultaneous Training of First- and Second-Order Optimizers in\n  Population-Based Reinforcement Learning",
          "hotspot_name": "PBT Injection Molding",
          "pdf_link": "http://arxiv.org/pdf/2408.15421v2",
          "published_date": "2024-08-27T21:54:26Z",
          "query": "injection molding PBT housing energy efficiency optimization cycle time reduction",
          "filename": "Simultaneous Training of First- and Second-Order Optimizers in Population-Based.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=injection%20molding%20pbt%20housing%20energy%20efficiency%20optimization%20cycle%20time%20reduction&start=0&max_results=10"
        },
        {
          "title": "Surrogate Modelling for Injection Molding Processes using Machine\n  Learning",
          "hotspot_name": "PBT Injection Molding",
          "pdf_link": "http://arxiv.org/pdf/2107.14574v1",
          "published_date": "2021-07-30T12:13:52Z",
          "query": "injection molding PBT housing energy efficiency optimization cycle time reduction",
          "filename": "Surrogate Modelling for Injection Molding Processes using Machine Learning.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=injection%20molding%20pbt%20housing%20energy%20efficiency%20optimization%20cycle%20time%20reduction&start=0&max_results=10"
        },
        {
          "title": "Towards new methods for process adjustments based on parts quality\n  measurements",
          "hotspot_name": "PBT Injection Molding",
          "pdf_link": "http://arxiv.org/pdf/1707.01765v1",
          "published_date": "2017-07-05T09:23:03Z",
          "query": "injection molding PBT housing energy efficiency optimization cycle time reduction",
          "filename": "Towards new methods for process adjustments based on parts quality measurements.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=injection%20molding%20pbt%20housing%20energy%20efficiency%20optimization%20cycle%20time%20reduction&start=0&max_results=10"
        },
        {
          "title": "Shrink-Perturb Improves Architecture Mixing during Population Based\n  Training for Neural Architecture Search",
          "hotspot_name": "PBT Injection Molding",
          "pdf_link": "http://arxiv.org/pdf/2307.15621v1",
          "published_date": "2023-07-28T15:29:52Z",
          "query": "injection molding PBT housing energy efficiency optimization cycle time reduction",
          "filename": "Shrink-Perturb Improves Architecture Mixing during Population Based Training for.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=injection%20molding%20pbt%20housing%20energy%20efficiency%20optimization%20cycle%20time%20reduction&start=0&max_results=10"
        },
        {
          "title": "Efficient quantum circuits for port-based teleportation",
          "hotspot_name": "PBT Injection Molding",
          "pdf_link": "http://arxiv.org/pdf/2312.03188v2",
          "published_date": "2023-12-05T23:39:04Z",
          "query": "injection molding PBT housing energy efficiency optimization cycle time reduction",
          "filename": "Efficient quantum circuits for port-based teleportation.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=injection%20molding%20pbt%20housing%20energy%20efficiency%20optimization%20cycle%20time%20reduction&start=0&max_results=10"
        },
        {
          "title": "Flexible and Comprehensive Patient-Specific Mitral Valve Silicone Models\n  with Chordae Tendinae Made From 3D-Printable Molds",
          "hotspot_name": "PBT Injection Molding",
          "pdf_link": "http://arxiv.org/pdf/1904.03704v1",
          "published_date": "2019-04-07T18:19:54Z",
          "query": "injection molding PBT housing energy efficiency optimization cycle time reduction",
          "filename": "Flexible and Comprehensive Patient-Specific Mitral Valve Silicone Models with Ch.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=injection%20molding%20pbt%20housing%20energy%20efficiency%20optimization%20cycle%20time%20reduction&start=0&max_results=10"
        },
        {
          "title": "Generalized Population-Based Training for Hyperparameter Optimization in\n  Reinforcement Learning",
          "hotspot_name": "PBT Injection Molding",
          "pdf_link": "http://arxiv.org/pdf/2404.08233v2",
          "published_date": "2024-04-12T04:23:20Z",
          "query": "injection molding PBT housing energy efficiency optimization cycle time reduction",
          "filename": "Generalized Population-Based Training for Hyperparameter Optimization in Reinfor.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=injection%20molding%20pbt%20housing%20energy%20efficiency%20optimization%20cycle%20time%20reduction&start=0&max_results=10"
        },
        {
          "title": "Efficient Algorithms for All Port-Based Teleportation Protocols",
          "hotspot_name": "PBT Injection Molding",
          "pdf_link": "http://arxiv.org/pdf/2311.12012v2",
          "published_date": "2023-11-20T18:49:16Z",
          "query": "injection molding PBT housing energy efficiency optimization cycle time reduction",
          "filename": "Efficient Algorithms for All Port-Based Teleportation Protocols.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=injection%20molding%20pbt%20housing%20energy%20efficiency%20optimization%20cycle%20time%20reduction&start=0&max_results=10"
        }
      ],
      "Aluminium Die Casting": [
        {
          "title": "An Innovative Line Balancing for the Aluminium Melting Process",
          "hotspot_name": "Aluminium Die Casting",
          "pdf_link": "http://arxiv.org/pdf/2504.02857v1",
          "published_date": "2025-03-29T14:31:33Z",
          "query": "die casting aluminium radiator energy consumption reduction process optimization techniques",
          "filename": "An Innovative Line Balancing for the Aluminium Melting Process.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=die%20casting%20aluminium%20radiator%20energy%20consumption%20reduction%20process%20optimization%20techniques&start=0&max_results=10"
        },
        {
          "title": "A process planning system with feature based neural network search\n  strategy for aluminum extrusion die manufacturing",
          "hotspot_name": "Aluminium Die Casting",
          "pdf_link": "http://arxiv.org/pdf/0907.0611v1",
          "published_date": "2009-07-03T12:08:13Z",
          "query": "die casting aluminium radiator energy consumption reduction process optimization techniques",
          "filename": "A process planning system with feature based neural network search strategy for.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=die%20casting%20aluminium%20radiator%20energy%20consumption%20reduction%20process%20optimization%20techniques&start=0&max_results=10"
        },
        {
          "title": "A Novel Approach for Establishing Connectivity in Partitioned Mobile\n  Sensor Networks Using Beamforming Techniques",
          "hotspot_name": "Aluminium Die Casting",
          "pdf_link": "http://arxiv.org/pdf/2308.04797v1",
          "published_date": "2023-08-09T08:35:00Z",
          "query": "die casting aluminium radiator energy consumption reduction process optimization techniques",
          "filename": "A Novel Approach for Establishing Connectivity in Partitioned Mobile Sensor Netw.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=die%20casting%20aluminium%20radiator%20energy%20consumption%20reduction%20process%20optimization%20techniques&start=0&max_results=10"
        },
        {
          "title": "Power Reduction in FM Networks by Mixed-Integer Programming. A Case\n  Study",
          "hotspot_name": "Aluminium Die Casting",
          "pdf_link": "http://arxiv.org/pdf/2310.19492v1",
          "published_date": "2023-10-30T12:32:50Z",
          "query": "die casting aluminium radiator energy consumption reduction process optimization techniques",
          "filename": "Power Reduction in FM Networks by Mixed-Integer Programming. A Case Study.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=die%20casting%20aluminium%20radiator%20energy%20consumption%20reduction%20process%20optimization%20techniques&start=0&max_results=10"
        },
        {
          "title": "Information-Theoretic Study of Time-Domain Energy-Saving Techniques in\n  Radio Access",
          "hotspot_name": "Aluminium Die Casting",
          "pdf_link": "http://arxiv.org/pdf/2303.17898v2",
          "published_date": "2023-03-31T08:57:20Z",
          "query": "die casting aluminium radiator energy consumption reduction process optimization techniques",
          "filename": "Information-Theoretic Study of Time-Domain Energy-Saving Techniques in Radio Acc.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=die%20casting%20aluminium%20radiator%20energy%20consumption%20reduction%20process%20optimization%20techniques&start=0&max_results=10"
        },
        {
          "title": "Power-Constrained Trajectory Optimization for Wireless UAV Relays with\n  Random Requests",
          "hotspot_name": "Aluminium Die Casting",
          "pdf_link": "http://arxiv.org/pdf/2002.09617v2",
          "published_date": "2020-02-22T04:09:51Z",
          "query": "die casting aluminium radiator energy consumption reduction process optimization techniques",
          "filename": "Power-Constrained Trajectory Optimization for Wireless UAV Relays with Random Re.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=die%20casting%20aluminium%20radiator%20energy%20consumption%20reduction%20process%20optimization%20techniques&start=0&max_results=10"
        },
        {
          "title": "Direct Reuse of Aluminium and Copper Current Collectors from Spent\n  Lithium-ion Batteries",
          "hotspot_name": "Aluminium Die Casting",
          "pdf_link": "http://arxiv.org/pdf/2210.07678v1",
          "published_date": "2022-10-14T10:01:44Z",
          "query": "die casting aluminium radiator energy consumption reduction process optimization techniques",
          "filename": "Direct Reuse of Aluminium and Copper Current Collectors from Spent Lithium-ion B.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=die%20casting%20aluminium%20radiator%20energy%20consumption%20reduction%20process%20optimization%20techniques&start=0&max_results=10"
        },
        {
          "title": "MEDPNet: Achieving High-Precision Adaptive Registration for Complex Die\n  Castings",
          "hotspot_name": "Aluminium Die Casting",
          "pdf_link": "http://arxiv.org/pdf/2403.09996v1",
          "published_date": "2024-03-15T03:42:38Z",
          "query": "die casting aluminium radiator energy consumption reduction process optimization techniques",
          "filename": "MEDPNet_ Achieving High-Precision Adaptive Registration for Complex Die Castings.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=die%20casting%20aluminium%20radiator%20energy%20consumption%20reduction%20process%20optimization%20techniques&start=0&max_results=10"
        },
        {
          "title": "Optimization of Solidification in Die Casting using Numerical\n  Simulations and Machine Learning",
          "hotspot_name": "Aluminium Die Casting",
          "pdf_link": "http://arxiv.org/pdf/1901.02364v2",
          "published_date": "2019-01-08T15:28:33Z",
          "query": "die casting aluminium radiator energy consumption reduction process optimization techniques",
          "filename": "Optimization of Solidification in Die Casting using Numerical Simulations and Ma.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=die%20casting%20aluminium%20radiator%20energy%20consumption%20reduction%20process%20optimization%20techniques&start=0&max_results=10"
        },
        {
          "title": "Optimizing IoT Energy Efficiency on Edge (EEE): a Cross-layer Design in\n  a Cognitive Mesh Network",
          "hotspot_name": "Aluminium Die Casting",
          "pdf_link": "http://arxiv.org/pdf/1901.05494v2",
          "published_date": "2019-01-11T02:33:01Z",
          "query": "die casting aluminium radiator energy consumption reduction process optimization techniques",
          "filename": "Optimizing IoT Energy Efficiency on Edge (EEE)_ a Cross-layer Design in a Cognit.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=die%20casting%20aluminium%20radiator%20energy%20consumption%20reduction%20process%20optimization%20techniques&start=0&max_results=10"
        }
      ],
      "PCBA Production": [
        {
          "title": "Separation of γ/π^0 showers at high energies",
          "hotspot_name": "PCBA Production",
          "pdf_link": "http://arxiv.org/pdf/hep-ex/9610005v3",
          "published_date": "1996-10-08T13:22:55Z",
          "query": "THT and SMD soldering PCBA production energy efficiency improvement quantitative efficiency gains",
          "filename": "Separation of γ_π_0 showers at high energies.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=tht%20smd%20soldering%20pcba%20production%20energy%20efficiency%20improvement%20quantitative%20efficiency%20gains&start=0&max_results=10"
        },
        {
          "title": "Polymerase/nicking enzyme powered dual-template multi-cycled G-triplex\n  machine for HIV-1 determination",
          "hotspot_name": "PCBA Production",
          "pdf_link": "http://arxiv.org/pdf/2006.15548v1",
          "published_date": "2020-06-28T09:14:10Z",
          "query": "THT and SMD soldering PCBA production energy efficiency improvement quantitative efficiency gains",
          "filename": "Polymerase_nicking enzyme powered dual-template multi-cycled G-triplex machine f.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=tht%20smd%20soldering%20pcba%20production%20energy%20efficiency%20improvement%20quantitative%20efficiency%20gains&start=0&max_results=10"
        },
        {
          "title": "YOLO algorithm with hybrid attention feature pyramid network for solder\n  joint defect detection",
          "hotspot_name": "PCBA Production",
          "pdf_link": "http://arxiv.org/pdf/2401.01214v1",
          "published_date": "2024-01-02T14:04:42Z",
          "query": "THT and SMD soldering PCBA production energy efficiency improvement quantitative efficiency gains",
          "filename": "YOLO algorithm with hybrid attention feature pyramid network for solder joint de.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=tht%20smd%20soldering%20pcba%20production%20energy%20efficiency%20improvement%20quantitative%20efficiency%20gains&start=0&max_results=10"
        },
        {
          "title": "SolderNet: Towards Trustworthy Visual Inspection of Solder Joints in\n  Electronics Manufacturing Using Explainable Artificial Intelligence",
          "hotspot_name": "PCBA Production",
          "pdf_link": "http://arxiv.org/pdf/2211.10274v1",
          "published_date": "2022-11-18T15:02:59Z",
          "query": "THT and SMD soldering PCBA production energy efficiency improvement quantitative efficiency gains",
          "filename": "SolderNet_ Towards Trustworthy Visual Inspection of Solder Joints in Electronics.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=tht%20smd%20soldering%20pcba%20production%20energy%20efficiency%20improvement%20quantitative%20efficiency%20gains&start=0&max_results=10"
        },
        {
          "title": "Optimization simulation of reflow welding based on prediction of\n  regional center temperature field",
          "hotspot_name": "PCBA Production",
          "pdf_link": "http://arxiv.org/pdf/2206.10119v1",
          "published_date": "2022-06-21T05:31:50Z",
          "query": "THT and SMD soldering PCBA production energy efficiency improvement quantitative efficiency gains",
          "filename": "Optimization simulation of reflow welding based on prediction of regional center.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=tht%20smd%20soldering%20pcba%20production%20energy%20efficiency%20improvement%20quantitative%20efficiency%20gains&start=0&max_results=10"
        },
        {
          "title": "DVQI: A Multi-task, Hardware-integrated Artificial Intelligence System\n  for Automated Visual Inspection in Electronics Manufacturing",
          "hotspot_name": "PCBA Production",
          "pdf_link": "http://arxiv.org/pdf/2312.09232v1",
          "published_date": "2023-12-14T18:56:54Z",
          "query": "THT and SMD soldering PCBA production energy efficiency improvement quantitative efficiency gains",
          "filename": "DVQI_ A Multi-task_ Hardware-integrated Artificial Intelligence System for Autom.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=tht%20smd%20soldering%20pcba%20production%20energy%20efficiency%20improvement%20quantitative%20efficiency%20gains&start=0&max_results=10"
        },
        {
          "title": "Miniaturized liquid metal composite circuits with energy harvesting\n  coils for battery-free bioelectronics and optogenetics",
          "hotspot_name": "PCBA Production",
          "pdf_link": "http://arxiv.org/pdf/2501.11016v1",
          "published_date": "2025-01-19T11:30:46Z",
          "query": "THT and SMD soldering PCBA production energy efficiency improvement quantitative efficiency gains",
          "filename": "Miniaturized liquid metal composite circuits with energy harvesting coils for ba.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=tht%20smd%20soldering%20pcba%20production%20energy%20efficiency%20improvement%20quantitative%20efficiency%20gains&start=0&max_results=10"
        },
        {
          "title": "A Fast Algorithm for Parabolic PDE-based Inverse Problems Based on\n  Laplace Transforms and Flexible Krylov Solvers",
          "hotspot_name": "PCBA Production",
          "pdf_link": "http://arxiv.org/pdf/1409.2556v2",
          "published_date": "2014-09-09T00:23:41Z",
          "query": "THT and SMD soldering PCBA production energy efficiency improvement quantitative efficiency gains",
          "filename": "A Fast Algorithm for Parabolic PDE-based Inverse Problems Based on Laplace Trans.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=tht%20smd%20soldering%20pcba%20production%20energy%20efficiency%20improvement%20quantitative%20efficiency%20gains&start=0&max_results=10"
        },
        {
          "title": "Convolutional Recurrent Reconstructive Network for Spatiotemporal\n  Anomaly Detection in Solder Paste Inspection",
          "hotspot_name": "PCBA Production",
          "pdf_link": "http://arxiv.org/pdf/1908.08204v1",
          "published_date": "2019-08-22T05:18:41Z",
          "query": "THT and SMD soldering PCBA production energy efficiency improvement quantitative efficiency gains",
          "filename": "Convolutional Recurrent Reconstructive Network for Spatiotemporal Anomaly Detect.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=tht%20smd%20soldering%20pcba%20production%20energy%20efficiency%20improvement%20quantitative%20efficiency%20gains&start=0&max_results=10"
        },
        {
          "title": "Grounding Stylistic Domain Generalization with Quantitative Domain Shift\n  Measures and Synthetic Scene Images",
          "hotspot_name": "PCBA Production",
          "pdf_link": "http://arxiv.org/pdf/2405.15961v1",
          "published_date": "2024-05-24T22:13:31Z",
          "query": "THT and SMD soldering PCBA production energy efficiency improvement quantitative efficiency gains",
          "filename": "Grounding Stylistic Domain Generalization with Quantitative Domain Shift Measure.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=tht%20smd%20soldering%20pcba%20production%20energy%20efficiency%20improvement%20quantitative%20efficiency%20gains&start=0&max_results=10"
        }
      ],
      "Steel Punching and Bending": [
        {
          "title": "Drawing of the wire of low-carbon steel: plasticity resource, optimal\n  reduction, structure, properties",
          "hotspot_name": "Steel Punching and Bending",
          "pdf_link": "http://arxiv.org/pdf/1412.0157v1",
          "published_date": "2014-11-29T21:42:44Z",
          "query": "punching and bending steel EMC shield material substitution studies resource reduction percentages",
          "filename": "Drawing of the wire of low-carbon steel_ plasticity resource_ optimal reduction.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=punching%20bending%20steel%20emc%20shield%20material%20substitution%20studies%20resource%20reduction%20percentages&start=0&max_results=10"
        },
        {
          "title": "Evidence of strong electron correlation effects and magnetic topological\n  excitation in low carbon steel",
          "hotspot_name": "Steel Punching and Bending",
          "pdf_link": "http://arxiv.org/pdf/2503.08822v1",
          "published_date": "2025-03-11T18:53:56Z",
          "query": "punching and bending steel EMC shield material substitution studies resource reduction percentages",
          "filename": "Evidence of strong electron correlation effects and magnetic topological excitat.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=punching%20bending%20steel%20emc%20shield%20material%20substitution%20studies%20resource%20reduction%20percentages&start=0&max_results=10"
        },
        {
          "title": "Bayesian Model Selection for Network Discrimination and Risk-informed\n  Decision Making in Material Flow Analysis",
          "hotspot_name": "Steel Punching and Bending",
          "pdf_link": "http://arxiv.org/pdf/2501.05556v1",
          "published_date": "2025-01-09T20:03:18Z",
          "query": "punching and bending steel EMC shield material substitution studies resource reduction percentages",
          "filename": "Bayesian Model Selection for Network Discrimination and Risk-informed Decision M.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=punching%20bending%20steel%20emc%20shield%20material%20substitution%20studies%20resource%20reduction%20percentages&start=0&max_results=10"
        },
        {
          "title": "Highly flexible electromagnetic interference shielding films based on\n  ultrathin Ni/Ag composites on paper substrates",
          "hotspot_name": "Steel Punching and Bending",
          "pdf_link": "http://arxiv.org/pdf/2005.04875v1",
          "published_date": "2020-05-11T06:09:53Z",
          "query": "punching and bending steel EMC shield material substitution studies resource reduction percentages",
          "filename": "Highly flexible electromagnetic interference shielding films based on ultrathin.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=punching%20bending%20steel%20emc%20shield%20material%20substitution%20studies%20resource%20reduction%20percentages&start=0&max_results=10"
        },
        {
          "title": "Air plasma key parameters for electromagnetic wave propagation at and\n  out of thermal equilibrium: applications to electromagnetic compatibility",
          "hotspot_name": "Steel Punching and Bending",
          "pdf_link": "http://arxiv.org/pdf/1902.07026v1",
          "published_date": "2019-02-19T12:38:27Z",
          "query": "punching and bending steel EMC shield material substitution studies resource reduction percentages",
          "filename": "Air plasma key parameters for electromagnetic wave propagation at and out of the.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=punching%20bending%20steel%20emc%20shield%20material%20substitution%20studies%20resource%20reduction%20percentages&start=0&max_results=10"
        },
        {
          "title": "Simulation of conventional cold-formed steel sections formed from\n  Advanced High Strength Steel (AHSS)",
          "hotspot_name": "Steel Punching and Bending",
          "pdf_link": "http://arxiv.org/pdf/1712.08037v1",
          "published_date": "2017-12-21T16:02:50Z",
          "query": "punching and bending steel EMC shield material substitution studies resource reduction percentages",
          "filename": "Simulation of conventional cold-formed steel sections formed from Advanced High.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=punching%20bending%20steel%20emc%20shield%20material%20substitution%20studies%20resource%20reduction%20percentages&start=0&max_results=10"
        },
        {
          "title": "Intelligent data collection for network discrimination in material flow\n  analysis using Bayesian optimal experimental design",
          "hotspot_name": "Steel Punching and Bending",
          "pdf_link": "http://arxiv.org/pdf/2504.13382v1",
          "published_date": "2025-04-18T00:04:12Z",
          "query": "punching and bending steel EMC shield material substitution studies resource reduction percentages",
          "filename": "Intelligent data collection for network discrimination in material flow analysis.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=punching%20bending%20steel%20emc%20shield%20material%20substitution%20studies%20resource%20reduction%20percentages&start=0&max_results=10"
        },
        {
          "title": "Radiation Shielding Properties of $Nd_{0.6}Sr_{0.4}Mn_{1-y}Ni_{y}O_{3}$\n  Substitute with Different Concentrations of Nickle",
          "hotspot_name": "Steel Punching and Bending",
          "pdf_link": "http://arxiv.org/pdf/2006.02525v1",
          "published_date": "2020-04-22T04:46:27Z",
          "query": "punching and bending steel EMC shield material substitution studies resource reduction percentages",
          "filename": "Radiation Shielding Properties of _Nd_0.6_Sr_0.4_Mn_1-y_Ni_y_O_3_ Substitute wit.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=punching%20bending%20steel%20emc%20shield%20material%20substitution%20studies%20resource%20reduction%20percentages&start=0&max_results=10"
        },
        {
          "title": "Neutron Activation Background in the NvDEx Experiment",
          "hotspot_name": "Steel Punching and Bending",
          "pdf_link": "http://arxiv.org/pdf/2307.12785v1",
          "published_date": "2023-07-24T13:33:15Z",
          "query": "punching and bending steel EMC shield material substitution studies resource reduction percentages",
          "filename": "Neutron Activation Background in the NvDEx Experiment.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=punching%20bending%20steel%20emc%20shield%20material%20substitution%20studies%20resource%20reduction%20percentages&start=0&max_results=10"
        },
        {
          "title": "Anisotropic behaviour law for sheets used in stamping: A comparative\n  study of steel and aluminium",
          "hotspot_name": "Steel Punching and Bending",
          "pdf_link": "http://arxiv.org/pdf/0801.3018v1",
          "published_date": "2008-01-19T07:48:14Z",
          "query": "punching and bending steel EMC shield material substitution studies resource reduction percentages",
          "filename": "Anisotropic behaviour law for sheets used in stamping_ A comparative study of st.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=punching%20bending%20steel%20emc%20shield%20material%20substitution%20studies%20resource%20reduction%20percentages&start=0&max_results=10"
        }
      ]
    },
    "total_papers": 40,
    "download_timestamp": "2025-06-24 00:07:36"
  },
  "processing_summary": {
    "total_processed": 40,
    "processing_timestamp": "2025-06-24 00:08:20.708978"
  }
}