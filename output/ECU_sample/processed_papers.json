{
  "processed_papers": {
    "An Innovative Line Balancing for the Aluminium Melting Process": {
      "full_text": " International Journal of Mechanical and Industrial Technology        ISSN 2348 -7593 (Online)  \nVol. 12, Issue 2, pp: (73 -84), Month: October 2024 - March 2025, Available at: www.researchpublish.com  \n \nPage | 73  \nResearch Publish Journals  Innovative Line Balancing for the Aluminium  \nMelting  Process  \nProf. Dr. Ray Wai Man Kong1, Ding N ing2, Theodore Ho Tin Kong3 \n1Adjunct Professor, S ystem Engineering Department, City University of Hong Kong, China  \n1Eagle Nice International (Holding) Ltd., Hong Kong, China  \n2Engin eering Doctorate Student, S ystem Engineering Department, City University of Hong Kong, China  \n3 Graduated Student, Master of Science in Aeronautical Engineering,  Hong Kong University of Science and Technology, \nHong Kong \n3 Thermal -acoustic (Mechanical) Design Engineer at Intel Corporation in Toronto, Canada  \nDOI: https://doi.org/ 10.5281/zenodo.15050721  \nPublished Date: 19 -March -2025  \nAbstract:   This research article explores the optimization of aluminium  extrusion processes through advanced line \nbalancing techniques, focusing on maximizing marginal profit by increasing melting and casting outputs. By \nemploying mixed integer linear programming (MI LP), we identify strategies to minimize idle costs and enhance \nproduction efficiency. The study demonstrates that increasing the daily cycle rate from 2 to 4.36 cycles results in a \nsignificant rise in daily marginal profit, calculated at USD67,786, after a ccounting for additional labour  costs. This \noptimization is achieved by expanding the workforce from 8 to 12 operators across two shifts, leading to a 50% \nincrease in labour  expenses. The findings reveal a remarkable 117.6% growth in marginal daily profit,  underscoring \nthe potential of automation and intelligent manufacturing in transforming the aluminium  extrusion industry.  \nInsights from cross -industry research, including Lean Methodology in the Modern Garment Industry, further \nillustrate the broader appl icability of these advancements. This study highlights the critical role of automation in \ndriving productivity and profitability in manufacturing sectors, paving the way for future innovations in aluminium  \nextrusion and beyond.  \nKeywords : Line Balancing, Pr oduction Plan, Aluminium  Extrusion , Automation, Aluminium  Manufacturing, Lean \nPractice .  \nI.   INTRODUCTION  \nAs a global market research company, the Aluminium  Extrusion Market Size was valued at USD 83.9 Billion in 2023 from \nMarket Research Future (MRFR) [1]  in Fig . 1, which is from the MRFR Database and Analyst Review. The Aluminium  \nExtrusion industry is projected to grow from USD 90.77 Billion in 2024 to USD 170.53 Billion by 2032, exhibiting a \ncompound annual growth rate (CAGR) of 8.20% during the forecast period (2024 - 2032).  The Aluminium  Extrusion \nMarket will be driven by a ris e in the need for strong, lightweight extruded items with high corrosion resistance, and \neconomic expansion, accelerated urbanisation  and expanding infrastructure projects are the key market drivers enhancing \nmarket growth.  \nThe aluminium  extrusion market h as been driven by the increasing demand for lightweight and durable extruded products. \nAluminium  extrusions are widely used in vari ous industries, including construction, automotive, aerospace, electronics, and \nconsumer goods, among others. One of the main  advantages of aluminium  extrusions is their lightweight nature. Aluminium  \nhas a high strength -to-weight ratio, making it an ideal material for applications where weight reduction is a critical factor, \nsuch as in the automotive and aerospace industries. Aluminium  is highly durable and corrosion -resistant, which makes it an \nattractive choice for products that need to withstand harsh environments or extreme weather conditions.  The aluminium   International Journal of Mechanical and Industrial Technology        ISSN 2348 -7593 (Online)  \nVol. 12, Issue 2, pp: (73 -84), Month: October 2024 - March 2025, Available at: www.researchpublish.com  \n \nPage | 74  \nResearch Publish Journals  frame can be used for the construction of the required material  for the iPad, iPhone and window.  The major materials of \nthe auto vehicles are the aluminium  alloy frame.  \nThe aluminium  alloy manufacturer has the most critical role in  the extrusion process , which  allows for the creation of \ncomplex shapes and designs, which ca n be used to meet the specific needs of different industries and applications.  \nII.    MANUFACTURING PROCESS OF ALUMINIUM ALLOY  \nManufacturing Process of Melting for Aluminium  Alloy Manufacture  \nAluminium  Alloy Melting is a major process in which a solid heats up and becomes a liquid state of aluminium . After the \nfeeding is completed, melting starts. During the smelting process, it is necessary to ensure that the melting is fast and \nuniform and that the te mperature near the flame is high, reaching more than 1200 ℃. The melting temperature is reaching \nover 700℃. To avoid local overheating and excessive temperature, the aluminium  alloy oxidation causes  a serious \ninconvenience to the later product refining. St irring should be carried out, and the unmelted furnace charge should be \nsqueezed into the aluminium  liquid so that almost all of it is immersed in the stirring to avoid local overheating. For the \nfinal immersion of Magnesium (Mg) and other metallic element s for various aluminium  alloys , the flame cannot be directly \nheated and melted, and because the aluminium  liquid is immersed in the raw material, the temperature is reduced, and the \nMg and other metallic elements are melted at a relatively low temperature,  which reduces the burning loss and improves \nthe combustion efficiency.  \nThe melting process chart in Fig. 2 is shown below to relate to the melting process. The machinery standard time and labour  \nstandard time are the critical time measurement s for the line balancing of the manufacturing process.  The input substance \nis the raw aluminium  and recycled aluminium , and the output is the aluminium  rod.  The aluminium  rod is a large and heavy \nstate of aluminium  to direct extrusion for reforming the va rious shapes of aluminium  to the related manufacturing processes \nfor the automobile, the frame of iPad & iPhone and window frame.  \n \nFigure 1 : Aluminium  Extrusion Market Research from MRFR Database and Analyst Review  \nThe problem  of Line Unbalancing in Aluminium  Melting  \nLine balancing in the aluminium  manufacturing industry is the technique of levelling the output of every operation in an \naluminium  alloy production line, so the output from the upstream operation can be optimized to pass through the \ndowns tream operation. There should neither be an accumulation of work between two processes (operation) nor a shortage \nof workpieces from the upstream workstation (previous work step) between the melting line and its inter -process. It is \nimportant to maintain t his balance because in a melting process line, the output of one process, as an aluminium  rod, is the \ninput of the next aluminium extrusion.  \n International Journal of Mechanical and Industrial Technology        ISSN 2348 -7593 (Online)  \nVol. 12, Issue 2, pp: (73 -84), Month: October 2024 - March 2025, Available at: www.researchpublish.com  \n \nPage | 75  \nResearch Publish Journals  A melting furnace and melting tank for stabilisation are not balanced; hence, there would be the following producti on \nproblems:  \n Reduced Efficiency:  \nIt means that an upstream aluminium  melting output is a downstream operation input. Because of this reason, the worker \nafter the melting process will not get the loading input as per their capacity of producing output, hence they will be \nunderutilized. In this case, it is to make matters wo rse, more machines and manpower will be allocated to increase \naluminium  rod sewing and cutting, but efficiency will fall even more.  \n Reduced the utilisation and efficiency of the melting furnace : \nThe melting furnace has poured the aluminium  liquid into the melting tank, and the melting furnace  is idle to wait for \nthe melting tank for several processes to the casting.  \n Energy wastage for keeping the high temperature in the melting furnace:  \nThe melting furnace is required to keep the high temperature and stir t he residue of the melting furnace. Once the melting \nfurnace is lower than the melting point of aluminium , the aluminium  solidifies to a solid state as steady stabilization of \nthe alloy structure.  It is not easy to melt and mix the new lot of aluminium  or recycled aluminium . \n \nFigure 2: Melting Process Chart for Aluminium  Alloy Manufacturing  \nIn the melting process, the operations in the melting furnace in Fig. 3 are defined as the melting workstation.  The operations \nin the melting tank are defined as the melting stabilization workstation  in Fig . 4.  The line balancing is required to make the \nbalance of the melting workstation and the stabilization workstation because these are related to the separated facility and \nequipment.  The workstation of stabilizatio n workstation includes the aluminium  rod casting in Fig. 4. \n \n International Journal of Mechanical and Industrial Technology        ISSN 2348 -7593 (Online)  \nVol. 12, Issue 2, pp: (73 -84), Month: October 2024 - March 2025, Available at: www.researchpublish.com  \n \nPage | 76  \nResearch Publish Journals   \nFigure 3 : Melting Furnace for Aluminium  Alloy Manufacturing  \n \nFigure 4 : Melting Stabilization and Casting for Aluminium  Alloy Manufacturing  \nThe traditional setup of the melting furnace line with the melting tank ratio is one-to-one, which is not followed by the line \nbalancing concept. The study of an aluminium  extrusion company is proposed for the use of the standard time ratio between \nthe melting furnace and the melting tank.  The below sect ion 4 shows  the case study how an aluminium  company refer to \nthe publication Journals from Prof Dr Ray WM Kong et al [ 2]. The imbalanced line balance  in the melting line  affects  the \nshortage of supply of aluminium  rods to the  aluminium  frame machining assembly.  For the aluminium  assembly, the \nupstream operation  is the supply of aluminium  rod and the downstream operation  is the aluminium  frame machining \nprocess .  It is not balanced the line balance; hence, the following shortage of supply causes  these  problems have  exist  as \nshown above points . \nIII.   LITERATURE REVIEW  \nProf. Dr Ray Wai Man Kong [3] has discussed strategies to optimize the mixed integer linear programming for line \nbalancing  and balance the capacity of machines, machine centres , and work centres  in the initial stages of line balancing to \nenhance output rates. In the context of aluminium  extrusion, merely increasing the capacity of individual machines or \nassembly lines does not necessarily improve overall production output and productivity due to line-balancing  challenges. \nThe article \"Lean Methodology for Lean Modernization\" provides insights into applying lean technology to develop future \nstate value stream mapping (VSM) and identify bottlenecks in the aluminium  extrusion process, thereby enhancing capacity \nand achieving balanced production.  \nIn the article on mixed integer linear programming in the Garment Line Balancing, there is a focus on the mathematical \noptimization method. For line balancing, Ocident Bongomin [ 4], the Assembly Line Ba lancing Problem (ALBP), also \n International Journal of Mechanical and Industrial Technology        ISSN 2348 -7593 (Online)  \nVol. 12, Issue 2, pp: (73 -84), Month: October 2024 - March 2025, Available at: www.researchpublish.com  \n \nPage | 77  \nResearch Publish Journals  known as assembly line design, is a family of combinatorial optimization problems widely studied for its simplicity and \nindustrial applicability. ALBP is NP -hard, encompassing the bin packing problem as a special case. ALBPs ar ise whenever \nan assembly line is configured, redesigned, or adjusted. In aluminium  extrusion, these problems are particularly pronounced \ndue to the unique balancing challenges posed by the extrusion process compared to other manufacturing lines, such as th ose \nfor trucks, buses, or machinery.  \nThe task involves distributing the total workload for producing aluminium  profiles among workstations along the line, \nadhering to strict or average cycle times. The general principles of line balancing include considera tions for machining, \nassembly, and disassembly environments; the number of product models (single -model, mixed -model, multimodal lines); \nand line layouts (basic straight lines, U -shaped lines, circular transfer lines).  \nThe assembly line balancing (ALB) pro blem has been extensively studied, as noted by Gary Yu -Hsin Chen [ 5]. The ALB \nmodel ensures that staff assignments balance the entire production process, effectively reducing production time or idle \ntime. In aluminium  extrusion, the mastery of skills by em ployees at each task is a critical indicator for achieving ALB. \nHowever, there is limited research on multifunctional workers with varying skill levels at workstations. Our research \nincorporates the Toyota Production System (TPS) principles, adapted for aluminium  extrusion, to optimize floor space, \nflexibility, and working conditions. This approach features U -shaped assembly lines and teams of workers managing \nextrusion processes on a single -piece flow basis.  \nChen et al. [ 6] address a multi -skill project sc heduling problem, which is relevant to aluminium  extrusion where projects \nare divided into tasks completed by skilled employees. Their multi -objective nonlinear mixed integer programming model \nconsiders employees' skill proficiency, multifunctional roles, and cell formation to minimize production cycle time. This \napproach, which accounts for real -world skill variations, effectively reduces production time through optimal personnel \nassignment and preferred production modes. The human factor introduces uncert ainty affecting actual cycle time, \nemphasizing the need for real -time dynamic line balancing in aluminium  extrusion.  \nHaile Sime & Prabir Jana (2018) [7] demonstrated the use of Arena simulation software to design and evaluate alternative \nproduction systems , optimizing resource utilization through effective line balancing. Markus Proster & Lothar Marz (2015) \nhighlighted the importance of dynamic balancing for high productivity in mixed -model assembly lines, applicable to \naluminium  extrusion where varying extrusion times for different profiles require adaptive strategies. Simulation tools can \nvisualize these methods, reducing complexity and enhancing transparency in planning extrusion lines.  \nGhosh and Gagnon (1989), along with Erel and Sarin (1998), provided detailed reviews on these topics. Configurations of \nextrusion lines for single and multiple products can be divided into single -model, mixed -model, and multi -model types. \nSingle -model lines extrude one product type, mixed -model l ines handle multiple products, and multi -model lines produce \nsequences of batches with intermediate setup operations (Becker & Scholl, 2006).  \nIV.   METHODOLOGY  \nA. Industrial Engineering  and Lean Technology to  Study the Line Balance of Aluminium  Extrusion  \nFollowing the Lean Methodology from the Lean Methodology for the Modern Garment Industry.  Industrial  engineering \napplies to the lean methodology and technology  in the line balancing of aluminium  extrusion manufacturing.  An industrial  \nengineer is working for the Here's how it is utilized:  \n(a) Work Measurement: Industrial engineering study involves conducting time and motion studies to measure the time \ntaken to perform each operation in the melting, stabilisation and casting processes . This cycle time data is related to \nthe machinery for calculating each operation's cycle time and relates  to the capacity . \n(b) Refining Process  in the Melting Analysis: the entire refining  process  is required to get the analysis report of aluminium  \nalloy comp osition  during the melting process at high temperatures . If the refinement cannot achieve the aluminium  \nalloy requirement, the refinement operation is required to be done again, repeated more times, to achieve the required \naluminium alloy composition and s trength. The simple  process  bottlenecks, inefficiencies, and areas of improvement \nin the refinement process can be identified by Value Stream Mapping. By understanding the process flow, the future \nstate of VSM can identify  opportunities for line balancing to optimize the line balancing between the melting furnace \noperation and the melting tank operation and optimize the  refinement time . \n  International Journal of Mechanical and Industrial Technology        ISSN 2348 -7593 (Online)  \nVol. 12, Issue 2, pp: (73 -84), Month: October 2024 - March 2025, Available at: www.researchpublish.com  \n \nPage | 78  \nResearch Publish Journals  (c) Capacity and Manpower Resource Allocation: Industrial engineers assess the aluminium  melting and casting workforce \nand eq uipment available in the manufacturing facility. They determine the number of operators required for each \noperation based on the involved machinery cycle time and capacity. The melting process relies on a melting furnace \nand machinery, but the refining and stirring operations are required for the skilful  operators to control the machine and \ncollect the sample of aluminium  alloy liquid for the metallic composition and strength te st. For the heavy machinery \nand aluminium  industry, it applies more than 1 operator to operate the facility and machinery to stir the aluminium  \nliquid and add the magnesium and other metals to mix the aluminium  alloy in the melting furnace.  The crows of \noperators are not fully loaded to operate the furnace and machine.  The lean practice with the industrial engineering \nconcept is a good tool to optimize the operators’ working time for sharing their time to involve more steps, reducing \ntheir waiting time an d idle time. This helps in allocating resources effectively and achieving a balanced line.  \n(d) Layout Design: Industrial engineers consider the layout of the time study of the melting process, stabilization process, \nrefinement process and casting,  which  has an  impact on efficiency  and output . They analyze the flow of materials, \nequipment placement, and operator movement. Optimizing  the layout , including batch layout  for setting the \noptimization of the aluminium  tank and the casting process.  The one melting fur nace to one melting tank and casting \nfacility is not an optimization.  To ensure the safety of the facility and not solidification of aluminium  liquid from the \nmelting furnace to the melting tank, the maximum ratio of the melting furnace to the melting tan k is 2 to 1 because of \nthe length of the connection pipe and the liquid flow rate between melting furnace to the melting tank  in Fig. 5.  \n(e) Continuous Improvement: Industrial engineering study emphasizes continuous improvement in line balanc e as referred \nto the Mixed Integer linear programming for Garment Line Balancing  and Lean Methodology . Industrial engineers \nmonitor the bottleneck operation of the refinery performance, collect data, and analyze  it to identify areas for further \noptimization. They implement changes, conduct follow -up studies, and refine the line-balancing  process to achieve \nhigher efficiency and productivity.  \n \nFigure 5 : Melting Line Comparison Diagram  \nV.   CASE STUDY FOR THE LINE BALANCING FOR MELTING PROCESS OPTIMIZATION  \nBy utilizing an industrial engineering study in line balancing aluminium  extrusion manufacturing, Compan y A in the case \nstudy  can optimize its production processes, reduce lead times, improve resource utilization, and enhance overall efficiency. \nThis results in increased productivity, cost savings, and improved customer satisfaction.  \nThe traditional melting process uses the melting furnace to li nk with the melting tank and the casting facility in series with \na ratio of 1:1. Aluminium  liquid in the melting furnace is poured into the melting tank by direct pipe connection. The time \nstudy of the traditional method was 2 times a day as shown in Fig. 6. \n International Journal of Mechanical and Industrial Technology        ISSN 2348 -7593 (Online)  \nVol. 12, Issue 2, pp: (73 -84), Month: October 2024 - March 2025, Available at: www.researchpublish.com  \n \nPage | 79  \nResearch Publish Journals  Figure 6: Traditional Time Schedule Melting Furnace and Melting Tank & Casting Chart  \nIn Fig. 7, the Time Schedule to Optimize the Melting Furnace and Melting Tank & Casting Chart is shown the Fig. 7, which \nmust be continuous, with no gaps in between. The materials flow through the layout in a loop shape. The hanger line is \nrequired to construct the hanger system and equipment.  The system is modernized to set up the control device to move the \nhanger between workstations and provide the just -in-time information to the manufacturing system.  The line balancing for \nthe hanger line can be optimised to increase production efficiency by increasing the throughput time based on increasing \nthe capacity of the bottleneck workstations in the process as the Lean Methodology for Garment Modernization that Prof \nDr Ray WM Kong mentioned [ 2]. \n \nFigure 7: Time Schedule to Optimize the Melting Furnace and Melting Tank & Casting Chart  \nThe plant  layout for optimization is  required to restructure the melting furnace to the melting tank with the casting facility.  \nThe plant layout has added the switch from the melting furnace to the melting tank with the casting facility. Aluminium  \nliquid can be poured  from two sets of meltin g furnaces to one set of melting tanks  and a casting facility.  \nProduction output before optimisation: 2 times/day  \nProduction output after optimisation: 4.36 times/day  \nThe increase in output  percentage : (4.36 times/day - 2 times/day) / 2 times/day x 100%  \nThe growth output percentage: 118%  \n International Journal of Mechanical and Industrial Technology        ISSN 2348 -7593 (Online)  \nVol. 12, Issue 2, pp: (73 -84), Month: October 2024 - March 2025, Available at: www.researchpublish.com  \n \nPage | 80  \nResearch Publish Journals  There is a n optimization in simple line balancing. The labour headcount is increased from 8 workers to 12 workers for 2 \nshifts a day. The labour cost has increased by 50%.  The maximum profit margin of each casting time o n 36 rods is over \nUSD28,800 (USD800 /rod). The additional profit margin from 2 times to 4.36 times a day is USD67,968.  To reduce the \nadditional labour cost fo r 4 workers , the total amount is USD182/day, so the net profit margin is USD67,786/day.  \nVI.   LINE  BALANCING MODEL ANALYSIS  \nThe main objective  of balancing the line is to maximize the marginal profit,  which requires reducing the idle cost of melting \nfurnaces. The increased daily output rates of aluminium alloy from the melting tank and the casting facility can provide \nmarginal profit, which also reduces the idle time of melting furnaces. The idle melting furnace is required to provide the \nenergy to keep the temperature of th e melting furnace because the drop temperature causes the aluminium  liquid to solidify \nat the bottom of the aluminium  residue. The change in temperature causes the melting furnace to crack based on the metallic \nstructure.  To keep the temperature from slow ing down to drop the temperature, the furnace is not destroy ed by \nthermoshocking, the energy is required to control and keep the expected temperature when the melting furnace is idle.  \nReferring to the mixed integer  linear program (MILP) for the garment lin e balancing, the optimization of linear \nprogramming is to find the optimized solution  from  Prof Dr Ray WM Kong [ 3], the number of melting cycle s in the melting \nprocess can be optimi zied to maximise the marginal profit.  \nBalancing in th e stage  of a single model finds a locally optimized solution in an iteration. The objective of the stage is to \nfind a solution(s) with a specified number of stations with a minimum cycle  time.  Solutions are considered locally optimized \nas the principal  objective is to  find a  solution which will define a smooth production by minimizing the objective function \nof melting workstation balance from Waldemar Grzechca [ 8]. The concept of ALBP, where the aim is to optimize the \nnumber of workstations with a predefined fixed cycl e time , is utilized in the formulation.  \nLinear  programming is adopted  to resolve the line balancing with constraints. The formula of linear programming is shown  \nin the following  standard format : \n𝑚𝑎𝑥 𝑋1,𝑋2,…,𝑋𝑛  (𝑧)= 𝑝1𝑥1+ …+ 𝑝𝑛𝑥𝑛  (1) \nsubject to   𝐴11𝑥1…+ 𝐴1𝑛𝑥𝑛 ≤ 𝑏1, \n ⋮  ⋮ \n  𝐴𝑚1𝑥1…+ 𝐴𝑚𝑛𝑥𝑛 ≤ 𝑏𝑚, \n 𝑥1,𝑥2,…,𝑥𝑛 ≥0 \nBy grouping the variables 𝑥1,𝑥2,…,𝑥𝑛 into a vector  x and constructing the following matrix and vectors from the problem \ndata, we can restate the standard form compactly as follows:  \n𝑚𝑎𝑥 𝑥  (𝑧)=𝑝′𝑥 \nsubject to 𝐴𝑥 ≤ 𝑏 ,   𝑥≥0 \n \nThe maximum  profit and cycle time is shown in the formula below : \n𝑃𝑟𝑜𝑓𝑖𝑡 𝑚𝑎𝑥 =max ( 𝑃𝑋𝑌−∑ 𝐼𝐶𝑥𝑥=𝑛\n𝑥=1 − ∑ 𝐼𝐶𝑦)𝑦=𝑛\n𝑦=1   (2) \n𝑀𝑎𝑥  (𝑃𝑟𝑜𝑓𝑖𝑡 )= max ( 𝑃𝑋𝑌−∑ 𝐼𝐶𝑥𝑥=𝑛\n𝑥=1 − ∑ 𝐼𝐶𝑦)𝑦=𝑛\n𝑦=1          ∀𝐶𝑇𝑥∈+𝑅 ，∀𝐶𝑇𝑦∈+𝑅  (3) \n𝑃𝑋𝑌= 𝑇𝐶𝑋𝑌∗𝑅𝑟 (4) \n𝑇𝐶𝑋𝑌=( 𝐶𝑇𝑥∗𝐸𝑓𝑓 𝑥%+𝐶𝑇𝑦∗𝐸𝑓𝑓 𝑦% )∗ 𝐸𝑓𝑓 𝑐𝑎𝑠𝑡% (5) \nWhere 𝑃𝑋𝑌 is the marginal profit of melting tank and casting  in USD,  \n 𝑇𝐶𝑋𝑌 is the output quantity of melting tank and casting in tons,  \n 𝐶𝑇𝑥 is the cycle time of melting  furnace  X per cycle in minutes,  \n 𝐶𝑇𝑦 is the cycle time of melting  furnace  Y per cycle in minutes,   International Journal of Mechanical and Industrial Technology        ISSN 2348 -7593 (Online)  \nVol. 12, Issue 2, pp: (73 -84), Month: October 2024 - March 2025, Available at: www.researchpublish.com  \n \nPage | 81  \nResearch Publish Journals   𝑅𝑟 is the marginal profit rate of melting tank and castin g in USD per ton,  \n𝐼𝐶𝑥 is the idle  cost of melting  furnace  X in minutes,  \n    𝐼𝐶𝑦 is the idle cost of melting  furnace  Y in minutes,  \n𝐸𝑓𝑓 𝑥% is the efficiency of output of melting  furnace  X after reduction of lose  \nduring manufacturing,  \n𝐸𝑓𝑓 𝑦% is the efficiency of output of melting  furnace  Y after reduction of lose  \nduring manufacturing,  \n𝐸𝑓𝑓 𝑐𝑎𝑠𝑡% is the efficiency of output of melting tank and casting after reduction  \nof lose during manufacturing,  \n \n∑ 𝐼𝐶𝑥𝑥=𝑛\n𝑥=1 = ∑(𝐶𝑎𝑝 𝑥− 𝐶𝑇𝑥𝑛\n1  𝑅𝑇𝑥) 𝐶𝑅 𝑥        ∀𝐶𝑇𝑥∈+𝑅 (6) \nwhere  𝐶𝑎𝑝 𝑥 is the daily available hours of melting  furnace  X in minutes,  \n    𝐶𝑇𝑥 is the cycle time of melting  furnace  X per cycle in minutes,   \n𝑅𝑇𝑥 is the number of time of melting  furnace  X in minutes per day,  \n𝐶𝑅 𝑥 is the idle cost rate of melting  furnace  X in USD per minute,  \n \n∑ 𝐼𝐶𝑦𝑦=𝑛\n𝑦=1 = ∑(𝐶𝑎𝑝 𝑦− 𝐶𝑇𝑦𝑛\n1  𝑅𝑇𝑦) 𝐶𝑅 𝑦      ∀𝐶𝑇𝑦∈+𝑅 (7) \nwhere  𝐶𝑎𝑝 𝑦 is the daily available hours of melting  furnace  Y in minutes,  \n    𝐶𝑇𝑦 is the cycle time of me lting furnace  Y per cycle in minutes,   \n𝑅𝑇𝑦 is the number of time of melting  furnace  Y in minutes per day,  \n𝐶𝑅 𝑦 is the idle cost rate of melting  furnace  Y in USD per minute,  \nsubject to  \n𝐶𝑇𝑥 𝑅𝑇𝑥≤  𝐶𝑎𝑝 𝑥  (8) \n𝐶𝑇𝑦 𝑅𝑇𝑦≤  𝐶𝑎𝑝 𝑦 (9) \nFirstly, the 𝑃𝑟𝑜𝑓𝑖𝑡 𝑚𝑎𝑥 is required to maximize the daily marginal profit as our goal. The  𝑃𝑋𝑌 is the profit of aluminium  rod \nper day,  which is 𝑃𝑋𝑌= 𝑇𝐶𝑋𝑌∗𝑅𝑟 , the 𝑇𝐶𝑋𝑌 total casting ouput per tank and casting equipment multiple the number of \ncycle in 𝑅𝑟 (number of cycle). The profit margin ( 𝑇𝐶𝑋𝑌) for each casting time means that the casting equipment can produce \nthe 36 rods each cycle, so the 𝑇𝐶𝑋𝑌 is USD800/ro d multiple 36 rods to be USD28,800 (USD800/rod)  per cycle of casting .  \nThe 𝑇𝐶𝑋𝑌 in the formula (5) is the discount the efficiency percentage in the casting facility and equipment,  𝐸𝑓𝑓 𝑐𝑎𝑠𝑡% . The \nmelting furnace x and the melting furnace x and f urnace  y have their  efficiency of output of melting furnace X after reduction \nof loss during the manufacturing process.  \nThe ∑ 𝐼𝐶𝑥𝑥=𝑛\n𝑥=1  is the sum idle labor cost of melting  furnace  x.  The melting  furnace  x is 1 set in the case study.  If the melting  \nfurnace  x is more than 1 set of melting  furnace s x to n sets , the total idle labor cost is summarized to n sets of melting  \nfurnace s x. The ∑(𝐶𝑎𝑝 𝑥− 𝐶𝑇𝑥𝑛\n1  𝑅𝑇𝑥) 𝐶𝑅 𝑥 means that the available of daily capacity , 𝐶𝑎𝑝 𝑥 in the melting  furnace  x \nminutes the utilized the metling furnace cycle time of melting furnace x  per cycle in minutes  𝐶𝑇𝑥  is multiplied by  the \nnumber of metling furnace x, 𝑅𝑇𝑥 and then the result  𝐶𝑎𝑝 𝑥− 𝐶𝑇𝑥𝑅𝑇𝑥 is utilizated total idle daily minutes to multiply  by \nthe idle cost 𝐶𝑅 𝑥 for the furnace x. The summarized the total idle cost for all furnace x in the  n sets. \nThe idle cost rate of melting furnace x,  𝐶𝑅 𝑥 is the standard idle labour  cost rate in USD per minute when it finds out the \nidle time in minutes to calculate the idle cost of furnace x.  International Journal of Mechanical and Industrial Technology        ISSN 2348 -7593 (Online)  \nVol. 12, Issue 2, pp: (73 -84), Month: October 2024 - March 2025, Available at: www.researchpublish.com  \n \nPage | 82  \nResearch Publish Journals  In the constraint, the total daily utilization time is the cycle time of furnace x to multiply  the number of time of melting \nfurnace x in minutes per day, 𝑅𝑇𝑥.  The daily total utilizatio n time is less than daily capacity of furnace, so it calls the \n𝐶𝑇𝑥 𝑅𝑇𝑥≤  𝐶𝑎𝑝 𝑥. \nThe∑ 𝐼𝐶𝑦𝑦=𝑛\n𝑦=1  is the sum idle labor cost of melting furnace y.  The melting furnace y is 1 set in the case study.  If the melting \nfurnace y is more than 1 set of melting furnaces y to n sets , the total idle labor cost is summarized to n sets of melting \nfurnaces  y. The ∑(𝐶𝑎𝑝 𝑦− 𝐶𝑇𝑦𝑛\n1  𝑅𝑇𝑦) 𝐶𝑅 𝑦  means that the available of daily capacity, 𝐶𝑎𝑝 𝑦 in the melting furnace y \nminutes the  utilized the me lting furnace cycle time of melting furnace y  per cycle in minutes 𝐶𝑇𝑦 is multiplied  by the \nnumber of metling furnace y, 𝑅𝑇𝑦 and then the result  𝐶𝑎𝑝 𝑦− 𝐶𝑇𝑦𝑅𝑇𝑦 is utilizated total idle daily minutes to multiply  by \nthe idle cost 𝐶𝑅 𝑦 for the furnace y. The summarized the total idle cost for all furnace y in the  n sets. \nThe idle cost rate of melting furnace y,  𝐶𝑅 𝑦 is the standard idle labour  cost rate in USD per minute when it finds out the \nidle time in minutes to calculate the idle cost of furnace y. \nIn the constraint, the total daily utilization time is the cycle time of furnace y multiplied by the number of times  of melting \nfurnace y in minutes per day, 𝑅𝑇𝑦.  The daily total utilization time is less than daily capacity of furnace, so it calls the  \n𝐶𝑇𝑦 𝑅𝑇𝑦≤  𝐶𝑎𝑝 𝑦. \nHence, the above formula from (1) to (9) has shown the detail of calculation for the linear progamming in the line balancing.  \nIn the case study, the increase in daily marginal profit is attributed to the enhanced output from melting and casting \nprocesses, as demonstrated by the following calculation:  \nThe increase in daily marginal profit is calculated as:  \nIncrement in daily marginal profit =  (New optimized daily cycles - Original daily cycles) *Output cost per cycle  per \nday - Additional operators' cost  \n \nSubstituting the values:  \n= (4.36 cycles  – 2 cycles) * USD28,800/cycle/day – (USD182/day)  \n= USD67,968/day  – USD182/day  \n= USD67,786/day  \n \nThe growth in marginal daily profit is calculated as:  \n = (USD125,386 – USD57,600) / USD57,600  \n =  117.6%  \nIn this optimization scenario, simple line balancing was achieved by increasing the labour  headcount from 8 to 12 workers \nacross two shifts per day, result ing in a 50% increase in labour  costs. The maximum profit margin per casting cycle of 36 \nrods exceeds USD28,800 (USD800 per rod). The additional profit margin from increasing production from 2 to 4.36 cycles \nper day amounts to USD67,968. After accounting f or the additional labour  cost of USD182 per day for the 4 extra workers, \nthe net profit margin is USD67,786 per day, reflecting a 117.6% growth in marginal daily profit.   \nVII.   CONCLUSION  \nLinear programming, particularly mixed integer linear programming (MILP), offers several benefits for line balancing in \naluminium  extrusion processes. Here are the key benefits and conclusions of applying linear programming to this research \nand development  of the aluminium  extrusion industry:  \n1. Optimization of Resources:  \n Linear programming helps in optimizing the allocation of resources, such as melting furnaces and casting facilities, \nto maximize output and minimize idle time. This ensures that resources are  used efficiently, reducing waste and \noperational costs.   International Journal of Mechanical and Industrial Technology        ISSN 2348 -7593 (Online)  \nVol. 12, Issue 2, pp: (73 -84), Month: October 2024 - March 2025, Available at: www.researchpublish.com  \n \nPage | 83  \nResearch Publish Journals  2. Maximization of Marginal Profit:  \n By optimizing the number of melting cycles and balancing the line, linear programming can help increase daily output \nrates  and greater utilization , thereby calculatin g the maximizing marginal profit. This is achieved by reducing idle \nlabour  costs and ensuring continuous operation of melting furnaces x and y. \n3. Reduction of Idle Time:  \n The approach minimizes idle time for melting furnaces  (furnaces x and y in n furnace) , which is crucial for maintaining \nthe necessary temperature to prevent aluminium  solidification and potential damage to the furnace  as a crash of furnace \nand melting tanks . This leads to more consistent production and less downtime.  \n4. Improved Production Flow:  \n Linear programming aids in achieving a smooth production flow by determining the optimal number of cycle times  \nof the melting process . This reduces bottlenecks and ensures a balanced workload across the production line.  \n5. Enhanced Decision -Making:  \n The use of linear programming provides a data -driven approach to decision -making, allowing aluminium  extrusion \nmanagers to evaluate different scenarios and choose the most efficient production strategy.  \n6. Flexibility and Scalability:  \n The methodology can be adapt ed to various production models, whether single -model, mixed -model, or multi -model \nlines, providing flexibility to accommodate changes in production demands or product types.  \nThe application of linear programming, particularly MILP, in aluminium  extrusion line balancing , offers significant \nadvantages in optimizing production processes. By focusing on maximizing marginal profit and minimizing idle costs, \nlinear programming ensures efficient resource utilization and enhances overall productivity. The approach  provides a \nstructured framework for balancing production lines, reducing idle time, and maintaining the operational integrity of melting  \nfurnaces. Ultimately, this leads to improved profitability, reduced operational risks, and a more agile production sys tem \ncapable of adapting to market demands and technological advancements.  In conclusion, effective line balancing in garment \nassembly operations is essential for resolving the issue of excessive work -in-process (WIP) inventory and improving the \nproduction output and efficiency, which often arises from an unbalanced production line. By systematically analyzing and \noptimizing the distribution of tasks among operators, organizations and factory planners can achieve a more synchronized \nworkflow that minimizes b ottlenecks and reduces idle time. This not only leads to a smoother production process but also \nsignificantly decreases the accumulation of WIP, thereby lowering storage costs and enhancing overall operational \nefficiency. Ultimately, implementing line -balancing techniques fosters a leaner manufacturing environment, improves \nresponsiveness to market demands, and contributes to higher levels of productivity and profitability in the garment industry.  \nThe research article on aluminium  extrusion can draw insights from cross -industry studies, highlighting the critical role of \nautomation and intelligent manufacturing in boosting productivity and output. The development of innovative technologies, \nsuch as the \"Design and Experimental Study  of Vacuum Suction Grabbing Technology to Grasp Fabric Piece\" by Prof. Dr. \nRay WM Kong et al. [9]  and in the K. M. Batoo (Ed.), Science and Technology: Developments and Applications, Innovative \nVacuum Suction -grabbing Technology for Garment Automation from  Prof. Dr. Ray WM Kong et al. [10] . Similarly, the \n\"Design of a New Pulling Gear for the Automated Pant Bottom Hem Sewing Machine\" by Prof. Dr. Ray WM Kong et al. \n[11] demonstrates how automation can enhance production rates in hem sewing machines. Looking  ahead, automation \npresents a significant opportunity to advance the aluminium  extrusion industry, paralleling its transformative impact on \ngarment and electronics manufacturing sectors.  ",
      "metadata": {
        "filename": "An Innovative Line Balancing for the Aluminium Melting Process.pdf",
        "hotspot_name": "Aluminium_Die_Casting",
        "title": "An Innovative Line Balancing for the Aluminium Melting Process",
        "published_date": "2025-03-29T14:31:33Z",
        "pdf_link": "http://arxiv.org/pdf/2504.02857v1",
        "query": "aluminium die casting energy efficiency optimization"
      }
    },
    "MEDPNet_ Achieving High-Precision Adaptive Registration for Complex Die Castings": {
      "full_text": "MEDPNet: Achieving High-Precision Adaptive\nRegistration for Complex Die Castings\nYu Du1, Yu Song1, Ce Guo2, Xiaojing Tian1*, Dong Liu2,\nMing Cong2\n1*School of Mechanical Engineering, Dalian Jiaotong University, 794\nHuanghe Road, Dalian, 116028, Liaoning, China.\n2School of Mechanical Engineering, Dalian University of Technology, 2\nLinggong Road, Dalian, 116024, Liaoning, China.\n*Corresponding author(s). E-mail(s): tzy@djtu.edu.cn;\nAbstract\nDue to their complex spatial structure and diverse geometric features, achiev-\ning high-precision and robust point cloud registration for complex Die Castings\nhas been a significant challenge in the die-casting industry. Existing point cloud\nregistration methods primarily optimize network models using well-established\nhigh-quality datasets, often neglecting practical application in real scenarios.\nTo address this gap, this paper proposes a high-precision adaptive registration\nmethod called Multiscale Efficient Deep Closest Point (MEDPNet) and intro-\nduces a die-casting point cloud dataset, DieCastCloud, specifically designed to\ntackle the challenges of point cloud registration in the die-casting industry. The\nMEDPNet method performs coarse die-casting point cloud data registration\nusing the Efficient-DCP method, followed by precision registration using the\nMultiscale feature fusion dual-channel registration (MDR) method. We enhance\nthe modeling capability and computational efficiency of the model by replacing\nthe attention mechanism of the Transformer in DCP with Efficient Attention\nand implementing a collaborative scale mechanism through the combination of\nserial and parallel blocks. Additionally, we propose the MDR method, which\nutilizes multilayer perceptrons (MLP), Normal Distributions Transform (NDT),\nand Iterative Closest Point (ICP) to achieve learnable adaptive fusion, enabling\nhigh-precision, scalable, and noise-resistant global point cloud registration. Our\nproposed method demonstrates excellent performance compared to state-of-the-\nart geometric and learning-based registration methods when applied to complex\ndie-casting point cloud data.\n1arXiv:2403.09996v1  [cs.CV]  15 Mar 2024Keywords: Complex Die Castings, point cloud registration, efficient Attention,\nmultiscale feature fusion\n1 Introduction\nComplex Die Castings are critical components in industries such as manufacturing,\ntransportation, and defense, characterized by intricate structures and diverse forms.\nHigh-quality three-dimensional reconstruction of their overall surfaces through point\ncloud registration plays a vital role in enhancing product molding quality and ensuring\nsafety in subsequent use. Recent work has made substantial progress in fully automatic,\n3D feature-based point cloud registration. At first glance, benchmarks like 3DMatch\n[1] appear to be saturated, with multiple state-of-the-art (SoTA) methods reaching\nnearly 95% feature matching recall and successfully registering over 80% of all scan\npairs [35]. However, due to the complexity of the spatial structure of Die Castings and\ntheir susceptibility to complex background interferences such as casting reflections,\noil contamination, machining marks, etc., there is currently no effective method to\nachieve high-precision point cloud registration of die-casting data. We believe that a\nhigh-precision adaptive method is the key to addressing this issue.\nCurrently, representative point cloud registration methods can be broadly catego-\nrized into two main types: those based on geometric properties[24][29][6][7]] and those\nbased on deep learning[35][3][36][27][8]. The method for point cloud registration, as\nshown in Fig 1, aims to calculate the optimal transformation parameters (R, T) (three\nrotation angles in R and three translation components in T) from the common parts\nof the data known as correspondences[49]. In recent years, with the rapid development\nof deep learning[4][12], it has found widespread application in point cloud registration\ntasks. Deep learning-based registration algorithms, including DCP[8], PointNetLK[32],\nGeoTransformer[18], etc., have significantly improved the speed and performance of\npoint cloud registration tasks. However, these methods often require more computa-\ntional resources, and their performance is frequently constrained by the quality of the\ndataset, often leading to suboptimal results in practical applications and difficulty in\nachieving stable high-precision point cloud registration effects.\nRepresentative point cloud registration algorithms based on geometric proper-\nties include Iterative Closest Point (ICP)[24] and Normal Distributions Transform\n(NDT)[29]. Such methods have low hardware requirements, are easy to implement,\nexhibit strong interpretability, and do not involve time-consuming training processes.\nHowever, they face challenges such as sensitivity to local minima or poor generaliza-\ntion, reliance on manually crafted features to distinguish corresponding relationships,\nand significant impact from the designer’s experience and parameter tuning capabil-\nities. Additionally, these methods often consume considerable time, posing potential\nbottlenecks in real-time applications.\nIn addressing this challenge, given the intricate nature of die-cast components, we\nhave devised a highly efficient adaptive registration method and created the DieCast-\nCloud point cloud dataset to validate the efficacy of the approach.DCP method\nperforms well in point cloud registration tasks, but it still has certain stability issues\n2when faced with complex die-cast point cloud data with diverse surface feature vari-\nations. To address this problem, we introduce Efficient Attention[14] to replace the\nTransformer Attention[48] in DCP. By combining serial and parallel blocks, Efficient\nAttention efficiently captures global feature information and improves computational\nefficiency. Efficient Attention differs from traditional self-attention mechanisms in\nterms of implementation. Traditional self-attention mechanisms[42][22][41] generate\nan attention map for each position to aggregate input values and produce outputs. In\ncontrast, Efficient Attention does not generate separate attention maps for each posi-\ntion. Instead, it interprets the keywords of attention as global attention maps, with\neach global attention map corresponding to a semantic aspect of the entire input.\nEfficient Attention uses these global attention maps to aggregate values and generate\na global context vector. Then, each position uses a set of coefficients to weight the\nglobal context vector and adjust its own representation. This approach gives Efficient\nAttention advantages in terms of memory and computational efficiency, as it does\nnot require calculating similarities between each pair of positions, thereby reducing\ncomputational and storage complexities. Although the improved DCP network pro-\nvides initial registration for casting point clouds, it has certain limitations in terms of\naccuracy and stability, restricting its practical feasibility in industrial applications.\nTo address this issue, we propose a multi-scale adaptive fine registration method.\nBuilding upon a favorable initial pose obtained from the DCP network, we further\nenhance the precise registration of point cloud data through the fusion of multi-scale\nfeature information. In order to ensure the robustness and verifiability of fine regis-\ntration, we improve and integrate ICP and NDT. Specifically, we initially perform\nmulti-scale feature extraction on the die-casting point cloud data to avoid the impact\nof feature loss or noise interference on point cloud registration. Subsequently, we use\na dual-channel approach to obtain transformation matrices from NDT and ICP that\nhave undergone multi-scale feature fusion. We then apply nonlinear weighting to the\nobtained transformation matrices, endowing them with good adaptability and stable\nregistration accuracy.\n•We replaced the Transformer’s Attention in DCP with Efficient Attention and imple-\nmented a collaborative scale mechanism through a combination of serial and parallel\nblocks to improve both the modeling capability and computational efficiency of the\nmodel.\n•We propose a Multiscale feature fusion dual-channel precision registration(MDR)\nmethod and supported our experimental details through ablation experiments under\nvarious scenarios.\n•We established the point cloud dataset DieCastCloud to address the challenge of\nscarce high-quality point cloud data in the die casting industry.\n2 Related Work\nDCP (Deep Closest Point): DCP[8] is a representative learning-based method\nfor point cloud registration[32][18][9]. The primary objective of the DCP method is\n3Fig. 1 Illustrates the point cloud registration process for a die-cast part . **”View 1” and\n”View 2” correspond to point clouds X and Y under different views.** Through an encoder, the point\ncloud data of the die-cast part are converted into data in feature space. Then, from the common\nparts of the unaligned point cloud pairs, the optimal transformation parameters R, t are calculated\nto obtain the best transformation matrix T∗, where R represents the rotation angles, t represents the\ntranslation components, and T∗is the best transformation matrix.\nto address issues encountered by traditional methods such as 4PCS(4-points congru-\nent sets for robust pairwise surface registration)[2] and CPD(Coherent point drift)[7],\nwhich are prone to noise interference and susceptible to getting trapped in local\noptima. DCP utilizes deep neural networks[51] to learn representations of point clouds\nand employs these learned representations for point cloud registration. Specifically,\nthe DCP[8] method comprises two sub-networks: a feature extraction network and a\ntransformation prediction network. The feature extraction network is responsible for\nextracting local and global geometric features[38] from the input point clouds, while\nthe transformation prediction network predicts the rigid transformation required to\nalign two point clouds.\nTo tackle the matching problem in point cloud registration[1][19][9][2][50], DCP[8]\nadopts the approach of Pointer Networks[31]. Pointer Networks use attention mecha-\nnisms to select positions in the input sequence, addressing the challenge of predicting\ndiscrete labels. By predicting a position distribution at each output step, Pointer Net-\nworks can be considered as ”soft pointers” for selecting matching positions. The entire\nnetwork is differentiable, allowing for end-to-end training[55]. HSGM [21][16] proposes\na hierarchical similarity graph module to relieve the conflict of backbone networks\nand mine the discriminative features. Additionally, Transformer[48] models are uti-\nlized to learn contextual information of point clouds, enabling the model to capture\nglobal feature information. However, DCP still exhibits certain limitations when deal-\ning with point cloud data with significant initial pose differences. In the presence of\nadded Gaussian noise[52], although DCP[8] demonstrates better robustness compared\nto methods like FGR[15], it is still subject to some degree of influence.\nAn Adaptive Registration Method Based on Multimodal Data: The cur-\nrent trend in point cloud tasks is the increasing popularity of multimodal data[5][39].\nGeometry-based methods[23][43][45], in the context of point cloud registration, involve\nutilizing geometric features such as point positions, distances, and orientations to\nestablish correspondences and achieve alignment between point clouds. These meth-\nods typically aim to find the optimal rigid transformation to minimize geometric\n4disparities between point clouds, ensuring accurate registration. ICP (Iterative Clos-\nest Point)[24] and NDT (Normal Distributions Transform)[29] are two of the most\nrenowned geometry-based point cloud registration methods, particularly suitable for\nscenarios with local overlap and small-scale rigid transformations.\nICP (Iterative Closest Point)[24] is a classical method for point cloud registration,\nwith the primary goal of iteratively finding the optimal rigid transformation between\ntwo point clouds to align them as closely as possible[57][58]. The core idea of ICP\ninvolves iteratively mapping points from the target point cloud to the reference point\ncloud and updating the rigid transformation based on the corresponding mappings.\nThis iterative process continues until convergence is achieved, ultimately realizing\nthe best possible alignment between the two point clouds. NDT (Normal Distribu-\ntions Transform)[29] is a method used for point cloud registration, and its core idea\ninvolves describing the local structure of each point cloud by modeling the normal\ndistribution of points. By mapping each point in the point cloud to its corresponding\nGaussian distribution[53], NDT represents the point cloud as a set of probability den-\nsity distributions[56]. During the registration process, the method adjusts the rigid\ntransformation[54] by minimizing the disparity in probability density distributions\nbetween the two point clouds, thereby achieving optimal point cloud alignment[59][60].\nWhile both geometric-based methods[24][29][2][45] and their variants[50][23] have\nbecome increasingly mature, integrating them into practical engineering applications\nremains a highly challenging task. The ICP method performs well in scenarios with\nlocal overlap and small-scale rigid transformations but is sensitive to noise and prone to\ngetting stuck in local optima. The NDT method demonstrates advantages in handling\nlarge-scale[20], sparse, or point clouds with complex geometric structures; however, its\nperformance is constrained by the choice of parameters.\n3 Proposed Method\n3.1 Overview\nGiven two sets of point clouds X={xi∈R3|i= 1, . . . , N }andY={yi∈\nR3|i= 1, . . . , M }, the objective is to estimate rigid transformations Ti={Ri, ti}for\ni= 1, . . . , N to align these two point clouds. In Fig 2, we present the architecture\nof MEDPNet. In brief, we embed the acquired die-casting part point cloud data into\na high-dimensional space using DGCNN[10] and encode the contextual information\nwith the Efficient Attention[14] module, finally estimating the alignment using a dif-\nferentiable SVD layer][47]. In which, xP\niis the embedding of point iin the P-th layer,\nandhP\nθis a nonlinear function in the P-th layer parameterized by a shared multilayer\nperceptron (MLP). The forward mechanism is given by:\nxP\ni=hP\nθ(xP−1\ni) (1)\nWe input the collected unaligned input point clouds X and Y into the same space,\nwhere we embed each point of the two input point clouds individually, and iterate over\nthe features of each point in the input point clouds, represented by the aggregation\n53D SensorRobot𝑿\n𝒀DGCNNSharedParameterTransformerℱ𝒳ℱ𝒳ℱ𝒴ℱ𝒴Φ𝒳Φ𝒴Pointer𝑌!𝑚(𝒙\",𝒴)𝑹#$=𝑽𝑼!𝑡#$=−𝑹#$𝒙0+𝒚0\t(a) Efficient DCP\n(b) Transformer(b) DGCNN\nTransfor-mation Matrix(b) Efficient Transformer\n(c) MDRFig. 2 The architecture of MEDPNet. In the diagram, part (a) shows the structure\nof Efficient DCP, part (b) illustrates the composition of Efficient Attention, and part (c)\noutlines the framework of the MDR method, with a detailed exposition of its details. The\nMEDPNet method collects point cloud data of the same die-casting part under different\npostures through a robotic arm equipped with a 3D sensor, inputs the unaligned point cloud\npairs into Efficient DCP for preliminary registration, and then refines the alignment through\nMDR to preserve essential feature information.\nfunction as:\nFx={xP\n1, xP\n2, . . . , xP\ni, . . . , xP\nN}, (2)\nand\nFy={yP\n1, yP\n2, . . . , yP\ni, . . . , yP\nN} (3)\nDGCNN constructs a k-NN (k-nearest neighbor) graph M, nonlinearly acquires edge\nvalues at edge endpoints, and performs per-vertex aggregation at each layer. Unlike\nPointNet[33], which extracts independent information from each point, DGCNN\nexplicitly incorporates local geometric shapes into its representation. This is achieved\nthrough the forward mechanism:\nxP\ni=f({hP\nθ(xP−1\ni, xP−1\nj)∀j∈Ni}) (4)\nwhere Nirepresents the set of neighbors of point iin the k-NN graph, ensuring that\nlocal geometric features are considered during the aggregation process. In the task of\ndie-casting part point cloud registration, DGCNN achieves higher quality registration\nperformance by leveraging these local geometric information.\n6Softmax\nQ K VOutput Feature Map\nInput Feature MapN × CN × C N × C C × NC × CN × CFig. 3 Illustration of the architecture of efficient attention .Where the input feature map\nundergoes a transformation into three distinct components: Queries ( Q), Keys ( K), and Values ( V).\nThese components facilitate a self-attention schema by computing attention scores between Qand\nK, followed by a softmax normalization to acquire a probabilistic weight distribution. The weighted\nsum of these probabilities with Vculminates in the output feature map, encapsulating a dynamic\nrepresentation of salient features pivotal for subsequent layers of the network to process. This mech-\nanism underpins the network’s capacity to accentuate pertinent information within the feature space\nselectively.\n3.2 Efficient Attention\nBefore introducing Efficient attention, let’s first discuss the concept of dot-product\nattention. Dot-product attention is a fundamental attention mechanism commonly\nused in models like Transformers. For a given query vector Q, key vector K, and value\nvector V, the computation of dot-product attention is as follows:\nAttention( Q, K, V ) = softmax\u0012QKT\n√dk\u0013\nV (5)\nHere, dkis the dimensionality of query/key vectors, QKTrepresents the dot product\nbetween query vector Qand key vector K, and it is scaled by√dkto stabilize gra-\ndient magnitudes. The softmax function[46] normalizes the dot product results into\nattention weights, which are then used to weight the value vector Vto generate the\nfinal output. This mechanism allows the model to dynamically allocate attention based\non the similarity between queries and keys, capturing relationships between different\npositions in the input sequence.\nThe principle of efficient attention is to optimize the traditional attention mecha-\nnism, particularly in addressing the challenges of high computational complexity and\nsignificant memory consumption when processing long sequence data. By reducing\nredundancies in computation and employing more efficient computational strategies,\nsuch as low-rank factorization and kernel techniques, it approximates the key operation\nQKTin the standard self-attention mechanism, as shown in Fig 3.\n7In the standard self-attention mechanism, the input sequence X∈Rn×dundergoes\nlinear transformations to obtain queries Q, keys K, and values V, where Q=XW Q,\nK=XW K,V=XW V, and WQ,WK,WV∈Rd×dkare the corresponding weight\nmatrices, with dkbeing the feature dimension.\nTraditionally, attention weights are obtained by computing QKTand applying the\nsoftmax function, i.e.,\nAttention( Q, K, V ) = softmax\u0012QKT\n√dk\u0013\nV (6)\nThis step has a computational complexity of O(n2dk), which for long sequence data,\nresults in significant computational burden and memory requirements.\nEfficient attention introduces an approximation technique to reduce this complex-\nity, specifically, it uses a form\nAttention( Q, K, V )≈softmax\u0012ϕ(Q)ϕ(K)T\n√dk\u0013\nV (7)\nfor approximation, where ϕ(·) is a nonlinear function mapping to a lower-dimensional\nfeature space, effectively reducing the required computational power and storage space.\nThis mapping not only reduces the need for direct computation of QKTbut also, by\nselecting an appropriate ϕfunction, can lower the computational complexity of the\nattention mechanism from O(n2dk) toO(nmd k), where mis the dimension of the\nmapped lower-dimensional space, significantly smaller than the length of the input\nsequence n.\nPrecisely for these reasons, Efficient attention significantly enhances computational\nand storage efficiency in die-cast parts point cloud registration tasks by optimizing\nthe attention mechanism. It effectively manages long-sequence dependencies, accu-\nrately captures changes and spatial relationships between point clouds, achieves\nhigh-precision registration, and broadens the application scope in resource-constrained\nenvironments.\n3.3 Adaptive Multi-scale Patch Matching for Registration\nFirst, let’s review the geometric point cloud registration method. Its core idea involves\niteratively optimizing the rigid transformation {R, t}, where Ris the rotation matrix\nandtis the translation vector. The objective is to minimize the distance Tbetween\ntwo point clouds, achieving their alignment. The expression for this can be given as\nfollows:\nT∗= arg min\nTNX\ni=1∥f(xi)−yi∥2(8)\nour approach primarily adopts the ICP (Iterative Closest Point) method and the NDT\n(Normal Distributions Transform) method. The core idea of the ICP method is to\nachieve point cloud registration by iteratively optimizing the rigid transformation {R,\n8Fig. 4 Adaptive parameter optimization . First, we input the formula to be optimized into\nan MLP and use backpropagation to optimize the weights. Next, through a self-updating filtering\nmechanism, we iterate to find the smallest registration error, iteratively updating the adaptive hyper-\nparameter ε.\nt}to minimize the distance Tbetween point clouds. Its expression is:\nT∗= min\nR,tX\ni∥Rxi+t−yi∥2(9)\nHere, ICP primarily aligns point clouds through iterative optimization. In the process\nof minimizing the objective function, adjustments to Randtare made to bring the\ntwo point clouds as close together as possible in space.\nThe NDT method describes the local structure of point clouds by modeling the\nnormal distribution of each point. Its optimization objective is formulated as:\nT∗= min\nR,tX\ni1\n2(µxi−µyi)T\n2Σ−1\ni(µxi−µyi) (10)\nµxiandµyidenote the mean of the normal distribution corresponding to points in\ntwo point clouds, and Σ irepresents the covariance matrix of the normal distribution.\nThe goal is to adjust Randtto make the two point clouds as consistent as possible\nin terms of normal distribution, minimizing the objective function.\nMulti-scale Feature Fusion Module: In both ICP (Iterative Closest Point) and\nNDT (Normal Distributions Transform), we introduced a multiscale feature fusion\nmodule aimed at addressing key challenges in point cloud registration, such as local\nminima and sensitivity to initial values. Our approach first utilizes the Feature Pyra-\nmid Network (FPN) [17] method to generate multiple scales of point clouds through\n9downsampling. At each scale, the ICP algorithm is independently applied to find the\noptimal rigid transformation. Subsequently, the coarse-scale registration results are\npropagated to finer scales, resulting in the final registration outcome and transforma-\ntion matrix. We have successfully implemented feature fusion on die-cast point cloud\npairs at kdifferent scales.\nThe multiscale ICP method aims to minimize the registration error across all scales\nkand corresponding points iby finding the optimal rigid transformation Rkandtk.\nThis approach allows us to comprehensively tackle registration challenges at multiple\nscales, thereby enhancing the algorithm’s robustness in diverse scale environments.\nThe expression for this method is given by:\nT∗= min\nRk,tkX\nkX\ni∥Rkxk\ni+tk−yk\ni∥2(11)\nsimilarly, multiscale Normal Distributions Transform (NDT) minimizes the cumulative\nerror of the normal distribution for corresponding points at each scale, utilizing the\nMahalanobis distance metric (measured by the difference between the inverse covari-\nance matrix and the normal distributions). This optimization seeks to find the optimal\nrigid transformation Rkand translation vector Tkat each scale. This enables precise\nregistration of point cloud Xwith target point cloud Yin the normal distribution\nacross multiple scales through adjustments in rigid transformation and translation. It\ncan be expressed as:\nT∗= min\nRk,TkX\nkX\ni1\n2(µk\nxi−µk\nyi)TΣ−1\ni(µk\nxi−µk\nyi) (12)\nDual Channel Fusion Module: Previous studies have primarily focused on\nenhancing the performance of the ICP (Iterative Closest Point) and NDT (Normal Dis-\ntributions Transform) methods, yet they have overlooked the importance of ensuring\nstability under conditions of high precision. This issue becomes particularly evident\nwhen dealing with complex die-cast point cloud data, where the ICP algorithm may\nperform excellently on point cloud data ”a,” while the NDT algorithm shows superior\nperformance on point cloud data ”b.” To overcome this challenge, we propose a novel\ndual-channel fusion module. Through multi-scale feature fusion, this module enables\nthe ICP and NDT methods to obtain rigid transformation matrices T1andT2, respec-\ntively. We input 300 pairs of rigid transformation matrices from the ICP and NDT\nmethods into an MLP for learnable self-feedback weighting and iterate the weights of\nthe optimal registration results by minimizing the registration error, as shown in Fig\n4. The MLP consists of three fully connected layers, with neuron counts of 32, 64, and\n32, respectively. Here, we opt for the Huber loss function, expressed as:\nLδ(a) =(\n1\n2a2for|a| ≤δ,\nδ(|a| −1\n2δ) otherwise.(13)\n10Where a=l−ˆlrepresents the prediction error, i.e., the difference between the actual\nvalue land the predicted value ˆl.δis a threshold parameter that determines the point\nat which the loss function transitions from squared error to linear error. When the\nabsolute value of the error is less than or equal to δ, the loss function behaves like the\nsquare of the error (similar to MSE), imposing a heavier penalty for smaller errors\nto encourage more precise fitting. Conversely, when the absolute value of the error\nexceeds δ, the loss function becomes linear (similar to MAE), reducing the penalty for\nlarger errors and enhancing the model’s robustness.\nAlthough the merged matrix Tobtained at this point demonstrates certain reli-\nability, repeated tests have shown that the model’s accuracy decreases when facing\nunfamiliar samples. We hypothesize that this instability might be due to the limited\nsample size, making it difficult for the model to learn the complete features of die-\ncast part point clouds. However, due to the irreplicability of die-cast samples and\nthe industrial production cycle’s inability to accommodate the training duration for a\nlarge volume of samples, we introduced the hyperparameter ε. Initially, we hoped to\ndirectly obtain optimal weights and εthrough the MLP mechanism, but the limited\ntraining samples and excessive number of parameters to be optimized led to unsatis-\nfactory results. To address this, we added a self-updating filtering mechanism on top\nof the MLP. With the determination of optimal weights W∗\n1andW∗\n2, we use the root\nmean square error (RMSE) feedback from each iteration to determine the correspond-\ningε, choosing the εassociated with the minimum RMSE as the input for the next\niteration, as shown in Fig 4.The formula for RMSE is as follows:\nRMSE =vuut1\nNNX\ni=1(li−ˆli)2 (14)\nWhere Nis the number of samples, liis the actual value of sample i, and ˆliis the\npredicted value for sample i.Finally,we can obtain:\nT∗=W∗\n1·T1+W∗\n2·T2+ε∗(15)\nIn this setup, we achieved learnable adaptive registration through a multilayer per-\nceptron and a self-updating filtering mechanism, obtaining desirable results on the\ndie-cast part point cloud dataset DieCastCloud.\n4 Experiment and Analysis\nIn this section, we will conduct comparative experiments to assess the effectiveness\nof our approach. We first introduce the details of the experiments in Section 4.1.\nIn Section 4.2, we evaluate the Efficient DCP method on our die-cast dataset and\nperform ablation experiments to ensure the method’s effectiveness. In Section 4.3, we\nintroduce the an adaptive registration method based on multimodal data, providing\ncorresponding experiments at each step. In Section 4.5, we will present an overall\nintroduction to our method, MEDPNet (Multimodal Efficient Deep Closest Point),\nand compare it with the current state-of-the-art methods.\n114.1 Implementation Details\nThis experiment utilizes Open3D[30] 1.2.0 and PCL 1.9.1 to implement algorithm\nexecution in Python and C++. The experimental platform is Ubuntu 18.04 system,\nwith PyTorch[37] version 1.8.1, CUDA version 11.1, GPU=RTX 3090 (24GB) * 1,\nCPU=15 vCPU AMD EPYC 7642 48-Core Processor. To test the generalization of\ndifferent models, we will split DieCastCloud into training and testing sets.\nDue to the complexity of the surface features of die cast parts, unlike the approach\nin PointNet[33] experiments of uniformly sampling 1024 points on the model’s outer\nsurface, we opted to sample 4096 points. This decision was based on an understanding\nof the complexity of die cast surface features, aiming to more comprehensively preserve\nthe point cloud’s feature information, thereby enhancing the accuracy and reliability\nof subsequent registration.\nTo ensure consistency and standardization in data processing, we performed a\nseries of preprocessing steps on the collected point cloud data. Initially, the point cloud\ndata was centered at the origin and scaled to fit within a unit sphere. Throughout\nthis process, we only used the three-dimensional coordinates (x, y, z) of the points as\ninput features, without introducing any additional attribute information, to accurately\nassess the model’s ability to recognize and process geometric shapes themselves.\nThe initial pose has a critical impact on point cloud registration. To better quantify\nthe performance of coarse registration, we employed multiple error metrics, including\nMean Squared Error (MSE), Root Mean Squared Error (RMSE), and Mean Absolute\nError (MAE), to ensure the reliability of the method. Moreover, considering prac-\ntical applications in the die casting industry, we primarily focused on Root Mean\nSquared Error (RMSE) and registration time (s) during fine registration, where all\nangle measurements were made in degrees ( °).\n4.2 Datasets\nThe DieCastCloud dataset contains 2,000 point cloud data, including 5 different types\nof die-cast parts. This dataset is randomly divided into a training set and a test\nset, with proportions of 0.8 and 0.2, respectively. In practical applications in the die-\ncasting industry, the purpose of point cloud registration is to enhance the completeness\nof the point cloud data while preserving key features, to ensure high-precision 3D\nreconstruction and facilitate product quality control. Unlike other datasets[28][25],\nhere, to ensure the practical feasibility of the method, the overlap rate of point cloud\ndata in DieCastCloud is set to be greater than 85%.\nWe utilized the UR16e robotic arm equipped with the high-precision 3D laser\nscanner CIRRUS 3D 300 to collect point cloud data of die-cast parts, and we named\nthe resulting dataset DieCastCloud. The point cloud data in DieCastCloud covers the\nmain external surfaces of the die-cast parts, including complex geometric features such\nas pipes and holes. Additionally, we processed and filtered the collected raw point\ncloud data to obtain a richer sample set.\nFinally, in the creation process of DieCastCloud, point cloud data was enhanced\nthrough techniques such as rotation, translation, scaling, and random erasure to\nincrease the diversity of the data and enhance the model’s generalization capabilities.\n12Specifically, we randomly rotated the point cloud data at random angles along any\naxis and translated it in any spatial direction, with the translation range controlled\nwithin [-800mm, 800mm]. The scaling ratio was constrained to between [0.95, 1.05] of\nthe original point cloud size.\n4.3 Efficient of DCP\nIn this experiment, we use DCP-v2, which incorporates a Transformer, as our\nbaseline. We compare the performance of Efficient DCP with other cutting-edge\ndeep learning-based point cloud registration methods, including PointNetLK[32],\nGeoTransformer[18], PRNet[36], DeepGMR[9], and DCP[8]. We randomly split 2000\ndie-cast component point clouds from DieCastCloud into validation and test sets,\nutilizing different point cloud data during the training and testing periods.During\ntraining, we sampled the point clouds and applied a random rigid transformation\nalong each axis, with rotations uniformly sampled within [0, 60 °] and translations in\nthe range of [-150mm, 150mm]. The source point cloud and the point cloud after the\nrigid transformation were used as the input to the network.\nTo ensure a fair comparison among these methods, we follow the convention and\nuse performance metrics including Mean Squared Error (MSE) for rotation angles\n(MSE(R)), Mean Squared Error for translation directions (MSE(t)), Root Mean\nSquared Error for rotation angles (RMSE(R)), Root Mean Squared Error for transla-\ntion directions (RMSE(t)), Mean Absolute Error for rotation angles (MAE(R)), and\nMean Absolute Error for translation directions (MAE(t)), to guarantee the reliability\nof the experiments.\nTable 1 assesses the performance of our method and its counterparts in this exper-\niment. In this study, Efficient DCP adopts a structure that integrates DGCNN with\nEfficient Transformer, with a learning rate of 0.001, 200 epochs, train batch sizes\nof 32 and train batch sizes of 10, and employs Stochastic Gradient Descent (SGD)\nas the optimizer. Across all evaluated performance metrics, Efficient DCP showcases\noutstanding performance.\nTable 1 Evaluating the performance of Efficient DCP against other advanced methods\nModel MSE(R) MSE(t) RMSE(R) RMSE(t) MAE(R) MAE(t)\nPointNetLK 51.271578 0.114432 6.842565 0.089526 7.664845 0.045454\nGeoTransformer 24.352470 0.001422 5.898481 0.002177 2.974119 0.084997\nPRNet 82.665150 0.014432 12.54549 0.114551 4.859481 0.072361\nDeepGMR 29.159647 0.008747 3.861095 0.084411 3.784151 0.048944\nDCP 24.372444 0.009330 4.851226 0.017721 2.311324 0.027983\nEfficient DCP(Ours) 4.822984 0.000231 2.196129 0.015187 1.350260 0.008338\nPerformance metrics include Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and\nMean Absolute Error (MAE) in both translation and rotation directions. Boldfaced numbers high-\nlight the best performance and the second best are underlined .\n134.4 Multiscale Feature Fusion Dual-channel Precision\nRegistration\nIn this section, we conducted comparative experiments to validate our choice of the\nIterative Closest Point (ICP) and Normal Distributions Transform (NDT) methods.\nIn real industrial scenarios, registration time needs to be controlled within 60 seconds.\nTo better evaluate the feasibility of the methods, we used Root Mean Square Error\n(RMSE) in millimeters and registration time in seconds as performance evaluation\nmetrics, with the results shown in Table 2. Additionally, to more intuitively understand\nthe impact of rotation angles ( °) and translation distances (mm) on various methods,\nwe conducted comparative experiments to test the performance of each method at\nspecific rotation angles and translation distances, with results presented in Table 3.\nFinally, we elucidated the advantages of our dual-channel precision registration method\nbased on multi-scale features.\nTable 2 Performance on clean and noisy samples\nData clean Data noisy\nMethod RMSE(mm) Time(s) RMSE(mm) Time(s)\nSAC-IA 0.128 47.64 0.206 53.76\n4PCS 0.168 13.32 0.273 16.11\nNDT 0.158 4.33 0.182 5.64\nICP 0.153 12.47 0.197 18.12\nMDR(ours) 0.092 25.92 0.148 29.41\nPerformance metrics primarily include Root Mean Square Error (RMSE) and registration time (s).\nBoldfaced numbers highlight the best performance and the second best are underlined .\nTable 3 Testing the impact of different rotation angles and translation distances on the\nexperimental results\nData Rotate ( °) Data Translation (mm)\nMethod 10 20 30 100 500 1000\nSAC-IA 0.004 0.008 0.015 0.003 0.007 0.015\n4PCS 0.002 0.007 0.011 0.010 0.014 0.022\nNDT 0.007 0.009 0.012 0.001 0.003 0.010\nICP 0.002 0.004 0.008 0.003 0.008 0.017\nMDR(ours) 0.001 0.001 0.003 0.001 0.002 0.002\nWe tested the Root Mean Square Error (RMSE) of each method when the rotation angles were\n10°, 30 °, and 90 °, and the translation distances were 100mm, 500mm, and 1000mm, respectively.\nBoldfaced numbers highlight the best performance and the second best are underlined .\nWe first evaluated the performance of SAC-IA[45], 4PCS[2], NDT[29], ICP[24],\nand MDR on the DieCastCloud dataset. To assess the robustness of these methods,\nwe introduced noise with an intensity of 0.1 into the DieCastCloud dataset and then\n14Input\nPointNetLK DCP Efficient DCP NDT MEDPNet(ours) ICP MDR\nClean\nNoisy\nInputPointNetLK NDT DCP MDR MEDPNet(ours) ICP Efficient DCP\nFig. 5 Registration result visualization . We visualized the registration results of various\nmethods under clean and noisy samples, ranking them in descending order according to the\nroot mean square error.\nconducted comparative experiments to evaluate the stability of each method under\nnoisy conditions. By incorporating a multi-scale feature fusion module, MDR pos-\nsesses more comprehensive feature information compared to other methods, thereby\nachieving higher registration accuracy and noise resistance. As shown in Table 2, MDR\ndemonstrates high performance while meeting the registration time requirements in\npractical applications.\nNext, we tested the effects of rotation angles and translation distances on several\nalgorithms. We rotated the point cloud data by 10 °, 30°, and 90 °around any spatial\naxis and translated it by 100mm, 500mm, and 1000mm in any spatial direction. As\ncan be seen from Table 3, ICP showed a notable performance for different angles\nof rotation, while NDT performed better in facing translation issues and exhibited\n15greater robustness in dealing with spatial position changes. Our method demonstrated\nthe best registration performance compared to the other methods.\nTable 4 The registration performance of\nvarious methods under clean and noisy samples\nMethod Data clean Data noisy\nPointNetLK 6.84315 15.9744\nDCP 4.85126 5.87391\nNDT 4.27784 9.76239\nEfficient DCP (Ours) 2.19618 3.71646\nICP 2.07729 6.73248\nMDR (Ours) 1.94877 4.67442\nMEDPNet (Ours) 1.17245 1.32954\nHere, we arrange the methods in descending order\naccording to their root mean square error (RMSE)\non clean samples. Boldfaced numbers highlight\nthe best performance and the second best are\nunderlined .\n4.5 Influence of MEDPNet\nOverall, the MEDPNet method achieves high-quality registration results by initially\napplying Efficient DCP for coarse registration of unaligned point cloud pairs, followed\nby fine-tuning through MDR. To assess the accuracy and robustness of our method,\nwe conducted tests on both clean and noisy samples, with our experimental results\npresented in Table 4. We selected root mean square error (RMSE) as the performance\nmetric to comprehensively account for both variance and bias. The experimental out-\ncomes indicate that our method not only ensures high accuracy but also maintains\nrobustness across different conditions.\n4.6 Visualization\nIn this section, we present a visual comparison of the performance of various advanced\nmethods against our MEDPNet approach, as illustrated in Fig 5. We selected three\ntypical types of die casting samples and conducted experiments under both noise-\nfree and noisy conditions to ensure the practicality of our method. Our comparison\nmainly includes PointNetLK[32], DCP[8], ICP[24], NDT[29], and our improved meth-\nods Efficient DCP, MDR, and MEDPNet, arranged in descending order according to\nroot mean square error (RMSE). As can be observed in Fig 5, MEDPNet achieves\nstate-of-the-art performance on both clean and noisy samples.\n5 Conclusion\nIn the intricate domain of die casting, where complex spatial structures and het-\nerogeneous geometric features prevail, the quest for precise and resilient point cloud\n16registration represents a formidable challenge. Traditional methodologies predomi-\nnantly hinge on high-caliber datasets, endeavoring to enhance registration fidelity\nthrough network model optimization, yet frequently neglecting the nuances of real-\nworld deployment. Addressing this lacuna, the present exposition delineates the\nMultiscale Efficient Deep Closest Point (MEDPNet) modality, coupled with the estab-\nlishment of DieCastCloud, a bespoke point cloud dataset specifically designed to\nmitigate the application impediments of point cloud registration within the die casting\nsphere.\nThe MEDPNet initially conducts coarse registration based on Efficient-DCP, sub-\nsequently transitioning to advanced precision registration via the Multiscale feature\nfusion dual-channel registration (MDR) method. By replacing the traditional Trans-\nformer’s attention mechanism with Efficient Attention, it introduces a Multiscale\nfeature fusion dual-channel precision registration (MDR) technique. This technique\nminimizes registration errors by adaptively optimizing the final transformation matrix\nusing multilayer perceptrons (MLP), resulting in an adaptive, scalable, and highly\nrobust global point cloud registration framework.\nAlthough our method achieves excellent registration results for die casting point\nclouds, there are still some shortcomings. Firstly, due to the large size of Die Castings,\nas well as the presence of occlusions, blind spots, and the influence of machining\nmarks, collecting high-quality point cloud data often requires a significant amount\nof time. Furthermore, for different Die Castings, it is necessary to adjust the data\ncollection strategy of the robot. We believe that an intelligent die casting point cloud\ngenerator is key to solving this problem. Secondly, in the step of precise registration,\nwe utilize unsubsampling point cloud data, where the parameter count of each point\ncloud reaches the tens of millions level. In actual industrial production, how to reduce\ncomputational costs is an urgent problem that needs to be addressed.\n6 Acknowledgements\nThis work was supported by both the Unveiling the Top Technical Research Project\nof Dalian City (2023JB11GX001) and the Key Special Projects of the National Key\nR&D Program (2022YFB3706802).",
      "metadata": {
        "filename": "MEDPNet_ Achieving High-Precision Adaptive Registration for Complex Die Castings.pdf",
        "hotspot_name": "Aluminium_Die_Casting",
        "title": "MEDPNet: Achieving High-Precision Adaptive Registration for Complex Die\n  Castings",
        "published_date": "2024-03-15T03:42:38Z",
        "pdf_link": "http://arxiv.org/pdf/2403.09996v1",
        "query": "aluminium die casting energy efficiency optimization"
      }
    },
    "Optimization of Solidification in Die Casting using Numerical Simulations and Ma": {
      "full_text": "Optimization of Solidi\fcation in Die Casting using\nNumerical Simulations and Machine Learning\nShantanu Shahane1, Narayana Aluru, Placid Ferreira, Shiv G Kapoor,\nSurya Pratap Vanka\nDepartment of Mechanical Science and Engineering\nUniversity of Illinois at Urbana-Champaign\nUrbana, Illinois 61801\nAbstract\nIn this paper, we demonstrate the combination of machine learning and three\ndimensional numerical simulations for multi{objective optimization of low\npressure die casting. The cooling of molten metal inside the mold is achieved\ntypically by passing water through the cooling lines in the die. Depending\non the cooling line location, coolant \row rate and die geometry, nonuni-\nform temperatures are imposed on the molten metal at the mold wall. This\nboundary condition along with the initial molten metal temperature a\u000bect\nthe product quality quanti\fed in terms of micro-structure parameters and\nyield strength. A \fnite volume based numerical solver is used to determine\nthe temperature-time history and correlate the inputs to outputs. The objec-\ntive of this research is to develop and demonstrate a procedure to obtain the\ninitial and wall temperatures so as to optimize the product quality. The non-\ndominated sorting genetic algorithm (NSGA{II) is used for multi{objective\noptimization in this work. The number of function evaluations required for\n1Corresponding Author Email: shahaneshantanu@gmail.com\nPreprint submitted to Journal of Manufacturing Processes January 6, 2020arXiv:1901.02364v2  [cs.CE]  3 Jan 2020NSGA{II can be of the order of millions and hence, the \fnite volume solver\ncannot be used directly for optimization. Therefore, a multilayer perceptron\nfeed{forward neural network is \frst trained using the results from the numer-\nical solution of the \ruid \row and energy equations and is subsequently used\nas a surrogate model. As an assessment, simpli\fed versions of the actual\nproblem are designed to \frst verify results of the genetic algorithm. An in-\nnovative local sensitivity based approach is then used to rank the \fnal Pareto\noptimal solutions and select a single best design.\nKeywords: Die Casting, Deep Neural Networks, Multi{Objective\nOptimization\n1. Introduction\nDie casting is one of the popular manufacturing processes in which liquid\nmetal is injected into a permanent metal mold and solidi\fed. Generally, die\ncasting is used for parts made of aluminum and magnesium alloys with steel\nmolds. Automotive and housing industrial sectors are common consumers of\ndie casting. In such a complex process, there are several input parameters\nwhich a\u000bect the \fnal product quality and process e\u000eciency. With advances\nin computing hardware and software, the physics of these processes can be\nmodeled using numerical simulation techniques. Detailed \row and tempera-\nture histories, micro-structure parameters, mechanical strength etc. can be\nestimated from these simulations. In today's competitive industrial world,\nestimating the values of input parameters for which the product quality is\noptimized has become highly important. There has been extensive research\nin numerical optimization algorithms which can be coupled with detailed\n2numerical simulations in order to handle complex optimization problems.\nSolidi\fcation in casting process has been studied by many researchers.\nMinaie et al. [1] have analyzed metal \row during die \flling and solidi\fcation\nin a two dimensional rectangular cavity. The \row pattern during the \flling\nstage is predicted using the volume of \ruid (VOF) method and enthalpy equa-\ntion is used to model the phase change with convection and di\u000busion inside\nthe cavity. They have studied the e\u000bect of gate location on the residual \row\n\feld after \flling and the solid liquid interface during solidi\fcation. Im et al.\n[2] have done a combined \flling and solidi\fcation analysis in a square cavity\nusing the implicit \flling algorithm with the modi\fed VOF together with the\nenthaply formulation. They studied the e\u000bect of assisting \row and opposite\n\row due to di\u000berent gate positions on the residual \row. They found that the\nliquid metal solidi\fes faster in the opposite \row than in the assisting \row sit-\nuation. Cleary et al. [3] used the Smoothed Particle Hydrodynamics (SPH)\nto simulate \row and solidi\fcation in three dimensional practical geometries.\nThey demonstrated the approach of short shots to \fll and solidify the cavity\npartially with insu\u000ecient metal so that validation can be performed on par-\ntial castings. Plotkowski et al. [4] simulated the phase change with \ruid \row\nin a rectangular cavity both analytically and numerically. They simpli\fed\nthe governing equations of the mixture model by scaling analysis followed\nby an analytical solution and then compared with a complete \fnite volume\nsolution.\nRecently, there has been a growing interest in the numerical optimization\nof various engineering systems. Poloni et al. [5] applied neural network with\nmulti{objective genetic algorithm and gradient based optimizer to the de-\n3sign of a sailing yacht \fn. The geometry of the \fn was parameterized using\nBezier polynomials. The lift and drag on the \fn was optimized as a function\nof the Bezier parameters and thus, an optimal \fn geometry was designed.\nElsayed and Lacor [6] performed a multi{objective optimization of a gas cy-\nclone which is a device used as a gas-solid separator. They trained a radial\nbasis function neural network (RBFNN) to correlate the geometric param-\neters like diameters and heights of the cyclone funnel to the performance\ne\u000eciency and the pressure drop using the data from numerical simulations.\nThey further used the non-dominated sorting genetic algorithm (NSGA{II)\nto obtain the Pareto front of the cyclone designs. Wang et al. [7] optimized\nthe groove pro\fle to improve hydrodynamic lubrication performance in or-\nder to reduce the coe\u000ecient of friction and temperature rise of the specimen.\nThey coupled the genetic algorithm (GA) with the sequential quadratic pro-\ngramming (SQP) algorithm such that the GA solutions were provided as\ninitial points to the SQP. Stavrakakis et al. [8] solved for window sizes for\noptimal thermal comfort and indoor air quality in naturally ventilated build-\nings. A computational \ruid dynamics model was used to simulate the air\n\row in and around the buildings and generate data for training and test-\ning of a RBFNN which is further used for constrained optimization using\nthe SQP algorithm. Wei and Joshi [9] modeled the thermal resistance of a\nmicro-channel heat exchanger for electronic cooling using a simpli\fed thermal\nresistance network model. They used a genetic algorithm to obtain optimal\ngeometry of the heat exchanger so as to minimize the thermal resistance\nsubject to constraints of maximum pressure drop and volumetric \row rate.\nHusain and Kim [10] optimized the thermal resistance and pumping power of\n4a micro-channel heat sink as a function of geometric parameters of the chan-\nnel. They used a three dimensional \fnite volume solver to solve the \ruid\n\row equations and generate training data for surrogate models. They used\nmultiple surrogate models like response surface approximations, Kriging and\nRBFNN. They provided the solutions obtained from the NSGA{II algorithm\nto SQP as initial guesses. Lohan et al. [11] performed a topology optimization\nto maximize the heat transfer through a heat sink with dendritic geometry.\nThey used a space colonization algorithm to generate topological patterns\nwith a genetic algorithm for optimization. Amanifard et al. [12] solved an\noptimization problem to minimize the pressure drop and maximize the Nus-\nselt number with respect to the geometric parameters and Reynolds number\nfor micro-channels. They used a group method of data handling type neural\nnetwork as a surrogate model with the NSGA{II algorithm for optimization.\nEsparza et al. [13] optimized the design of a gating system used for gravity\n\flling a casting so as to minimize the gate velocity. They used a commercial\nprogram (FLOW3D) to estimate the gate velocity as a function of runner\ndepth and tail slope and the SQP method for optimization. Patel et al. [14]\nmodeled and optimized the wear behavior of squeeze cast products. Three\ndi\u000berent optimization methods (genetic algorithm, particle swarm optimiza-\ntion and desirability function approach) are combined with neural network\nas a surrogate model. The training data for neural network is obtained from\nexperiments and nonlinear regression models.\nIn this paper, we consider the heat transfer and solidi\fcation processes\nin die casting of a complex model geometry. The computer program [15]\nsolves the \ruid \row and energy equations, coupled with the solid fraction{\n5temperature relation, using a \fnite volume numerical method. When not\nsigni\fcant, natural convection \row is neglected and only the energy equa-\ntion is solved. The product quality is assessed using grain size and yield\nstrength which are estimated using empirical relations. The solidi\fcation\ntime is used to quantify the process e\u000eciency. The molten metal and mold\nwall temperatures are crucial in determining the quality of die casting. The\nwall temperature is typically nonuniform due to the complex mold geometries\nand asymmetric placement of cooling lines. This nonuniformity can be mod-\neled by domain decomposition of the wall and assigning single temperature\nvalue to each domain. Neural networks are trained using the data generated\nfrom the simulations to correlate the initial and wall temperatures to the\noutput parameters like solidi\fcation time, grain size and yield strength. The\noptimization problem formulated with these three objectives is then solved\nusing genetic algorithm. The procedure illustrated here can be applied to any\npractical mold geometry with a complex distribution of wall temperatures.\n2. Numerical Model Description\nThe numerical model incorporates the e\u000bects of solidi\fcation and heat\ntransfer in die casting. Since the common die casting geometries have thin\ncross-sections, the solidi\fcation time is of the order of seconds and hence,\nthe e\u000bect of natural convection has been found to be negligible. Thus, the\nmomentum equations of the liquid metal are not solved in this work. The\nenergy equation which can be written in terms of temperature has unsteady,\n6di\u000busion and latent heat terms.\n\u001aCp@T\n@t=r\u000f(krT) +\u001aLf@fs\n@t(1)\nwhere,Tis temperature, \u001ais density,Cpis speci\fc heat, kis thermal con-\nductivity,Lfis latent heat of fusion, fsis solid fraction and tis time. The\nGulliver-Scheil equation (2) [16] relates solid fraction to temperature for a\nbinary alloy.\nfs(T) =8\n>>>>><\n>>>>>:0 if T >Tliq\n1 if T <Tsol\n1\u0000\u0010\nT\u0000Tf\nTliq\u0000Tf\u0011 1\nkp\u00001otherwise(2)\nwhere,kpis partition coe\u000ecient, Tfis freezing temperature, Tsolis solidus\ntemperature and Tliqis liquidus temperature.\nSecondary Dendrite Arm Spacing (SDAS) is a microstructure parameter\nwhich can be used to estimate the 0.2% yield strength. The cooling rate\nat each point in the domain is computed by numerically solving the energy\nequation and solid fraction temperature relation (eqs. (1) and (2)). The\nfollowing empirical relations link the cooling rate to SDAS and yield strength.\nSDAS =\u00152=A\u0015\u0012@T\n@t\u0013B\u0015\n[in\u0016m]: (3)\nwhere,A\u0015= 44:6 andB\u0015=\u00000:359 are based on the model for microstructure\n7in aluminum alloys [17].\n\u001b0:2=A\u001b\u0015\u00001=2\n2+B\u001b (4)\nwhere,\u001b0:2is in MPa,\u00152(SDAS) is in \u0016m,A\u001b= 59:0 andB\u001b= 120:3 [18].\nGrain size estimation is based on the work of Greer et al. [19]. The grain\ngrowth rate is given by:\ndr\ndt=\u00152\nsDs\n2r(5)\nwhere,ris the grain size, Dsis the solute di\u000busion coe\u000ecient in the liq-\nuid andtis the time. The parameter \u0015sis obtained using invariant size\napproximation:\n\u0015s=\u0000S\n2\u00190:5+\u0012S2\n4\u0019\u0000S\u00130:5\n(6)\nSis given by\nS=2(Cs\u0000C0)\nCs\u0000Cl(7)\nwhere,Cl=C0(1\u0000fs)(kp\u00001)is solute content in the liquid, Cs=kpClis solute\ncontent in the solid at the solid-liquid interface and C0is the nominal solute\nconcentration. Hence, from the partition coe\u000ecient ( kp) and estimated solid\nfraction (fs), eqs. (5) to (7) are solved to get the \fnal grain size.\n8(a) Geometry\n (b) Mesh: 334000 Elements\nFigure 1: Clamp: 16.5 cm x 9 cm x 3.7 cm [15]\nEquations (1) to (7) are solved numerically using the software OpenCast\n[15] with a \fnite volume method on a collocated grid. The variations of\nthermal conductivity, density and speci\fc heat due to temperature are taken\ninto account. Most practical die casting geometries are complex and require\nunstructured grids. In our work, we have \frst generated a tetrahedral mesh\nusing GMSH [20] and then divided into a hexahedral mesh using TETHEX\n[21]. The details of the numerical algorithm and veri\fcation and validation of\nOpenCast are discussed in previous publications [15, 22]. A model geometry\nrepresenting a clamp [15] has been considered to illustrate the methodology.\nFigure 1 shows the clamp geometry with a mesh having 334000 hexahedral\nelements. It is important to assess the e\u000bects of natural convection. Hence,\nthe clamp geometry is simulated for two cases viz. with and without natural\nconvection. Figures 2 and 3 plot grain size and yield strength contours with\nidentical process conditions for both the cases. Since the solidi\fcation time\nis around 2.5 seconds, the velocities due to natural convection in the liquid\nmetal are observed to be negligible. It is evident from the contours that there\nis no signi\fcant e\u000bect of natural convection and hence, it is neglected in all\n9our further simulations.\n(a) Without Natural Convection\n (b) With Natural Convection\nFigure 2: Clamp: Grain Size ( \u0016m)\n(a) Without Natural Convection\n (b) With Natural Convection\nFigure 3: Clamp: Yield Strength (MPa)\n3. Optimization\nIn die casting the mold cavity is \flled with molten metal and solidi\fed.\nThe heat is extracted from the cavity walls by \rowing coolants (typically wa-\nter) through the cooling lines made inside the die. The quality of the \fnished\n10product depends on the rate of heat extraction which in turn depends on the\ntemperature at the cavity walls. Due to complexity in the die geometry, the\nwall temperature varies locally. An optimal product quality can be achieved\nif the temperature distribution on the cavity walls and initial \fll temperature\nare set properly. Thus, in this work, the following optimization problem with\nthree objectives is proposed:\nMinimizeff1(Tinit;Twall);f2(Tinit;Twall);f3(Tinit;Twall)g\nsubject to 900\u0014Tinit\u00141100 K and 500\u0014Twall\u0014700 K(8)\nwhere,f1= solidi\fcation time, f2= max (grain size) and f3=\u0000min(yield\nstrength). Minimizing the solidi\fcation time increases productivity. Re-\nduction in grain size reduces susceptibility to cracking [23] and improves\nmechanical properties of the product [24]. Thus, minimization of the max-\nimum value of grain size over the entire geometry is set as an optimization\nobjective. Higher yield strength is desirable as it increases the elastic limit of\nthe material. Hence, the minimum yield strength over the entire geometry is\nto be maximized. For convenience, this maximization problem is converted\nto minimization by multiplying by minus one. This explains the third ob-\njective function f3. All the objectives are functions of the initial molten\nmetal temperature ( Tinit) and mold wall temperature ( Twall). The initial\ntemperature is a single value in the interval [900 ;1100] K which is higher\nthan the liquidus temperature of the alloy. As discussed before, the mold\nwall temperature need not be uniform in die casting due to locally varying\nheat transfer to the cooling lines. Thus, in this work, the wall surface is\ndecomposed into multiple domains with each domain having a uniform tem-\n11perature boundary condition which is held constant with time during the\nentire solidi\fcation process. If the die design with cooling line placement\nand coolant \row conditions are included, the thermal analysis of the die can\nalso be done to identify these domains. Due to the lack of this information,\nthe wall is decomposed into ten domains using the KMeans classi\fcation al-\ngorithm from Scikit Learn [25]. Figure 4a shows the domain decomposition\nwith ten domain tags and \fg. 4b shows a random sample of the boundary\ntemperature with a single temperature value assigned uniformly to each do-\nmain. Thus, the input wall temperature ( Twall) is a ten dimensional vector\nin the interval [500 ;700] K which is lower than the solidus temperature of\nthe alloy. Hence, this is a multi{objective optimization problem with three\nminimization objectives which are a function of eleven input temperatures.\n(a) Domain Numbers\n (b) Randomly Assigned Values\nFigure 4: Domain Decomposition of the Boundary and Random Value Assignment\n12(a) Time: 0.037 s\n (b) Time: 0.294 s\n (c) Time: 0.731 s\n (d) Time: 1.88 s\nFigure 5: Clamp Temperature Contours (K) (Solidi\fcation Time: 3.02 s)\n(a) Time: 0.037 s\n (b) Time: 0.294 s\n (c) Time: 0.731 s\n (d) Time: 1.88 s\nFigure 6: Clamp Solid Fraction Contours (Solidi\fcation Time: 3.02 s)\nFigures 5 and 6 plot temperature and solid fraction for di\u000berent time steps\nduring solidi\fcation for the sample shown in \fg. 4b with Tinit= 986 K. It\ncan be seen that the temperature and solid fraction contours are asymmetric\ndue to non-uniform boundary temperature. For instance, domain number\n10 is held at minimum temperature and thus, region near it solidi\fes \frst.\nFigure 7 plots \fnal yield strength and grain size contours. The cooling rates\nand temperature gradients decrease with time as solidi\fcation progresses.\nHence, the core regions which are thick take longer time to solidify. As the\ngrains have more time to grow, the grain size is higher in the core region\nand correspondingly, the yield strength is lower. These trends along with the\nasymmetry are visible in \fg. 7.\n13(a) Yield Strength (MPa)\n (b) Grain Size ( \u0016m)\nFigure 7: Clamp Microstructure Parameters\n3.1. Genetic Algorithm\n3.1.1. Single Objective Optimization\nGenetic algorithms (GAs) are global search algorithms based on the me-\nchanics of natural selection and genetics. They apply the `survival of the\n\fttest' concept on a set of arti\fcial creatures characterized by strings. In\nthe context of a GA, an encoded form of each input parameter is known as\na gene. A complete set of genes which uniquely describe an individual (i.e.,\na feasible design) is known as a chromosome. The value of the objective\nfunction which is to be optimized is known as the \ftness. The population of\nall the individuals at a given iteration is known as a generation. The overall\nsteps in the algorithm are as follows:\n1. Initialize \frst generation with a random population\n2. Evaluate the \ftness of the population\n3. Select parent pairs based on their \ftness (better \ftness implies higher\nprobability of selection)\n144. Perform crossover to generate an o\u000bspring from each parent pair\n5. Mutate some genes randomly in the population\n6. Replace the current generation with the next generation\n7. If termination condition is satis\fed, return the best individual of the\ncurrent generation; else go back to step 2\nThere are multiple strategies discussed in the literature for each of the\nabove steps [26, 27]. A brief overview of the methods used in this work is\ngiven here. The population is initialized using the Latin Hypercube Sam-\npling strategy from the python package pyDOE [28]. Fitness evaluation is\nthe estimation of the objective function which can be done by full scale com-\nputational model or by surrogate models. The number of objective function\nevaluations are typically of the order of millions and thus, step 2 becomes\ncomputationally most expensive step if a full scale model is used. Instead, it\nis common to use a surrogate model which is much cheaper to evaluate. In\nthis work, a neural network based surrogate model is used, details of which\nare provided in section 3.2. Tournament selection is used to choose the parent\npairs to perform crossover. The idea is to choose four individuals at random\nand select two out of them which have better \ftness. Note that since the\noptimization is cast as a minimization problem, lower \ftness value is desired.\nUniform crossover is used to recombine the genes of the parents to generate\nthe o\u000bspring with a crossover probability of 0.9. Random mutation of the\ngenes of the entire generation is performed with a mutation probability of\n0.1. Thereafter, the old generation is replaced by this new generation. Note\nthat the elitist version of GA is used which passes down the \fttest individual\nof the previous generation to this generation as it is. Elitism was found help-\n15ful as it ensures that the next generation is at least as good as the previous\ngeneration.\n3.1.2. Multi{Objective Optimization\nThe simultaneous optimization of multiple objectives is di\u000berent than\nthe single objective optimization problem. In a single objective problem, the\nbest design which is usually the global optimum (minimum or maximum)\nis searched for. On the other hand, for multi{objective problem, there may\nnot exist a single optimum which is the best design or global optimum with\nrespect to all the objectives simultaneously. This happens due to the con-\n\ricting nature of objectives i.e., improvement in one can cause deterioration\nof the other objectives. Thus, typically there is a set of Pareto optimal solu-\ntions which are superior to rest of the solutions in the design space which are\nknown as dominated solutions. All the Pareto optimal solutions are equally\ngood and none of them can be prioritized in the absence of further informa-\ntion. Thus, it is useful to have a knowledge of multiple non-dominated or\nPareto optimal solutions so that a single solution can be chosen out of them\nconsidering other problem parameters.\nOne possible way of dealing with multiple objectives is to de\fne a single\nobjective as a weighted sum of all the objectives. Any single objective op-\ntimization algorithm can be used to obtain an optimal solution. Then the\nweight vector is varied to get a di\u000berent optimal solution. The problem with\nthis method is that the solution is sensitive to the weight vector and choosing\nthe weights to get multiple Pareto optimal solutions is di\u000ecult for a practical\nengineering problem. Multi{objective GAs attempt to handle all the objec-\ntives simultaneously and thus, annihilating the need of choosing the weight\n16vector. Konak et al. [29] have discussed various popular multi{objective GAs\nwith their bene\fts and drawbacks. In this work, the Non-dominated Sorting\nGenetic Algorithm II (NSGA{II) [30] which is a fast and elitist version of\nthe NSGA algorithm [31] is used. The NSGA{II algorithm to march from a\ngiven generation of population size Nto a next generation of same size is as\nfollows:\n1. Select parent pairs based on their rank computed before (lower rank\nimplies higher probability of selection)\n2. Perform crossover to generate an o\u000bspring from each parent pair\n3. Mutate some genes randomly in the population thus forming the o\u000b-\nspring population\n4. Merge the parent and o\u000bspring population thus giving a set of size\n2\u0002N\n5. Evaluate the \ftness of the population corresponding to each objective\n6. Divide the population into multiple non-dominated levels also known\nas fronts\n7. Compute the crowding distance for each individual along each front\n8. Sort the population based on front number and crowding distances and\nrank them\n9. Choose the best set of Nindividuals as next generation (i.e., Nindi-\nviduals with lowest ranks)\n10. If termination condition is satis\fed, return the best front of the current\ngeneration as an approximation of the Pareto optimal solutions; else\ngo back to step 1\n17Before iterating over the above steps, some pre-processing is required. A\nrandom population of size Nis initiated and steps 5{8 are implemented to\nrank the initial generation. The parent selection, crossover and mutation\nsteps are identical to the single objective GA described in section 3.1.1. The\nalgorithms for remainder of the steps can be found in the paper by Deb et al.\n[30]. Ranking the population by front levels and crowding distance enforces\nboth elitism and diversity in the next generation.\n3.2. Neural Network\nThe \ftness evaluation step of the GA requires a way to estimate the\noutputs corresponding to the set of given inputs. Typically, the number of\ngenerations can be of the order of thousands with several hundreds of popu-\nlation size per generation and thus, the total number of function evaluations\ncan be around hundred thousands or more. It is computationally di\u000ecult\nto run the full scale numerical estimation software. Thus, a surrogate model\nis trained which is cheap to evaluate. A separate neural network is trained\nfor each of the three optimization objectives (eq. (8)). Hornik et al. [32]\nshowed that under mild assumptions on the function to be approximated,\na neural network can achieve any desired level of accuracy by tuning the\nhyper-parameters. The building block of a neural network is known as a\nneuron which has multiple inputs and gives out single output by performing\nthe following operations:\n1. Linear transformation: a=Pn\ni=1wixi+b; where,fx1;x2;:::;xngaren\nscalar inputs, wiare the weights and bis a bias term\n2. Nonlinear transformation applied element-wise: y=\u001b(a); where,yis\na scalar output and \u001bis the activation function\n18Multiple neurons are stacked in a layer and multiple layers are connected\ntogether to form a neural network. The \frst and last layers are known as\ninput and output layers respectively. Information \rows from the input to\noutput layer through the intermediate layers known as hidden layers. Each\nhidden layer adds nonlinearity to the network and thus, a more complex\nfunction can be approximated successfully. At the same time, having large\nnumber of neurons can cause high variance and thus, blindly increasing the\ndepth of the network may not always help. The number of neurons in the\ninput and output layers is de\fned by the problem speci\fcation. On the other\nhand, number of hidden layers and neurons has to be \fne tuned to have low\nbias and low variance. The interpolation error of the network is quanti\fed\nas a loss function which is a function of the weights and bias.\nName of\nNetworkNo. of\nHidden LayersNo. of Neurons\nper Hidden LayerLearning RateNo. of\nEpochsL2\u0015Dropout\nFactor\nSol. Time 4 50 0.001 300 0.004 0\nMax. Grain 6 75 0.001 300 0.005 0.2\nMin. Yield 4 25 0.003 400 0.01 0\nTable 1: Neural Network Hyper{Parameters\nName of Network Training Error Tesing Error Validation Error\nSol. Time 0.90% 1.01% 1.08%\nMax. Grain 1.29% 2.04% 1.92%\nMin. Yield 0.25% 0.38% 0.43%\nTable 2: Average Percent Training, Testing and Validation Errors\n19(a) Solidi\fcation Time\n (b) Max. Grain Size\n (c) Min. Yield Str.\nFigure 8: Neural Networks: Error Estimates for 200 Testing Samples\nA numerical optimization algorithm coupled with gradient estimation by\nthe backpropagation algorithm [33] is used to estimate the optimal weights\nand bias which minimize the loss function for a given training set. This is\nknown as training process. An approach described by Goodfellow et al. [34]\nis used to select the hyper{parameters and train the neural network. Mean\nsquare error is set as the loss function. A set of 500 random samples is used\nfor training and two di\u000berent sets of 200 each are used for validation and test-\ning. The number of inputs and outputs of the problem specify the number\nof neurons in the input and output layers respectively. Here, there are three\nneural networks with 11 input neurons and one output neuron each (11 tem-\nperatures and three objectives mentioned in eq. (8)). The number of hidden\nlayers and hidden neurons, learning rate, optimizer, number of epochs and\nregularization constant are the hyper{parameters which are problem speci\fc.\nThey are varied in a range and then chosen so that both the training and\nvalidation errors are minimized simultaneously. The accuracy of prediction\nis further checked on an unseen test data. This overall approach is used for\ntraining the neural network with low bias and low variance. In this work,\n20the neural network is implemented using the Python library Tensor\row [35]\nwith a high level API Keras [36]. After testing various optimizers available\nin Keras, it is found that the Adam optimizer is the most suitable here. The\nparameters \f1= 0:9 and\f2= 0:999 are set as suggested in the original\npaper by Kingma and Ba [37] and the `amsgrad' option is switched on. The\nactivation functions used for all the hidden and output layers are ReLU and\nidentity respectively. This choice of activation functions is generally recom-\nmended for regression problems [34]. A combination of L2 regularization\nwith dropout is used to control the variance. The training is stopped when\nthe loss function changes minimally after a particular epoch. This is known\nas the stopping criteria. After varying the hyper-parameters in a range, it is\nfound that for the values listed in table 1, the training, testing and validation\nerrors (table 2) are simultaneously minimized. Moreover, since all the three\nerrors are low and close to each other, it shows that the bias and variance\nare low. Figure 8 plots percent relative error for 200 testing samples. It can\nbe seen that all the three neural networks are able to predict with acceptable\naccuracy.\n4. Assessment of Genetic Algorithm on Simpler Problems\nIt is di\u000ecult to visualize the variation of an output with respect to each\nof the eleven inputs. Hence in this section, two problems are considered\nwhich are simpli\fed versions of the actual problem. In the \frst case, the\nboundary temperature is set to a uniform value and hence, there are only\ntwo scalar inputs: ( Tinit;Twall). For the second case, the initial temperature\nis held constant ( Tinit= 1000 K) and the boundary is split into two domains\n21instead of ten. Thus, again there are two scalar inputs: ( T(1)\nwall;T(2)\nwall).T(1)\nwallis\nassigned to domain numbers 1{5 and T(2)\nwallto domain numbers 6{10 (\fg. 4a).\nThe ranges of wall and initial temperatures are the same as before (section 3).\nSuch a simpli\fed analysis gives an insight into the actual problem. Moreover,\nsince these are problems with two inputs, the optimization can be performed\nby brute force parameter sweep and compared to the genetic algorithm. This\nhelps to \fne tune the parameters and assess the accuracy of the GA.\n4.1. Single Objective Optimization\nIn this section, all the objectives are analyzed individually. A two dimen-\nsional mesh of size 40,000 with 200 points in each input dimension is used for\nthis analysis. The outputs are estimated from the neural networks for each\nof these points. Figure 9 plots the response surface contours for each of the\nthree objectives with their corresponding minima. The minima are estimated\nfrom the 40,000 points. The XandYaxes are initial and wall temperatures,\nrespectively. When the initial temperature is reduced, the total amount of\ninternal energy in the molten metal is reduced and thus, the solidi\fcation\ntime decreases. The amount of heat extracted is proportional to the tem-\nperature gradient at the mold wall which increases with a drop in the wall\ntemperature. Thus, the drop in wall temperature reduces the solidi\fcation\ntime. Hence, the minimum of solidi\fcation time is attained at the bottom\nleft corner (\fg. 9a). The grain size is governed by the local temperature\ngradients and cooling rates which are weakly dependent on the initial tem-\nperature. Thus, it can be seen that the contour lines are nearly horizontal\nin \fg. 9b. On the other hand, as wall temperature reduces, the rate of heat\nextraction rises and hence, the grains get less time to grow. This causes a\n22drop in the grain size. Thus, the minimum of the maximum grain size is on\nthe bottom right corner (\fg. 9b). The contour values in \fg. 9c are negative\nas maximization objective of the minimum yield strength is converted into\nminimization by inverting the sign. The minimum is at the top right corner\nof \fg. 9c. Figure 10 has similar plots for the second case of constant initial\ntemperature and split boundary temperature. Figure 10c shows the e\u000bect of\nnonuniform boundary temperature. The minimum is attained at wall tem-\nperatures of 500 K and 700 K since the local gradients and cooling rates vary\ndue to the asymmetry in the geometry. This analysis shows the utility of the\noptimization with respect to nonuniform mold wall temperatures.\n23(a) Solidi\fcation Time (s)\n (b) Max. Grain Size ( \u0016m)\n(c) Min. Yield Str. (MPa)\nFigure 9: Parameter Sweep: Uniform Boundary Temperature\n24(a) Solidi\fcation Time (s)\n (b) Max. Grain Size ( \u0016m)\n(c) Min. Yield Str. (MPa)\nFigure 10: Parameter Sweep: Split Boundary Temperature with Tinit= 1000 K\nTable 3 lists the optima for the single objective problems estimated from\nparameter sweep and GA. Note that since the 200 K range is divided into\n200 divisions, the resolution of the parameter sweep estimation is 1 K. For\nall the six cases, the outputs and corresponding inputs show that the GA es-\ntimates are accurate. The GA parameters are varied in the following ranges:\n[25{100] generations, [10{50] population size, [0.75{0.9] crossover probability\nand [0.05{0.2] mutation probability. Similar ranges have been used in the\n25literature [38{40]. Parameter values of 50 generations with population size\nof 25 and crossover and mutation probability of 0.8 and 0.1 respectively are\nfound to give accurate estimates as shown in table 3. The elitist version of\nGA is used which passes on the best individual from previous generation to\nthe next generation.\nProblem\nTypeOptim.\nObjectiveOptim.\nTypeParam. Sweep GA Estimates\nInputs (K) Output Inputs (K) Output\nUniform\nB.C.Solid. Time (s) Min. 900, 500 1.962 900.2, 500.2 1.962\nMax. Grain ( \u0016m) Min. 1100, 500 22.41 1099.1, 500.3 22.42\nMin. Yield (MPa) Max. 1100, 700 145.4 1099.9, 699.9 145.4\nTinit=\n1000 KSolid. Time (s) Min. 500, 500 1.982 500.6, 500.3 1.982\nMax. Grain ( \u0016m) Min. 500, 500 22.49 500.1, 500.1 22.50\nMin. Yield (MPa) Max. 500, 700 140.9 500.1, 699.9 140.8\nTable 3: Single Objective Optimization: Genetic Algorithm Estimates compared with\nParameter Sweep Values for Two Input Problems\n4.2. Bi-Objective Optimization\nIn this section, two objectives are taken at a time for each of the two\nsimpli\fed problems de\fned in section 4. As before, a two dimensional mesh\nof size 40,000 with 200 points in each input dimension is used for this analysis.\nThe outputs are estimated from the neural networks for each of these points.\nThe feasible region is the set of all the attainable designs in the output space\nwhich can be estimated by the parameter sweep. For a minimization problem,\na designd1is said to dominate another design d2if all the objective function\nvalues ofd1are less than or equal to d2. The design space can be divided\nin two disjoint sets SpandSdsuch thatSpcontains all the designs which do\nnot dominate each other and at least one design in Spdominates any design\n26inSd[41].Spis called as the Pareto optimal or non-dominated set whereas,\nSdis called as the non-Pareto optimal or dominated set. Since the designs\nin Pareto optimal set are non-dominated with respect to each other, they all\nare equally good and some additional information regarding the problem is\nrequired to make a unique choice out of them. Thus, it is useful to have a list\nof multiple Pareto optimal solutions. Another way to interpret the Pareto\noptimal solutions is that any improvement in one objective will worsen at\nleast one other objective thus, resulting in a trade-o\u000b [42].\n(a) Parameter Sweep\n (b) NSGA{II\nFigure 11: Uniform Boundary Temperature: Solidi\fcation Time vs Min. Yield Strength\n27(a) Parameter Sweep\n (b) NSGA{II\nFigure 12: Uniform Boundary Temperature: Max. Grain Size vs Min. Yield Strength\nThe blue region in the left parts of \fgs. 11 to 14 indicates the feasible\nregion. Using a pairwise comparison of the designs in the feasible region\nobtained by parameter sweep, the Pareto front is estimated which is plot-\nted in red. The right side plots of \fgs. 11 to 14 show the Pareto fronts\nobtained using NSGA{II. The NSGA parameters are varied in the following\nranges: [25{100] generations, [500{1500] population size, [0.75{0.9] crossover\nprobability and [0.05{0.2] mutation probability. The population size used\nin this work is kept higher than the literature [43] to get a good resolution\nof the Pareto front. It can be seen that both the estimates match which\nimplies that the NSGA{II implementation is accurate. A population size of\n1000 is evolved over 50 generations with crossover and mutation probability\nof 0.8 and 0.1 respectively. Existence of multiple designs in the Pareto set\nimplies that the objectives are con\ricting. This can be con\frmed from the\nsingle objective analysis. For instance, consider the two objectives solidi\fca-\n28tion time and minimum yield strength in the uniform boundary temperature\ncase. From \fgs. 9a and 9c it can be seen that individual minima are attained\nat di\u000berent corners. Moreover, the directions of descent are di\u000berent for each\nobjective and thus, at some points, improvement in one objective can worsen\nother. This e\u000bect is visible on the corresponding Pareto front plot in \fg. 11.\n(a) Parameter Sweep\n (b) NSGA{II\nFigure 13: Split Boundary Temperature with Tinit= 1000 K: Solidi\fcation Time v/s Min.\nYield Str.\n29(a) Parameter Sweep\n (b) NSGA{II\nFigure 14: Split Boundary Temperature with Tinit= 1000 K: Max. Grain Size v/s Min.\nYield Str.\n5. Results of Multi{Objective Optimization Problem with Eleven\nInputs\nAfter veri\fcation of the NSGA{II implementation on simpli\fed prob-\nlems, multi{objective design optimization with eleven inputs is solved. As\ndiscussed before, some additional problem information is required to choose\na single design from all the Pareto optimal designs. In die casting, there is a\nlot of stochastic variation in the wall and initial temperatures. Shahane et al.\n[15] have performed parameter uncertainty propagation and global sensitiv-\nity analysis and found that the die casting outputs are sensitive to the input\nuncertainty. Thus, from a practical point of view, it is sensible to choose a\nPareto optimal design which is least sensitive to the inputs. In this work,\nsuch an optimal point is known as a `stable' optimum since any stochastic\nvariation in the inputs has minimal e\u000bect on the outputs. A local sensitiv-\n30ity analysis is performed to quantify the sensitivity of outputs towards each\ninput for all the Pareto optimal designs. For a function f:Rn!Rmwhich\ntakes input x2Rnand produces output f(x)2Rm, them\u0002nJacobian\nmatrix is de\fned as:\nJf[i;j] =@fi\n@xj81\u0014i\u0014m;1\u0014j\u0014n (9)\nAt a given point x0, the local sensitivity of fwith respect to each input can\nbe de\fned as the Jacobian evaluated at that point: Jf(x0) [44]. Here, there\nare eleven inputs and two outputs. Thus, the 2 \u000211 Jacobian is estimated\nat all the Pareto optimal solutions evaluated using the neural networks with\na central di\u000berence method. Then, the L1norm of the Jacobian given by\nthe sum of absolute values of all its components is de\fned as a single scalar\nmetric to quantify the local sensitivity.\nTo begin with, two pairs of objectives are chosen: fsolidi\fcation time,\nminimum yield strength gandfmaximum grain size, minimum yield strength g.\nFor both of these cases, a population size of 500 with 5000 generations is set.\nFigure 15 plots the Pareto fronts colored by the value of Jacobian norm at\neach design. It can be seen that the norm varies signi\fcantly and thus, rank-\ning the designs based on the sensitivity is useful. The design with minimum\nnorm is chosen and marked on the Pareto fronts as a stable optimum. Note\nthat minimum norm is observed at the end of the Pareto front. However,\nthe norm is low on the entire left vertical side of the Pareto front. Hence,\nit may be a good idea to choose the design near shown `knee' region which\nhas similar value of the objective on the X{axis but much lower value of the\nobjective on the Y{axis.\n31(a) Solidi\fcation Time v/s Min. Yield\nStr.\n(b) Max. Grain Size v/s Min. Yield Str.\nFigure 15: Pareto Front for Two Objectives (Colored by Jacobian Norn)\nThe next step is to perform the complete multi{objective optimization\nanalysis as mentioned in eq. (8). NSGA{II is used with a population size of\n2000 evolved over 250 generations. Figure 16 plots the Pareto optimal de-\nsigns in a three dimensional objective space colored by the value of Jacobian\nnorm at each design. Since it is di\u000ecult to visualize the colors on the three\ndimensional Pareto front, a histogram of norm of the Jacobian at each of\nthese designs is also plotted in \fg. 17. It can be seen that the norm varies\nfrom 0.95 to 11.1. The histogram is skewed towards the left which implies\n32that multiple designs are stable. The stable optimum is:\nInputs:Tinit= 1015:8 K\nTwall=f500:7;502:8;500:0;501:5;500:5;\n503:6;643:6;508:8;502:3;500:7gK\nOutputs: Solidi\fcation Time = 1 :99 s\nMax Grain Size = 22 :39\u0016m\nMin Yield Strength = 137 :95 MPa(10)\nFigure 16: Pareto Front for Three Objectives (Colored by Jacobian Norn)\n33Figure 17: Histogram of Local Sensitivity of Designs on the Pareto Front for 3 Objectives\n6. Conclusions\nThis paper presents an application of multi{objective optimization of the\nsolidi\fcation process during die casting. Although the procedure is illus-\ntrated for a model clamp geometry, the process is general and can be applied\nto any practical geometry. Practically, it is not possible to hold the entire\nmold wall at a uniform temperature. The \fnal product quality in terms of\nstrength and micro-structure and process productivity in terms of solidi\fca-\ntion time depends directly on the rate and direction of heat extraction during\nsolidi\fcation. Heat extraction in turn depends on the placement of coolant\nlines and coolant \row rates thus, being a crucial part of die design. In this\n34work, the product quality is assessed as a function of initial molten metal and\nboundary temperatures. Knowledge of boundary temperature distribution in\norder to optimize the product quality can be useful in die design and process\nplanning. NSGA{II, which is a popular multi{objective genetic algorithm,\nwas used for the optimization process. Since the number of function evalua-\ntions required for a GA is extremely high, a deep neural network was used as\na surrogate to the full computational \ruid dynamics simulation. The training\nand testing of the neural network was completed with less than thousand full\nscale \fnite volume simulations. The run time per simulation using OpenCast\nwas about 20 minutes on a single processor i.e., around 333 compute hours for\n1000 simulations. All the simulations were independent and embarrassingly\nparallel. Thus, a multi{core CPU was used to speed up the process with-\nout any additional programming e\u000bort for parallelization. Computationally,\nthis was the most expensive part of the process. Subsequent training and\ntesting of the neural network took a few minutes. Implementation of GA is\ncomputationally cheap since the evaluation of a neural network is a sequence\nof matrix products and thus, was completed in few minutes. Hence, it can\nbe seen that the strategy of coupling the GA and neural network with \fnite\nvolume simulations is computationally bene\fcial.\nIn this work, the wall is divided into ten domains. Together with the\ninitial temperature, this is an optimization problem with eleven inputs. Both\nsingle and multi{objective genetic algorithms were programmed and veri\fed\nwith parameter sweep estimation for simpli\fed versions of the problem. The\nsingle objective response surfaces were used to get an insight regarding the\ncon\ricting nature of the objectives since the individual optimal solutions\n35were completely di\u000berent from each other. Moreover, the solidi\fcation time,\nmaximum grain size and minimum yield strength varied in the ranges [2, 3.5]\nseconds, [22, 34] microns and [134, 145] MPa respectively for the given inputs.\nThis showed the utility of the simultaneous optimization of all the objectives\nsince there was a signi\fcant scope for improvement. After estimating multiple\nPareto optimal solutions, a common question is to choose a single design. The\nstrategy of choosing the design with minimum local sensitivity towards the\ninputs was found to be practically useful due to the stochastic variations\nin the input process parameters. Overall, although die casting was used\nas an example for demonstration, this approach can be used for process\noptimization of other manufacturing processes like sand casting, additive\nmanufacturing, welding etc.\nAcknowledgments\nThis work was funded by the Digital Manufacturing and Design Inno-\nvation Institute with support in part by the U.S. Department of the Army.\nAny opinions, \fndings, and conclusions or recommendations expressed in this\nmaterial are those of the author(s) and do not necessarily re\rect the views\nof the Department of the Army. The authors would like to thank Beau Glim\nof North American Die Casting Association (NADCA) and Alex Monroe of\nMercury Castings for their insightful suggestions.\n36References",
      "metadata": {
        "filename": "Optimization of Solidification in Die Casting using Numerical Simulations and Ma.pdf",
        "hotspot_name": "Aluminium_Die_Casting",
        "title": "Optimization of Solidification in Die Casting using Numerical\n  Simulations and Machine Learning",
        "published_date": "2019-01-08T15:28:33Z",
        "pdf_link": "http://arxiv.org/pdf/1901.02364v2",
        "query": "aluminium die casting energy efficiency optimization"
      }
    },
    "Recy-ctronics_ Designing Fully Recyclable Electronics With Varied Form Factors": {
      "full_text": "Recy-ctronics: Designing Fully Recyclable Electronics With\nVaried Form Factors\nTingyu Cheng\nGeorgia Institute of Technology\nAtlanta, USA\ntcheng32@gatech.eduZhihan Zhang\nUniversity of Washington\nSeattle, USA\nzzhihan@cs.washington.eduHan Huang\nGeorgia Institute of Technology\nAtlanta, USA\nhhuang449@gatech.edu\nYingting Gao\nGeorgia Institute of Technology\nAtlanta, USA\nygao617@gatech.eduWei Sun\nChinese Academy of Sciences\nBeijing, China\nsunwei2017@iscas.ac.cnGregory D. Abowd\nNortheastern University\nBoston, USA\ng.abowd@northeastern.edu\nHyunJoo Oh\nGeorgia Institute of Technology\nAtlanta, USA\nhyunjoo.oh@gatech.eduJosiah Hester\nGeorgia Institute of Technology\nAtlanta, USA\njosiah@gatech.edu\nFigure 1: General recycling process for Recy-ctronics. 1: Materials preparation of PVA (substrate material) and liquid metal\n(electrode material). 2: Different recyclable interactive devices with various form factors (sheet, foam and tube). 3: The dissolving\nand separation process of the devices. 4: Material extraction for remaking new devices.\nABSTRACT\nFor today’s electronics manufacturing process, the emphasis on\nstable functionality, durability, and fixed physical forms is designed\nto ensure long-term usability. However, this focus on robustness\nand permanence complicates the disassembly and recycling pro-\ncesses, leading to significant environmental repercussions. In thispaper, we present three approaches that leverage easily recyclable\nmaterials—specifically, polyvinyl alcohol (PVA) and liquid metal\n(LM)—alongside accessible manufacturing techniques to produce\nelectronic components and systems with versatile form factors. Our\nwork centers on the development of recyclable electronics through\nthree methods: 1) creating sheet electronics by screen printing LMarXiv:2406.09611v1  [cs.HC]  13 Jun 2024Cheng, et al.\ntraces on PVA substrates; 2) developing foam-based electronics by\nimmersing mechanically stirred PVA foam into an LM solution; and\n3) fabricating recyclable electronic tubes by injecting LM into mold\ncasted PVA tubes, which can then be woven into various structures.\nTo further assess the sustainability of our proposed methods, we\nconducted a life cycle assessment (LCA) to evaluate the environ-\nmental impact of our recyclable electronics in comparison to their\nconventional counterparts.\nCCS CONCEPTS\n•Human-centered computing →Ubiquitous and mobile com-\nputing systems and tools .\nKEYWORDS\nRecyclable electronics, transient electronics, sustainable computing,\nubiquitous computing\n1 INTRODUCTION\nThe aspiration for 21st-century computing is its seamless integra-\ntion into our everyday life ( e.g.,smart lock, smart home hub), a\nvision steadily coming to fruition with the proliferation of today’s\ncomputational devices, serving users both locally and through the\ninternet. However, the current landscape of connected devices (an\naverage of 22 per U.S household in 2022) and global e-waste (53.6\nmillion metric tons (Mt) by 2020) paints a bleak picture. The rapid\nexpansion of IoT (Internet of Things) devices has resulted in environ-\nmental hazards surpassing our capacity to manage them sustainably.\nAt the same time, as we venture into emerging fields like AI (Artifi-\ncial Intelligence) and strive for fully immersive user experiences,\nthe number of devices and subsequent waste will only escalate.\nEstimates indicate IoT devices could reach a trillion by 2035. This\nraises concerns about how to handle the disposal of these devices\nwhen they eventually become obsolete or malfunction. Addressing\nthese questions is essential to building a sustainable future where\ntechnological advancement aligns with responsible environmental\npractices.\nHuman-Computer Interaction (HCI) has a long history of proto-\ntyping various interactive devices, with a recent rise in promoting\nthe development of sustainable ones. Traditionally, a significant\nportion of interactive devices rely on plastic housings ( e.g.,ABS,\nHDPE), conventional PCBs with FR4 and SMD components, and\nbatteries containing harmful elements like lithium and lead. Im-\nproper processing or disposal of these components can pose serious\nenvironmental hazards, contributing to plastic waste, E-waste, and\nlithium battery pollution. Recognizing the environmental impact,\nresearchers in HCI have been actively advocating for sustainable\npractices and reducing reliance on conventional materials in device\nmanufacturing. For example, researchers have proposed transient\nand decomposable devices with more sustainable material options\nsuch as chitosan, beeswax to make wireless charged heaters or\ntransient sensors [ 8,36]. The focus of these works lies in transiency\nand biodegradability, prioritizing environmental sustainability over\nfunctionality or longevity.\nIn this work, we develop and integrate a set of materials, fab-\nrication tools, and manufacturing processes to enable the acces-\nsible fabrication of recyclable electronic interactive devices. Wewould not only view the disintegration of the electronics as the\nend of their lifespan, but rather as marking the beginning of an-\nother circuit’s physical and functional reconfiguration. Our aim\nis that these devices are designed for recycling efficiency— unlike\ntraditional electronics— eliminating the need for sophisticated ma-\nchinery or intricate chemical processes. To this end, we have chosen\ncommercially available room-temperature liquid metals (RTLMs),\nparticularly eutectic gallium-indium (EGaIn), for their conductive\nproperties, and poly(vinyl alcohol) (PVA) as both substrate and\nencapsulation materials in the creation of recyclable electronics.\nOur innovations extend beyond traditional thin-sheet electronics,\nintroducing three distinct form factors: sheets, foam, and tubes. We\nalso introduce design strategies to customize devices’ mechanical\nproperties, ranging from rigid sheet electronics to flexible, com-\npressible interactive foams, and highly stretchable tube sensors and\nactuators. Each form factor is tailored to meet different application\nneeds while ensuring complete recyclability, showcasing our com-\nmitment to reducing electronic waste through innovative design\nand material selection.\nThe main contributions of this paper are:\n•Integration of PVA and LM to enable the creation of fully\nand easily recyclable electronics across three distinct form\nfactors: sheet, foam and tube.\n•Introduction of a formulation facilitating the production of\nthree types of recyclable electronics with various properties:\nrigid, flexible, and stretchable.\n•Development of a process to recycle three different forms\nof electronics with high recycling rates.\n•A comparative life cycle assessment (LCA) case study for\nthe recyclable sheet-based proximity sensor, validating the\nenvironmental benefits of the proposed approach and demon-\nstrating ways to guide design decisions about sustainability\nwithout a full LCA.\n2 RELATED WORK\n2.1 Printed Electronics\nPrinted electronics represents a cutting-edge technology that in-\nvolves fabricating electronic devices and circuits through various\nprinting methods, such as inkjet printing [ 31], screen printing [ 9]\nand 3D printing [ 41,42]. These printing techniques can also be com-\nbined with different pre/post-treatments for versatile use cases. For\ninstance, conformally transfer printed heat-shrinkage film-based\nelectronics can be applied to complex 3D surfaces [ 35]. Creating pre-\nstrain or cutting patterns can enable stretchable electronics [ 19,37].\nPrinted electronics offer several advantages, including the creation\nof lightweight, mostly flexible or stretchable electronic components,\ncircuits, or even systems. The manufacturing approach can also\nbe more sustainable by favoring more on additive over subtractive\nmethods.\nWithin the Human-Computer Interaction (HCI) community,\nrapid prototyping is a key theme for printed electronics research.\nOne pioneering example is Instant Inkjet Circuits , which introduced\nthe use of commodity inkjet printers for quickly prototyping func-\ntional devices [ 17]. Building on this accessible and diverse design\nconcept, researchers have utilized this technique to fabricate variousRecy-ctronics: Designing Fully Recyclable Electronics With Varied Form Factors\nsensors [ 7,34,46], antennas [ 22], displays [ 33] and even heating-\nbased actuators [ 32]. Moreover, researchers have demonstrated the\nscalability of printed electronics beyond the normal machine sizes.\nFor example, Sprayable introduced a spraying technique for proto-\ntyping large-scale sensors and displays [ 43], while Duco introduced\na system that leverages a hanging wall robot to draw large-scale\ncircuits directly on everyday surfaces, using multiple tools and\nsubstrates.\n2.2 Recyclable Electronics\nWith the remarkable advancements in fabricating diverse electron-\nics, spanning various form factors, scales, and functionalities, the\nissue of disposing of these devices has become a pressing concern,\nyet not much effort has been devoted to this problem within the\nHCI community. Song et al. proposed a method to manufacture\nwireless heating interfaces that can decompose in backyard condi-\ntions within 60 days [ 36].Functional Destruction is another recent\nwork demonstrated three different approaches (PVA, beeswax and\nedible chocolate) to make electronics that are physically and func-\ntionally transient [ 8]. Lazaro Vasquez et al. focused on mycelium,\nwhich is a fast-growing vegetative part of a fungus, to replace the\nplastic housing for traditional PCBs, but most of these work aims\nto make electronics with disposable or degradable manners instead\nof recyclable [ 20]. The only two works, which are both from the\nsame research group, try to emphasize recyclability for electronics.\nArroyos et al. introduced an end-to-end digital fabrication process\nfor creating a computer mouse with a biodegradable printed circuit\nboard and case, utilizing materials like Jiva Soluboard and PVA\n3D printing filament [ 2], where the electronic components can\nbe reused in this case. The other work expands the recyclability\nfrom the components to the substrate and the electrode material.\nZhang et al. present vPCB, using vitrimer as the replacement for\nconventional circuit boards with a high recycling rate [47].\nIn contrast, other research fields such as materials science and\nelectrical engineering have delved extensively into recycling elec-\ntronics. Khrustalev et al., for instance, deviated from conventional\nthermoset materials like fiberglass and epoxy and instead used\nthermoplastic materials (3D printing PLA) as the substrate, show-\ncasing how electronics can be fabricated and recycled efficiently\n[18].3Rproject also aims to design resilient, repairable, and recy-\nclable electronics through a combination of novel biphasic printable\nLM composites and reversible tough block co-polymers [ 38]. LM,\nknown for its excellent conductivity and intrinsic fluid nature, has\nbeen widely used as an electrode option for recyclable electronics.\nTeng et al. demonstrated two papers utilizing LM as the conductive\nmaterial and PVA and beeswax as the substrate materials, respec-\ntively [ 39,40]. The beeswax approach stands out as it features a\nself-destructing-recycling process where the LM traces react with\nthe sodium hydroxide within the device. These projects share a\ncommon goal of adapting straightforward recycling approaches\nthat avoid complicated procedures or tooling while achieving high\nrecyclability rates.\n2.3 Sustainable Computing\nThe proliferation of miniaturized computational devices has brought\nup a range of environmental hazards, from plastic waste, E-wasteto unsustainable power consumption. The call for sustainability\nechoes across industries, urging designers, researchers and mak-\ners to adopt more environmentally conscious materials and prac-\ntices [ 5,6,13,21,27,49]. For material selection, even when eco-\nfriendly options like PLA hold promise, it is always challenging to\nsource from responsible manufacturers and find proper disposal\navenues. Another critical area drawing from the use of traditional\nbatteries [ 15,24]. While harvesting energy from various sources\nshows potential, it alone cannot ensure continuous device opera-\ntion, prompting the need for solutions like energy buffering with\ncapacitors to enable intermittent computing [ 3,12]. In the pursuit\nof sustainable alternatives, batteryless energy-harvesting devices\nhave emerged as promising contenders, challenging the dominance\nof battery-powered counterparts with their eco-friendliness and re-\nduced maintenance requirements [ 4,11,14,16]. Lastly, the concept\nof Life Cycle Assessment (LCA) is a cradle-to-grave method to study\na device’s environmental impact, which has guided researchers to\nevaluate whether a device is more or less sustainable in a more\nquantitative manner [21].\n3 GENERAL RECY-CTRONICS DESIGN RULES\nElectronic devices have become pervasive in our daily lives, offer-\ning convenience while also giving rise to an inescapable tension\nbetween the rising quantity of these devices and the resulting waste\nthey generate at the end of their life cycle. A central challenge in\nthe realm of electronics recycling today stems from the prevailing\nfocus on functionality, durability, and affordability during device\ncreation, often at the expense of considering their sustainability\nat the end of their lifespan. Many devices were designed without\nprioritizing characteristics like \"ease of repair\" and \"recyclability.\" In\nthis section, we set three general design criteria for Recy-ctronics.\n(1)Recyclability over functionality: To enable \"greener\" elec-\ntronics, we will prioritize materials selection and fabrica-\ntion approaches that can ensure an easy, high-rate recycling\nprocess but meanwhile secure devices’ functionality.\n(2)Versatile form factors and mechanical versatility: We aim to\nuse recyclable materials to enable a wide range of electron-\nics including form factors and mechanical properties.\n(3)Highly accessible approaches and tools: For Recy-ctronics, to\nsupport a broad spectrum of users, we expect our materials\nand tools selection can be low-cost, off-the-shelf parts, and\neasy to use.\n3.1 Recycling approach for Recy-ctronics\nThe definition of electronics recycling nowadays is indeed broad,\nwhich is mainly referred to as e-waste recycling, pertaining to the\nretrieval of valuable materials and components from discarded elec-\ntronic devices. However, when viewed through a more fundamental\nmaterial lens, recycling processes can be further classified based\non whether the items retain their original functionality. Recycling\nplastics as an example, approximately a decade ago, diverse strate-\ngies for recycling plastics were grouped into different categories: 1)\nprimary, 2) secondary, 3) tertiary, and 4) energy recovery [ 1]. Pri-\nmary recycling mainly involves reprocessing materials to generate\nsubstances with the same intended function. In contrast, secondary\nrecycling yields materials repurposed for uses differing from theCheng, et al.\nFigure 2: General design strategy. (a) Load versus strain curve under uniaxial deformation for different types of PVA and Cyclic\nloading to increasing strains as a function of strain. (b) Demonstration of PVA’s mechanical flexibility (rigid, highly flexible,\nstretchable) for versatile types of devices (sheet, foam, tube).\noriginal plastic, often referred to as \"downcycling\". Tertiary recy-\ncling predominantly employs chemical processes, encompassing\nmethods like hydrolysis, pyrolysis, hydrocracking, and gasification,\nto recover individual components or monomers. This approach\npresents opportunities for \"upcycling\" the original materials, lead-\ning to the production of potential higher-grade products. The fourth\nmethod, energy recovery, also termed quaternary recycling or incin-\neration, involves harnessing heat as a form of energy recuperation.\nRecy-ctronics focuses mainly on primary and can be extended to\nsecondary recycling processes. This means prioritizing electron-\nics that can be recycled while retaining their initial functionality,\nwith minimal or controlled levels of impurities. Of particular sig-\nnificance is our emphasis on facilitating recycling ease withoutany complicated procedures or chemical involvement, which can\ngreatly enhance the electronics prototyping experience [10].\n3.2 Versatile electronics from simple materials\nElectronics encompass a spectrum of components including semi-\nconductors like silicon, metals like copper and gold, plastics, ceram-\nics, glass, and rare earth elements. These materials are combined to\nform structures such as printed circuit boards (PCBs), connected by\nsolder joints and powered by battery systems. Each component’s\ncharacteristics, encompassing materials, shapes, and sizes, exhibit\nvariations. In this paper, Recy-ctronics not only highlights ease\nof recycling and the quality of recycled materials, we also empha-\nsize the flexibility, which we mean the capacity of materials to be\nadapted into diverse interactive devices.Recy-ctronics: Designing Fully Recyclable Electronics With Varied Form Factors\nFigure 3: Sheet fabrication and basic characterization. (a) Fabrication process for making recyclable electronics with thin-sheet\nform factor. (b) Contact angle between LM and different substrates. (c) Resolution testing for trace width and gap width. (d,e)\nDifferent device and pattern design primitives.\nPolyvinyl Alcohol (PVA) stands as a synthetic polymer originat-\ning from vinyl acetate, synthesized via a distinct polymerization\nprocess. Unlike conventional polymerization routes, PVA emerges\nthrough the dissolution of polyvinyl acetate (PVAc) in alcohol like\nmethanol, followed by treatment with an alkaline catalyst such\nas sodium hydroxide. This unique process grants PVA remarkable\nattributes including high water solubility, adeptness in forming\nfilms, and recyclability [ 28]. These qualities have made PVA to be a\nfavored material for transient electronics, serving as substrates and\nencapsulation mediums [ 40], which also made us choose PVA as the\nmain material for Recy-ctronics. Besides the easy-to-dissolve nature\nof PVA, the mechanical versatility of PVA is also another key factor.\nAs illustrated in Figure 2 a, showcasing how we can tailor the me-\nchanical property of PVA through simple adjustments in material\ncomposition and fabrication approaches. In general, for making PVA\nbased substrates, a blend of glycerol/glycerin, de-ionized water, and\nPVA powder constitutes the primary components. Each component\nplays a different role in enabling diverse material properties.\nAs shown in Figure 2 a, we have conducted a series of tests on\ndiverse PVA samples with varying compositions. As shown in the\nfirst graph of Figure 2 a, if with the same recipe (glycerin: PVA:\nde-ionized = 1: 6: 30), PVA featuring a higher molecular weight\n(i.e.M.W. 88,000-97,000) exhibits a higher elastic modulus, ultimate\nstress, and meanwhile demonstrates lower stretchability. Interest-\ningly, within the same figure, if no glycerin is added, PVA behaves\nas a non-elastic material with clear yield stress and much higherYoung’s modulus, making it a great option as a rigid sheet mate-\nrial. Additionally, the overall stretchability can be augmented by\nincreasing the proportion of glycerin, which is a typical plasticizer,\nduring the PVA mixing process. As shown in the second graph of\nFigure 2 a, it is clear when more glycerin is added to the system,\nmore elastic and \"softer\" the material will become. Furthermore,\non the right side of Figure 2 a, we have carried out cyclic testing\nfor PVA samples with a recipe of glycerin: PVA: de-ionized water\nratio of 3: 6: 30. The sample underwent two cycles of increasing\nstrain levels: 10%, 20%, 30%, 40%, 50%, 60%, 70%, and 80%. These\ntests revealed minimal plastic deformation of the sample during the\nprocess. All the testing specimens were prepared in dog-bone shape\n(ASTM D412A) and tested on a materials testing machine (Instron\nUniversal Testing Systems 68SC-05) at a strain rate of 20mm/min.\nIn this section, we aim to provide a simple strategy to easily tailor\nthe mechanical property of PVA, which is a crucial step to enable\na wide range of devices and applications. In general, as shown in\nFigure 2 b, by varying the composition of PVA, one can achieve\nelectronic materials from rigid to flexible to highly stretchable. This\nhigh tunability can also favor different forms of interactive devices\nwe aim to make. For example, for making sheet-based electron-\nics, the more rigid and non-elastic recipes can be used, while for\nfabricating foam-based electronics, we will mostly use the flexible\nrecipes to enable the compressing behavior for foams, and for tubes,\nwe prefer more stretchable recipes due to the typical stretchable\napplication domains for this specific form factor.Cheng, et al.\nFigure 4: Sheet application. (1) Recyclable Sheet-based proximity sensor that responds to finger position. (2) Recyclable RFID\ntags used in two different application scenarios, including recyclable RFID clothing price tags and marathon RFID trackers.\nBoth the proximity sensor and RFID tags are collected and recycled.\n4 SHEET\nEven in today’s electronics landscape, various form factors are\npresent, yet planar geometry remains the main form factor (PCB\nboard). Our initial exploration centers on using the Recy-ctronics\napproach to make some of today’s electronics more sustainable.\nAs shown in the initial step of Figure 3 a, both PVA and LM are\nprepared through mixing and stirring processes. In the formulation\nof PVA, a blend of glycerin, de-ionized water, and PVA powder\nconstitutes the primary components. For crafting flexible and mod-\nerately stretchable thin sheets, a typical recipe involves a glycerin:\nPVA powder: de-ionized water ratio of 1: 6: 30. The LM involves the\ncombination of pure Gallium and Indium in a weight ratio of 3: 1.\nEmploying a magnetic stirrer (Thermo Scientific Cimarec stirring\nhot plate), PVA is subjected to mixing at 80 °C for approximately\n2 hours, while LM undergoes a 160 °C overnight stirring process.\nOnce both materials attain the desired state, a film applicator (GLTL\nfour-sided coating applicator) is utilized to form a PVA film. This\nfilm is allowed to cure either at room temperature overnight or in\nan oven for 4 hours at 60 °C depending on thickness. Subsequently,\na stencil (Selizo low tack transfer paper) is applied to the PVA sheet,\nonto which LM is evenly brushed with the assistance of a rubber\nroller. The stencil is removed post circuit completion (shown in\nFigure 3 a). We conducted preliminary tests to examine the printing\nresolution for trace and gap widths, achieving a minimum trace\nwidth of 0.1mm and a smallest gap width of 0.2mm (Figure 3 c).\nMoreover, we present several printed electronics device design-\nLED, inductor, capacitor and heater pattern (Figure 3 d). We have\nalso explored some pattern designs including interconnected circles,\nsquares and waves with different density—showcasing the design\nand fabrication potential inherent in the utilization of PVA and LM(Figure 3 e). It also shows great adhesion and intrinsic mechanical\ncompatibility between PVA and the room-temperature LM. Usually,\nbecause of the high surface energy of LM, it barely gets attached to\nother surfaces. As shown in Figure 3 b, the contact angle of PVA\nand LM is 52.4±4.2◦(ramé-hart Contact Angle Goniometer), indi-\ncating a relatively good adhesive between PVA and LM, and as a\ncomparison, the contact angle between LM and the copy paper is\nalso measured, which is much smaller (24.4 ±2.5◦). With this great\nadhesion between LM and PVA sheet, we have also achieved a sheet\nresistance of 0.013±0.002Ω/𝑠𝑞, where 20 samples with 40cm by\n0.5cm dimension were tested (B&k Precision 891 LCR Meter).\n4.1 Application 1: Recyclable Sheet Electronics—\nfrom multiple components to minimal\ncomponents\nOur primary goal is to enable the transformation of the current\nPrinted Circuit Boards (PCBs) into easily recyclable alternatives\nusing PVA and LM. In this section, we highlight two distinct circuit\ndesigns to demonstrate the versatility of Recy-ctronics in prototyp-\ning electronics with varying component complexities—from those\nwith many components to those with minimal components.\nAs shown in the top row of Figure 4, we first introduce a prox-\nimity sensor circuit featuring an OPB733TR, which integrates both\nan infrared LED and an NPN phototransistor. This sensor acti-\nvates additional LEDs as a hand or object passes over it, with the\nlights dimming once the object is removed. This interaction is man-\naged by a standard Microcontroller Unit (MCU), specifically an\nATMEGA328P-PU. To enhance compatibility with the sheet circuit,\na socket is added for the MCU, ensuring a flat connection between\nthe MCU and the sheet substrate. This assembly is bonded usingRecy-ctronics: Designing Fully Recyclable Electronics With Varied Form Factors\ncommercial silver epoxy. This particular circuit design incorporates\ntwelve electronic components in total, including the phototransis-\ntor, the MCU, four Surface-Mount Device (SMD) LEDs, and six SMD\nresistors. The PVA substrate used in this specific circuit design has\na dimension of 5.5cm x 4.5cm x 0.5mm, made from a mixture exclud-\ning glycerin (PVA: de-ionized water = 1: 5). Also, all components\nare designed for easy disassembly and reuse, and the substrate dis-\nsolves entirely in water within ten minutes, enabled by the PVA\ncircuit’s recyclable nature (shown on the right side of Figure 4).\nBesides the proximity sensing circuit, which can exemplify most\nof today’s circuit, there are also electronics that have minimal to\nno components but serve a crucial role in our everyday life. Passive\nRFID technology is one typical example, which has gained wide-\nspread adoption in various everyday scenarios (such as key fobs,\ndoor access, chipless payments, etc.), primarily due to its techno-\nlogical benefits. Firstly, it operates without the need for a battery.\nWhen an RFID reader is in proximity, the tag becomes instantly\npowered, enabling it to transmit the data stored within it back to\nthe reader. Secondly, its design is notably slim, enabling seamless\nintegration of the RFID tag into thin and flat devices (like clothing\nprice tags, apartment key fobs, etc.). Lastly, the production cost of\nthe RFID tag is very economical, facilitating large-scale manufac-\nturing. These benefits make RFID not only a promising commercial\nproduct in almost all industry branches, also draws attention in\nusing RFID for interaction, including tracking 20 objects and iden-\ntifying their movements [ 23], utilising RFID to build interaction\nwith smart devices [22].\nHowever, the widespread adoption of RFID tags has also brought\nabout concerns regarding their end-of-life waste management. Re-\ncent research highlights that selected logistics centers alone con-\ntribute to an annual generation of 5.7 tons of e-waste from discarded\nradio frequency identifiers on received pallets, with this waste con-\ntaining 139 kg of metal components. Our primary objective is to\naddress this issue by introducing easily recyclable RFID tags, specif-\nically targeting two key scenarios. Illustrated in the second row\nof Figure 4, we have affixed recyclable ultra-high-frequency (UHF)\nRFID tags to clothing price tags and marathon name tags. After\nthese RFID tags have fulfilled their purpose, users can conveniently\ndeposit them into designated collection bins (depicted in the third\ncolumn of Figure 4), initiating a process of dissolution and subse-\nquent recycling.\nAs shown in Figure 4, we fabricated the RFID tags utilizing PVA\nas the substrate, overlaying it with an LM pattern. Employing the\nImpinj Monza 4QT chip, which can either be retrieved prior to\nRFID device dissolution or recycled subsequently, added to the\nfabrication process. Our measurement approach employed an ultra-\nhigh-frequency (UHF) RF reader, specifically the ImpinJ Speedway\nR420 RFID reader paired with the Laird S9028PCR antenna, encom-\npassing the radio frequency range spanning 902MHz to 958MHz.\nLeveraging the Octane SDK, we established a comprehensive sys-\ntem that not only reads UHF RFID tags but also furnishes pertinent\nfeedback. As presented in the central column of Figure 4 b, we\nconducted replicative scenarios involving the utilization of our re-\ncyclable RFID tags in shopping and marathon runner information\ntracking. Remarkably, in both scenarios, the recyclable RFIDs con-\nsistently exhibited robust detectability by our reader systems. Also,\nthe fabrication process is similar as the proximity sensor example.However, considering most of today’s commercial RFID tags are in\na more flexible form, so we fabricated the RFID on a more flexible\nsubstrate recipe (glycerin: PVA: de-ionized water = 1: 6: 30), which\ncan be more conformally attached to the price tag or marathon\nname tag.\n5 FOAM\nBesides sheet form factors, which are more of a conventional form\nfactor for printed circuits, foam, with its inherent porous compo-\nsition and responsive elasticity, has garnered significant attention\nin HCI, offering diverse avenues for crafting interactive devices.\nNakamaru et al. showcase foam devices coated with conductive\nink, enabling volumetric alterations for detecting a spectrum of\nuser inputs: compression, bending, torsion, and even shearing [ 29].\nIn this section, our study aims to use PVA and LM to enable fully\nrecyclable interactive foam structures.\nAs shown in Figure 5 a, the fabrication process starts by prepar-\ning both the PVA material and LM. The sequential steps are similar\nto the ones described in the sheet fabrication section. However, a\nmajor difference arises in the formulation for PVA and how we coat\nthe LM. To ensure the foam’s stability during the curing phases of\nPVA, we reduce the use of water content coupled with an increased\nglycerin proportion. The main recipe we adopt in this study covers\na blend of glycerin: PVA: de-ionized water in a ratio of 1: 2: 4. Once\nthe PVA solution is prepared, a mechanical stirrer operating at\napproximately 2000rpm for 6 minutes serves to transform the PVA\nsolution into a spongy and porous configuration. It’s noteworthy\nthat we had also tested adding nano-cellulose fibers (NCF) and\nsurface surfactants (such as Sodium dodecyl sulfate) which can\neffectively maintain the structure of the recyclable foam during\nthe curing process, meaning no foam bubbles will collapse during\nthe curing step but will complicate the material synthesizing and\nalso the subsequent recycling process. After the complete drying\nprocess of the PVA foam, which is thickness dependent (typically\nspanning 1 day at room temperature for a 2mm foam), laser cut-\nting can be employed to shape the foam into versatile geometries.\nConcluding the process, as shown in Figure 5 a, step 4, the foam is\nimmersed in LM, with an additional vacuum treatment to back-fill\nthe LM into the foam’s internal voids. Figure 5 b and c show two\ntesting results, including the pore concentration for the 1: 2: 4 foam,\nwhich is mainly around 55000nm (AutoPore 9520 Mercury intru-\nsion porosimeter) and also the conductivity and weight changes\nacross different back-fill times. It is worth noting that as we back-fill\nmore times, LM will flow and attach to more voids within the PVA\nfoam, making the foam more conductive. For shaping the foams\ninto different geometries, not only laser cutting, foams can also\nbe mold-casted, shown in Figure 5 d and e, we have introduced\ndifferent recyclable interactive foams with various sizes, geometries\nor even surface textures.\n5.1 Foam-based Electronics Primitives\nFoam, with its unique bouncy property, can enable unique multi-\nfunctional sensing capabilities. Here, we present four innovative\nconductive foam designs. We initiated our exploration by showcas-\ning a basic square foam configuration as a pressure-sensitive unit,\nwhich can detect different levels of user-applied pressures (FigureCheng, et al.\nFigure 5: Foam fabrication and basic characterization. (a) Fabrication process for making recyclable foam electronics. (b)\nThe pore diameter for the 1:2:4 foam recipe. (c) The conductivity testing for the interactive foam. (d,e) Different types and\ndimensions of recyclable interactive foams through mold casting or laser cutting.\n6 a). This foam pressure sensor is able to distinguish between light,\nmoderate, and firm touches. Different than this fully LM-soaked\npressure sensor, in Figure 6 b, we introduce a selectively conductive\nfoam. Here, we utilized a laser cutter to cut the four corners of the\nfoam, selectively immerse these units into LM to make them con-\nductive, and then reattach them back to the original piece by using\nPVA solution as glue. This device now functions as a controller,\noffering four distinct input options. Both the pressure sensor and\ncontroller implementations rely on self-capacitance mechanism\nand the Arduino CapacitanceSensor Library, with data acquisition\nhandled by an Arduino UNO (shown in Figure 6 a and b), and 1M Ω\nresistor was used in the RC circuit.\nExpanding beyond capacitive sensing, we showcase a mechanical-\ncontact-based switch in Figure 6 c, where we present a three-layer\nfoam structure comprising a fully conductive top layer, an insulating\nmiddle layer, and a bottom layer housing two separated conductive\ncomponents. Utilizing the foam’s exceptional compressibility, apply-\ning pressure to the top conductive layer will make contact with the\ntwo lower terminal layers, which further completes a closed-loop\ncircuit. This configuration has the potential to serve as a functional\nlight switch. While bending sensors have been frequently explored,\nit becomes challenging to use highly conductive LM, which has\nminimal resistance variance during bending. In Figure 6 d, thanks\nto the laser cutting approach, we try to overcome this challenge\nby laser-cutting a strain-gauge-like pattern to amplify resistance\nchanges along the bending direction. Both the mechanical-contact\nswitch and the enhanced bending sensor (Figure 6 c and Figure\n6 d) are implemented using Arduino UNO and voltage-dividingmechanisms. We have also included the characterization data for\nboth sensors on the right side of Figure 6 c and d.\n5.2 Application 2: Recyclable Interactive Pen\nTip\nFoam has many distinctive qualities, encompassing its softness,\ncushioning attributes, and adaptability to the contours of objects\nor bodies. One of the most notable characteristics when interacting\nwith foam-based devices is the bouncy tactile feedback. Applying\nthis mechanical property, we present an interactive pen featuring\nfour distinct interactive and recyclable pen tips. We aim to address\nthe existing gap in current drawing software, which typically offers\na wide array of pen tools, while conventional interactive pens, such\nas the Apple Pencil, only provide rigid tips that do not replicate\nthe tactile sensations associated with specific tools. As illustrated\nin Figure 7 a 1-4, we demonstrate four pen tips with varying foam\nproperties designed to mimic the appearance of different drawing\ntools, such as a pencil, ballpoint pen, and brush. Also, we employed\nvarious formulations to create PVA foams with adjustable levels\nof hardness, enabling these four interactive pen tips to provide\nusers with sensations closely resembling real pen tools. As shown\nin Figure 7 b, these pen tips can be affixed to 3D-printed pens, with\na conductive foil and wire used to establish a capacitive coupling\nbetween the pen tips and the user’s hand. The formulations utilized\nfor creating hard, medium, soft and brush pen tips are as follows:\nglycerin: PVA: de-ionized water ratios of 0.5: 2: 4, 1: 2: 4, and 2.5:\n2: 4, and 1: 2: 4 (small PVA molecules) respectively. We have also\nincluded the young’s modulus of these four different foams in FigureRecy-ctronics: Designing Fully Recyclable Electronics With Varied Form Factors\nFigure 6: Foam sensing primitives. (a, b) Capacitive sensing based pressure sensor and controller and (c, d) Resistive sensing\nbased mechanical contact switch and bending sensor.\n7 c, showing their distinct hardness. It is important to note that\nonly the pen tips are fully recyclable in this application, while the\npen holder and the pen’s body are 3D printed using black PLA\nmaterials.\n6 TUBE\nBesides sheet and foam, Recy-ctronics is introducing a third form\nfactor: tube. Tubes, tailored by their dimensions and diameter-to-\nlength ratios, are versatile in applications, allowing for the creation\nof bending actuators [ 26] or integration into textiles for sensing\npurposes [ 25]. This section focuses on utilizing PVA and LM-based\ntubes to develop various types of hand-weaving recyclable devices.\nThe method for fabricating these recyclable tubes begins with the\npreparation of PVA solution and LM, similar to the processes for\nsheet and foam. We exemplify three different recipes for making\ndifferent types of tubes in the third graph of Figure 8 f, where by\nvarying the mix ratios, it is possible to produce tubes ranging from\nrigid and flexible to highly stretchable. While for most of the tubes\nused in this paper, a mixture ratio of glycerin: PVA: de-ionized\nwater = 3: 2: 3.5 is preferred for achieving high elasticity in this\npaper.\nThe fabrication technique for tubes is straightforward, as illus-\ntrated in Figure 8 a. A simple casting method involves dipping a\ncarbon fiber rod into the PVA solution. The process is complete\nonce the PVA tube has cured and the curing time is similar to curing\nPVA sheet which is also thickness dependent. As demonstrated in\nFigure 8 b, tubes of varying diameters can be produced by using\ncarbon fiber rods of different sizes. Additionally, as shown in Figure\n8 c, applying multiple layers of PVA allows for the creation of tubes\nwith various wall thicknesses. While this tube casting method islimited to producing tubes up to one meter in length, corresponding\nto the length of carbon fiber rod that is available, longer tubes can\nbe fabricated by joining multiple tubes end-to-end. This is shown\nin Figure 8 d, where a 3-meter tube was assembled by connecting\nseveral tubes together using PVA as an adhesive.\nIn this paper, we present two types of tubes for making inter-\nactive devices: hollow tubes and LM-filled tubes. The stretchable\nLM-filled tubes can be utilized as strain sensors. Through experi-\nmentation, we detail the process by stretching these tubes to a 50%\nstrain results in a resistance increase from approximately 0.74 Ω\nto 0.82 Ω(shown in first graph of Figure 8 f). Also, we explore the\npossibility of using LM-filled tubes for connecting LEDs. Unlike the\ncircular tubes, we chose to use squared tubes that can align more\ncompatibly with the geometry of standard SMD LEDs (as illustrated\non the right side of Figure 8 e). Beyond sensors and LEDs, which\nboth rely on LM-filled tubes, we also showcased the application\nof hollow PVA tubes in the development of recyclable pneumatic\nactuators. To enhance the functionality of these tube actuators, we\nadd a constraining layer by using fishing lines, which will limit the\naxial tube deformation and only the longitudinal deformation is\nallowed. Besides, on the left side of Figure 8 e, we have investigated\nthe design consideration for using tubes for fabricating devices.\nTubes that are overly flexible with thin walls may easily get sharp\nbends, obstructing the flow of LM or air. Conversely, tubes that\nare too rigid pose difficulties in bending and integration into tex-\ntiles, limiting their applicability in wearable technology and smart\nfabrics.\nFor building recyclable tube-based interactive devices, we present\ntwo applications. Firstly, we introduce a strain-sensor enabled hap-\ntic ring, as shown on the left side of Figure 9. This device comprisesCheng, et al.\nFigure 7: Recyclable interactive pen tip application. (a) From 1-4, we showcase recyclable interactive pen tips with different\nhardness and tactile feelings, from hard, medium, soft to brush feelings. (b) The disassembly look of the interactive pen for\nmobile devices with replaceable pen tips. (c) Young’s modules for the foams with different material compositions.\nten PVA tubes, each has 1.5mm in internal diameter (ID), 2.9 mm\nin outer diameter (OD), and 10cm in length. These tubes are hand-\nwoven using PVA-based knots for secure connections. Five of these\ntubes are filled with LM, serving as strain sensors to detect users’\nfinger movements. Additionally, a circular-shaped pneumatic actu-\nator, fabricated from a thicker PVA tube (3mm ID, 6mm OD, and\n20cm long), is integrated at the device’s tail end. The pneumatic\ntube’s axial movement is restrained with fishing wire, allowing the\nactuator to expand or contract around the finger. As a result, when\na user bends their finger, the ring can automatically get loosened,\nenabling easy removal (illustrated at the bottom of Figure 9).\nSecondly, we demonstrate an interactive LED setup, showcased\non the right side of Figure 9. This setup is crafted by weaving\nfour LM-filled PVA wires with conventional cotton yarns. Among\nthese, two wires function as capacitive sensors, which we simply\nimplemented the self-capacitance mechanism and the Arduino Ca-\npacitiveSensor library is used. The remaining two LM-filled PVA\nwires serve as connectors for two SMD LEDs, where two squared\nPVA tubes are cast to fit the size of the LEDs. This configuration\nallows for the individual activation and deactivation of each LED\nby interacting with the corresponding capacitive sensing wire.7 RECYCLING PROCESS\nWe have introduced Recy-ctronics for crafting interactive devices\nacross varying form factors. These devices not only offer diverse\ninteraction capabilities but are also designed with full recyclabil-\nity in mind. The recycling process is the same for all three form\nfactors—sheet, foam, and tube, while recycling rates, duration, and\npractical viability might differ among different form factors.\nThe recycling process follows prior literatures [ 39,40,45], which\nstarts with detaching electronic components if there are any ( e.g.,\nSMD, MCU, battery) from the substrates. Any material remnants\non the pins of the components can be delicately eliminated using\ntweezers or by partially immersing them in water. Any compo-\nnents can be further dried out to prevent electrical failure under\nthe condition of 80-90 °C for one day in a laboratory oven. Then the\nentire device undergoes immersion in water until complete dissolu-\ntion is achieved. Throughout this process, the dissolved device is\nsubjected to thorough rinsing with fresh water, a step conducted\n2-3 times to ensure the complete dissolution of PVA. The resulting\nPVA-enriched water is then collected, following the separation of\nthe PVA-rich water from the residual LM fraction, approximatelyRecy-ctronics: Designing Fully Recyclable Electronics With Varied Form Factors\nFigure 8: Tube fabrication and basic characterization. (a) Fabrication process for making recyclable tube electronics. (b-d)\nRecyclable tube with different diameters, wall thickness and length. (e) Three types of tube-based devices, including LED,\nsensor and actuator. (f) Basic electrical and mechanical testing results for tubes.\n5ml solution of 1mol% NaOH is added to the LM. The quantity of\nNaOH solution is contingent upon the device’s size. This NaOH\nserves the role of removing the oxidized layer on the LM, a thin\nGa2O3shell, thereby enabling the seamless recycling of the LM\nand ensuring high recycling efficiency. With the removal of the\noxidized layer and the transformation of the LM into a spherical\nform, one can retrieve the LM by using a syringe.\nTo assess the recycling performance across three distinct form\nfactors, we carried out comprehensive recycling testing. This en-\ncompassed recycling an LED pattern on a thin sheet, a cylindrical\nconductive foam, and a 4cm pipe. The recycling process was exe-\ncuted with six samples for each design, documenting and presenting\nthe pre- and post-recycling weights of PVA and LM. As shown Fig-\nure 10, all three sample groups achieved a high recycling rate over\n95% on average for LM, where we have achieved 97.5%, 96.1% and\n98.6% for sheet, foam and tube respectively. Also the loss of PVA\nduring the recycling process is higher, where we obtained 87.4%,\n78.1% and 83.2% for sheet, foam and tube respectively. Among all\nthe three types of interactive devices, the tube group displayed thehighest recycling rate for LM. This lower recycling rate for LM can\npotentially come from the foam’s highly porosity structure, which\nis mostly covered by LM, inducing more oxidized area compared to\nsheets and tubes.\nBeyond the individual device recycling test phase, we expanded\nour scope to include a batch of devices accumulated over two\nmonths, leading to the recovery of 17.4 grams of LM, as detailed\nin Figure 10 b. This batch consisted of various device types ( e.g.,\nsheets, foams, tubes) and miscellaneous non-device waste, totaling\n716 grams. Within approximately 30 hours, all materials dissolved\nin water, which is later rinsed three times to transport the PVA-rich\nwater from the LM remnants. It is worth noting that we paid more\nattention to recycling LM for this experiment and saved the PVA\nsolutions in containers for later prototyping use.\nThis lab-scale recycling initiative revealed several insights: (1)\nComplexity in Bulk Processing : Recycling a multitude of devices\nintroduces complexities that we did not encounter in single-device\nrecycling. The diverse mixture resulted in the emergence of unex-\npected contaminants such as silver epoxy particles, paper scraps,Cheng, et al.\nFigure 9: Two tube-based applications. (1) A fully recyclable sensor embedded haptic ring, and (2) An hand-weaved interactive\nLED.\nand wires, requiring further filtration through a 200 mesh filter\n(around 75 microns). (2) Necessity for Improved Sorting : The\nheterogeneous nature of the waste highlighted the need for a better\nsorting system. Varied material compositions with different ratios\nof glycerin, PVA, and de-ionized water, suggest pre-recycling cat-\negorization could stabilize the material composition of recycled\noutputs. (3) Observations on LM Recycling : The bulk recycling\nprocess yielded a higher presence of oxidized substances and an\nincrease in the resistance of the recycled LM, with a sheet resis-\ntance of 0.019±0.011Ω/𝑠𝑞(20 pieces of 40cm by 0.5cm samples),\ncompared to the original LM’s 0.013 ±0.002Ω/𝑠𝑞, which might stem\nfrom the extended environmental exposure during waste accumu-\nlation. (4) Challenges in Dissolving PVA : We have observed\nthat the dissolving process dramatically slowed down after several\nhours, and we had to add more water to the dissolving tank. Also,\napproaching the end of the dissolving stage, we picked up some\nremained PVA fragments and separately dissolved them in a beaker\nwith hot water. Overall, this experience with recycling accumulated\nwaste underlines the method’s simplicity and efficiency, making it\nparticularly suitable for maker environments.\n8 COMPARATIVE LIFE CYCLE ASSESSMENT\nWe undertake an environmental impact comparison (Figure 11)\nto juxtapose Recy-ctronics against conventional rigid PCB-based\nscenario using the case of the recyclable sheet-based proximity\nsensor presented in Figure 4. This comparison utilizes the recent\nconcept of a comparative life-cycle assessment (LCA) [ 48] to un-\nderscore the environmental benefits of Recy-ctronics. FR-4, themost common PCB substrate, serves as the baseline for compari-\nson. We assume identical dimensions to our substrate, i.e., 45mm\nx 55mm, and a standard thickness of 1.6 mm per dielectric layer.\nVarious common LCA boundaries exist, including “cradle-to-gate”\nassessments, which encompass raw material extraction through\nproduct manufacturing, and “cradle-to-grave”, which extends to\ntransport to the consumer, usage throughout the product’s lifetime,\nand end-of-life disposal. Given our focus on end-of-life recycling,\nwe adopt a “cradle-to-grave” analysis as the primary scope of this\ncomparison study.\nIn the initial phase of conducting comparative LCA, we compile\na parts inventory of the recyclable sheet-based proximity sensor cir-\ncuit and an FR-4 printed circuit board assembly (PCBA). As demon-\nstrated in the previous section, the Recy-ctronics sheet effectively\nsupports the standard MCU and sensor operation. Consequently,\nelectronic components across both sensors can be canceled out in\nthis environmental impact comparison as they are mostly iden-\ntical. Thus, the sole distinction between Recy-ctronics sheet and\nconventional FR-4 circuitry in the raw material phase lies in the\nsubstrate, and conductive traces on the circuit board, which are\nPVA and LM for Recy-ctronics, and copper-clad FR-4, respectively.\nThe environment impact of these parts is approximately equal in\nthe raw material stages.\nThe first step of comparative LCA is to create a parts inventory\nof the recyclable sheet-based proximity sensor and an FR-4 sensor,\nwe have shown in the previous section that Recy-ctronics sheet can\nsupport the successful operation of a standard MCU and sensor,\nso the electronic components across could be canceled out in this\nenvironmental impact comparison as they are identical. Therefore,Recy-ctronics: Designing Fully Recyclable Electronics With Varied Form Factors\nFigure 10: Recycling process. (a) All three form factors-sheet, foam and tube will undergo immersion in the water until complete\ndissolution. Then the LM and PVA will be separately collected to conclude the recycling process. (b) Lab-scale recycling\nexperiment over two months.\nthe only difference between Recy-ctronics sheet and a conventional\nFR-4 circuit is the substrate and the conductive traces on the circuit\nboard, and their corresponding manufacturing and end-of-life stage.\nWe qualify them as PVA and LM for Recy-ctronics, and copper-clad\nFR-4, they are comparable in the raw material stages.\nIn the manufacturing stage, Recy-ctronics and conventional\nPCBs each possess pros and cons. For instance, Recy-ctronics em-\nploys screen printing to pattern circuits, a method that is fasterfor low-quantity manufacturing and requires less expensive equip-\nment and ecosystem. However, it lacks scalability compared to the\nconventional heat press and etching process, which can produce\nlarge quantities of parts or multiple boards simultaneously. From an\nenvironmental standpoint, screen printing proves more beneficial,\nas its required equipment consumes significantly less electricity,\nwhich contributes almost half of the global warming potential in\nconventional PCB manufacturing [ 30], compared to conventional\nPCB manufacturing equipment in factories [47].Cheng, et al.\nFigure 11: Comparative environmental impact analysis of Recy-ctronics sheet and conventional FR4 PCB.\nWe assume similar transportation and use life stages through-\nout the device’s lifespan, as they are the same device. When both\ndevices eventually reach the end-of-life stage. While FR-4 becomes\ne-waste, Recy-ctronics enables the reuse of its substrate (PVA) and\nconductive traces (LM), along with electronic components, owing to\nan accessible recycling process. As discussed before, the recycling\nprocess for Recy-ctronics is very straightforward with a high recy-\ncling rate which only requires dissolving the device by immersing it\nin the water, reducing the oxidized layer outside LM and separating\nthe two materials. This entire process is highly accessible without\ninvolving complicated tools/steps. For recycling FR-4, the collected\nPCBs are dismantled and shredded, followed by crushing and grind-\ning to reduce them into finer particles. Subsequent stages involve\nair separation, screening, and magnetic and electrostatic separation\ntechniques to isolate copper from non-metallic materials. Chemical\nprocesses, such as hydrometallurgical methods, are employed to\nleach out copper, which is then refined for reuse. Meanwhile, the\nfiberglass component is processed separately to recover fiberglass\nmaterials for further applications [44].\nIn summary, Recy-ctronics offers substantial environmental ben-\nefits and potentials compared to the conventional, non-recyclable\nFR-4, particularly at the end-of-life stage.\n9 DISCUSSION, LIMITATION AND FUTURE\nWORK\nRecy-ctronics aims to make interactive devices across a broad spec-\ntrum of form factors for various use cases, and with complete re-\ncyclability as the core feature in mind. Here, we reflect on several\ncritical factors for Recy-ctronics, including what elements remain\nnon-recyclable in the system, how robust PVA-based devices are,\nand how we encapsulate the devices.9.1 What are not recyclable\nIn this work, we introduced both PVA and LM to make fully re-\ncyclable electronics with three distinct form factors. Meanwhile,\nmost of the electronic components can also be recycled for reuse,\neven submerged in the water. One can follow the drying process\ndescribed previously in the paper by placing chips and components\nin a glass petri dish inside a laboratory oven at 80-90 °C for one day.\nThe major non-recyclable part in our system is the silver epoxy\nwhich is used to connect different components to the LM circuit\nor wires, and currently we will need to remove the residue for the\nsilver epoxy from both the components and the substrate before\nwe recycle them. Usually silver epoxy contains thermosetting poly-\nmers that form cross-linked networks when cured, providing the\nadhesive with its structural integrity and mechanical strength but\nmaking the recycling process challenging. We envision this can\nbe tackled by developing more recyclable type of solder, for exam-\nple instead of using thermosetting polymers as the base material,\nwe can also use more recyclable materials, such as PVA which is\nalready used in this paper.\n9.2 Robustness for PVA\nRecy-ctronics aims to develop electronics that are transient and\nfully recyclable, prioritizing sustainability over functionality. How-\never, ensuring the system’s reliability and preventing malfunction\nor early failure are also very important. In this work, PVA is the\nmain water-soluble material and initiates the dissolution process,\nmaking the system sensitive to water or even moisture. As also de-\nscribed in previous work, the tunability of PVA’s dissolvability can\nrely on different parameters such as the molecule weight of PVA or\ndifferent types of PVA [ 8]. While we believe the water solution na-\nture of PVA made the system not an ideal option for water-related\napplications ( e.g.,under-water electronics). Moreover, moisture canRecy-ctronics: Designing Fully Recyclable Electronics With Varied Form Factors\nsignificantly impact the mechanical and electrical performance of\nPVA-based systems. In this study, we explore the use of PVA mixed\nwith glycerin—a simple polyol compound characterized by three\nhydroxyl (OH) groups. These groups can facilitate the formation of\nhydrogen bonds with water molecules, including those present in\nenvironmental moisture. This results in noticeable changes in our\nsamples’ softness or stiffness depending on weather conditions. To\nmitigate these issues, one strategy involves storing the samples in\ncontrolled conditions, such as within airtight bags. Alternatively, in\nthe future, we plan to explore the application of hydrophobic coat-\nings to enhance the water resilience of the PVA substrate, thereby\nbroadening the potential applications of our sustainable electronic\nsystems.\n9.3 Encapsulation and sealant\nRecy-ctronics introduces three types of interactive devices, each re-\nquiring different levels of encapsulation. For sheet-type electronics,\nencapsulation can be achieved by applying an additional thin layer\nof prepared PVA film. This is done by adhering the PVA film to the\nsubstrate layer and slightly spraying the PVA surface with a mist of\nwater. Upon drying, the film forms a tight bond with the PVA layer,\ncompleting the encapsulation process. For devices incorporating\nadditional electronic components, the PVA film needs to get laser-\ncut to create spaces for the components before adhering it to the\nPVA substrate using the same water mist method. For foam-based\nelectronics, achieving a conformal encapsulation that follows the\nfoam’s geometry is essential. To accomplish this, we apply another\nlayer of PVA solution directly onto the foam using a brush. We\nspecifically adopted an elastic formulation for this layer, which is\nglycerin, PVA, and water in the proportions of 3: 2: 3.5, ensuring\nthat the foam’s movement is not restricted. This specific formula-\ntion also has a reduced water content to prevent further dissolution\nof the already cured PVA foam during the brushing step. Regarding\nthe tube-shaped devices, an additional coating layer is not usually\nneeded. However, applying some PVA at the points where the PVA\ntube connects to wires will further enhance the stability of the wire\nconnections.\n10 CONCLUSION\nIn this paper, we have introduced a set of readily accessible materi-\nals, such as polyvinyl alcohol (PVA) and liquid metal (LM), along\nwith innovative fabrication methods, including screen printing, me-\nchanical stirring, and mold casting. These enable the prototyping of\ninteractive devices in three distinct form factors: sheet, foam, and\ntube. Each of these device types can undergo a simple immersion\nin water for an efficient dissolution and recycling process, with an\nexceptional recycling rate. Furthermore, we proposed a material\ndesign strategy that empowers users to tailor the mechanical prop-\nerties of these devices, ranging from rigid to flexible and stretchable.\nWe hope Recy-ctronics can enable a broad spectrum of users to cre-\nate versatile interactive devices with sustainability at the forefront,\nadvocating for an environmentally conscious approach prioritizing\nrecyclability over functionality.",
      "metadata": {
        "filename": "Recy-ctronics_ Designing Fully Recyclable Electronics With Varied Form Factors.pdf",
        "hotspot_name": "PBT_Injection_Molding",
        "title": "Recy-ctronics: Designing Fully Recyclable Electronics With Varied Form\n  Factors",
        "published_date": "2024-06-13T22:43:47Z",
        "pdf_link": "http://arxiv.org/pdf/2406.09611v1",
        "query": "PBT injection molding LCA environmental impact"
      }
    },
    "Space debris through the prism of the environmental performance of space systems": {
      "full_text": "The Journal of Space Safety Engineering  \nSpace debris through the prism of the environmental performance of space systems:\nthe case of Sentinel-3 redesigned mission \n--Manuscript Draft-- \n \nManuscript Number: JSSE-D-20-00055\nArticle Type: SI: 1st IOC2019\nSection/Category: Space Environment Hazards\nKeywords: life cycle assessment;  environmental impact;  ecodesign;  space sustainability;  space\nindustry;  space systems engineering\nCorresponding Author: Thibaut Maury\nInstitut des Sciences Moléculaires\nTalence, Aquitaine FRANCE\nFirst Author: Thibaut Maury\nOrder of Authors: Thibaut Maury\nSara Morales Serrano\nPhilippe Loubet\nGuido Sonnemann\nCamilla Colombo\nLuisa Innocenti\nAbstract: Like any industry, space activities generate pressures on the environment and strives\ntowards more sustainable activities. A consensus among the European industrial\nstakeholders and national agencies in the Space sector is emerging on the need to\naddress eco-design through the prism of the environmental Life Cycle Assessment\n(LCA) methodology. While the use of LCA is being implemented within the sector, the\ncurrent scope disregards the potential environmental impact in term of debris\ngenerated by space missions on the orbital environment. The paper highlights the\nrelevance of applying LCA holistically during the design phase of space systems,\nconsidering potential impacts occurring in the orbital environment during the  utilisation\nand  disposal stages  of a space mission. Based on the comparison of two mission\ndesigns, the aim is to consider potential emission of space debris into the LCA\nframework as a way of measuring the resource security for orbits and potential\nenvironmental impacts occurring in case of collision.\nSuggested Reviewers:\nOpposed Reviewers:\nPowered by Editorial Manager® and ProduXion Manager® from Aries Systems CorporationSpace debris through the prism of the environmental performance of space systems : \nthe case of Sentinel -3 redesigned mission  \nThibaut Maury (1), Sara Morales Serrano (2), Philippe Loubet (1), Guido Sonnemann (1), Camilla Colombo (3), \nand Luisa Innocenti (4) \n(1) Univ . Bordeaux, CNRS, Bordeaux INP, ISM, UMR 5255, F -33400 Talence (France)   \n(2) European Space Agency, ESTEC, Keplerlaan 1, NL -2201 AZ Noordwijk (The Netherlands)  \n(3) Politecnico di Milano, Department of Aerospace Science and Technology, I -20156 Milan (Italy)  \n(4) European Space Agency, HQ, Rue Mario Nikis 8 -10, F -75015 Paris (France)  \nABSTRACT  \nLike any industry, space activities generate  pressures on the environment and strives towards more sustainable \nactivities. A consensus among the  European  industrial stakeholders and national agencies in the Space sector is \nemerging on the need to address eco -design through the prism of the environme ntal Life Cycle Assessment (LCA) \nmethodology.  While the  use of  LCA is being implemented  within the sector, the current scope disregards the potential  \nenvironmental impact in term of debris generated by space missions on the orbital environment. The paper h ighlight s \nthe relevanc e of applying LCA holistically  during the design phase of space systems , considerin g potential impacts  \noccurring in  the orbital environment during  the utilisation and disposal  stage s of a space mission . Based on the \ncomparison of two mission designs, the aim is to  consider potential emission of space debris into the  LCA framework \nas a way of measuring the resource security for orbits and potential environmental impacts occurring in case of \ncollision.  \n1 INTRODUCTION  \n1.1 Environmental life cycle assessment (LCA) for s pace missions  \nAs highlighted during the COP21 in Paris, of the 50 essential variables used to assess Earth's climate, 26 are monitored  \nfrom satellite observations [1]. Though, r egarding environmental legislation, the industrial stakeholders o f the space \nsector are not targeted by specific international binding commitment s. Nevertheless, environmental performance has \nbecome a criterion in purchasing decisions.  \nConsidering guidelines for the evaluation of environmental impacts  of space activitie s, several actors of or related to \nthe European space industry , such as the European Space Agency (ESA) and its associated  ‘Clean Space initiative’ \nor ArianeGroup,  have identified  environmental Life Cycle A ssessment  (LCA), according to ISO14040/44 , as the  most \nappropriate methodology to measure and minimise  their environmental impact  [2]. Since the last three decades , the \nenvironmental LCA methodology is considered as a relevant methodology to support decision -makers in the \nevaluation of the environmental impacts linked to the design, manufacturing, transporting, and disposing of the goods \nand services [3]. It compiles and evaluates the inputs, outputs and the potential environmental impacts of a product \nsystem throughout its life cycle. As a multi -criteria methodology, LCA studies avoid the ‘burden -shifting pollution’ \nwhich consists of transferring impact from an environmental impact category to another, or from a life cycle stage to \nanother. LCA shows how a specific functionality can be achieved  in the most environmentally friendly way among a \npredefined list of alternatives, or in which parts of the life cycle it is particularly important to improve a product to \nreduce its environmental impacts.  \nContractual requirements have been placed in ESA’s funded projects to take into account their environmental impacts \nand promote the development of ‘green ’ technologies through the implementation of the Life Cycle Assessment (LCA) \nmethodology.  In particular, a n LCA study of the new Ariane 6 launch er in exploitation phase is currently performed  \n[4]. A requirement to perform such assessment from early phases  of the project has also been  included for  two \ncompeting Earth Explorer 9 concepts (i.e. FORUM and SKIM) as well as for the standard platf orm for all future \nSentinel missions in the frame of the Copernicus program.  \nHowever, s pace systems deal with a strong particularity, which  adds new aspects regarding the scope of the LCA \nframework (see  Figure 1). Rocket launches are the only human activity that crosses all segments of the atmosphere \nand stays “out” of the natural environment . Environmental impacts of space systems could occur  not only in the \nconventional Earth environment  but also in the outer space, further referred to  as the orbital environment . Manuscript File \nFigure 1. Life cycle of a space mission . Potential impact regarding debris proliferation in the orbital environment is \nhighlighted.  \nConsequently, the scope of environmental LCA for space missions should be extended to cover the phases dealing \nwith in -orbit operations, i.e. the use phase and the end-of-life (EoL)  including post -mission disposal  of the space \nmissions.  By analogy to conventional environmental impacts, the potential generation of fragments in the orbital \nenvironment  is considered as  an emission of an environmental stressor damaging the orbital resource. In this way, the \npotential impacts occu rring in the orbital environmental  should be assessed  to provide a holistic analysis of space \nmissions.  \n1.2 Objectives  and outline of the paper  \nThe objective of this paper is to demonstrate the relevance of integrating space -debris related impacts in the \nmethodological framework of environmental LCA for space mission s (see Figure 2). For such purpose, the LCA space \ndebris indicator previously developed  by Maury et al.  [5] in compliance with  the LCA standards  [6] will be described \nand applied to a theoretical case study.  For the first time  [2], we propose to merge  this approach dealing with  space \ndebris related impact s and the conventional  Earth environmental  impacts  computed in LCA  (such as climate change, \ntoxicity, etc.) for a space mission.  \nThe chosen study  case is the space mission of Sentinel -3 from the ESA, as its LCA has already been carried out in the \nframe of an eco -design study called ‘GreenSat’ [7–9]. In this study, two design options concerning the choice of \npropellant were assessed: the use of (i) Hydrazine and then, (ii) LMP -103s as an eco-design measure  [10]. This \nalternative solution would reduce obsolescence risks (due to the European REAC H regulation) and could reduce the \ntime the satellite needs to spend in the clean rooms. The LCA showed that the use of alternative LMP -103s could also \ndecrease  conventional  environmental impacts such as climate change. Furthermore,  the choice of propellant , due to \nits higher theoretical efficiency,  also implies  the possibility of re -entry of the spacecraft at the  end-of-life life (with \nthe same size of propellant tanks)  and therefore its  associated  exposure to space debris. Thus,  it is considered as an \nappropriate study case to include space debris related impacts in the LCA.  \nAfter briefly presenting  the LCA structure, the goal & scope of the study is defined with a particula r focus on the two \nscenarios described in the ‘GreenSat’ study. Then, the inventory data collection and the impact assessment methods, \nincluding the space debris related impact  used in the frame of this study are presented . Based on the previous system \ndefinition and calculation steps , the results obtained are exposed . Finally, the main outcomes,  challenges and \nopportunities regarding the environmental impact assessment of  space mission s are exposed and discussed.  \n2 MATERIAL & METHODS  \nLCA is carried out in four distinct phases as defined in ISO  14040/44 . It starts with an explicit description of (i) the \ngoal and scope  of the study before providing (ii) an inventory of flows  from and to the environment for a product \nsystem during the life cycle inventory (LCI ) phase. Inventory analysis is followed by  (iii) impact  assessment  during \nthe life cycle impact assessment phase  (LCIA). This phase of LCA aims at evaluating the significance of potential \nimpacts based on LCI flow results. Finally, (iv) the interpretation phase  is based on the identification of the significant \nissues, limitations, and recommendations and shall be integrated systematically at each step of the LCA study.  \n2.1 Goal & Scope of the LCA study  \nGoal of the study. The goal of the study is to compare the environmental impacts, including space debris related \nimpacts related to two design alternatives of the Sentinel -3B mission.  \n \nFigure 2 - Life cycle impact assessment framework. A new impact category is added for LCA of space mission, \ncorresponding to the potential generation of fragments in the orbital environment after a break -up (only collision risk \nis considered) . \nScope of the study . The original Sentinel -3 LCA study was performed following requirements of ESA Space system \nLCA guidelines  [11] which require to consider all segment s related to a space mission . Here , we only focus our \nanalysis on space & ground segments  as presented  in Figure 3.  \nThe launcher  represents around 99% and the spacecraft 1% of the mass of a complete space mission. Therefore , the \nenvironmental impacts related to the launcher (production, launch campaign, launch event) is the major hotspot  [12]. \nHowever, in a first approach, we focus on the design, production, utilisation  and disposal p hases of the spacecraft \nonly because the ecodesign options are exclusively targetting specific design choices related to the spacecraft. The \npotential mass increase due to design changes is considered as  negligible and do not influence the launcher product \nsystem . The infrastructures are also disregarded  because the ecodesign practices related to  this segment are more in \nline with facility management optimisation , for instance , site management actions  regarding the utilities,  than \nassessment of space system s design  and manufacturing processes.   \nConsidering  the utilisation  phase, the nominal operational lifetime  for Sentinel -3 is expected to be 7,5 years. However,  \nthe amount of embedded propellant reaches 120 kg of hydrazine  and can cover an extended timespan, ensuring on -\norbit operation for 12,5 years. The expected Post-Mission Disposal  (PMD) scenario is an uncontrolled re -entry thanks \nto the decrease of the perigee.   \nDescription of the scenarios . In the fr ame of the GreenSat project, the replacement of Hydrazine propellant embedded \nin Sentinel -3B was identified as a potential ecodesign improvement.  Due to its high human toxicity potential, the \nhydrazine is currently targeted by the European REAC H regulation  on chemical substances . It implies strong safety \nmeasures during handling and loading of the propellant into the spacecraft as well as the cleaning operations . The \nauthors of the GreenSat study suggested replacing  Hydrazine (N 2H4) propellant by LMP -103s  (composition:  ~ 60% \nof Ammonium di-Nitrimide , ~20% Methanol, ~6% of Ammonia  as well as water  for mass balance ). In term of \nperformance, LMP -103s features a 6% higher  specific impulse  (Isp) than Hydrazine (235s vs 220s) and a 24% higher \ndensity in liquid for m (1.24kg/L vs 1kg/L), meaning that more propellant can be stored within the same tank volume . \nTherefore , we define the following scenarios:  \n- Baseline  scenario : use of Hydrazine as propellant  \n- GreenSat redesigned scenario : use of LMP -103s as propellant  \n \nFigure 3. System boundaries of a space mission  according to the specific segments of a space mission based on [13]. \nFor this specific study on Sentinel -3B, only Space & Ground segment were included in the scope. In the space segment, \ntwo different propellants are studied (Hydrazine vs LMP).  Also, t he disposal phase of the spacecraft is further explored \nin this paper.  \nWe analyse the delta -V budget needed to performed a 12,5 -year mission followed by potential  PMD manoeuvres  to \nbe considered during the disposal stage . The same approach was already proposed in [5,14]  for Sentinel -1A spacecraft. \nWe consider  equivalent  characteristics for both scenarios described above  apart from the propellant used.  It should be \nnoted that we assume a negligible mass change for the propulsion system as stated in [10]. The potential PMD  options \nare proposed hereafter in Table 1 based on the OSCAR tool simulation s of the ESA -DRAMA v3 software  [15]. \nTable 1. Potential post-mission disposal  options to be considered for Sentinel -3 spacecraft  \nSentinel -3 Tot. ∆V  \nbudget (m/s)  ∆V  \n12,5-year \nmission (m/s)  ∆V for direct  \ndeorbiting (m/s)  Budget for delayed \nreentry < 25  years \n(m/s)  Remaining ∆V for final \nboost (controlled \nreentry)  \nBaseline  \n120 kg Hydrazine  238 \n200 203 \n(too high for  \nboth scenarios)  77 \n(too high for  \nthe baseline sc.)  0 \nAlternative  \n165 kg LMP -103s 342 65  \n(not enough for full  \ncontrolled reentry)  \nFunctional unit . The functional unit corresponding to space missions ha s been debated since the beginning of the \nLCA application to the space sector. Most of the time, a space mission is designed for a unique purpose (Earth \nobservation, telecommunications, science). For this reason, obtaining a functional unit which enables th e comparison \nbased on the ‘function’ of a satellite is challenging guidelines [11]. To compare the environmental impacts of both \nspace systems including manufacturing, operational lifetime in orbit (i.e. use phase) and PMD phase, the following \nfunctional unit was chosen: “Complete a 12,5 -year mission for the Sentinel -3 spacecraf t including  the deorbit  from \nits operational orbit (inc=98°, h=800 km) to the upper part of the atmosphere (h=120 km)”.  \n2.2 Life cycle i nventory  (LCI)  data collection  \nConventional LC I. The ‘GreenSat’ study is based on a previous  ESA -funded  LCA study  performed by Deloitte  [8] \nfor which four iterations  in terms of  industrial data collection  were cond ucted . A fifth iteration was performed in the \nframe of ‘GreenSat’ by Thales Alenia Space (as prime contractor) and Deloitte to complete and improve the inventory \nmodel with specific industrial data. Also, the recent development and use of the ESA space -specific LCI database \n[16,17]  brings an added -value, particularly dealing with propellant manufact uring.  To answer the functional unit,  the \nfollowing propellant quantit ies (based on  Table 1), are needed: (i) Baseline scenario, use of all the remainin g hydrazine \ncorresponding to 17 kg; (ii) GreenSat redesigned scenario, use of around 35kg of LMP -103s to perform a 25 -year \nreentry.  \nLCI parameters  for ‘degradative  use of orbits ’. The inventory  corresponds to the orbital surface occupied  by the \nproduct system under study (i.e. the spacecraft) multiplied by its respective on -orbit lifetime, expressed in m2·years . \nIn addition, the launch mass of the spacecraft shall be considered. The inventory variables  which  correspond to \ndesign parameters  are given by the  Eq. 1.  \n𝐼𝑛𝑣𝑒𝑛𝑡𝑜𝑟𝑦 =𝐴𝑐∙𝑀∙∑ 𝑡𝑖\n𝑂𝑟𝑏𝑖𝑡𝑠           [𝑚2∙𝑘𝑔∙𝑦𝑟𝑠] Eq. 1 \n𝐴𝑐 is the average cross -section area of the S/C . M is the launch mass  of the spacecraft (in kg). The dwelling time in \norbit 𝑡𝑖 is mainly dependent on the mission lifetime and the area -to-mass ratio which allows quantifying the effect of \norbital mec hanics perturbations. Besides , the mass of the  spacecraft is also the main parameter involved in the \ncalculation of the number of debris generated when a break -up occur s. ∑ (𝑡𝑖) 𝑂𝑟𝑏𝑖𝑡𝑠  expresses the sum of the dwelling \ntime into each orbital cell i crossed by the trajectory of the spacecraft. This on -orbit lifetime covers the nominal time \nof the mission (use stage ) plus the post -mission disposal duration representing the End -of-Life (EoL) phase.  \nThe trajectories for the mission lifetime and the  theoretical PMD scenarios , as described in Table 1, are propagated \nthanks to the OSCAR tool of the ESA -DRAMA software v3 [15]. The results are shown in Figure 4. \n \nFigure 4. Semi -major axis of the spacecraft during the operational time of the mission and potential  post-mission \ndisposal  obtained using the ESA -DRAMA software [15]. The delta -V values  previously calculated  for the PMD \nmanoeuvres  are indi cated.  The direct re -entry scenario is a discarded option because of the delta -V budget required.  \n2.3 Conventional life cycle impact assessment (LCIA): selected indicators  \nThe selected LCIA indicators for ESA funded stud ies are broadly described  in the  ESA  Handbook  [11]. Most of them  \nare based  on the ILCD 2011 recommended methods  [18]. Due to confidentiality aspects , we only present in this paper \nthe relative impact contributions of the processes for the five indicators listed hereafter:  \n Global Warming Potential  (in tonnes CO 2 eq.) retrieved from CML2002 impact method  [19] and based on \nthe green house gas quantification method proposed by the Intergovernmental Panel on Climate Change [20]. \n Freshwater Aquatic Ecotoxicity Potential  (in disappeared fraction of species )  and Human Toxicity Potential  \n(in disability -adjusted life years ) which quantif y the severity of observed effects of substances emitted  \nrespectively in an ecosystem  integrated over area and time  and, on human health , giving more weight  to \ndeath and irreversible problems than to reversible temporary problems (e.g. a skin or  respiratory irritation ). \nBoth indicator s are based on the USEtox consensus model [21].  \n Fossil Resource s Depletion Potential  (in gi ga-joules of  fossil  energy consumed)  and Mineral Resource s \nDepletion Potential  (in kg Sb eq.) represent  the Abiotic Depletion Potential  of the CML2002 method  [19]. \nThis indicator  charac teris es the depletion of energetic and mineral resources based on the extraction \nrate and the reserve base estimates . \nIt should be noted that t he Ozone Depletion Potential (ODP) is a key indicator in the space sector as the space  activities \nare the  only ones responsible for direct emissions within the ozone layer.  Previous ESA LCA stud ies [12] showed that  \nthe launch event (which is part of the launch segment) is by far the main contributor  on this indicator , i.e. close to \n100%, of the impact for a complete space mission scope . Since the launch segment is not in cluded in  the scope of this \npaper , we choose to disregard this impact category.  \n2.4 Specific LCIA indicator for ‘degradative use of the orbital resource ’  \nThe methodology associated with the development of the LC IA indicator for space debris is widely addressed in \n[5,22] . We propose here to present  the equations leading to the  characteri sation  factors (CFs)  presented in Figure 6. \nThese CFs express the potential generation of space debris from a spacecraft, and are discreti sed for each orbita l cell. \nCFs are  computed as the product between (i) the  exposure factor (XF) that depicts the risk that the spacecraft \nencounters a space debris, (ii)  a 𝛼 coefficient  that depicts the number of debris generated by a collision event  \ndepending of the mass of spacecraft and (iii) the  severity factor (SF)  that depicts the fate of the potential fragments \ngenerated  (Eq. 2).  \n𝐶𝐹𝑖= 𝑋𝐹𝑖  α  𝑆𝐹𝑖       [potential fragments ∙years ∙kg-1] Eq. 2 \nExposure factor (XF). We define the exposure factor  (𝑋𝐹 𝑖) in Eq. 3 as the average flux of space debris passing \nthrough a targeted circular orbit i of the LEO region for one year.  The orbital stress  caused by space debris should be \nassessed for the LEO region to obtain s patially differentiated  factors  since  each orbit presents a different state which \nallows to classify and differentiate them accordingly.  It is done  by computing the flux of the catalogued objects in \neach LEO orbits as done in previous studies [23–25]. It represents the background population (i.e. explosion and \ncollision fragments, rocket bodies, dead and active spacecraft , etc.). \n𝑋𝐹𝑖=𝜙ℎ,𝑖𝑛𝑐,𝑡                     [#.𝑚−2.𝑦𝑟−1] Eq. 3  \nwhere  𝑋𝐹 is the exposure factor for particular circular orbit i; 𝜙 is the  relative flux of catalogued  particles provided \nby the ESA ’s reference model MAS TER -2009 at a given altitude (h), inclination (inc) and interval of time (t) averaged \non a 35 -year period  based on a ‘business -as-usual’ perspective . The 35 -year period has been chosen with the aim of \ncovering the orbital lifetime of a satellite completing a 10 -year mission and a 25 -year Post -Mission Disposal as \nrequired by the international standard [26]. All debris wh ose size is higher than 1 cm is accounted for . However, the \nsize of the debris taken into account during the mission lifetime  (i.e. util isation phase ) is only 1 𝑐𝑚<𝑋𝐹 𝑖,𝑚𝑖𝑠𝑠𝑖𝑜𝑛 <\n10 𝑐𝑚 representing non -trackable objects for which collision avoidance maneuver is not possible . \nAlpha coefficient  (α). Using  the NASA break -up model [27,28] , it is possible to express the number of fragments \n>10 cm generated from a collision per kg of spacecraft depending on  the initial launch mass. The relation is presented \nin Figure 5. Since the number of fragments/kg depends on the mass of the spacecraft, we propose to consider a fixed \nratio ( 𝛂 coefficient) for a given class of spacecraft  (between  1.000 and 1.500 kg as for Sentinel -3): 0.86 fragments/kg.  \n \nFigure 5. Fragments >  10 cm released per kg of spacecraft according to the mass of the spacecraft using the NASA \nbreak -up model [27,28] . For sentinel -3 the class [1.000; 1.500] is chosen.  \nSeverity factor (SF). According to [29], the percentage of fragments > 10 cm released at an altitude h (km) and still \non-orbit after a given time t (yrs) follows the  Eq. 4: 00.20.40.60.811.21.41.61.8\n0 500 1000 1500 2000 2500Fragments / kgspacecraft\nMass of the spacecraft (kg)      𝛂 [1.000; 1.500] kg  ~ 0,86 fragments / kg  𝑃(𝑡,ℎ)=𝑒− 𝑡\n128 .3−0.585892 ∙ℎ+0.00067 ∙ℎ2              [%]  Eq. 4 \nWhere 𝑃 is the percentage of fragments (in %) still in orbit after a period 𝑡 (in years) and h is the initial altitude of \nrelease (in km).  \nThe cumulative residence time of debris  into orbits is obtained by the integral of 𝑃(𝑡,ℎ) over a given interval of time. \nHere, we choose the following time interval: [0:200] yrs.  The polynomial part of the Eq. 4 is later expressed as  𝜏 and \ncan be considered as a constant in the integral  which is only time -dependent. Thus, the severity factor  (𝑆𝐹𝑖) for a \nbreak -up occurring  in given orbit  i, is given in  Eq. 5. It represents the cumulative survivability of a fragment with \nrespect to its altitude of emission expressed in ye ars. \n𝑆𝐹𝑖,200  𝑦𝑟𝑠=∫ 𝑒− 𝑡\n𝜏= 200 𝑦𝑟𝑠\n0 𝑦𝑟[−𝝉∙𝒆− 𝒕\n𝝉]\n 𝟎 𝒚𝒓𝒔 𝟐𝟎𝟎  𝒚𝒓𝒔\n [years]   \nEq. 5 \nCFs and computation of i mpact score (IS). Combining Eq. 3, Eq. 5 and the alpha coefficient, we obtain the CFs \ncalculated for a given circular orbit i and presented  in Figure 6. \nFor a specific spacecraft (i.e. product system) with a defined mission and PMD scenario (see Figure 4 & Eq. 1), we \nobtain the following impact score ( IS) presented in Eq. 6:    \n𝐼𝑆𝑚𝑖𝑠𝑠𝑖𝑜𝑛 =𝐴𝑐∙𝑀∙∑ 𝑡𝑖∙𝐶𝐹𝑖  𝑜𝑟𝑏𝑖𝑡𝑠\n𝑖         [potential fragments ∙years]  Eq. 6 \n \nFigure 6. Potential fragments -year per kg generated as characterisation factors for the LCA indicator . The value of \n𝜶 equals 0,8 6 fragments·kg-1 as the interval [1. 000-1.500 kg] is considered for the spacecraft.  Calculations are made  \nfor each of the 3330 discretised circular orbits . \n3 RESULTS  \n3.1 Baseline Sentinel -3 environmental  profile  \nThe relative contribution of the different stages under study are presented in Figure 7. The detailed definition, \nqualification and production stage contributes by around 50% for  most of the impact categories , except mineral \nresource depletion potential (~100 %) and space debris impact category (0%).  Within the design and production phase,  \nthe office work  (i.e. electricity and natural gas consumption) is  by far the major  contributor (>50% for all the  impact  \ncategories ) except  for the mineral resource  depletion driven by the materials used for satellites. This score (~100%) \nis lead by the production of the PV  system  (for solar cells ) using Germanium which is the most important \nPotential fragments·years·kg-1\n semiconductor material, in weight, of a spacecraft. Assembly, Integration and Testing ( AIT) accounts for around 20% \nof the global warming potential within the production stage. T he scores related to the toxicity/ecotoxicity categories  \nare driven by the production of the electronic components embedded in the payload that account for around 30 %.  \nThe utilisation stage  of the spacecraft  contributes to 25 to 35%% of the environmental impacts for the overall \ncategories including space debris generation potential . During this phase, the electricity consumption of the data \ncentre and servers are majors contributors  for the global warming potential, toxicity and ecotoxicity potential as well \nas fossil resource depletion.  \nOthers life cycle steps  (i.e. phases A+B, E1a and F ) have a low contribution to the conventional LCA impact \ncategories . Electricity consumption for office work  (related to R&D)  during pre -design and design stages has a rather \nlow contribution to the environmental indicators, mainly because of the highly decarbonised electricity mix in France \nand for the ESA Technical cent re (where 100%  of renewable electricity is used) . It is also the case for the Phase E1a , \nwhich includes  launch and early orbit phase  (LEOP)  and commissioning  with activities at ESA Operation Center \n(100% renewable electricity  too).  \nFinally, the disposal phase  contribution equal s to zero on all conventional indicators because of the lack of model \nrelated to the emissions  on the Earth surface and  into the atmosphere during the spacecraft  re-entry . \nRegarding the space debris related impacts (i.e. relative sc ore on degradative use of orbits), 36% of the impact comes \nfrom the utilisation  phase . The potential annual impact during the operational phase is more important than the \ndisposal phase . It is mainly due to the coordinate s of the operational orbit (i.e. SS O, 800 km, 98°) for which  the \nexposure factor  faces one the highest value in the LEO  region , even considering  only 1cm< debris size<10cm  to take \ninto account collision avoidance manoeuvres for bigger debris . Also, the lack of efficient atmospheric drag  at such \naltitude gives a high value in term of severity factor . Nevertheless, the small amount of propellant available at the EoL \nof the mission (17 kg) does not allow to target a 25 -years reentry. As a consequence, t he long duration of the disposal \nphase (aro und 89 years) compared to the initial  lifetime of the mission results in a higher score (64%)  of the disposal \nphase  even if the exposure & severity factors  of the orbits crossed during the PMD are lower.  \n \nFigure 7. Environmental profile of the Sentinel -3 mission , including the relative contributions of utilisation and \ndisposal phases in term of space debris potential. Impact scores for conventional categories are retrieved from [10].  \n3.2 Comparison of the impact score for the baseline and ‘GreenSat’ consi dering  the ‘degradative \nuse’  of the orbital resource  \nFigure 8 shows the potential number of fragments years  generated by  both scenarios . We observe that the impact is \n2.3 times higher for the baseline scenario which considers 120 kg of hydrazine than wit h the redesigned ‘GreenSat’ 0%20%40%60%80%100%\nGlobal warming\npotentialHuman toxicity\npotentialFreshwater\naquatic\necotoxicity\npotentialFossil resources\ndepletion\npotentialMineral\nresources\ndepletion\npotentialPotential\ndegradative use\nof the orbital\nresource\nA + B - FEASIBILITY + PRELIMINARY DEFINITION\nC + D - DETAILED DEFINITION + QUALIFICATION AND PRODUCTION\nE1a - LAUNCH AND COMMISSIONING - Spacecraft-related activities\nE2 - UTILISATION PHASE\nF - DISPOSAL PHASEscenarios using 165 kg of LMP -103s. Alternatively, we also compute the probability of collision only considering the \nrelevant XFs. We obtained the following probabilit ies of collision  (conservative approach) : 6,6%, 10,5% and 27, 4%, \nrespectively for a 12,5 -year mission with direct deorbiting (discarded option due to the delta -V budget), with a delayed \nreentry in less than 25 years (i.e. ‘GreenSat’) and with a lowering perigee manoeuvre  with remaining propellant  (i.e. \nBaseline scen ario).  Besides, we notice that , in the case of GreenSat , the contribution of the utilisation phase \ncorresponds to around 87% of the overall impact (versus 36% previously).  It is due to a shorter PMD lifetime in the \norbital environment  while the operational  time remains the same.  \n \nFigure 8. Comparison of the impact scores obtained for the baseline (i.e. PMD duration 89 years) and the ‘GreenSat’ \nscenarios (i.e. PMD <25 years ). The difference between both “E2 – Utilisation phase”  comes from the launch mass \nincreased in the ‘GreenSat’ scenario (1195 kg vs 1150  kg for the baseline scenario with hydrazine).  \n3.3 Global comparison of the LCA results  \nWhile the replacement of hydrazine by LMP -103s significantly reduces the  potential impact on the orbital \nenvironmental, the scores obtained for conventional environmental impacts in both scenarios  are retrieved from [10]  \nand compared in Figure 9. The results show that no significant burden shifting  (i.e.transferring environmental impact \nfrom one category to another) occurs.  Even if the impacts related manufacturing stage is higher in the case of the new \nLMP -103s propellant (due to the larger  amount of propellant  by +32%), the difference  when  considering the full scope \nof the study is comp ensated mainly by a shortened launch preparation  into clean rooms.  It stems from more limited \nsafet y me asures  regarding ‘GreenSat’ scenario  during the loading propellant operations and cleaning processes. \nIndeed, hydrazin e is targeted by the REAC H regulation, which is not the case for the LMP -103s. \n \nFigure 9. Comparative environmental impacts of Sentinel -3B at mission level for Baseline and ‘GreenSat scenario’. \nThe figure highlights the absence of a noticeable ‘burden shifting ’ between the impact categories.  Hence, the GreenSat \nscenario can be considered as a r elevant ecodesign improvement.  0.00E+002.00E+034.00E+036.00E+038.00E+031.00E+041.20E+041.40E+041.60E+04\nBaseline Sentinel-3B 'GreenSat' scenarioPotential fragments -yearsE2 - Utilisation phase\nF - Disposal phase\n0%10%20%30%40%50%60%70%80%90%100%\nBaseline\nGreensat\nBaseline\nGreensat\nBaseline\nGreensat\nBaseline\nGreensat\nBaseline\nGreensat\nBaseline\nGreensat\nGlobal Warming\nPotentialHuman Toxicity\nPotentialFreshwater Aquatic\nEcotoxicity\nPotentialFossil Resources\nDepletion PotentialMineral Resources\nDepletion PotentialSpace Debris\nPotential- 56.7% 4 CONCLUSION & PERSPECTIVES  \nThis LCA study  demonstrate s the added value of  holistically address ing the environmental performance of space \nmission, i.e. not only in term of conventional environmental impacts but also assessing the potential impact on the \norbital environment. Based on the previous results obtained in the frame of the ‘GreenSat’ ecodesign study and the \nscores computed for the potential degr adative use  of the orbital environment, the replacement of the Hydrazine by \nLMP -103s propellant appears as a relevant ecodesign improvement for the sentinel -3 mission. The full environmental \nprofile  computed  represents a relevant input to be used by decisi on-makers at the early design stage of space mission.  \nHaving more propellant dedicated to the PMD would  also help the satellite to initiate a controlled re -entry  aiming to \ncomply with the casualty risk threshold required by the  space debris mitigation sta ndards. Nevertheless,  additional \ninvestigations should be carried out for a detailed comparison of both propulsion systems.  Sentinel -3 spacecraft was \ndesigned with  “light” 1N hydrazine thrusters, while LMP -103s propulsion system could require n ew thruster  \nequipment and addition of a re -pressurisation module to cope with the needs of a controlled re -entry .  \nGoing further, relevant mission parameters could be taken into account to perform a comprehensive multi -criteria \noptimisation when designing space missi ons and the associated spacecraft . For instance, minimum  performance \nthreshold s should be considered when defining the functional unit of the system. It is particularly the case for the \nreliability rate (e.g.  90% reliability at the end of  the mission) as it could greatly  influence the use phase  and the EoL \ndurations  (e.g. when mission extensions are decided)  [30]. It is also the case for  the casualty  risk related to the \natmospheric re -entry  as mentioned above.  \nFinally, as stated by Donella Meadows [31]: “indicators arise from values (we measure what we care about), and they  \ncreate values (we care about what we  measure)”.  The need of consistent metrics for space sustainability assessment  \nis a raising concern among  the aerospace community as demonstrated by the on -going work of Letizia et a l. [32]. In \nthis context, the LCA methodology should be considered as a starting point guidin g ecodesign  effort for space systems \nin a broader space sustainability perspective.  \n5 ACKNOWLEDGEMENT S \nThe authors acknowledge the support of the French National Association for Technical Research (CIFRE Convention \n2015/1269). The research on the LCA  indicat or for  space debris was also cofounded by the R&T department of the \nArianeGroup in the frame of the Eco -space project.  \nWe also acknowledge the project COMPASS “Control for Orbit Manoeuvring through Perturbations for Application \nto Space Systems” (Grant agr eement No 679086) funded by the European Research Council (ERC) under the \nEuropean Union Horizon 2020 research and innovation programme.  \nThe authors are grateful of the work previously performed by Th ales Alenia Space and Deloitte C onsulting in the \nframe o f the ESA funded project ‘GreenSat’  (ESA -TEC -SOW -003752 ). \n6 REFERENCES  \n[1] CEOS, ESA, Satellite earth observations in support of climate information challenges, 2015. \nhttp://eohandbook.com/cop21/files/CEOS_EOHB_2015_COP21.pdf.  \n[2] T. Maury, P. Loubet, S.M. Serrano, A. Gallice, T. Maury, P. Loubet, S.M. Serrano, A. Gallice, Application \nof environmental Life Cycle Assessment ( LCA ) within the space sector: a state of the  art, Acta Astronaut. \n(2020). https://doi.org/10.1016/j.actaastro.2020.01.035.  \n[3] J. Guinée, R. Heijungs, G. Huppes, A. Zamagni, P. Masoni, R. Buonamici, T. Ekvall, T. Rydberg, Life cycle \nassessment: past, present, and future., Environ. Sci. Technol. 45 ( 2011) 90 –96. \nhttps://doi.org/10.1021/es101316v.  \n[4] A. Gallice, T. Maury, E. Olmo, Environmental impact of the exploitation of the Ariane 6 launcher system, \nin: Clean Sp. Ind. Days, ESA - Clean Space, ESTEC, Noordwijk, 2018. \nhttps://indico.esa.int/event/23 4/contributions/3918/attachments/3115/3826/2018CSID_AGallice_Environme\nntalLifeCycleImpactAnalysisOfA6ExploitationPhase.pdf.  \n[5] T. Maury, P. Loubet, M. Trisolini, A. Gallice, G. Sonnemann, C. Colombo, Assessing the impact of space \ndebris on orbital resourc e in life cycle assessment: A proposed method and case study, Sci. Total Environ. \n(2019). https://doi.org/10.1016/j.scitotenv.2019.02.438.  [6] T. Maury, P. Loubet, J. Ouziel, M. Saint -Amand, L. Dariol, G. Sonnemann, Towards the integration of \norbital space  use in Life Cycle Impact Assessment, Sci. Total Environ. 595 (2017) 642 –650. \nhttps://doi.org/doi.org/10.1016/j.scitotenv.2017.04.008.  \n[7] ESA -ESTEC, Statement of Work - Greensat, Noordwijk, The Netherlands, 2016.  \n[8] N. Thiry, A. Chanoine, Lessons learned  from the Sentinel 3 LCA and Applications to a GreenSat, in: Clean \nSp. Ind. Days, ESA - Clean Space, Noordwijk, The Netherlands, 2017. \nhttps://indico.esa.int/indico/event/181/session/3/contribution/70.  \n[9] N. Thiry, A. Chanoine, P. Duvernois, J. Bitar, Gre enSat, in: Clean Sp. Ind. Days, ESA - Clean Space, \nESTEC, Noordwijk, 2018. \nhttps://indico.esa.int/event/234/contributions/4024/attachments/3075/3779/CSID2018_NThiry_GreenSat.pdf\n. \n[10] N. Thiry, P. Duvernois, F. Colin, A. Chanoine, GreenSat : Final Report, 2019.  \n[11] ESA LCA Working Group, Space system Life Cycle Assessment (LCA) guidelines, 2016.  \n[12] A. Chanoine, Environmental impacts of launchers and space missions, in: Clean Sp. Ind. Days, ESTEC, \nNoordwijk, 2017. https://doi.org/10.17226/11935.  \n[13] ECSS, ECSS -M-ST-10 – Space Project Management, Project Planning and Implementation, 2009.  \n[14] C. Colombo, F. Letizia, M. Trisolini, H.G. Lewis, A. Chanoine, P. Duvernois, J. Austin, S. Lemmens, Life \ncycle assessment indicator for space debris, in: 7th Eur . Conf. Sp. Debris, ESA Space Debris Office, \nDarmstadt, 2017.  \n[15] ESA’s Space Debris Office, Debris Risk Assessment and Mitigation Analysis (DRAMA) Software User \nManual, 2019. https://sdup.esoc.esa.int/drama/downloads/documentation/DRAMA -Software -User -\nManual.pdf.  \n[16] J.B. Pettersen, H. Bergsdal, E.J. Silva, M.M. Bjørnbet, M.A. Estrela, P.A. Chaves, D7 Space specific \nmaterials and processes – final report. ESA Contract No 4000111095/14/NL/GLC, 2015.  \n[17] J.B. Pettersen, E.J. Silva, H. Bergsdal, C. Solli, D 7 LCA of space propellants – final report. ESA Contract \nNo 4000112710/14/NL/GLC/as, 2016.  \n[18] European Commission - Joint Research Centre, Institute for Environment and Sustainability, International \nReference Life Cycle Data System (ILCD) Handbook - Recom mendations for Life Cycle Impact \nAssessment in the European Context, Publications Office of the European Union, 2011. \nhttps://doi.org/10.278/33030.  \n[19] J.B. Guinée, M. Gorée, R. Heijungs, G. Huppes, R. Kleijn, L. van Oers, A. Wegener Sleeswijk, S. Suh, H.  \nUdo de Haes, H. de Bruijn, R. van Duin, M. Huijbregts, Handbook on Life Cycle Assessment - Operational \nGuide to the ISO Standards, Kluwer Academic Publishers, Dordrecht, The Netherlands, 2002. \nhttp://www.springer.com/us/book/9781402002281.  \n[20] IPCC, Four th Assessment Report (AR4), Geneva, Switzerland, 2007. \nhttps://doi.org/10.1256/004316502320517344.  \n[21] R.K. Rosenbaum, T.M. Bachmann, L.S. Gold, M.A.J. Huijbregts, O. Jolliet, R. Juraske, A. Koehler, H.F. \nLarsen, M. MacLeod, M. Margni, T.E. Mckone, J. Pay et, M. Schuhmacher, D. Van De Meent, M.Z. \nHauschild, USEtox - The UNEP -SETAC toxicity model: Recommended characterisation factors for human \ntoxicity and freshwater ecotoxicity in life cycle impact assessment, Int. J. Life Cycle Assess. 13 (2008) 532 –\n546. h ttps://doi.org/10.1007/s11367 -008-0038 -4. \n[22] T. Maury, Consideration of space debris in the life cycle assessment framework, Université de Bordeaux, \n2019.  \n[23] C. Kebschull, J. Radtke, H. Krag, Deriving a priority list based on the environmental critical ity, Proc. 65th \nInt. Astronaut. Congr. (2014) 1 –9. \n[24] L. Anselmo, C. Pardini, Compliance of the Italian satellites in low Earth orbit with the end -of-life disposal guidelines for Space Debris Mitigation and ranking of their long -term criticality for the environment, in: \nActa Astronaut., Elsevier, 2015: pp. 93 –100. https://doi.org/10.1016/j.actaastro.2015.04.024.  \n[25] F. Letizia, C. Colombo, H.G. Lewis, H. Krag, Assessment of breakup severity on operational satellites, Adv. \nSp. Res. 58 (2016) 1255 –1274. ht tps://doi.org/10.1016/j.asr.2016.05.036.  \n[26] IADC, Inter -Agency Space Debris Coordination Committee, Space Debris Mitigation Guidelines, (2007) 1 –\n10. http://www.iadc -online.org/Documents/IADC -2002 -01, IADC Space Debris Guidelines, Revision 1.pdf.  \n[27] N.L. Johnson, P.H. Krisko, J.C. Liou, P. Anz -Meador, NASA’s new break -up model of EVOLVE 4.0, Adv. \nSp. Res. 28 (2001) 1377 –1384. https://doi.org/http://dx.doi.org/ 10.1016/S0273 -1177(01)00423 -9. \n[28] P. Krisko, Proper Implementation of the 1998 NASA Breakup M odel, Orbital Debris Q. News. 15 (2011) 1 –\n10. https://orbitaldebris.jsc.nasa.gov/quarterly -news/pdfs/odqnv15i4.pdf.  \n[29] H. Krag, F. Letizia, S. Lemmens, Space traffic management through the control of the space environment’s \ncapacity, in: 5th Eur. Work. S p. Debris Model. Remediat. CNES HQ, Paris, 2018.  \n[30] ESA, The role of the satellite reliability in successful missions and post mission disposal, Clean Sp. Blog. \n(2018).  \n[31] D. Meadows, Indicators and information systems for sustainable development, Hart land, VT, 1998. \nhttp://www.biomimicryguild.com/alumni/documents/download/Indicators_and_information_systems_for_su\nstainable_develoment.pdf.  \n[32] F. Letizia, S. Lemmens, B. Bastida Virgili, H. Krag, Application of a debris index for global evaluation of \nmitigation strategies, Acta Astronaut. 161 (2019) 348 –362. https://doi.org/10.1016/j.actaastro.2019.05.003.  \n ",
      "metadata": {
        "filename": "Space debris through the prism of the environmental performance of space systems.pdf",
        "hotspot_name": "PBT_Injection_Molding",
        "title": "Space debris through the prism of the environmental performance of space\n  systems: the case of Sentinel-3 redesigned mission",
        "published_date": "2022-07-13T15:58:25Z",
        "pdf_link": "http://arxiv.org/pdf/2207.06306v1",
        "query": "PBT injection molding LCA environmental impact"
      }
    },
    "The environmental value of transport infrastructure in the UK_ an EXIOBASE analy": {
      "full_text": "Engineering Sustainability\nThe environmental value of transport\ninfrastructure in the UK: an EXIOBASE analysis\nNikolaos Kalyviotis Dipl-Ing, MSc, MSPM, MBA, PhD\nAssistant Professor of Planning, Management and Evaluation of Programs,\nInvestments and Projects, Department of Planning and RegionalDevelopment, School of Engineering, University of Thessaly, Volos, Greece(Orcid: 0009-0006-5658-4262 ) (corresponding author: nkalyviotis@uth.gr)\nChristopher D. F. Rogers BSc, PhD, CEng, MICE, FCIHT, FISTT,\nSFHEA, Eur Ing\nProfessor Geotechnical Engineering, Department of Civil Engineering, University\nof Birmingham, Birmingham, UK (Orcid: 0000-0002-1693-1999 )Geoffrey J. D. Hewings BA, MA, PhD\nEmeritus Professor of Urban and Regional Planning, Economics and\nGeography and Regional Science, Regional Economics ApplicationsLaboratory, University of Illinois at Urbana–Champaign, Urbana, IL, USA(Orcid: 0000-0003-2560-3273 )\nFive life cycle assessment (LCA) methods to calculate a project ’s environmental value are described: ( a) process-based,\n(b) hybrid, ( c) pseudo, ( d) simplified, and ( e) parametric. This paper discusses in detail and compares the two methods\nwith the least inherent uncertainty: process-based LCA (a bottom-up methodology involving mapping and character-\nising all processes associated with all life cycle phases of a project) and a hybrid LCA (the EXIOBASE analysis, which\nincorporates top-down economic input –output analysis and is a wider sector-by-sector approach). The ‘bottom-up ’\nnature of process-based LCA, which quantifies the environmental impacts for each process in all life cycle phases of a\nproject, is particularly challenging when applied to the evaluation of infrastructure as a whole. Conversely, combining\nthe environmental impact information provided in EXIOBASE tables with the corresponding input –output tables\nallows decision makers to more straightforwardly choose to invest in infrastructure that supports positive environ-\nmental outcomes. Employing LCA and a bespoke model using Pearson ’s correlation coefficient to capture environ-\nmental interdependencies between the transport sector and the other four ‘economic infrastructures ’showed the\ntransport and energy sectors to be most closely linked. Both integrated planning and innovative technologies are\nneeded to radically reduce adverse environmental impacts and enhance sustainability across transport, waste, water,\nand communication sectors.\nKeywords: LCA/life cycle analysis/life cycle assessment/transport planning/UN SDG 8: Decent work and economic growth/UN SDG 9:\nIndustry, innovation and infrastructure/UN SDG 12: Responsible consumption and production\n1. Introduction\nModern society is wholly reliant on civil infrastructure, both tofacilitate civilised life and to do so while supporting a move to a\nmore sustainable, resilient, and liveable world —it is a critical de-\nterminant of how well society is progressing. The immediate polit-\nical perspective is that economic activities facilitated by energy,\nwater, waste, transportation, and communication infrastructures(which are indeed termed the ‘economic infrastructures ’) are\nessential for societal prosperity. However, it is also important to\nrecognise that the natural environment plays a crucial role in sup-porting these infrastructures and that it is vital for both economic\nprosperity and human life to flourish. Consequently, protection\nand enhancement of the natural environment should be prioritisedby governments and decision makers when advancing infrastruc-\nture engineering, which has the potential to deliver multiple forms\nof economic, social, and environmental value if systemicallydesigned with foresight in mind.To achieve this, it is important for decision makers to recognise\nthe diverse perspectives on environmental value: decisions should\nnot be based on a single definition or metric for the measurement\nof benefits. A further critical consideration for decision makers isthe context in which the assessment of the environmental value\noccurs. Properly defining the appropriate context will help deci-\nsion makers to understand the benefits and limitations of alterna-tive environmental value analysis methods. Here, it is helpful to\ndistinguish between environmental impacts (the outcomes deriv-\ning from design choices when implemented) and value (the com-ponents that feature in ‘business models ’, which balance all value\nrealised or compromised by a system intervention —a changed\npolicy, operational practice, a new construction project or what-ever; see Bouch et al., 2018\n). Business models should seek to cap-\nture all forms of value arising from a project: direct and indirect\neconomic, social, environmental, and cultural value (Rogers et al.,\n2023).\n1Cite this article\nKalyviotis N, Rogers CDF and Hewings GJD\nThe environmental value of transport infrastructure in the UK: an EXIOBASE analysis.Proceedings of the Institution of Civil Engineers –Engineering Sustainability,\nhttps://doi.org/10.1680/jensu.24.00023Research Article\nPaper 2400023Received 15/02/2024; Accepted 26/03/2025\nEmerald Publishing Limited: All rights reservedThis research addresses these issues and aims to broaden the con-\ntext in which environmental decisions are made. The scope of this\npaper includes a discussion of five life cycle assessment (LCA)\nmethods, with a detailed focus on the two methods with the leastuncertainty: process-based LCA and hybrid LCA (EXIOBASE).\nThe research reported herein applies these methods to the transport\ninfrastructure sector in the UK to provide a comprehensive sector-\nwide analysis while investigating the interdependencies between\nthe transport system and four other economic infrastructure sys-\ntems (energy, waste, water, and communication). The aim is to\nenhance sustainability through integrated planning.\nFirst, it defines environmental value in relation to civil infrastruc-\nture. It then provides a general discussion on environmental value\nand delves into the specific challenges associated with assessingthe environmental impact of civil infrastructures, moving from a\nbroad perspective to a detailed literature review. The research\naims to establish the most suitable method for assessing the envi-\nronmental impacts (hence value gained or lost) by comparing two\ntraditional assessment methods: process-based LCAs (e.g. see\nJones et al., 2017\n) and environmentally extended multi-regional\ninput –output (EE MRIO) tables, such as EXIOBASE 3 (Stadler\net al., 2018).\nThe starting hypothesis is that a hybrid LCA is a more effective\napproach for infrastructure and policy decision making, particu-larly when decisions are made at a higher level and a top-down\nanalysis is employed (e.g. focusing on the civil infrastructure of\nsociety as a whole rather than on specific infrastructure projects;\nsee Kalyviotis, 2022\n; Kalyviotis et al ., 2018). However, it is\nappreciated that each method has its own value for different appli-\ncations, and the benefits and limitations of each will be discussed\nin detail herein.\nInfrastructure is defined as a ‘large-scale physical resource made\nby humans for public consumption ’(Frischmann, 2012 :p .3 )\nessential for meeting the needs of people and ensuring the func-\ntionality of the economy. However, infrastructure projects usually\naffect ecosystems by altering, or even destroying, species ’habitats\n(Lederman and Wachs, 2014). These impacts are typically\naddressed on a project-by-project basis using tools such as the\nhabitat conservation plan (Lederman and Wachs, 2014). Habitat\nconservation plans are developed during the planning phase of an\ninfrastructure project, focusing on the effect of the constructionproject (Lederman and Wachs, 2014) rather than on the systemic\nimpacts of the infrastructure. At a systemic scale, new infrastruc-\nture will interact with existing infrastructures and the natural envi-\nronment, thereby amplifying the scope of its environmental\nimpacts.\nThe novelty of this research lies in its comprehensive comparative\nanalysis of process-based LCA and a hybrid LCA method utilising\nEXIOBASE. The study focuses on evaluating the uncertainties\nand the applicability of these methods to large-scale infrastructureprojects. Uniquely, it applies the EXIOBASE method to assess the\nenvironmental impact of the entire transport infrastructure sector\nin the UK rather than individual projects. In addition, a novel\nmodel employing Pearson ’s correlation coefficient has been devel-\noped to capture environmental interdependencies between thetransport system and other economic infrastructure systems. This\napproach is therefore innovative within the context of environ-\nmental impact assessment.\n2. Literature review\n2.1 Perspectives on environmental value\nAn important dimension of the literature on environmental valuerelates to the utility of nature. Environmental value has a multilat-\neral nature (Stamatopoulos, 2021\n). In the late 1990s, the notion\nemerged that environmental value is often not appreciated until itis disrupted or lost (Daily, 1997). For instance, the importance of\nforests in the hydrological cycle of an ecosystem became apparent\nonly after deforestation led to flash flooding, significant erosion,\nand other negative effects (Daily, 1997). These adverse effects\nimpact human, animal, and plant life within an ecosystem. This lit-\nerature highlights both the utility value and the environmental\nvalue lost when ecosystems are degraded due to human activity.More recent discussions concluded that environmental value\nencompasses the human ability to use nature kindly and responsi-\nbly, contributing equitably to quality of life for present and future\ngenerations through a cross-disciplinary collaboration between\nsocieties and human activities (Reser and Bentrupperbäumer,\n2005).\nCurrently, the debate on environmental value centres on whether it\nshould focus on the natural systems (e.g. nature, ecosystems, ani-mals, etc.) or on the benefits derived from these systems that sup-\nport growth and human life (Chan et al., 2016\n). These ideas relate\nto the tension between what is termed ‘strong and weak sustain-\nability practices ’in business and government. Strong sustainabil-\nity posits that humans are part of nature, fostering a partnership-\nstyle relationship, whereas weak sustainability suggests that\nhumans control nature, implying a relationship of human domi-nance over the environment (Landrum, 2018).\nAlthough both strong and weak sustainability theories aim to\nreduce negative impacts on the environment, decision makers\nexert a strong influence on the sustainability measures adopted.\nHow environmental values are viewed, understood, and incorpo-\nrated into decision making is influenced by societal perspectiveson natural systems. Chan et al. (2016)\nargue that the environment\nhas three dimensions of value: intrinsic, instrumental, and rela-\ntional. Intrinsic value refers to the value that the natural systems have\nfor the rest of the ecosystem, excluding ( ‘independent of people ’),\nwhile instrumental value is the value that ‘brings pleasure or sat-\nisfaction ’to people and relational value exists in the personal\nand social relationships that humans have with nature (Chanet al., 2016).Engineering Sustainability The environmental value of transport\ninfrastructure in the UK: an EXIOBASE analysisKalyviotis, Rogers and Hewings\n2Both strong and weak sustainability theories engage with the\ninstrumental value of natural systems (Landrum, 2018 ) since they\nextend to the delivery of ecosystem services (or satisfying needs).The weak sustainability model seeks to extract materials andresources from nature to a degree that minimises harm to the envi-ronment (Landrum, 2018), while the strong sustainability modelfocuses on extracting, recycling, and replenishing materials,resources, and energy in nature (Landrum, 2018). One challengewith sustainability theories is that they typically do not directly\nengage with the intrinsic and relational values of nature. The envi-\nronment has an inherent value that should be protected, independ-ent of human interaction or need (Gómez-Baggethun and Ruiz-Pérez, 2011). Culturally, spiritually, socially, and mentally,humans derive significant value from the environment in waysthat it is very challenging to quantify (Chan et al., 2016).\nHerein lies an important difference between economic and envi-\nronmental value; economic value can be transferred and replaced,but the intrinsic, instrumental, and relational value of the environ-ment cannot (Chan et al., 2016\n). When studying interdependen-\ncies, economic value in one sector (e.g. infrastructure) can be\ntransferred to another sector or industry, but environmental loss of\nvalue in one sector or industry cannot be rectified by environmen-tal improvements in another sector or industry. Damage to an eco-system by infrastructure systems is often permanent and has rippleeffects on the surrounding environment and, consequently, onsociety.\nThe Gaia hypothesis, proposed by Lovelock and Margulis (1974) ,\nposits that living organisms interact with their inorganic surround-ings to form a synergistic and self-regulating system that main-tains the conditions for life on Earth. This theory underscores the\nintrinsic value of natural systems, emphasising that the health of\nthe entire biosphere is crucial for the survival of its parts.Similarly, deep ecology, advocated by Naess (1973) and furtherdeveloped by thinkers such as Devall and Sessions (2001), pro-motes an eccentric view that recognises the inherent worth of allliving beings and the interconnectedness of all life forms and theenvironment. These perspectives highlight the importance ofviewing environmental value beyond mere utility, acknowledgingthe intrinsic and relational values that are essential for the well-being of ecosystems and human societies.\nFrom an assessment perspective, the multi-capitals assessment\napproach advocated by the Capitals Coalition (2024) offers a com-\nprehensive framework for evaluating the interrelated aspects ofthe environment, humans, society, and the economy. Thisapproach considers natural, social, human, and produced capitals,ensuring that the impacts and dependencies on various forms ofcapital are measured and valued. By integrating these differentcapitals, decision makers can develop more holistic and sustain-able strategies that recognise the intrinsic value of natural systemsand their critical role in supporting human and economic activ-ities. This method aligns well with the principles of the Gaiahypothesis and deep ecology, as it promotes a balanced and inter-\nconnected view of environmental value.\nThe concepts of doughnut economics, introduced by Raworth\n(2012) , provide another valuable framework for assessing environ-\nmental value at a system level. Doughnut economics visualises a\nsafe and just space for humanity, balancing the needs of all within\nthe planet ’s ecological limits. This model emphasises the impera-\ntive of creating economies that are regenerative and distributiveby design, ensuring that human activities do not overshoot envi-\nronmental boundaries while meeting social foundations. By focus-\ning on both ecological ceilings and social foundations, doughnuteconomics offers a practical approach to achieving sustainability\nthat aligns with the intrinsic and relational values highlighted by\nthe Gaia hypothesis and deep ecology.\nWhile both strong and weak sustainability theories engage withthe instrumental value of natural systems, they differ in theirapproach to resource management and the substitutability of natu-\nral capital. Weak sustainability relies on technological solutions\nand human ingenuity to mitigate environmental impacts, whereasstrong sustainability emphasises the preservation and sustainable\nmanagement of natural resources. In contrast, the Gaia hypothesis\nand deep ecology focus on the intrinsic value of natural systems,advocating for a holistic and interconnected view of the environ-\nment. These theories emphasise the importance of maintaining the\nhealth and stability of ecosystems, recognising that human healthand well-being are deeply intertwined with the health and well-being of the natural world. They call for a fundamental shift in\nhuman attitudes and behaviours towards nature, promoting a\nmore respectful and harmonious relationship with the naturalenvironment.\n2.2 The environmental impact of civil\ninfrastructure\nEnvironmental value has a direct relationship with civil infrastruc-\nture, which supports the natural environment ’s ability to sustain\nhuman life through its resources and provides conditions for eco-\nnomic prosperity. In addition, processes that reduce environmental\ndamage (e.g. renewable energy sources, water collection and dis-tribution, and food production and distribution) require civil infra-structure (Frischmann, 2012\n). In other words, civil infrastructure\nhas a reciprocal relationship with environmental value, as it relies\non the environment for materials, energy, and water while alsobeing necessary for protecting the environment.\nThis research defines environmental value in this context as the\nbenefits derived from infrastructure systems that contribute to\nenvironmental sustainability and overall health and well-being.This concept encompasses several factors, including the reduction\nof carbon dioxide (CO\n2) emissions, minimisation of pollution, and\nthe promotion of resource efficiency. Environmental value is notan isolated metric but is deeply interconnected with economic andsocial values. For example, sustainable transport infrastructure canEngineering Sustainability The environmental value of transport\ninfrastructure in the UK: an EXIOBASE analysisKalyviotis, Rogers and Hewings\n3lead to significant environmental benefits by reducing greenhouse\ngas (GHG) emissions and improving air quality, which in turn\nenhances public health and reduces healthcare costs.\nFollowing this approach, we suggest that instrumental and rela-\ntional value form part of social value and should be studied sepa-\nrately (out of the scope of this study). We highlight the importance\nof a holistic approach to infrastructure development, where envi-\nronmental considerations are integrated into the decision making\nprocess alongside economic and social factors. By recognisingand quantifying these interdependencies, policymakers can de-\nvelop more robust and sustainable business models for infrastruc-\nture projects. The effectiveness of a policy should be evaluated\nnot only in terms of emissions but also its impact on society and\nthe urban environment (Triantafyllopoulos, 2024\n). This approach\nensures that investments not only provide economic returns but\nalso contribute to the long-term sustainability and resilience of\ncommunities. By balancing these values, infrastructure systems\ncan better serve the needs of society while preserving the environ-\nment for future generations.\nA review of relevant literature revealed over 600 publications on\nthe environmental impact of the construction industry and its proj-\nects. These studies range from simple to complex analyses of pa-\nrameters for estimating the environmental impact and show that\nmaterials can account for up to 80 %–90%of the total structure ’s\nembodied GHG emissions (D ’Amico and Pomponi, 2018 ; Kang\net al., 2015; Zhang and Wang, 2016).\nSome studies present simple comparative analyses of environmen-\ntal impact factors for typical buildings or structures (see Asif\net al., 2017 ; Atmaca, 2017; Blok et al., 2020; Ding and Forsythe,\n2013; Park et al., 2014; Puskas and Moga, 2015), while others\nfocus on specific types of structures (see Connolly et al., 2018;\nLolli et al., 2019; Niu and Fink, 2019; Robertson et al., 2012) or\nroad and pavement infrastructure (see Balieu et al ., 2019; Park\nand Kim, 2019; Said and Al-Qadi, 2019). These analyses arebased on a structural analysis of the system of interest without\ndeveloping new models or theories. For example, Park et al .\n(2014) compared different construction methods and smart frame\napplications in tall buildings. Robati et al. (2019) and Wang et al.\n(2020) used statistical methods such as boxplots to analyse the\nuncertainty in material quantity calculations. Hodková et al .\n(2012) highlighted that inappropriate use of LCA data could lead\nto unreliable results, suggesting that initial environmental impact\nassessments should avoid using LCA databases to reduce uncer-\ntainty. The environmental impact can be transformed into emis-\nsions using any database after the structural analysis is complete.\nStatistical analysis of numerous civil projects should provide a\nrange of values to cover the uncertainty of the results (Robati\net al., 2019; Wang et al., 2020). Thus, the optimal way to study\nthe environmental impact of civil infrastructure is to maintain a\ndatabase of factors impacting the natural environment fornumerous construction projects, transforming these factors into\nenvironmental value as late as possible in the study process.\nOther studies compared the environmental impact of different con-\nstruction options for civil infrastructure. For instance, Balieu et al.\n(2019) compared different types of electrified road infrastructures,\nLangston et al. (2018) compared refurbished with new projects,\nand Lemma et al. (2020) and Meil et al. (2006) compared different\ndesigns for the same project. Grant and Ries (2013) studied the\nlife expectancy and replacement of different elements in existing\nstructures. These comparisons did not involve modelling but dis-\ncussed how different choices affect environmental impact without\nnumerical representation. While comparing construction options is\nstraightforward, numerical modelling, which relies on quantifica-\ntion of the dependency between environmental impact and theinfluencing parameters, is challenging since often the variables\ncannot be quantified accurately and/or with certainty.\nEstablishing the necessary parameters for civil infrastructure usu-\nally requires a high level of detail. De Wolf et al. (2016 , 2020)\ncombined different parameters with embodied carbon coefficients\nto develop a database estimating global warming potential. The\nparameters studied were both qualitative (e.g. type of structure,\nmain structural material, rating scheme certification, region, or\ncountry) and quantitative (e.g. size by floor area, height by number\nof floors, number of occupants, and span). Some authors decom-posed infrastructure into structural components and estimated\nemissions using geometric data combined with loading analyses\n(Collings, 2006; Du and Karoumi, 2013, 2014). The geometric\ncharacteristics and inputs affect the material quantities and emis-\nsions generated. Other authors studied different structural compo-\nnents, such as beams, columns, walls, and slabs, suggesting\noptimisation systems to reduce construction materials and CO\n2\nemissions (Hong et al., 2010; Lagaros, 2018). Shafiq et al. (2015)\nexamined how CO 2emissions from concrete and steel in buildings\nare affected by the dimensions of structural elements.\nA holistic model that includes the entire system of interest is a\ngood approach. Such models can be for simple structures (e.g.\nwater tanks, Sanjuan-Delmás et al., 2015 ) or more complex struc-\ntures (D ’Amico and Pomponi, 2018; Shafiq et al., 2015). Sanjuan-\nDelmás et al. (2015) studied how concrete and steel quantities are\naffected by volume, diameter, height, wall thickness, shape, and\nmaterial reinforcement, presenting results on a Cartesian coordi-nate system. D ’Amico and Pomponi (2018) created a complex\nsustainability tool to optimise steel quantities in complex construc-\ntions, presenting results by changing one specific parameter while\nholding other parameters constant, yet this omitted consideration\nof other potentially important factors (such as temperature, surface\narea, thickness, and thermal properties): being comprehensive is\nproblematic.\nAnother approach is the integration of LCA and the Material\nCircularity Index (MCI), which provides a comprehensive frameworkEngineering Sustainability The environmental value of transport\ninfrastructure in the UK: an EXIOBASE analysisKalyviotis, Rogers and Hewings\n4for evaluating the combined influences of environmental perform-\nance and circularity (Rigamonti and Mancini, 2021 ). LCA quantifies\nenvironmental impacts throughout a product ’s life cycle, while\nMCI measures adherence to circular economy principles such as\nreuse and recycling (Dervishaj and Gudmundsson, 2024). This\ncombined approach helps identify trade-offs between impactsand circularity, supporting balanced strategies and the develop-\nment of regulations that promote sustainability. However, this\napproach is challenging to be used on infrastructure systems dueto their size.\nBased on the above discussion, loadings, geometric parameters,\nand material quantities of any structure within the infrastructure\nsystem must be established to estimate emissions correctly.Combining the reasoning of De Wolf et al . (2016\n, 2020),\nD’Amico and Pomponi (2018), and Tecchio et al. (2019a, 2019b),\nit is clear that studying the environmental impact of a civil infra-structure system with many projects requires a vast number of pa-\nrameters for each structure, making the bottom-up approach\nimpractical.\n3. Methodology\n3.1 Methods and assumptions\nThe four major stages within LCA, as certified and supported bythe requirements established in the standards of the InternationalOrganization for Standardization (ISO), are ‘(1) goal and scope\ndefinition, (2) life cycle inventory (LCI), (3) life cycle impact\nassessment (LCIA), and (4) life cycle interpretation ’(ISO, 2006\n:\np. 2). The imprimatur of ISO contributes to their acceptance by\nthe international community and other stakeholders (Rebitzer\net al., 2004).\nAccording to Olugbenga et al.( 2 0 1 9 ) , there are five types of LCA\nmethods (see the Video with the five methods online and Figure 1).\n/H17039Process-based LCA: this is a bottom-up methodology performed\nby mapping and characterising ‘all processes associated with all\nlife cycle phases of a project ’(Jones et al., 2017 ).\n/H17039Hybrid LCA: this method incorporates both top-down\neconomic input –output analysis-based (sector-by-sector wider\nanalysis) and process-based LCA (Chester and Horvath,\n2010 ) to address data gaps when data are available only forpart of the process or to expand the boundaries of analysis\n(García de Soto et al., 2017; Jones et al., 2017).\n/H17039Pseudo LCA: this method is based on a mix of primary data\nand data from literature to calculate GHG emissions. Wheresystem data are not readily available, simplified andparametric LCA approaches are adopted (Bueno et al., 2017\n;\nWestin and Kågeson, 2012).\n/H17039Simplified LCA: this approach compares the environmental\nimpact of infrastructure with the condition of no infrastructurewithin a given area (Bueno et al., 2017\n).\n/H17039Parametric LCA: in this method, specific system parameters\nare statistically modelled to calculate emissions associated\nwith the system.\nThe two methods with the least uncertainty in assessing environ-\nmental impacts will be discussed in the following section:\nprocess-based life cycle assessments and a hybrid LCA tool, EEMRIO tables (Figure 1\n).\n3.2 Process-based life cycle assessment\nA process-based LCA is a method for quantifying the environmen-\ntal impacts of all processes associated with an existing or potential\nproduct or service throughout each stage of its life (Finkbeiner\net al., 1998 ; Jones et al., 2017). This method is the initial approach\nto conducting a life cycle evaluation of a good, service, or system,\nfocusing on a scientific analysis of the inputs (material and energy\nbalance) and outputs (emissions and wastes; Finkbeiner et al .,\n2006). The process-based LCA process is a detailed methodology\nfor assessing the environmental impacts of a specific infrastruc-\nture system (Ayres, 1995; Trunzo et al., 2019). It is considered a\n‘bottom-up ’method, which is inherently attractive, yet because\nit quantifies the environmental impacts for each process in alllife cycle phases of a project (Olugbenga et al., 2019), by its na-\nture, it is challenging to evaluate infrastructure as a whole. To\nassess the performance of the civil infrastructure of a society,\nLCA should be applied to individual infrastructure projects,\nproducts, and services and then scaled up to the level (i.e. scale)of interest. Therefore, detailed information is required on all\nmaterials and energy used, their supply chains, and the operation,\nmaintenance, and use of every piece of infrastructure being ana-\nlysed. More specifically, performing a process-based LCA at the\nlevel of interest in this research requires extensive informationon the extraction, transportation, and assembly of every material\nused on each site, as well as the recycling or disposal of each\ncomponent of an infrastructure project at the end of its life.\nThis method enables a high level of environmental detail to be\nobtained if the analysis focuses on single products or projects\n(Beylot et al ., 2020 ). However, many obstacles arise during its\napplication that can restrict the evaluation of impacts. First, due to\ncircular economy practices now more routinely adopted in the pur-suit of enhancing environmental value, this method requires many\nassumptions and decisions to define the goal and scope of the\nanalysis, which can create limitations on the number of compo-\nnents (materials, processes, or flows) and sub-emissions that canProcess-\nbasedHybrid Pseudo Simpliﬁed ParametricLCA methods\nIncreasing amount of data that goes into the LCA methodsIncreasing level of data uncertainty and assump/g415ons\nFigure 1. LCA methods ranking based on data requirements and\nuncertainty (Olugbenga et al., 2019 )Engineering Sustainability The environmental value of transport\ninfrastructure in the UK: an EXIOBASE analysisKalyviotis, Rogers and Hewings\n5be practically considered (Ayres, 1995; Beylot et al., 2020). This\nmakes a comprehensive process-based LCA a very complicatedand time-consuming process (Beylot et al., 2020).\nSecond, this method requires extensive primary data from each\ncomponent within the civil infrastructure. Therefore, if there is\ninsufficient or unreliable data, the analysis will involve anincreased level of complexity and uncertainty in determining all\nthe input and output elements, thus reducing the method ’s utility\n(Ayres, 1995\n; Beylot et al., 2020; Trunzo et al., 2019). The data\nrequired for this method comes from both public databases and\ncustomers. Public databases provide inventory information of spe-\ncific materials, energy, and processes (Finkbeiner et al., 2006; Lee\nand Inaba, 2004), while customer data are collected from surveys\nand manufacturer ’s assumptions that provide insight into a spe-\ncific product (Finkbeiner et al ., 2006; Lee and Inaba, 2004).\nBecause this data comes from customer ’s assumptions, it is often\n‘proprietary, unpublished or confidential ’, and therefore cannot be\nverified with other credible databases (Ayres, 1995: p. 8). Thelarger the system analysis, the greater the data deriving from mul-\ntiple sources, making the result prone to overstatement (Ayres,\n1995). Third, the process-based LCA ignores real human behav-iour and, consequently, the economic market and activities related\nto global commerce (Gutowski, 2018).\nBased on the above, the process-based LCA method is appropri-\nate for detailed analysis of specific, single and small-scale infra-\nstructure projects where socio-economic factors are not required\nto be evaluated. This challenge was one of the main reasons theprocess-based method was not used in this research, as the scope\nof the research is to estimate the environmental impact of an\nentire region (UK) and infrastructure type/sector (transport,energy, water, waste, and communication).\n3.3 EXIOBASE input –output tables\nEE MRIO tables are tools for analysing the environmental impact\nof economic activities, including infrastructure provision, and\ntheir interdependencies across multiple sectors (Stadler et al .,\n2018 ). EXIOBASE 3 is an EE MRIO database that combines esti-\nmates of the quantity of products supplied and used within differ-\nent sectors of the economy (input –output tables) with estimates\nfor aggregated emissions to the environment. An input –output ta-\nble is a tool used in economics to represent the flow of goods and\nservices between different sectors of an economy (Avelino et al.,\n2021; Langarita and Cazcarro, 2022). EXIOBASE is a ‘global\nmulti-regional input –output database ’with a high level of sector\ndisaggregation and economic foundation (Stadler et al ., 2018).\nThis database has been developed in three different versions\nthrough specific projects: the EXIOPOL project developed\nEXIOBASE1 (2000), the CREEA project created EXIOBASE2(2007), and the DESIRE project built the latest version,\nEXIOBASE3 (1995 –2011; Tukker et al., 2018). EXIOBASE com-\nbines two forms of analysis: monetary (euros) and supply use(tonnes and terajoules, among others) using a range of statisticalmodels (Merciai and Schmidt, 2018). The EE MRIO tables\nmethod combines elements of hybrid LCA with parametric LCAmethods, as it is based on economic factors and a range of statisti-cal models. An EE MRIO table requires the corresponding input –\noutput table for estimating environmental impacts related to theconsumption of products (Tukker et al., 2018; Van Roekel et al.,\n2017).\nEXIOBASE 3 describes the environmental impacts of complexglobal and cross-sectoral relationships and can therefore informpolicymakers regarding the use of resources and the discharge ofemissions to the environment (Stadler et al., 2018\n). EXIOBASE 3\ncaptures both material and service exchanges between sectors (e.g.\ninfrastructure). This tool is important for national and regional de-cision makers due to its high level of aggregation and coverage ofenvironmental impacts. Therefore, EXIOBASE 3 allows decisionmakers to take system-wide decisions on a large scale.\nEXIOBASE demonstrates a high level of consistency in the mac-roeconomic sector and international data sources by allowing theestimation of emission factors associated with specific consump-tion activities within and outside of any particular country (Stadleret al., 2018\n; Tukker et al., 2018).\nHowever, there are some limitations. First, the latest version of\nthis technique aims to cover global data, but only detailed infor-\nmation from 43 out of 195 countries is included, grouping the\nremaining countries into five world regions, which may lead tooverestimation (Yang et al., 2017\n). Second, its time series is built\nfor 1995 –2011, thus opening up several analytical options, includ-\ning time series analysis and structural decompositions restricted tothese years (Tukker et al., 2018). Third, the classification of emis-\nsions requires assumptions in which several elementary flows arenot reported in the environmental extensions (Beylot et al., 2020).\nThis prevents the quantification of the entirety of ecologicalimpacts for several impact categories and indicates that thismethod achieves a low level of detail or approximate sector reso-\nlution (Beylot et al., 2020; Suh and Nakamura, 2007). This condi-\ntion may influence the assessment results and necessitates asensitivity analysis (Beylot et al., 2020).\nTherefore, EXIOBASE 3 is a very powerful tool for decision mak-\ners looking to maximise environmental value. However,EXIOBASE 3 cannot provide decision makers with a level ofdetail sufficient to determine whether one single infrastructureproject is better than another in terms of environmental value.Estimating the environmental value of a single infrastructure pro-ject using the EXIOBASE 3 would require a very complex top-down analysis.\n3.4 Critical analysis of methodologies\nLCA and EXIOBASE tables capture the environmental value of civilinfrastructure in different ways. According to Chan et al. (2016)\n,\nenvironmental value is defined as the intrinsic, instrumental, andEngineering Sustainability The environmental value of transport\ninfrastructure in the UK: an EXIOBASE analysisKalyviotis, Rogers and Hewings\n6relational value found within nature. In other words, environmental\nvalue is a multi-faceted concept, and there is merit in analysing the\nstrengths and weaknesses of both LCA and EXIOBASE tables for\neach facet —instrumental, intrinsic and relational —of environmental\nvalue individually to gain a comprehensive understanding of the\noverall results, as they relate to civil infrastructure.\nNeither method can be said to be superior in capturing nature ’s\nintrinsic value, as this depends on the goals and the scale of envi-\nronmental interventions (Chan et al., 2016 ). Most of the environ-\nmental impacts (and hence value gained or lost) assessed by\nEXIOBASE 3 and LCA pertain to the instrumental value of the\nenvironment. LCAs are useful in identifying whether investment\nin a specific project or product will affect any of the current ‘uses\nof ecosystem goods ’(Daily, 1997). However, when assessed in\nisolation, an infrastructure project may not cause a substantial\nenvironmental impact, whereas several similar projects in aggre-\ngate could result in larger consequences. For this reason, EE\nMRIO tables are more significant in protecting or enhancing the\nenvironment ’s instrumental value. Governments need to manage\nshared resources among their citizens and industries to ensure thatpeople are served fairly by civil infrastructure, industry is sup-\nported economically, and the environmental value of the area is\nprotected (Gómez-Baggethun and Ruiz-Pérez, 2011). These types\nof decisions can be made with support from EXIOBASE 3.\nNeither process-based LCA nor EXIOBASE 3 captures the rela-\ntional value of the environment. People, cultures, and societies have\ndifferent relationships with many aspects of nature (Chan et al.,\n2016 ). These relationships are challenging to quantify and cannot\nbe measured by either process-based LCAs or EXIOBASE 3. Civil\ninfrastructure can either contribute to or detract from these relation-\nships. For example, when placed in an inappropriate location, infra-\nstructure can permanently scar culturally significant land or damage\npolitical relationships (Dona and Singh, 2017).\nIn addition to Chan ’s approach, both process-based LCA and\nEXIOBASE are methods used to quantify environmental foot-\nprints using standard mathematical functions (Crawford et al .,\n2018 ). Nevertheless, the emphasis of each method will vary\ndepending on the goal, assumptions, interest, and componentssuch as resolution granularity and unit measurement (Beylot et al.,\n2020; Castellani et al., 2019). Table 1 shows the most notable dif-\nferences between both approaches.\nThe above discussion leads to a working hypothesis that process-based LCA is appropriate for small-scale physical civil infrastruc-tures without functionality analysis and within limited and re-\nstricted boundaries. Furthermore, this process is suitable if time\nand effort are invested in finding accurate data for each elementwithin the system, including information provided by industrymanufacturers, to ensure the accuracy of the environmental assess-ment of infrastructure. This method is oriented towards engineers\nand mid-level decision makers. In contrast, the authors consider\nEXIOBASE suitable for macro-scale civil infrastructure assess-ment in an overall context, including physical characteristics,components, and social and economic activities within and outsideof the infrastructure ’s geographical location. EXIOBASE is ori-\nented towards government authorities and international agencies\nthat make high-level decisions. However, both methods should becombined when small-scale civil infrastructure or specific materialmust be evaluated to estimate environmental impacts that havesocio-economic interdependencies. The combination of both willcapture missing areas of consumption and provide better and more\nunderstandable results.\n3.5 Summing up the research methodology\nEnvironmental value, and its protection and enhancement, is a\ncomplex issue that affects all countries, industries, and economicactivities. It encompasses the intrinsic, instrumental, and relationalvalue of nature. Both traditional process-based LCA and EEMRIO tables are valuable tools for evaluating environmental\nimpacts at different scales, with process-based LCA being particu-\nlarly useful for making specific decisions about products or proj-ects. However, EXIOBASE 3 is more suitable for addressingenvironmental issues (identifying and characterising environmen-tal impacts and assessing environmental value gained or lost) asso-\nciated with large-scale interventions and is therefore a better\nresource for policymakers, who seek to implement significantchanges and preserve environmental value both domestically andin their trading partner countries.\nTable 1 Comparison of process-based LCA and EXIOBASE\nProcess-based LCA EXIOBASE\nSimple methodology (Ayres, 1995 ) High level of sector disaggregation (Stadler et al., 2018 )\nTime and effort-consume, but a high level of environmental\ndetail of single products (Beylot et al., 2020 )Environment accounts, micro and macro-scale scenarios (Merciai & Schmidt, 2018 )\nLimited to the areas of consumption and materials\n(Castellani et al., 2019 )\n/H17039Open database source (Finkbeiner et al., 2006 )\n/H17039Private sources from industries (sometimes\nconfidential) (Finkbeiner et al., 2006 )\n/H17039No socio-economic data (Gutowski, 2018 )Many products and consumption areas are included in the same database (EXIOBASE\nConsortium, 2015)\n/H17039Input–output table (MR-IOT); monetary (Beylot et al., 2020 ; Tukker et al., 2018)\n/H17039Multi-regional environmentally extended supply-use table (MR-SUT); supply-use\n(Beylot et al., 2020 ; Tukker et al., 2018)\n/H17039Process-based LCA data; coefficients and statistics(Beylot et al., 2020 ; Tukker\net al., 2018)Engineering Sustainability The environmental value of transport\ninfrastructure in the UK: an EXIOBASE analysisKalyviotis, Rogers and Hewings\n74. Interpretation, analysis, and discussion\nThe main challenge in applying these methods is how to track\ndependencies. Dependencies were studied inductively by examining\nthe correlation between each type of value of the different types ofinfrastructures, noting that correlation between two variables does\nnot necessarily imply causality (Field, 2009\n: pp. 619 –620). Thus,\nwhile two variables can certainly be related with causality, makingthis assumption and using a superficial interpretation together with\nthe correlation may lead to incorrect conclusions. Accordingly, a\ncausal relationship (interdependence) between two correlated varia-bles was verified with a rational assumption, using the Pearson cor-\nrelation coefficient for (Field, 2009) this study:\nr¼sxy\nsx/C1sy¼Xv\ni¼1ðxi/C0x/C0Þ/C1ðyi/C0y/C0Þ\nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃXv\ni¼1ðxi/C0x/C0Þ2Xv\ni¼1ðyi/C0y/C0Þ2s\nwhere:\n/H17039If−0.3<r<0.3, there is no linear correlation.\n/H17039If−0.5<r/C20−0.3 or 0.3 /C20r<0.5, there is a weak linear\ncorrelation.\n/H17039If−0.7<r/C20−0.5 or 0.5 /C20r<0.7, there is a medium linear\ncorrelation.\n/H17039If−0.8<r/C20−0.7 or 0.7 /C20r<0.8, there is a strong linear\ncorrelation.\n/H17039If−1<r/C20−0.8 or 0.8 /C20r<1, there is a very strong linear\ncorrelation.\n/H17039Ifr=±1, there is a perfect linear correlation.\nFor this, the existence of linear correlation, meaning requals more\nthan 0.3 or less than −0.3, and the size of the correlation (weak,\nmedium, strong, or very strong) were important.\nThe interplay between transport and energy emphasises the envi-\nronmental benefits of adopting advancements such as electricvehicles powered by renewable energy and implementing inte-\ngrated policies targeting both sectors (Cartone et al., 2021\n;W e i\net al., 2021). Improved waste management, such as recycling auto-\nmotive materials, can reduce energy use and emissions (Lee et al.,\n2024). The transport and water sectors are closely linked, with\nshipping activities affecting water quality and water availabilityinfluencing transport operations; innovative technologies and sus-\ntainable planning can mitigate these impacts (David and Gollasch,\n2015; Makkonen and Inkinen, 2021). In addition, integrating ICT,such as intelligent transportation systems and IoT technologies,\nenhances transport efficiency, reduces emissions, and supports re-\nsilience to climate change (Di Martino et al., 2017).\nThe EXIOBASE 3 database includes 85 types of emissions for\nboth water and air pollution (Stadler et al ., 2018 ). EXIOBASE\n3 includes EE MRIO tables linked with the economic input –outputtables of each country (Stadler et al., 2018). The range of emis-\nsions considered was, inductively, reduced to those relevant to thefocus of this study to make the development of the theory (deduc-tion) manageable —the emissions herein are studied for water and\nwaste pollution, while the sectors of interest are transport, energy,\nwater, waste, and communication. It was checked by which input –\noutput group (IOG) each emission is produced to determine itsrelevance to this study and, because of its (environmental) focus,the separation of the sectors/IOGs for analysis differs from the\nseparation of sectors/IOGs done in the economic input –output\ntables.\nThe EXIOBASE 3 database provides the empirical data used toconclude which types of emissions are produced by the transport,energy, water, waste, and communication sectors. The theory isthen derived from the resultant observations, as induction com-\nmands (Ghauri and Grønhaug, 2010\n; May, 2011).\nThe transport sector includes the IOGs of transportation (eco-\nnomic approach): ( a) Transport via Railways; ( b) Other land trans-\nport; ( c) Transport via pipelines; ( d) Sea and coastal water\ntransport; ( e) Inland water transport; ( f) Air transport and the\nIGOs of manufacturing of vehicles and transport related services\n(engineering approach); ( g) Manufacture of motor vehicles,\ntrailers and semi-trailers; ( h) Manufacture of other transport equip-\nment; ( i) Sale, maintenance, repair of motor vehicles, motor\nvehicles parts, motorcycles, motor cycles parts and accessories;\nand ( j) Retail sale of automotive fuel.\nThe energy sector includes ( a) Production of electricity by coal,\n(b) Production of electricity by gas, ( c) Production of electricity\nby nuclear, ( d) Production of electricity by hydro, ( e) Production\nof electricity by wind, ( f) Production of electricity by petroleum\nand other oil derivatives, ( g) Production of electricity by biomass\nand waste, ( h) Production of electricity by solar photovoltaic, ( i)\nproduction of electricity, ( j) Transmission of electricity, ( k)\nDistribution and trade of electricity, and ( l) Manufacture of gas;\ndistribution of gaseous fuels through mains.\nThe water sector includes ( a) Steam and hot water supply and ( b)\nCollection, purification and distribution of water.\nThe communication sector includes ( a) post and telecommunications.\nThe waste sector includes ( a) Incineration of waste: Food, ( b)\nIncineration of waste: Paper, ( c) Incineration of waste: Plastic, ( d)\nIncineration of waste: Metals and Inert Materials, ( e) Incineration of\nwaste: Textiles, ( f) Incineration of waste: Wood, ( g) Incineration of\nwaste: Oil/Hazardous waste, ( h) Biogasification of food waste,\nincluding land application, ( i) Biogasification of paper, including\nland application, ( j) Biogasification of sewage sludge, including\nland application, ( k) Composting of food waste, including land appli-\ncation, ( l) Composting of paper and wood, including land applica-\ntion, ( m) Waste water treatment: Food, ( n)W a s t ew a t e rt r e a t m e n t :Engineering Sustainability The environmental value of transport\ninfrastructure in the UK: an EXIOBASE analysisKalyviotis, Rogers and Hewings\n8Other, ( o) Landfill of waste: Food, ( p) Landfill of waste: Paper, ( q)\nL a n d f i l lo fw a s t e :P l a s t i c ,( r) Landfill of waste: Inert/metal/hazard-\nous, ( s) Landfill of waste: Textiles, and ( t) Landfill of waste: Wood.\nTo conclude inductively, 26 emissions were found to be produced\nby the transport sector based on the empirical data (see Table 2 ).\nThe next step was to seek a connection with the theory, as induc-tion demands. Of the 26 emissions, 23 are calculated by the engi-neering combustion model calculation of emissions from roadtransport (COPERT) (Laou, 2013); two (arsenic (As) and mercury(Hg)) are produced only by water transport, and the last (non-methane volatile organic compound(NMVOC)) is a non-combustion pollutant (Stadler et al., 2018).\n4.1 Environmental infrastructure\ninterdependencies\nDifferent methodologies were used for calculating the EXIOBASE\nemissions of each type of industry, meaning that any correlation willbe a result of dependency between the different industries. In addi-\ntion, each country or area has different sizes of industry developed\nover its lifecycle.\nThe key challenge is how to cross-correlate the environmental\ncoefficients to determine their relationships. Pearson correlation isa common method to expose the correlation between series, but\nsince the data were calculated by developing time series ofdetailed EE MRIO tables, the issues of spurious regression or spu-\nrious correlation should be eliminated.\nSpurious regression was reduced as follows: Stadler et al. (2018)\nremoved all the perfectly correlated indicators (14 indicators) a\npriori, and the remaining 105 indicators, which also showed very\nhigh correlations, were reduced using principal component analy-\nsis (PCA) and an optimisation methodology based on the PCAresults. This approach eliminated the correlation based on the cal-\nculating indicators and the within-series dependence. This can be\nobserved even if coefficients from the same group of sectors arecompared with Pearson ’s correlation, as the correlation is not high\nin every case and is never perfect, despite using similar indicators.\nIn addition, the research did not use the chronological develop-\nment of the data (time series), but the regional development (same\nyear and different country). Each country has a different trajectoryover time, has different legislation, and is at a different stage of\ndevelopment. This means it is possible to accept the linearity of\nthe data (linear regression analysis) and assume this linear rela-tionship extends to the total world activity, enabling the applica-\ntion of the Pearson correlation method. This is a safe assumption,\nTable 2. Emissions produced by the transport sector\nEmissionType of emissions (European\nEnvironment Agency, 2006 )Assessment method (EuropeanEnvironment Agency, 2006\n)Literature/theory\nCO2 Fuel-related pollutants Fuel consumption COPERT model (Laou, 2013 )\nCH4 Non-regulated pollutant Emissions coefficients COPERT model (Laou, 2013 )\nN2O Non-regulated pollutant Emissions coefficients COPERT model (Laou, 2013 )\nSOx Non-regulated pollutant Fuel consumption COPERT model (Laou, 2013 )\nNOx Regulated pollutant Emissions coefficients COPERT model (Laou, 2013 )\nNH3 Non-regulated pollutant Emissions coefficients COPERT model (Laou, 2013 )\nCO Regulated pollutant Emissions coefficients COPERT model (Laou, 2013 )\nBenzo(a)pyrene Fuel-related pollutants T otal percentage of volatile organic com pound COPERT model (Laou, 2013 )\nBenzo(b)fluoranthene Fuel-related pollutants T otal percentage of volatile organic com pound COPERT model (Laou, 2013 )\nBenzo(k)fluoranthene Fuel-related pollutants T otal percentage of volatile organic com pound COPERT model (Laou, 2013 )\nIndeno(1,2,3-cd)pyrene Fuel-related pollutants T otal percentage of volatile organic com pound COPERT model (Laou, 2013 )\nPCDD_FaFuel-related pollutants T otal percentage of volatile organic com pound COPERT model (Laou, 2013 )\nNMVOCbNon-regulated pollutant T otal percentage of volatile organic com pound COPERT model (Laou, 2013 )\nPM10 Regulated pollutant Emissions coefficients COPERT model (Laou, 2013 )\nPM2.5 Regulated pollutant Emissions coefficients COPERT model (Laou, 2013 )\nTSPcRegulated pollutant Emissions coefficients COPERT model (Laou, 2013 )\nAs Heavy metals n/a (Stadler et al., 2018 )\nCd Heavy metals Fuel consumption COPERT model (Laou, 2013 )\nCr Heavy metals Fuel consumption COPERT model (Laou, 2013 )\nCu Heavy metals Fuel consumption COPERT model (Laou, 2013 )\nHg Heavy metals n/a (Stadler et al., 2018 )\nNi Heavy metals Fuel consumption COPERT model (Laou, 2013 )\nPb Heavy metals Fuel consumption COPERT model (Laou, 2013 )\nSe Heavy metals Fuel consumption COPERT model (Laou, 2013 )\nZn Heavy metals Fuel consumption COPERT model (Laou, 2013 )\nNMVOCb(non-\ncombustion)Non-regulated pollutant n/a (Stadler et al., 2018 )\naPCDD_F, polychlorinated dibenzodioxins (PCDDs) and polychlorinated dibenzofurans (PCDFs)\nbNMVOC, non-methane volatile organic compounds\ncTSP , total suspended particlesEngineering Sustainability The environmental value of transport\ninfrastructure in the UK: an EXIOBASE analysisKalyviotis, Rogers and Hewings\n9as the authors of EXIOBASE 3 also use it, and EXIOBASE 3 is\nwidely used by input –output modellers worldwide. The unknown\nactivity rate is estimated for each year by applying linear regres-\nsion with a constant offset parameter. Mathematically, it is not the\nideal method, but it compensates for a small part of the missingdata where real values are absent (Stadler et al., 2018\n:S 3–S7). In\naddition, the input –output models are linear and do not assume\nincreasing or decreasing returns to scale.\nTo summarise, the IOGs that belong to the same sector group arecorrelated because a similar methodology was used to estimate the\nemissions produced. Therefore, these correlations are rejected\nsince it is not possible to establish whether they are correlatedbecause of the estimation methodology, meaning that the results\nare biased.\nTables with the calculated correlations between the sectors of\ninterest for each pollutant were created. Table 3\nis an example\ntable for CO 2. The sectors within the same group were high-\nlighted and removed (see Column 3 in Table 3). Then, the num-\nber of missing data points was checked, and if the missing dataexceeded 10 %of the total, the connection was also removed\n(see Column 2 in Table 3). For example, EnergyNuclear may be\ndependent on other sectors, but since many of the areas studieddo not have this type of energy, it is not possible to include the\nconnection.\nThe IOGs glossary of the first column of Table 3 is as follows.\n/H17039TRail is the ‘Transport via Railways ’IOG of EXIOBASE\n/H17039TOther is the ‘Other land transport ’IOG of EXIOBASE\n/H17039TPipelines is the ‘Transport via pipelines ’IOG of\nEXIOBASE\n/H17039TSea is the ‘Sea and coastal water transport ’IOG of\nEXIOBASE\n/H17039TManufMotor is the ‘Manufacture of motor vehicles, trailers,\nand semi-trailers ’IOG of EXIOBASE\n/H17039TManufOther is the ‘Manufacture of other transport\nequipment ’IOG of EXIOBASE\n/H17039TAir is the ‘Air transport ’IOG of EXIOBASE\n/H17039TSaleFuel is the ‘Sale, maintenance, repair of motor vehicles,\nmotor vehicles parts, motorcycles, motorcycles parts andaccessories ’IOG of EXIOBASE\n/H17039EnergyCoal is the ‘Production of electricity by coal ’IOG of\nEXIOBASE\n/H17039EnergyGas is the ‘Production of electricity by gas ’IOG of\nEXIOBASE\n/H17039EnergyNuclear is the ‘Production of electricity by nuclear ’\nIOG of EXIOBASE\n/H17039EnergyWind is the ‘Production of electricity by wind ’IOG of\nEXIOBASE\n/H17039EnergyHydro is the ‘Production of electricity by hydro ’IOG\nof EXIOBASE\n/H17039EnergySolar1 is the ‘Production of electricity by solar\nphotovoltaic ’IOG of EXIOBASE\n/H17039EnergySolar2 is the ‘Production of electricity by solar\nthermal ’IOG of EXIOBASE/H17039EnergyTransm is the ‘Transmission of electricity ’IOG of\nEXIOBASE\n/H17039Energynec is the ‘Production of electricity not elsewhere\nclassified ’IOG of EXIOBASE\n/H17039EnergyPetrol is the ‘Production of electricity by petroleum\nand other oil derivatives ’IOG of EXIOBASE\n/H17039EnergyBiomass is the ‘Production of electricity by biomass\nand waste ’IOG of EXIOBASE\n/H17039EnergyOcean is the ‘Production of electricity by tide, wave,\nocean ’IOG of EXIOBASE\n/H17039EnergyGeoth is the ‘Production of electricity by Geothermal ’\nIOG of EXIOBASE\n/H17039EnergyTransm is the ‘Transmission of electricity ’IOG of\nEXIOBASE\n/H17039EnergyDistrib is the ‘Distribution and trade of electricity ’\nIOG of EXIOBASE\n/H17039Communic is the ‘Post and telecommunications ’IOG of\nEXIOBASE\n/H17039NWaterSupply the ‘Steam and hot water supply ’IOG of\nEXIOBASE\n/H17039NWaterDistrib is the ‘Collection, purification, and distribution\nof water ’IOG of EXIOBASE\n/H17039WIFood is the ‘Incineration of waste: Food ’IOG of\nEXIOBASE\n/H17039WIPaper is the ‘Incineration of waste: Paper ’IOG of\nEXIOBASE\n/H17039WIMetal is the ‘Incineration of waste: Metals and Inert\nMaterials ’IOG of EXIOBASE\n/H17039WITextile is the ‘Incineration of waste: Textiles ’IOG of\nEXIOBASE\n/H17039WIWood is the ‘Incineration of waste: Wood ’IOG of\nEXIOBASE\n/H17039WIOil is the ‘Incineration of waste: Oil/Hazardous waste ’\nIOG of EXIOBASE\n/H17039WWFood is the ‘Wastewater treatment, food ’IOG of\nEXIOBASE\n/H17039WWOther is the ‘Wastewater treatment, other ’IOG of\nEXIOBASE\n/H17039WCFood is the ‘Composting of food waste, incl. land\napplication ’IOG of EXIOBASE\n/H17039WCPaper is the ‘Composting of paper and wood, incl. land\napplication ’IOG of EXIOBASE\n/H17039WBPaper is the ‘Biogasification of paper, incl. land\napplication ’IOG of EXIOBASE\n/H17039WBFood is the ‘Biogasification of food waste, incl. land\napplication ’IOG of EXIOBASE\n/H17039WBSewage is the ‘Biogasification of sewage slugde, incl.\nland application ’IOG of EXIOBASE\n/H17039WLFood is the ‘Landfill of waste: Food ’IOG of EXIOBASE\n/H17039WLPaper is the ‘Landfill of waste: Paper ’IOG of\nEXIOBASE\n/H17039WLPlastic is the ‘Landfill of waste: Plastic ’IOG of\nEXIOBASE\n/H17039WLMetal is the ‘Landfill of waste: Inert/Metal/Hazardous ’\nIOG of EXIOBASE\n/H17039WLTextile is the ‘Landfill of waste: Textiles ’IOG of\nEXIOBASE\n/H17039WLWood is the ‘Landfill of waste: Wood ’IOG of EXIOBASEEngineering Sustainability The environmental value of transport\ninfrastructure in the UK: an EXIOBASE analysisKalyviotis, Rogers and Hewings\n10Table 3. CO 2emission generation correlation between sectors\nInput –output groups Correlation —CO2 Data robustaType\nWIPlastic–WITextile 1.000 x W-W\nWBFood–WBPaper 0.999 X W-W\nWIMetal–WCFood 0.999 x W-W\nWIMetal–WIOil 0.993 x W-W\nNWaterSupply–WIMetal 0.993 x N-W\nNWaterSupply–WCFood 0.992 x N-W\nWLFood–WLTextile 0.991 \u0013 W-W\nWLPaper–WLTextile 0.990 \u0013 W-W\nWLFood–WLPaper 0.990 \u0013 W-W\nWLPlastic–WLWood 0.957 \u0013 W-W\nWIOil–WCFood 0.928 x W-W\nNWaterSupply–WLMetal 0.919 \u0013 N-W\nNWaterSupply–WIOil 0.919 x N-W\nWIMetal–WLMetal 0.919 x W-W\nEnergyTransm–EnergyDistrib 0.918 \u0013 E-E\nWCFood–WLMetal 0.913 x W-W\nWLFood–WLWood 0.893 \u0013 W-W\nWLPaper–WLWood 0.888 x W-W\nWIOil–WLMetal 0.887 x W-W\nWIFood–WIPaper 0.874 x W-W\nWLTextile–WLWood 0.872 \u0013 W-W\nEnergyCoal–EnergyDistrib 0.846 \u0013 E-E\nEnergyTransm–NWaterDistrib 0.841 \u0013 E-N\nEnergyCoal–EnergyTransm 0.828 \u0013 E-E\nWWFood–WWOther 0.816 \u0013 W-W\nTManufMotor–TPipelines 0.803 \u0013 T- T\nEnergyNuclear–EnergyWind 0.767 x E-E\nEnergyDistrib–NWaterDistrib 0.761 \u0013 E-N\nWLPaper–WLPlastic 0.753 \u0013 W-W\nWLFood–WLPlastic 0.750 \u0013 W-W\nEnergyOcean–WCPaper 0.731 x E-W\nTSaleFuel–Communic 0.725 \u0013 T- C\nWLPlastic–WLTextile 0.715 \u0013 W-W\nEnergyCoal–NWaterDistrib 0.707 \u0013 E-N\nTRail–Communic 0.617 \u0013 T- C\nEnergyPetrol–TAir 0.598 \u0013 T- E\nWWFood–WLFood 0.591 \u0013 W-W\nTSea–WIPlastic 0.591 x T- W\nTSea–WITextile 0.591 x T- W\nEnergyGeoth–TPipelines 0.589 x E-T\nWWFood–WLPaper 0.579 \u0013 W-W\nWIOil–WWOther 0.579 x W-W\nWWFood–WLTextile 0.565 \u0013 W-W\nTManufMotor–EnergyGeoth 0.556 \u0013 T- E\nTOther–TAir 0.554 \u0013 T-T\nTManufMotor–TManufOther 0.553 \u0013 T-T\nEnergyBiomass–EManufGas 0.547 \u0013 E-E\nEnergySolar2–EnergyGeoth 0.545 x E-E\nWWOther–WLMetal 0.538 \u0013 W-W\nEnergyNuclear–NWaterDistrib 0.537 x E-N\nEnergyDistrib–TOther 0.492 \u0013 E-T\nTManufOther–TPipelines 0.489 \u0013 T-T\nEnergyNuclear–TAir 0.488 x E-T\nWIFood–WIPlastic 0.487 x W-W\nWIFood–WITextile 0.487 x W-W\nTManufMotor–TRail 0.484 \u0013 T-T\nEnergyTransm–TOther 0.483 \u0013 E-T\nEnergyNuclear–EnergyTransm 0.481 x E-E\nEnergyNuclear–TOther 0.477 x E-T\n(continued on next page)Engineering Sustainability The environmental value of transport\ninfrastructure in the UK: an EXIOBASE analysisKalyviotis, Rogers and Hewings\n11The third column of Table 3 shows the environmental dependence\nbetween the sectors of transport (T), communication (C), energy\n(E), water (N), and waste (W), for example T –C means that trans-\nport and communication sectors are dependent for the specificIOG. A network diagram was developed to represent the findings\n(see Figure 2), integrating statistical significance levels as a meth-\nodological approach (see Niavis et al., 2024).\nThe same process was repeated for the other 25 emissions, the\nresults being detailed in Table 4 and represented in Figure 3.\n5. Concluding discussion\nThe statistical inference of EXIOBASE demonstrated that there\nare emission generation interdependencies that can be quantified.It is important to note that the linearity assumptions influence the\nanalysis, yet LCA and EXIOBASE remain widely accepted.\nVery many of the emissions generated by energy and land transport sec-\ntors are correlated (CO\n2, methane (CH 4), nitrous oxide (N 2O), sulfur\noxide (SO X), nitrogen oxide (NO X), ammonia (NH 3), carbon monoxide(CO), Benzo(a)pyrene, Benzo(b)fluoranthene, Benzo(k)fluoranthene,\nIndeno(1,2,3-cd)pyrene, NMVOC, particulate matter (PM10): inhalableparticles, with diameters that are generally 10 μma n ds m a l l e r( P M\n10),\nparticulate matter (PM2.5): inhalable particles, with diameters that are\ngenerally 2.5 μma n ds m a l l e r( P M 2.5), triple superphosphate (TSP), cad-\nmium (Cd), chromium (Cr), copper (Cu), nickel (Ni), lead (Pb), sele-nium (Se), zinc (Zn), and NMVOC (non-combustion)), while feweremissions correlate with air transport (CO\n2,C H 4,S O X,N H 3,C O ,\nBenzo(k)fluoranthene, Indeno(1,2,3-cd)pyrene, NMVOC, PM 10,P M 2.5,\nTSP, Cd, Cr, Cu, Ni, Pb, Se, Zn), and even fewer still (CO 2,N H 3,C O ,\nBenzo(b)fluoranthene, Benzo(k)fluoranthene, and TSP) correlate with\nwater transport sectors. In general, many of the emissions gener-\nated by land transport correlate with those from the communicationsector (CO\n2,N 2O, SO X,N O X,N H 3, CO, Benzo(a)pyrene,\nBenzo(b)fluoranthene, Benzo(k)fluoranthene, NMVOC, PM 10,\nPM 2.5,T S P ,C r ,C u ,P b ,S e ,a n dZ n )a n dw a t e rs e c t o r( C O 2,N O X,\nNH 3, CO, NMVOC, PM 10,P M 2.5,T S P ,C d ,C u ,P b ,S e ,Z n ,a n d\nNMVOC (non-combustion)). Relatively few of the emissions gener-ated by the waste and land transport sectors are correlated (CO\n2,\nNH 3, CO, Benzo(a)pyrene, and NMVOC (non-combustion)). WaterTable 3. Continued\nInput –output groups Correlation —CO2 Data robustaType\nNWaterDistrib–TOther 0.474 \u0013 N-T\nEnergyBiomass–TPipelines 0.468 \u0013 E-T\nTManufMotor–EnergyWind 0.464 \u0013 T- E\nEnergyPetrol–TOther 0.463 \u0013 T- E\nEnergynec–WWOther 0.462 \u0013 E-W\nEnergyWind–NWaterDistrib 0.462 x E-N\nEnergyPetrol–EnergyDistrib 0.475 \u0013 E-E\nEnergynec–WWFood 0.455 x E-W\nNWaterSupply–WWOther 0.449 x N-W\nEnergyWind–EnergyTransm 0.444 x E-E\nEnergyCoal–EnegryPetrol 0.442 \u0013 E-E\nEnergyWind–TPipelines 0.440 x E-E\nWBFood–WBSewage 0.434 x W-W\nTSea–WIWood 0.430 x T- W\nWBPaper–WBSewage 0.430 x W-W\nWIMetal–WWOther 0.428 x W-W\nWWFood–WLMetal 0.421 x W-W\nEnergySolar1–EnergySolar2 0.420 x E-E\nWCFood–WWOther 0.420 x W-W\nEnergyNuclear–EnergyDistrib 0.410 x E-E\nEnergyNuclear–EnegryPetrol 0.409 x E-E\nWIOil–WWFood 0.407 x W-W\nTSaleFuel–WLPaper 0.407 x T- W\nTRail-TSaleFuel 0.406 \u0013 T-T\nEnergyGas–Communic 0.405 \u0013 E-C\nEnergySolar1–TRail 0.401 x E-T\nTSaleFuel–WLFood 0.396 \u0013 E-W\nTSaleFuel–WLTextile 0.387 \u0013 T- W\nEnergyGas–TRail 0.385 \u0013 E-T\nWBFood–WCPaper 0.381 x W-W\nWWFood–WLWood 0.381 x W-W\nEnergyGas–TWaterLand 0.378 \u0013 E-T\nWBPaper–WCPaper 0.377 x W-W\naThe connection is included in the analysis if fewer than 10% of the data points used in the correlation are missing ( \u0013); if more than 10% are missing, the connection is\nremoved (x)Engineering Sustainability The environmental value of transport\ninfrastructure in the UK: an EXIOBASE analysisKalyviotis, Rogers and Hewings\n12transport emissions correlate with those from the water (CO 2and\nNH 3), communication (CO 2), and waste (CO 2,N H 3,a n dC u )s e c t o r s .\nFinally, there are air transport emissions that correlate with the water\n(CO 2,N H 3, Cd, and Pb), communication (CO 2,N O X,a n dC O ) ,a n d\nwaste (CO 2,S O X,a n dN H 3) sectors.\nThe correlation of emissions between the transport and energy sectors\nis a vital area of research, as both sectors significantly contribute tothese environmental impacts. Although data reveal a weak to moder-ate dependency, nearly all IOGs within these sectors exhibit at least\none correlation with IOGs from the other (the exceptions being two\nIOGs in the transport sector and one in energy). These findingsbroadly align with established expectations in the literature but aremade specific herein.\nThe relationship between the transport and waste sectors is\ncomplex. Transport directly correlates weakly with the IOGs\nof food and textile waste landfills while indirectly influencingand being influenced by other waste IOGs that correlate withthese two. However, given the absence of direct correlations\nof transport with other IOGs, drawing definitive conclusions ischallenging. Through these indirect interactions, transport also con-\nnects with the energy and water sectors by way of the waste sector.\nConversely, waste management practices can directly impact the\nenvironmental footprint of the transport sector.\nThe connection between land transport and water resources is rela-\ntively weak, limited to a correlation with the steam and hot water\nsupply IOG. This IOG also correlates with the energy sector, cre-\nating an indirect interaction between energy and transport.\nThe interconnection between transport and communication is evident,\nwith medium to strong correlations found in rail transport and the sale,\nmaintenance, and repair of motor vehicles, parts, and accessories IOGs.\nIntra-sectoral interdependencies often exhibit strong or very strong\ncorrelations. However, water demonstrates a strong to very strong\ndependency on energy, while communication shows a medium to\nstrong dependency on transport.\nIn conclusion, the correlations between transport and the other eco-\nnomic infrastructures (energy, waste, water, and communication)Figure 2. Network diagram of CO 2emission generation correlation between sectorsEngineering Sustainability The environmental value of transport\ninfrastructure in the UK: an EXIOBASE analysisKalyviotis, Rogers and Hewings\n13underscore the interconnected nature of environmental impacts.\nThese relationships point to opportunities for targeted i nnovations to\nsignificantly reduce emissions. Integrated policies and advanced tech-nologies can amplify these benefits, with methodologies such as\nPearson correlation and PCA ensuring reliable and actionable\ninsights. Future research should further explore these interdependen-cies, utilising advancements in data analytics and modelling to de-velop holistic strategies for sustainable development.\nA more in-depth environmental analysis is required to understand\nhow the production of different pollutants interacts, as these rela-tionships are essentially the result of economic analysis (similar tohybrid LCA methods) based on quantitative data rather thanqualitative. Taking this argument further, future research should\nfocus on developing integrated models that combine quantitativeand qualitative data to better understand the cause –effect relation-\nships between different pollutants produced by transport infra-\nstructure. This approach can help policymakers make informed\ndecisions on promoting sustainable transport infrastructure. In addi-tion, expanding the scope of studies to include diverse geographical\nregions will enhance the generalisability of findings and support the\ndevelopment of globally applicable environmental policies.\nAcknowledgements\nThe authors gratefully acknowledge the University of Birmingham,\nthe University of Illinois at Urbana –Champaign, the University ofTable 4. Environmental infrastructure interdependencies\nEmission Transport dependency Air Land Water\nCO2 Energy; communication;\nwater; wasteEnergy; communication;\nwater; wasteEnergy; communication;\nwater; wasteEnergy; communication;\nwater; waste\nCH4 Energy Energy Energy —\nN2O Energy; communication — Energy; communication —\nSOx Energy; communication;\nwater; wasteEnergy; waste Energy; communication —\nNOx Energy; communication;\nwater; wasteCommunication Energy; communication;\nwater—\nNH3 Energy; communication;\nwater; wasteEnergy; water; waste Energy; communication;\nwater; wasteEnergy; Water; Waste\nCO Energy; communication;\nwater; wasteEnergy; communication Energy; communication;\nwater; waste—\nBenzo(a)pyrene Energy; communication;\nwaste— Energy; Communication;\nWaste—\nBenzo(b)fluoranthene Energy; communication;\nwaste— Energy; communication Energy\nBenzo(k)fluoranthene Energy; communication Energy Energy; communication Energy\nIndeno(1,2,3-cd)\npyreneEnergy; communication;\nwaterEnergy Energy —\nPCDD_F (Missing values) (Missing values) (Missing values) (Missing values)\nNMVOC Energy; communication;\nwaterEnergy Energy; communication;\nwater—\nPM10 Energy; communication;\nwaterEnergy Energy; communication;\nwater—\nPM2.5 Energy; communication;\nwaterEnergy Energy; communication;\nwater—\nTSP Energy; communication;\nwaterEnergy Energy; communication;\nwaterEnergy\nAs (Missing values) (Missing values) (Missing values) (Missing values)\nCd Energy; water Energy; water Energy; water —\nCr Energy; communication Energy Energy; communication —\nCu Energy; communication;\nwaterEnergy Energy; communication;\nwaterWaste\nHg (Missing values) (Missing values) (Missing values) (Missing values)\nNi Energy Energy Energy —\nPb Energy; communication;\nwaterEnergy; water Energy; communication;\nwater—\nSe Energy; communication;\nwaterEnergy Energy; communication;\ncater—\nZn Energy; communication;\nwaterEnergy Energy; communication;\nwater—\nNMVOC (non-\ncombustion)Energy; water; waste — Energy; water; waste —Engineering Sustainability The environmental value of transport\ninfrastructure in the UK: an EXIOBASE analysisKalyviotis, Rogers and Hewings\n14Thessaly and the financial support of the UK Engineering and Physical\nSciences Research Council under grant numbers EP/K012398\n(iBUILD: Infrastructure Business Models, Valuation and Innovation\nfor Local Delivery), EP/J017698 (Transforming the Engineering of\nCities to Deliver Societal and Planetary Wellbeing, known as Liveable\nCities), and EP/R017727 (UK Collaboratorium for Research on\nInfrastructure and Cities). This research informed a newly awardedgrant from the Medical Research Council for the Healthy Low-Carbon\nTransport Hub (Grant No. MR/Z506382).",
      "metadata": {
        "filename": "The environmental value of transport infrastructure in the UK_ an EXIOBASE analy.pdf",
        "hotspot_name": "PBT_Injection_Molding",
        "title": "The environmental value of transport infrastructure in the UK: an\n  EXIOBASE analysis",
        "published_date": "2025-04-26T11:33:03Z",
        "pdf_link": "http://arxiv.org/pdf/2504.20098v1",
        "query": "PBT injection molding LCA environmental impact"
      }
    },
    "PCB Renewal_ Iterative Reuse of PCB Substrates for Sustainable Electronic Making": {
      "full_text": "PCB Renewal : Iterative Reuse of PCB Substrates for\nSustainable Electronic Making\nZeyu Yan\nzeyuy@umd.edu\nUniversity of Maryland\nCollege Park, Maryland, USAAdvait Vartak\navartak@terpmail.umd.edu\nUniversity of Maryland\nCollege Park, Maryland, USAJiasheng Li\njsli@umd.edu\nUniversity of Maryland\nCollege Park, Maryland, USA\nZining Zhang\nznzhang@umd.edu\nUniversity of Maryland\nCollege Park, Maryland, USAHuaishu Peng\nhuaishu@umd.edu\nUniversity of Maryland\nCollege Park, Maryland, USA\nFigure 1: PCB Renewal : a single piece of FR-4 was renewed for four iterations across three projects, significantly reducing\nresource consumption compared to engraving new circuits for each iteration.\nABSTRACT\nPCB (printed circuit board) substrates are often single-use, lead-\ning to material waste in electronics making. We introduce PCB\nRenewal , a novel technique that “erases” and “reconfigures” PCB\ntraces by selectively depositing conductive epoxy onto outdated\nareas, transforming isolated paths into conductive planes that sup-\nport new traces. We present the PCB Renewal workflow, evaluate\nits electrical performance and mechanical durability, and model its\nsustainability impact, including material usage, cost, energy con-\nsumption, and time savings. We develop a software plug-in that\nguides epoxy deposition, generates updated PCB profiles, and calcu-\nlates resource usage. To demonstrate PCB Renewal ’s effectiveness\nand versatility, we repurpose a single PCB across four design itera-\ntions spanning three projects: a camera roller, a WiFi radio, and anESPboy game console. We also show how an outsourced double-\nlayer PCB can be reconfigured, transforming it from an LED watch\nto an interactive cat toy. The paper concludes with limitations and\nfuture directions.\nCCS CONCEPTS\n•Social and professional topics →Sustainability ;•Hardware\n→Printed circuit boards ;•Human-centered computing →\nSystems and tools for interaction design .\nKEYWORDS\nPCB Prototyping, Sustainability, Reuse, Renewal, FabricationarXiv:2502.13255v1  [cs.HC]  18 Feb 2025Yan, et al.\n1 INTRODUCTION\nPrinted circuit boards (PCBs) are critical components in nearly all\nelectronic devices. However, the obsolescence of PCBs, from their\ndesign process to end-of-life disposal, has become an increasingly\nsignificant source of electronic waste (e-waste).\nThe design of a functional PCB typically involves multiple stages,\nincluding software simulation, circuit validation (e.g., via bread-\nboarding), and prototyping with custom PCB batches. While the\nsimulation and breadboard validation phases generate minimal\ne-waste—since engineers test functionality digitally or reconfig-\nure reusable components like breakout boards, through-hole elec-\ntronics, and jumper wires—the subsequent PCB prototyping stage\ninevitably contributes to e-waste production.\nPCBs are made using a subtractive fabrication method, where\ncopper layers are permanently etched from laminated substrates\n(e.g., FR-4, a fiberglass-reinforced epoxy), making the process in-\nherently irreversible . During PCB prototyping, minor errors—such\nas flaws in electronic design automation (EDA) schematics or mis-\nmatches between PCB dimensions and their housing—are often\ndiscovered. While these issues may be small and easily corrected\ndigitally, they are physically embedded into the substrates, ren-\ndering the entire prototype (or batches, if outsourced to factories)\nunusable. This necessitates the repeated production of new PCBs,\nwhile discarded ones contribute to e-waste.\nMass-produced PCBs further exacerbate the e-waste problem\nwhen devices reach the end of their life cycle. In 2022, less than 23%\nof globally generated e-waste was formally collected and recycled.\nEven when PCBs are recycled, their inherently irreversible fabri-\ncation process forces them into centralized waste streams, where\nthey are processed indiscriminately. As a result, they are rarely\nrepaired, repurposed, or reused—even though many PCBs and their\nsubstrates remain functional [4].\nThese e-waste challenges have garnered attention in the HCI\ncommunity, as evidenced by sustainable making and unmaking\nworkshops at UIST and CHI [ 56,66], and a dedicated TOCHI special\nissue [ 60]. Recent work has also called for a reimagining of end-\nusers’ roles, emphasizing their potential not only as consumers\nbut also as active participants in PCB recycling and reuse [ 43].\nAdditionally, researchers have advocated for the development of\nnew processes, tools, and infrastructure to address e-waste and\npromote sustainable practices [67].\nIn this paper, we contribute to sustainable PCB practices by\nproposing a reversible PCB substrate fabrication process that en-\nables the “erasure” and “reconfiguration” of copper layouts. Central\nto this process is the additive restoration of removed copper areas\nusing conductive fillers, such as conductive epoxy, to renew the PCB\nsubstrate for fresh trace patterns. Analogous to a correction pen\noverwriting mistakes on paper, our approach extends the lifespan\nof PCB substrates by enabling physical re-editing to correct design\nerrors or remove obsolete traces. This transforms what would oth-\nerwise become e-waste into new designs (Figure 1). We call this\napproach PCB Renewal .\nIn the remainder of this paper, we introduce the workflow of\nPCB Renewal , providing a detailed examination of conductive\nfiller materials and the key fabrication processes involved in the\nrenewal of the commonly used PCB substrate FR-4. We validateour approach through a series of experiments that evaluate key\nelectrical parameters, including conductivity, current capacity, sol-\nder joint durability, and the number of renewal iterations a single\nFR-4 board can undergo. These experiments demonstrate that the\nrenewed substrate exhibits electrical performance comparable to\nthat of raw FR-4. To assess the sustainability impact of PCB Re-\nnewal , we present a quantitative analysis model that compares\nPCB Renewal with the fabrication of new circuits using raw FR-4.\nThis model includes estimates of material usage, cost, time, and\nenergy consumption. To help end-users incorporate PCB Renewal\ninto their workflow to save PCB substrates during prototyping or\nrepurpose PCB designs in general, we develop an EDA software\nplug-in. This plug-in allows end users to update a circuit design\nwith changes visualized across iterations, evaluate the sustainabil-\nity impact of specific renewed designs, and generate the fabrication\nprofiles required for renewal.\nPCB Renewal can be applied to PCBs fabricated either in-house\nor through outsourcing. To demonstrate its versatility, we provide a\ndetailed account of a single PCB reused across four in-house design\niterations for three distinct projects: a wireless camera roller, a WiFi\nradio, and an ESPboy game console. Additionally, we demonstrate\nthat an outsourced double-layer PCB, originally made for an LED\nwatch, can be renewed and repurposed for a cat toy using the\nPCB Renewal process. We report the sustainability impact of each\ndesign iteration for all examples. We conclude with a discussion on\nthe limitations of PCB Renewal and its potential future directions.\n2 RELATED WORK\nOur work is inspired by a substantial body of prior research in\nsustainable human-computer interaction (SHCI), methods for recy-\ncling or reusing electronic and electronic waste, as well as technical\nexplorations in PCB substrate repair and renewal.\n2.1 Sustainability in HCI: Making and\nPrototyping\nThe notion of Sustainable Interaction Design (SID) was introduced\nby Blevis [ 9] over a decade ago, providing a foundational framework\nfor addressing environmental impacts and human behavior in the\ndesign of interactive technologies. This concept has since evolved\ninto the broader field of SHCI.\nEarly discussions in SHCI often centered on mobile applications\nand their influence on end-users’ daily behaviors, such as reduc-\ning energy consumption through persuasive computing [ 24,25].\nMore recently, attention has shifted to the environmental impact\nof making and physical prototyping [ 56,66], driven by the democ-\nratization of personal fabrication tools and the growing maker\nmovement [31, 55].\nSeveral studies have explored end-users’ (creative) approaches\nto engaging with wasted physical materials in daily activities. For\nexample, Yan et al. [ 67] have presented a qualitative research that\nmaps out the sustainability practices, challenges and opportunities\nin modern makerspace setups and have called for new tools and\ninfrastructure to support making sustainably. Kim and Paulos [ 36]\nhave proposed a reuse composition framework, based on online\nsurveys and observations, to inspire the creative reuse of material\nwaste. Dew and Rosner [ 21] have conducted design explorationsthat examine how designers conceptualize, manage, and rework\nwaste materials in educational makerspaces. Similarly, Maestri and\nWakkary [ 45] have studied the intersection of repair and creativ-\nity within household settings. These ideas have since evolved into\nbroader concepts, such as unmaking [ 58], uncrafting [ 50], and un-\nfabricating [ 65], which employ speculative or participatory design\nlenses to explore the afterlife of objects and materials.\nAlongside the exploration of reusing daily waste, HCI researchers\nhave begun investigating the use of decomposable and biodegrad-\nable materials in making. For example, several projects have pro-\nposed using edible materials [ 11,57] or substances derived from\nfood waste [ 54] as construction materials for molding and 3D print-\ning. Microbe-based materials, such as yeast [ 7] and fungi [ 34,62],\nas well as biomaterials derived from living organisms, including al-\ngae [ 6] and cellulose-based fibers [ 27,37], have also been proposed\nas building materials for the prototyping of interactive devices.\nIn addition, new fabrication processes and tools have been devel-\noped to support more sustainable making practices. For example,\nFilament Wiring [ 20] and Substiports [ 63] introduce alternative\nfabrication pipelines that repurpose wasted 3D printing filament or\nfailed prints for new designs. EcoThreads [ 74] and Desktop Biofibers\nSpinning [ 40] have developed new machines and processes to make\nwater-dissolvable yarns easily accessible for sustainable textile ap-\nplications.\nOur work is greatly inspired by the aforementioned advance-\nments in sustainable making, with a specific focus on the processes\ninvolved in PCB making. As discussed in the introduction, PCBs\nare among the largest contributors to e-waste. Our work aims to\nreduce this environmental impact.\n2.2 Supporting the Reuse and Recycling of\nElectronics\nE-waste recycling requires interdisciplinary research and collabo-\nrative practices.\nIn the electronics management industry, the primarily focus is\non infrastructure and large-scale processes that can extract raw\nmaterials from PCB scrap. For example, chemical and mechanical\ntechniques are used to recover valuable materials, including refrac-\ntory metals and elements of the platinum group found in standard\nPCB waste [ 29,48]. Although effective, these industrial and cen-\ntralized approaches void the opportunities for PCBs that might be\nrepurposed, repaired, or reused, and they may fall short as more\nindividuals become involved in creating electronics through the\ndemocratization of making tools.\nRecent HCI literature points out that many end users are no\nlonger just consumers of physical artifacts but also their creators.\nConsequently, they bear greater responsibility for managing the ma-\nterial waste generated during the individual making process [ 43,67].\nIn this context, much of the HCI research focuses on promoting\nthe reuse and recycling of electronics at the individual level. For\nexample, the CurveBoards project [ 73] proposes a custom-shaped\nbreadboard design that is versatile for rapid prototyping with form-\nspecific requirements. CircuitGlue [ 38] reduces waste in prototyp-\ning by allowing easy integration and reuse of off-the-shelf com-\nponents. SolderlessPCB [ 68] demonstrates a pressure-based PCBassembly method using 3D printed or CNC-made housings, allow-\ning easy disassembly and reuse of surface-mounted components.\necoEDA [ 42] shows how interactive circuit design software, by in-\ntegrating early-stage suggestions for utilizing recyclable electronic\ncomponents from stock PCBs, can facilitate the reuse of electronics\nthroughout the design process.\nNew, more environmentally friendly PCB materials and compo-\nsitions have also been explored. For example, transesterification\nvitrimers have been proposed as PCB substrate materials, which\ncan be recycled through polymer swelling, achieving a 98% poly-\nmer recovery [ 71]. Several studies have investigated PCB substrates\nbased on paper [ 15,35,64], wood [ 33], and water-soluble materi-\nals [2,8,26]. Water-soluble materials are particularly interesting in\nthe context of sustainable electronics, as their degrading processes\nare controllable. This enables the creation of transient electronic\nprototypes [ 30,70] with programmable lifespans, simplifying the\nrecycling of materials once they are no longer needed [16, 17, 59].\nOur work also aims to reduce material waste from PCBs. How-\never, instead of focusing on new materials that may not be readily\navailable to many, we seek to improve the workflow of the existing\nFR-4 substrate-based PCB manufacturing process. Our approach\nrelies solely on off-the-shelf conductive epoxy and CNC engraving\nmachines, which have become more affordable and widely available\nin makerspaces. As a result, our method has the potential to be\nadopted at scale.\n2.3 PCB Substrate Repair and Renewal\nAlthough PCBs are generally considered irreversible, several so-\nlutions have been proposed to repair minor errors or shorts. For\nexample, jumper wires can restore electrical continuity between\ndisconnected points [ 18], while conductive ink pens enable tem-\nporary, ad-hoc circuit repairs [ 13]. However, these methods are\nprimarily effective for minor fixes, such as bridging gaps over short\ndistances, and are not suitable for more complex repairs that require\nremoving multiple conductors or altering component footprints\nand placements.\nSeveral studies have investigated methods for fixing regional\ncircuit errors. For example, Chen et al. [ 14] have developed a local\nelectroplating technique to repair constrictions in copper traces.\nLim et al. [ 41] have proposed repairing broken circuit traces us-\ning reduced graphene oxide on a laser direct writing platform.\nLange [ 39] has demonstrated the use of UV and IR lasers to trim\nfuzzy edges of conductor shapes on PCBs, reducing the defect rates\nin PCB products. However, these approaches focus on repairing\ndefects in PCB traces rather than addressing circuit design errors\nthrough rerouting or editing existing circuits.\nPrior to our work, preliminary explorations have demonstrated\nthe potential of using conductive filler deposition to modify or\nrepair existing circuit diagrams on substrates. For example, Self-\nhealing UI [ 51] has introduced a composite material capable of au-\ntonomously repairing circuit wiring made of multiwall carbon nan-\notubes by leveraging the dynamic cross-linking properties of poly-\nborosiloxane polymers. However, carbon nanotubes are hazardous\nand require specialized handling, and circuits made with this com-\nposite are limited to low-fidelity prototypes. Circuit Eraser [ 52] has\nproposed using a standard eraser to remove circuit traces printedYan, et al.\nwith conductive ink, facilitating rapid iteration of circuit design.\nSilver Tape [ 15] enables circuit trace repair via tape transfer of\ninkjet-printed silver ink. Furthermore, Marghescu et al. [ 22] and\nDrumea et al. [ 46] have evaluated the current-carrying capacity of\nsectional circuit traces made with nickel and silver paste, confirm-\ning the potential of PCB repair using conductive pastes.\nBuilding upon previous research, we investigate the additive\nmethod of paste deposition as an alternative to the conventional\nsubtractive PCB engraving process. This approach enables the re-\nnewal of circuit boards originally fabricated using methods such\nas CNC engraving or photochemical etching. Furthermore, our\nmethod enables the editing of large conductive areas, allowing an\nexisting PCB designed for a specific purpose to be repurposed for\ndifferent projects. This, therefore, increases the opportunity to reuse\notherwise wasted PCBs, reducing unnecessary e-waste.\n3PCB RENEWAL\nPCB Renewal is a simple yet effective approach to repurposing\nPCB substrates that would otherwise be discarded. It helps reduce e-\nwaste during PCB prototyping by enabling the correction of design\nerrors, such as incorrect circuit trace connections or component\nmisplacements, directly on faulty PCBs. Additionally, PCB Renewal\nfacilitates the reuse of obsolete outsourced PCBs, particularly open-\nsource designs. By updating trace areas that are no longer needed,\nit provides retired PCBs with new functionalities.\nAs illustrated in Figure 2, the core of the renewal process is\nthe selective deposition of conductive filler material into isolation\ngrooves to “erase” existing circuit traces or pads, allowing new\nconductive traces to be re-engraved. PCB Renewal assumes access\nto conductive epoxy as a filler material and a CNC or laser-cutting\nmachine for modifying the PCB substrate. To support this process,\nan EDA software plug-in (Section 6) has been developed to compare\nnew circuit designs with the existing layout and apply selective\nmodifications only where necessary.\nBy preserving existing copper conductors and much of the fiber-\nglass substrate, PCB Renewal significantly reduces material waste,\nmanufacturing costs, and energy consumption while maintaining\na fabrication time comparable to producing a new PCB. Its core\nrefilling and re-engraving processes are independent of board type,\nmaking it suitable for both in-house and factory-made PCBs, as\nwell as single- and double-sided designs, though creating new vias\nfor double-sided PCBs requires manual effort.\nIn the following sections, we use in-house PCBs with FR-4 sub-\nstrates to explore key considerations and experiments related to\nPCB Renewal . In Section 7, we showcase examples of repairing and\nrepurposing PCB substrates fabricated both in-house and through\noutsourced manufacturers.\n3.1 Material\nThe key to PCB Renewal is refilling the isolation grooves of a\nPCB substrate to restore the conductive plane. This requires the\nconductive filler material to exhibit high conductivity, comparable\nto that of the original copper conductors. In addition, the filler\nmaterial must form a robust bond with the PCB substrate while\npossessing physical properties that allow for controlled and precise\ndeposition.\nFigure 2: PCB Renewal principle illustration: a) initial PCB\nengraved, b) selectively depositing conductive filler into iso-\nlation grooves, c) re-engraving new circuit trace.\nOur search for suitable materials began with solder wire and\nsolder paste, widely accessible conductive materials known for\ntheir excellent electrical conductivity. However, these materials are\ndesigned primarily to create strong metal-to-metal bonds between\nelectronic components and copper circuit pads. Specifically, they\nexhibit high surface tension in their liquid state and are formulated\nto form metallurgical bonds exclusively with unoxidized metal\nsurfaces [ 32]. As fiberglass is inert to metallurgical bonding, solder\ntends to flow toward the copper surface rather than settling in\nisolation grooves.\nIn contrast to solder, conductive epoxy products are widely used\nin PCB screen printing and plotting processes. These polymer-based\nconductive epoxies exhibit high electrical conductivity for circuit\ntraces and cure to a glassy state rather than transition to a high-\nsurface-tension liquid, as is the case with solder. This property\nallows for uniform bonding to both metallic and inert substrates.\nConductive epoxies are formulated with a variety of fillers, in-\ncluding silver, nickel, copper, carbon, and graphene. Notably, silver-\nbased epoxies are available in single-part formulations that require\nno mixing and do not need specialized curing treatments, such as\nformic acid fumes, laser processing, or flash lamp exposure. There-\nfore, we surveyed a range of off-the-shelf, single-part, thermoset\nsilver-based conductive epoxies, as shown in Table 1.\nWe considered four technical criteria when selecting the appro-\npriate conductive epoxy. First, the curing temperature must not\nexceed the maximum operating temperature of commonly used\nPCB substrates such as FR-4 ( 150◦Cfor Tg150 FR-4). Second, we\nprioritized materials with lower volume resistivity to maximize the\ncurrent-carrying capacity of the traces passing through epoxy-filled\nareas. Therefore, we targeted a volume resistivity of the conductive\nepoxy that does not exceed 10µΩ·cm, which is within the same\norder of magnitude as copper. Third, the viscosity of the material\nat room temperature is critical. Through empirical testing, we ob-\nserved that excessive viscosity hinders efficient flow and proper\nfilling of the filler material in tiny engraved grooves, resulting in\npoor mechanical bonding and unreliable electrical connections (Fig-\nure 3a). On the other hand, excessively low viscosity causes the\nepoxy to flow away from the intended deposition areas or spread\nunevenly along the engraved grooves (Figure 3b). Based on our\nexperiments, we determined that a room-temperature viscosity of\napproximately 10- 15Pa·ssatisfies our requirements. Fourth, to\nsimplify the filler deposition process, we exclusively considered\nsingle-part conductive epoxy. This choice eliminates the need for\nmixing and minimizes material waste from residual mixtures.Table 1: Silver Epoxies Surveyed\nName (code) Volume resistance ( µΩ·cm) Viscosity ( Pa·s) Curing time ( min) Curing temp (◦C)\nVoltera Conductor 3 127 not reported 15 170\nAA-duct 2979 30 65 15 150\nACI FS0142 6 15 15 150\nDM-SIP-3072S 7.5 10 10 150\nMetalon ®HPS-021LV 10.4 2.6 30 150\nFigure 3: Illustration of conductive epoxy behavior at differ-\nent viscosities: a) excessively high viscosity, b) excessively\nlow viscosity.\nThese criteria led to the selection of two materials from the tested\nsilver-based conductive epoxies: ACI FS0142 and DM-SIP-3072S.\nBased on material availability at the site the research was conducted,\nACI FS0142 was chosen for all samples in this study unless otherwise\nnoted. This heat-cured, single-part epoxy is specifically designed for\nPCB screen printing, has a viscosity of 15Pa·sat room temperature,\nand cures at 150◦Cin 15 minutes.\nNote that the goal of this search was to identify one conductive\nfiller that meets our design requirements for PCB Renewal . This\nsurvey is not exhaustive, and other materials may perform equally\nwell or better.\n3.2 Fabrication Pipeline\nThe fabrication pipeline for renewing a PCB consists of four main\nsteps: desoldering, depositing, curing, and engraving. We illustrate\nthis process (Figure 4) by correcting an in-house PCB with a trace\nthat was incorrectly connected due to a design error. Specifically,\nthe example circuit includes an ATtiny85, a toggle switch, a JST\nconnector, an LED, and a resistor that was mistakenly connected\nto the wrong pin of the ATtiny. During the renewal process, the\nincorrect trace is rerouted to connect to the correct pin, which\nis programmed to control the LED’s blinking. As noted earlier,\nwhile we used a single-sided, CNC-milled PCB as the walkthrough\nexample, our method is applicable to double-sided PCBs and those\nmanufactured through outsourcing.\nStep 1 — Desoldering :PCB Renewal begins with desoldering the\ncomponents from the old PCB (Figure 4a-b). This step is essential\nbecause material deposition, curing, and new trace engraving can\nonly be performed safely on a bare board. Although components\nfar from affected areas might theoretically remain in place during\nsmall, localized modifications, we recommend fully removing all\nFigure 4: Fabrication workflow: a) old board, b) desoldering,\nc) manual epoxy deposition with a stencil„ d) epoxy curing, e)\nengraving new traces, f) new functional PCB with a modified\ntrace.\ncomponents. The heat curing process often reaches the solder’s\nsoaking temperature range, potentially compromising connection\nperformance if components are left on the board.\nStep 2 — Depositing : After desoldering, conductive epoxy is de-\nposited into the engraved grooves to be restored. This process can\nbe performed manually, similar to applying solder paste, or using a\nCNC machine with a paste extruder add-on. In our case, we use a\nsyringe with a 23-gauge tapered blunt tip to manually deposit the\nconductive epoxy. An optional stencil can be generated from our\nsoftware plugin (see Section 6). The stencil profile features openings\nthat align with the isolation areas to be restored (Figure 4c). When\napplying epoxy, it is important to ensure that there are no visible\ngaps between the epoxy and the adjacent copper to prevent open\ncircuit spots on the updated board. Excess material can be removed\nmanually before peeling the stencil off the board or with a CNC\nmilling machine after the epoxy has cured.\nStep 3 — Curing : Once the epoxy is applied, the board is cured by\nplacing it in a convection oven or on a soldering hot plate. We cure\nthe epoxy at 150◦Cfor 15 minutes using a hot plate (see Figure 4d).\nStep 4 — Engraving : After curing, the board is allowed to cool to\nroom temperature before being placed on the CNC milling machine\nto engrave the updated traces (Figure 4e). An alignment bracket is\nused to position the bottom-left corner of the board at the machine\norigin. The engraving profile, obtained from the software plug-in,\nis then imported and aligned with the machine origin in the CNCYan, et al.\ncontrol software. Since cured silver epoxy is softer than the FR-4\nsubstrate, the engraving Gerber file and G-code can be generated\nusing the same tooling and settings as a standard FR-4 PCB. In this\nproject, all samples are engraved using a Bantam Tools desktop\nCNC milling machine [5].\n4 PERFORMANCE CHARACTERIZATION\nAsPCB Renewal introduces conductive materials other than cop-\nper for creating new PCB traces, it is essential to evaluate its electri-\ncal and mechanical performance to determine whether it can serve\nas a reliable iterative PCB making approach. To this end, this section\noutlines a series of experiments designed to evaluate PCB Renewal\n’s performance, focusing on factors such as fabrication resolution in\nepoxy areas, electrical conductivity at copper-epoxy intersections,\nthe current-carrying capacity of the traces, soldering performance,\nand the maximum number of renewal cycles achievable with this\nmethod.\n4.1 Fabrication Resolution\nIn CNC-engraved PCBs, the bonding strength between the con-\nductive and dielectric layers is inversely related to the minimum\ntrace width. Thinner traces are more prone to delamination from\nthe fiberglass substrate. Consumer-grade CNC milling machines\ngenerally recommend trace widths of at least 10 mil (where 1 mil\nis one-thousandth of an inch or 0.0254 mm) [1]. Renewed PCBs,\nwhich bond conductive epoxy to the fiberglass substrate through\nheat curing, may exhibit different bonding strengths compared to\ncopper in standard FR-4. To determine the minimal engravable\ntrace width for renewed PCBs, we conducted an experiment using\nvarying trace widths in a conductive epoxy pour.\nFigure 5: Fabrication resolution: trace engraving was at-\ntempted on a conductive epoxy pour at various trace widths.\nWe began by engraving a rectangular area on an FR-4 board to a\ndepth of 0.15mm, which is the typical depth for creating PCBs with\ndesktop CNC machines. The engraved area was then filled with\nconductive epoxy, leveled to flush with the surrounding copper,\nand cured on a hot plate. Once the epoxy was fully cured, 10mm\ncircuit traces with contact pads at both ends were engraved directly\nonto the epoxy surface. The trace widths ranged from 2 to 20 mil,\nincreasing in 2-mil increments. Each width was tested three times,\nwith the results shown in Figure 5. Traces narrower than 6 mil\nfailed in all three attempts, while those 6 mil and above consistently\nsucceeded, aligning with the recommended minimum trace widthfor CNC copper circuits. In practice, we recommend designing\ncircuit traces with the widest width that a design can accommodate\nto ensure optimal reliability.\n4.2 Electrical Conductivity\nA renewed PCB contains circuit traces made of silver epoxy or a\nhybrid of silver epoxy and copper. To understand how variations in\nmaterial composition affect trace conductivity, we conducted two\nsets of experiments.\n4.2.1 Conductive Epoxy Trace Conductivity. To evaluate the con-\nductivity performance of the silver epoxy traces, we used traces\nwith widths of 6 mil and above from the samples fabricated in\nSection 4.1. Since the actual width of the engraved traces is in-\nfluenced by manufacturing errors, we measured the actual trace\nwidth using a microscope stage, interpolating measurements to 0.1\nmil. The resistance of each trace was measured using a Keysight\n3446SECU digital multimeter. The average measured trace width\nand resistance for each specified trace width were calculated from\nmeasurements taken across three individual traces. The average\ntrace widths were rounded to two decimal places, while the av-\nerage resistance values were rounded to three decimal places, as\npresented in Table 2.\nTable 2: Conductivity of Conductive Epoxy Traces\nNominal width (mil) Measured width (mil) Resistance ( Ω)\n6 3.47 0.287\n8 6.83 0.134\n10 9.50 0.136\n12 10.93 0.108\n14 12.20 0.105\n16 14.37 0.102\n18 16.33 0.101\n20 18.03 0.088\nAs shown in the table, all the traces exhibit a resistance of less\nthan 0.3 Ωper centimeter, with the majority below 0.15 Ω, making\nthem suitable for implementing most low-frequency DC circuit\nfunctionalities.\n4.2.2 Material Interface Conductivity. PCB Renewal creates bond-\ning seams between copper and epoxy, through which current flows.\nTo assess the reliability of these seams, we conducted an experi-\nment simulating real-world conditions to evaluate the quality of\nthe connections at these points.\nWe began with five parallel grooves, each 15 mil wide—the small-\nest typical square end mill diameter used for circuit boards with\ndesktop CNC milling machines. Then, conductive epoxy was de-\nposited in each groove. After curing, we engraved five 20 mil traces\nperpendicular to the grooves. As a result, each trace contained 10\nepoxy-copper bonding seams for investigation (Figure 6a). We mea-\nsured the resistance of all 50 seams using a Keysight 3446SECU\ndigital multimeter, probing as closely as possible to both sides of\neach seam (Figure 6). The seams consistently exhibited a resistance\nof0.146 Ω with a standard deviation of 0.0345 Ω , demonstrating thatFigure 6: Material interface experiment: a) hybrid material\ntraces (20 mil wide), b) illustration of measurement points.\nhybrid-material circuit traces can achieve electrical performance\ncomparable to pure copper traces.\n4.2.3 Current Capacity. Introducing an additional material into\ncircuit trace formation can result in localized thermal accumulation\ndue to uneven resistance. To evaluate the performance of conduc-\ntive epoxy traces under high-current conditions, we tested the\ncurrent-carrying capacity of the traces fabricated in Sections 4.1\nand 4.2.2. Fixed currents of 1 A,3 A, and 5 Awere applied to each\ntrace using a bench power supply, and the temperature was moni-\ntored with a thermal camera. All experiments were conducted at a\nroom temperature of 22◦C.\nFigure 7: Current capacity experiment–thermal camera im-\nages of: a) 6 mil trace under 1A, b) 8 mil trace under 3A, c) 20\nmil trace under 5A, d) hybrid material 20 mil trace under 5A.\nWe observed that the temperature increase of all traces remained\nbelow 23◦Cunder a current of 1 A(Figure 7a). When subjected to\n3 A, 6-mil traces fused within three seconds, while the remaining\ntraces exhibited a maximum temperature rise of 66◦C(Figure 7b).\nUnder a 5 Aload, traces narrower than 20 mils fused in five seconds.\nHowever, the 20-mil traces remained functional, with a temperature\nincrease below 120◦C, which is within the 150◦CTg rating of the\nFR-4 board (Figure 7c). These results indicate that traces producedby our method have sufficient current-carrying capacity for low-\ncurrent DC signal circuits. For applications requiring currents above\n3 A, a minimum trace width of 20 mils is recommended.\nFurthermore, we observed that the hybrid traces fabricated in\nSection 4.2.2 exhibited higher temperature increases at the con-\nductive epoxy segments. However, at the same current levels, the\ntemperature rise did not exceed that of traces made entirely from\nconductive epoxy (Figure 7d).\n4.3 Solder Joint Performance\nIn addition to circuit traces, PCB assemblies must ensure both con-\nductivity and mechanical durability at solder joints. The renewed\nPCB design will likely incorporate solder pads partially or entirely\nmade of silver epoxy. We investigated the conductivity and strength\nof the solder joint using 0805 resistors and their corresponding sol-\nder pads. Following a process similar to that in Section 4.1, we\nfabricated silver epoxy-based traces with solder pads designed for\n0805 resistors. The resistors were soldered (Figure 8a) to six samples\nusing low-temperature solder paste [ 12], as recommended by the\nsilver epoxy manufacturer. Three samples were soldered using a\nhot plate, while the other three were soldered with a hot air blower.\nFigure 8: Solder joint experiment: a) an example of a solder\njoint experiment sample, b) the probing location adopted\nwhen measuring solder joint resistance, c) force gauge press-\ning on the soldered resistor at 30-degree angle, d) epoxy trace\nfailure while the solder joint stays intact.\nThe resistance of the solder joint was measured by probing\nthe solder pad and the corresponding resistor terminal, using a\n3446SECU digital multimeter (Figure 8b). For comparison, we fabri-\ncated another set of samples on copper substrates with identical\ntrace and pad geometry, soldering 0805 resistors using the same\nsolder paste and soldering methods. For both copper and epoxy\npads, the hot air blower and hot plate methods produced similar\nsolder joint resistance (Table 3, rows 1 and 2). The difference inYan, et al.\nTable 3: Solder Joint Conductivity and Strength\nSolder equipment Hot plate Hot air All samples\nCopper conductivity ( Ω) 0.17 0.18 0.18\nEpoxy conductivity ( Ω) 0.22 0.27 0.24\nCopper strength ( N) 87.67 71.40 79.53\nEpoxy strength ( N) 39.32 39.21 39.27\nresistance between solder joints on copper and epoxy pads was less\nthan 0.1 Ω, a negligible value that does not affect the functionality\nof DC or AC signal circuits.\nIn addition to conductivity, we used the same set of samples to\nevaluate the strength of the solder joints. Pressure was applied to\nthe soldered resistors at a 30-degree angle (Figure 8c) until they de-\ntached from the solder pad. A DFS100 force gauge recorded the peak\nforce value. The samples soldered on epoxy pads broke off with an\naverage force of 39.27 N, with a standard deviation of 16.16 N, while\nthose soldered on copper pads withstood an average of 79.53 N,\nwith a standard deviation of 17.51 N. Note that for the silver epoxy\nsamples, all break points occurred at the interface between the\nepoxy layer and the fiberglass, while the solder joints themselves\nremained intact (Figure 8d). In practice, we recommend avoiding\npure silver pads; however, if their use is necessary, increasing the\npad size and the width of the connecting traces can help mitigate\nthe risk of delamination. Additionally, during testing, all connection\npoints remained intact and functional, even after multiple drops\nfrom a height of 1.5 m.\n4.4 Number of Renewal Iterations\nIn theory, an FR-4 substrate can be renewed indefinitely, provided\nthat the newly engraved grooves consistently and completely re-\nmove the previous epoxy at the exact same Z-height. However, in\npractice, achieving this level of machining precision is not feasible.\nTo successfully renew a PCB, the engraving depth for new traces\nmust be set deeper than the epoxy deposited in the previous iter-\nation, which corresponds to the prior engraving depth. Based on\nempirical results, we recommend that with each renewal iteration,\nthe cutting depth be at least 0.05mmdeeper than the previous one.\nFigure 9: Multi-iteration renewal: a) original circuit, b)-f)\nsecond to sixth iteration of circuit modification, each rotated\nby 45 degrees counter-clockwise, g) the seventh iteration\nmodification with broken traces and pads, h) zoom in view\nof a broken pad at the interface of copper and epoxy.As the cutting depth gradually increases with each renewal iter-\nation on an FR-4 board, the trace is positioned progressively higher\nrelative to the bottom of the isolation grooves, making the circuit\ntraces more vulnerable during engraving. We tested the maximum\nnumber of renewal iterations using an octagon-shaped PCB. The\ninitial circuit consisted of an ATTiny85, a resistor, an LED, a JST\nconnector for a LiPo battery, and a mini toggle switch. It was origi-\nnally engraved with an isolation depth of 0.15mm. The minimum\nnominal trace width in the circuit was 16 mil (see Figure 9a). For\neach renewal iteration, we completely erased the old circuit by fill-\ning all engraved grooves with conductive epoxy, rotated the board\nby 45 degrees, and engraved the same circuit with an additional\n0.05mmisolation depth (see Figure 9b-f). We found that the circuit\ntraces remained functional until the seventh iteration, at which\npoint small solder pads and traces began to break (Figure 9g and h).\n5 MODELING THE SUSTAINABILITY IMPACT\nOFPCB RENEWAL\nThe primary goal of PCB Renewal is to promote sustainable PCB\nmaking by enabling the reuse of PCB substrates. To fully under-\nstand its impact, a detailed evaluation is essential. Ideally, a lifecycle\nassessment (LCA) [ 28] would be conducted to comprehensively as-\nsess the environmental effects of PCB Renewal . However, the\nvariability of each renewal scenario makes it difficult to generalize\nits impact. For example, if a new circuit design shares no traces with\nthe old one, the renewal process requires a near-complete removal\nof all old traces and the engraving of entirely new ones. Depending\non the PCB size, this may result in a trade-off, where a minor reduc-\ntion in FR-4 usage is offset by higher energy consumption for epoxy\ncuring, potentially negating any environmental benefits when an-\nalyzed quantitatively. Additionally, the lack of LCA data on most\nsilver-based epoxy products further complicates a comprehensive\nLCA evaluation in practice.\nTo address this, we adopted the DeltaLCA framework [ 72] and\ndeveloped a quantitative comparison model that evaluates key sus-\ntainability metrics commonly considered in LCA on a case-by-case\nbasis. This model estimates and compares material usage, cost, time,\nand energy consumption between renewing a PCB and fabricating\na new one from fresh FR-4. By analyzing these sustainability met-\nrics, end-users can make informed decisions, determining whether\nrenewing a PCB substrate is the more sustainable option or if fabri-\ncating a new one is preferable.\nNote that while this section focuses on modeling the sustainabil-\nity impact of PCB Renewal , the model is also integrated into the\nsoftware plug-in (Section 6). As a result, all modeling parameters—\nsuch as deposition path length and trace contour length—can be\ndirectly extracted from PCB design profiles, enabling the automatic\ncalculation of PCB Renewal ’s sustainability impact for each given\nPCB design.\n5.1 Modeling Material Usage and Cost\nDifferences\nWe chose to estimate material usage based on weight. While weight\nalone does not fully capture the material trade-offs between a PCB\nmanufactured using the renewal approach and one made with new\nsubstrate, it provides the most practical basis for comparison, giventhe lack of comprehensive carbon footprint data for most silver-\nbased conductive epoxies. In PCB Renewal , users are free to select\nany homemade or commercially available conductive filler follow-\ning our guidelines. However, variations in the filler’s composition,\nmanufacturing process, shipping distance, curing conditions, and\ncured material properties, along with other relevant factors, can\nsignificantly influence environmental impact metrics, including but\nnot limited to carbon emissions, energy footprint, and toxicity. For\nexample, the energy footprint associated with mining and produc-\ning different metal elements used in conductive materials can vary\nby several orders of magnitude [ 61]. Given these uncertainties, we\nprovide material usage data in terms of weight as a reliable and\nconservative basis for further environmental impact modeling. This\napproach ensures consistency and prevents both overestimation\nand underestimation of the environmental implications of adopting\nPCB Renewal .\nForPCB Renewal , the primary new materials introduced are\nconductive epoxy and, optionally, a deposition stencil sheet. The\nweight of epoxy required ( 𝑀𝑒) can be estimated by multiplying the\narea of the isolation grooves to be filled ( 𝐴𝑔) by the depth of the\ngrooves from the previous engraving iteration ( 𝑑) and the epoxy\ndensity (𝜌𝑒). We offset the deposition depth by 0.1mmby default\nto account for excess material. This parameter can be adjusted\nbased on actual deposition needs. The area of the stencil sheet\n(𝐴𝑠) corresponds to the surface area of the previous board design\n(𝐴𝑏_𝑜𝑙𝑑).\n𝑀𝑒=𝜌𝑒𝐴𝑔𝑑\n𝐴𝑠=𝐴𝑏_𝑜𝑙𝑑\nWhen calculating material usage for engraving a circuit on a new\nsubstrate, neither epoxy nor stencil material is involved. Instead, a\nfresh piece of FR-4 is used, with an area ( 𝐴𝐹𝑅−4) that matches the\nnew board design ( 𝐴𝑏_𝑛𝑒𝑤).\n𝐴𝐹𝑅−4=𝐴𝑏_𝑛𝑒𝑤\nWe calculate the cost difference between the two methods (de-\nnoted as𝑃) based on the unit prices ( 𝑝𝑢) of each raw material and\nthe estimated material usage.\n𝑃𝑑𝑒𝑙𝑡𝑎 =𝑀𝑒𝑝𝑢_𝑒+𝐴𝑠𝑝𝑢_𝑠−𝐴𝐹𝑅−4𝑝𝑢_𝐹𝑅−4\nA negative value indicates monetary savings when using PCB Re-\nnewal , while a positive value indicates additional costs. Disposable\nhardware and equipment, such as tooling, double-sided tape, and\nglassware, are excluded from the material usage and cost estimation.\n5.2 Modeling Fabrication Time Differences\nThe fabrication time for creating a circuit on a new substrate is the\nsum of trace engraving time, determined by the path length ( 𝐿𝑡),\nand board outline cutting time. The feed rate ( 𝐹𝑡) depends on the\nengraving bit. The number of passes is determined by the ceiling\nof the fraction of the engraving depth ( 𝑑𝑡)—typically 0.15mm—and\nthe stepdown ( 𝛿𝑧𝑡), which also depends on the engraving bit. The\nboard outline engraving time is calculated in the similar manner,\nbased on the outline length ( 𝐿𝑜), feed rate ( 𝐹𝑜), board thickness ascutting depth ( 𝑑𝑜), and stepdown ( 𝛿𝑧𝑜). The total fabrication time\ncan be estimated as follows:\n𝑇𝐹𝑅−4=𝐿𝑡\n𝐹𝑡⌈𝑑𝑡\n𝛿𝑧𝑡⌉+𝐿𝑜\n𝐹𝑜⌈𝑑𝑜\n𝛿𝑧𝑜⌉\nThe fabrication time in PCB Renewal comprises several compo-\nnents: desoldering time, solder pad cleaning time, epoxy deposition\ntime, epoxy curing time, engraving time, and an optional laser cut-\nting time for manufacturing the deposition stencil. Desoldering\ntime and solder pad cleaning time are highly dependent on the\nequipment used and the operator’s skill level. In practice, desol-\ndering time ( 𝑇𝑑𝑒) requires user estimation based on their specific\nscenario. The default value for desoldering time is set to 1 minute,\nas all example circuits in our experiments were desoldered within\nthis time frame using a hot plate. The solder pad cleaning time\n(𝑇𝑐𝑙) is calculated as the number of solder pads on the old board\n(𝑛𝑝) multiplied by the estimated cleaning time per pad ( 𝑡𝑝). Based\non empirical experiments, the typical cleaning time per solder pad\nusing a soldering iron is approximately 6 seconds. This value is\nset as the default, but users can adjust it to match their skill level.\nEpoxy is deposited along the contours of the conductors designated\nfor removal, with the extruder moving at a constant rate during de-\nposition. The estimated deposition time ( 𝑇𝑑) is calculated based on\nthe total deposition path length ( 𝐿𝑑) and the feed rate ( 𝐹𝑑), which\nis set at 3 mm/sfor manual deposition.\n𝑇𝑑=𝐿𝑑\n𝐹𝑑\nEpoxy curing time ( 𝑇𝑐) is a fixed duration specified in the conduc-\ntive epoxy’s datasheet. Engraving time consists of the same two\ncomponents as engraving a new board: trace engraving time and\nboard outline cutting time. These are calculated using the methods\ndescribed above, with the corresponding path lengths denoted as 𝐿′\n𝑡\nfor trace engraving and 𝐿′𝑜for board outline modification cutting.\nThe primary difference lies in the trace engraving depth. In PCB\nRenewal , the new conductors must be engraved 0.05mmdeeper\nthan previous iterations (see Section 4.4). Since the current renewal\nis the𝑛𝑡ℎiteration, the engraving depth is:\n𝑑′\n𝑡=𝑑𝑡+0.05(𝑛−1)\nThe stencil cutting time is estimated based on the contour length\nof the conductors to be removed ( 𝐿𝑠) and the feed rate of a CO 2\nlaser cutter ( 𝐹𝑙). Hence, the time difference between renewing and\nengraving a new PCB is:\n𝑇𝑑𝑒𝑙𝑡𝑎 =𝑇𝑑𝑒+𝑛𝑝𝑡𝑝+𝐿𝑑\n𝐹𝑑+𝑇𝑐+𝐿𝑠\n𝐹𝑙+𝐿′\n𝑡\n𝐹𝑡⌈𝑑′\n𝑡\n𝛿𝑧𝑡⌉−𝐿𝑡\n𝐹𝑡⌈𝑑𝑡\n𝛿𝑧𝑡⌉+𝐿′𝑜−𝐿𝑜\n𝐹𝑜⌈𝑑𝑜\n𝛿𝑧𝑜⌉\n5.3 Modeling Energy Consumption Differences\nThe primary energy consumption arises from the epoxy heat cur-\ning process, as well as the power drawn by machinery for epoxy\ndeposition, engraving, and stencil fabrication. Energy consumption\nfor desoldering, pad cleaning, deposition, engraving and stencil\ncutting is calculated by multiplying the estimated time for each\nstage by its respective power consumption. Thus, the difference in\nenergy consumption can be expressed as:Yan, et al.\n𝐸𝑑𝑒𝑙𝑡𝑎 =𝑇𝑑𝑒𝑃𝑑𝑒+𝑛𝑝𝑡𝑝𝑃𝑖+𝐿𝑑\n𝐹𝑑𝑃𝑑+𝑇𝑐𝑃𝑐+𝐿𝑠\n𝐹𝑙𝑃𝑙\n+(𝐿′\n𝑡\n𝐹𝑡⌈𝑑′\n𝑡\n𝛿𝑧𝑡⌉−𝐿𝑡\n𝐹𝑡⌈𝑑𝑡\n𝛿𝑧𝑡⌉+𝐿′𝑜−𝐿𝑜\n𝐹𝑜⌈𝑑𝑜\n𝛿𝑧𝑜⌉)𝑃𝑒\nwhere,𝑃𝑖denotes the power required by the soldering iron.\n6 SOFTWARE\nThePCB Renewal software (open-sourced on GitHub1) serves\nthree main purposes: visualizing and enabling direct comparison\nof two circuit designs, generating stencil profiles for epoxy filling\nand milling profiles for selective trace renewal, and automatically\nestimating the material usage, cost, time, and energy consumption\nsavings or trade-offs of a given design. The software is developed\nas a plug-in for the open-source EDA software KiCAD. The plug-in\nuses KiCAD’s Python bindings2to access PCB data, shapely3for\ngeometric operations, and wxPython4for the user interface.\n6.1 Software Plug-in Features\nThe user interface includes a sequence of essential features: loading\nEDA files, aligning design layouts, selecting PCB layers for compar-\nison, executing the comparison process, conducting sustainability\nanalyses, and exporting cutting profiles. A responsive visualization\npanel remains active throughout the workflow, providing real-time\nupdates based on user interactions to ensure immediate feedback.\nFigure 10: User Interface: a) importing boards, b) board align-\nment, c) board comparison, d) sustainability analysis param-\neters made customizable for different machine and tooling\nadoption.\n1Software plug-in: https://github.com/zyyan20h/PCBRenewal.git\n2KiCAD Python Bindings: https://dev-docs.kicad.org/en/apis-and-binding/pcbnew/\n3shapely: https://shapely.readthedocs.io/en/stable/\n4wxPython: https://wxpython.org/index.htmlBoard Comparison. Our software allows users to load two Ki-\nCad PCB designs for comparison (Figure 10a). Because the designs\nmay vary in size and position, an optional feature enables users to\nalign them using selected reference points, such as the corners of\nboard outline bounding boxes or the geometric centers of electronic\ncomponent footprints (Figure 10b). Once aligned, the software ex-\necutes the comparison algorithm in the background and displays\nthe results in the visualization window.\nOutput and Analysis. After comparison, the software automati-\ncally exports the stencil profile and engraving pattern as fabrica-\ntion inputs to the same directory as the “new board” design. While\nexporting these files, the software also performs a sustainability\nanalysis for the given renewal scenario and displays the result in\nthe log at the bottom of the plug-in interface. Based on these results,\nusers can decide whether to proceed with PCB Renewal or create\na new PCB from scratch. Calculation parameters are initialized\nwith default values that match the machines and tools used in our\ndemonstrations. Users can reconfigure these parameters in a pop-\nup window by clicking the “Edit Analysis Params” button (Figure\n10d).\n6.2 PCB Design Comparison Algorithms\nThe circuit design comparison results are used as both fabrication\ninput and sustainability impact analysis data. This process requires\nhighly accurate output to ensure minimal fabrication errors and\nreliable analysis results. To achieve a precise comparison between\ntwo KiCad board designs, we developed a custom algorithm that\nextracts board information from KiCad and converts it into vector-\nbased geometries.\nWe used KiCAD’s Python bindings to access the board informa-\ntion. Every PCB component (e.g. pads, tracks, holes) incorporated\nin our comparison algorithm is represented as a user-defined in-\nstance to preserve the integrity of the original data. Each board\nis represented by an instance of a custom Board class. A Board\ninstance contains a collection of nets—groups of electrical nodes\n(or pins) and tracks that are electrically connected on copper layers.\nThese nets are stored in a nested hash map, 𝐻, where each key cor-\nresponds to a layer names (e.g., F.Cu for the front layer or B.Cu for\nthe back layer), and each key points to a list of nets present on that\nlayer. Nets are represented by a custom class, and each net instance\ncontains a list of tracks, a list of pads, and the layer name to which\nit belongs. When the boards are imported, we initialize the board\ninstances according to the layer and net information retrieved from\nKiCad. We refer to the two board instances as 𝐵𝑜𝑙𝑑and𝐵𝑛𝑒𝑤, and\ntheir respective net hash maps as 𝐻𝑜𝑙𝑑and𝐻𝑛𝑒𝑤.\nThe comparison is carried out in two steps. First, all nets from\nthe old board are compared against each net of the new board. This\nstep identifies nets with identical geometry and position, which\nremain unchanged and can be excluded from further comparison.\nNext, the remaining nets in both board instances are converted\ninto flat polygons and subjected to Boolean union operations within\neach board. A second round of comparison is then performed on\nthe resulting compound polygon outlines, producing the final com-\nparison results.Net Wise Comparison. This step takes 𝐻𝑜𝑙𝑑and𝐻𝑛𝑒𝑤, and gener-\nates two new hash maps, 𝐻𝑜𝑙𝑑_𝑢𝑛𝑖𝑞𝑢𝑒 and𝐻𝑛𝑒𝑤 _𝑢𝑛𝑖𝑞𝑢𝑒 , each con-\ntaining nets with unique geometries from their respective boards.5\nIf the two boards share no common nets, then 𝐻𝑜𝑙𝑑_𝑢𝑛𝑖𝑞𝑢𝑒 will\ncontain all the net instances from 𝐵𝑜𝑙𝑑and𝐻𝑛𝑒𝑤 _𝑢𝑛𝑖𝑞𝑢𝑒 will con-\ntain all the conductors from 𝐵𝑛𝑒𝑤. The pseudocode block below\nillustrates the pairwise comparison of each list of nets within the\ncorresponding layers of the boards.\nAlgorithm 1: compareNets( 𝐻𝑜𝑙𝑑,𝐻𝑛𝑒𝑤,𝑆)\nin:Hash map of nets on the old board 𝐻𝑜𝑙𝑑, Hash map of\nnets on the new board 𝐻𝑛𝑒𝑤, Layers selected for\ncomparison 𝑆\nout: Hash map of unique old nets 𝐻𝑜𝑙𝑑_𝑢𝑛𝑖𝑞𝑢𝑒 , Hash map of\nunique new nets 𝐻𝑛𝑒𝑤 _𝑢𝑛𝑖𝑞𝑢𝑒\nlocal: Flag denoting whether a net in the old board has an\nidentical match in the new board 𝐹\n(1)𝐻𝑜𝑙𝑑_𝑢𝑛𝑖𝑞𝑢𝑒←empty hash map\n(2)𝐻𝑛𝑒𝑤 _𝑢𝑛𝑖𝑞𝑢𝑒←empty hash map\n(3)for each layer𝐿in𝑆do:\n(4)𝐻𝑜𝑙𝑑_𝑢𝑛𝑖𝑞𝑢𝑒[𝐿]← empty list\n(5)𝐻𝑛𝑒𝑤 _𝑢𝑛𝑖𝑞𝑢𝑒[𝐿]←𝐻𝑛𝑒𝑤[𝐿]\n(6) for each old net𝑁𝑜𝑙𝑑in𝐻𝑜𝑙𝑑[𝐿]do:\n(7) 𝐹←FALSE\n(8) for each new net𝑁𝑛𝑒𝑤in𝐻𝑛𝑒𝑤 _𝑢𝑛𝑖𝑞𝑢𝑒[𝐿]do:\n(9) if𝑁𝑜𝑙𝑑=𝑁𝑛𝑒𝑤then:\n(10) 𝐹←TRUE\n(11) Pop𝑁𝑛𝑒𝑤from𝐻𝑛𝑒𝑤 _𝑢𝑛𝑖𝑞𝑢𝑒[𝐿]\n(12) Exit loop\n(13) if not𝐹then :\n(14) Append𝑁𝑜𝑙𝑑to𝐻𝑜𝑙𝑑_𝑢𝑛𝑖𝑞𝑢𝑒[𝐿]\n(15) Return𝐻𝑜𝑙𝑑_𝑢𝑛𝑖𝑞𝑢𝑒 ,𝐻𝑛𝑒𝑤 _𝑢𝑛𝑖𝑞𝑢𝑒\nWhen comparing two nets (line 9 in Algorithm 1), we verify that\nthe position and geometry of all pads and tracks in both nets are\nidentical.\nGeometric Comparison. In this step, we convert all remaining\nunique nets into polygons for further comparison. Algorithm 2\ndetails the parsing process for these remaining nets within a single\nboard.\nFor each net—whether it is to be removed from the old board\ndesign or engraved into the new one—the fabrication process fo-\ncuses on the isolation area outside that net, either covering it or\nremoving materials. The minimum width of the isolation area is\nusually defined in the design rule checking (DRC) configuration. To\ndetermine the midline of the isolation area, we offset the outlines\nof each net by half of the minimum isolation width. This midline\nconservatively represents any possible machining or deposition\npath outside the net. Within each board layer, we then compute\nthe Boolean union of all polygons generated for the leftover nets\nin that layer, and store the resulting path in a new hash map 𝑃.\n5We use the notation hashmap[key] ←value to represent inserting or updating a\nvalue associated with a specific key in the hash map, mirroring Python’s dictionary\nsyntax.Algorithm 2: createPaths( 𝐻,𝑆)\nin:Hash map of nets 𝐻, Layers to compare\nout: Hash map of paths 𝑃\nlocal: offset outline of an individual net 𝑝𝑛𝑒𝑡, compound\ngeometry of all net in a layer 𝑝\n(1)𝑃←empty hash map\n(2)for each layer𝐿in𝑆do:\n(3)𝑝←blank shape\n// place holder for the Boolean union paths\n(4) for each net𝑁in𝐻[𝐿]do:\n(5) 𝑝𝑛𝑒𝑡←offset outline of 𝑁\n(6) 𝑝←Boolean union of𝑝and𝑝𝑛𝑒𝑡\n(7)𝑃[𝐿]←𝑝\n(8)Return𝑃\nUsing Algorithm 2, we parse the leftover nets across all layers\nin both boards. We then apply Boolean subtraction between the\nparsing results of each layer from each board (Algorithm 3). This\nprocess yields paths for deposition ( 𝐷𝑝𝑎𝑡ℎ) and engraving ( 𝐸𝑝𝑎𝑡ℎ),\neach having a equal to the minimum isolation width defined in\nDRC.\nAlgorithm 3: comparePaths( 𝐻𝑜𝑙𝑑,𝐻𝑛𝑒𝑤,𝑆)\nin: Hash map of nets on the old board 𝐻𝑜𝑙𝑑, Hash map of\nnets on the new board 𝐻𝑛𝑒𝑤, Layers selected for\ncomparison 𝑆\nout: Hash map of paths to deposit 𝐷𝑝𝑎𝑡ℎ, Hash map of\npaths to engrave 𝐸𝑝𝑎𝑡ℎ\n(1)𝑃𝑜𝑙𝑑←createPaths( 𝐻𝑜𝑙𝑑,𝑆)\n(2)𝑃𝑛𝑒𝑤←createPaths( 𝐻𝑛𝑒𝑤,𝑆)\n(3)𝐷𝑝𝑎𝑡ℎ←empty hash map\n(4)𝐸𝑝𝑎𝑡ℎ←empty hash map\n(5)for each layer𝐿in𝑆do:\n(6)𝐷𝑝𝑎𝑡ℎ[𝐿]←Boolean subtraction of𝑃𝑜𝑙𝑑[𝐿]and\n𝑃𝑛𝑒𝑤[𝐿]\n(7)𝐸𝑝𝑎𝑡ℎ[𝐿]←Boolean subtraction of𝑃𝑛𝑒𝑤[𝐿]and\n𝑃𝑜𝑙𝑑[𝐿]\n(8)Return𝐷𝑝𝑎𝑡ℎ,𝐸𝑝𝑎𝑡ℎ\nNote that, between individual renewal iterations, certain traces\nand pads from the old board do not need to be “erased” if the\ncorresponding area is not utilized in the new board design. However,\nit is uncertain whether future iterations will make use of the areas\nthese traces and pads occupy. To preserve the potential for all future\nrenewal iterations, our software, by default, “erases” all undesired\nnets from the old board.\nSupport of Vias and Through-Hole Components. Vias are com-\npared within their own category across the two boards. When a\nvia from the old board is no longer used in the new design, it is\nreplaced with a hole in the engraving profile. These holes, along\nwith existing holes in the old board designed for through-hole com-\nponents and mechanical assembly, are considered outside board\nusable profile and do not support new traces and pads. If any newYan, et al.\ntraces or pads overlap with these areas, our software will generate\nan error message and a corresponding visualization layer in yellow\ncolor to alert the user.\nOutline Comparison. In addition to comparing the copper layers,\nthe plugin also compares the board outlines. It does this by by con-\nverting the board outlines into polygons and performing a Boolean\nsubtraction on those polygons. This results in a polygon that serves\nas a guide for trimming the old board to convert it into the new\none.\nThe plugin uses the shapely python library to perform geometric\nparsing and Boolean operations.\n7 EXAMPLE PCB RENEWAL SCENARIOS\nIn this section, we present a series of walkthrough examples. Sec-\ntions 7.1 through 7.3 showcase a single CNC-milled substrate being\nreused across four design iterations within three distinct projects.\nSection 7.4 further demonstrates that PCB Renewal can be applied\nto factory-made, double-layer PCBs. These examples highlight how\nPCB Renewal facilitates local alterations to circuit traces and board\noutlines, enabling error correction and functional updates. Addi-\ntionally, they demonstrate the versatility and range of electrical\nfunctionalities achievable with these updated hybrid material cir-\ncuits.\nWe report the sustainability analysis results for each example.\nFor trace engraving, we used a 1/64-inch square end mill, while\na 1/32-inch square end mill was used for outline engraving. The\ncorresponding tooling parameters were applied to estimate the\nfabrication time. Additionally, we measured the average power\nconsumption of our machines using an appliance wattage moni-\ntor. During operation, the CNC machine consumes approximately\n47 W for engraving, the hotplate averages 22 W for desoldering,\nthe solder iron used for pad cleaning consumes 21.5 W, the laser\ncutter requires 8 Wfor stencil cutting, and the heater operates at\nan average of 22 W during the heat-curing process. We set the des-\noldering time to 1 minute and the cleaning time for each solder pad\nto 3 seconds. These values are used as inputs for energy estimation.\nThe standardized analysis data are visualized in radar graphs for\neach renewal iteration.\n7.1 Iteration One and Two — Camera Roller\nIn this example, we created a camera roller designed to achieve\nfluid, dynamic shots, such as tracking, panning, and dollying. The\noriginal circuit board was developed to control two DC gear motors\nusing an ESP8266 microcontroller. However, an error was identified\nin the ESP8266 accessory circuit—its enable pin requires an external\npull-up resistor when resetting the board or entering download\nmode, preventing us from uploading code to the ESP8266.\nTo correct this, we needed to add a resistor and connect it to two\nexisting conductors, which also required relocating some compo-\nnents and traces. In a conventional PCB prototyping process, this\nwould have required manufacturing an entirely new PCB, as traces\ncannot be easily altered or added.\nWith PCB Renewal , however, we were able to make these minor\nadjustments directly on the existing prototype. This enabled us to\nimplement the necessary modifications without the waste of materi-\nals or energy required to fabricate a new board. The corrected PCBnow functions as intended, allowing control code to be uploaded\nand ensuring smooth operation of the camera roller (see the left\ncolumn of Figure 11).\nBetween the first and second design iterations, PCB Renewal\nallowed us to save 6402.90mgof FR-4, 71.91kJof energy and\n15.25min in fabrication time, while consuming only 4.06mgof\nsilver epoxy, reducing the cost of raw material by 98.4%.\n7.2 Iteration Three — WiFi Radio\nWith the camera roller design finalized, the prototype PCB was no\nlonger needed. However, much of its circuitry, especially the sec-\ntions supporting the ESP8266 microcontroller, remained potentially\nuseful for other projects. Instead of discarding the entire board, we\nselectively removed and updated only the necessary components\nof the camera roller PCB, repurposing it for a new project.\nIn this case, we transformed the otherwise obsolete camera roller\nPCB into a WiFi radio controller while retaining much of the origi-\nnal microcontroller circuitry. The modifications mainly involved\nswapping out the motor driver and connectors for an audio ampli-\nfier, speaker connections, and a potentiometer. We designed the\nnew circuit layout using KiCAD and utilized the PCB Renewal\nplug-in to evaluate the sustainable impact of updating the old board.\nWe then physically implemented the updated PCB by selectively\nremoving and updating the traces and pads, as well as reducing\nthe board size to fit the new radio design. The renewal process is\ndocumented in the middle column of Figure 11.\nIn addition to demonstrating how PCB Renewal can support the\nprototyping of a complete new project using an obsolete PCB, this\nWiFi radio example also showed that the renewed PCB, with circuit\ntraces made from hybrid materials, could support audible-frequency\ndata transmission while maintaining low noise levels.\nIn this design iteration, renewing the PCB allowed us to save\n5602.15mgof FR-4 and 32.03kJof energy while consuming only\n105mgof silver epoxy, reducing material cost by 74.6%. The fabrica-\ntion time is comparable to engraving a new piece of FR-4, with PCB\nRenewal taking only 3.89 minutes longer despite the additional\ndesoldering and cleaning steps.\n7.3 Iteration Four — ESPBoy Game Console\nOne FR-4 board can undergo multiple iterations across different\nprojects. Here, we demonstrate that the same PCB substrate can be\nreused for yet another new project, even after three prior iterations.\nSpecifically, we retrofitted the previous WiFi radio controller\ninto a game console based on the open-source ESPBoy design [ 23].\nIn this iteration, we repurposed the WiFi radio circuit as the mother-\nboard of the ESPBoy assembly, retaining the ESP8266 circuitry and\nadding two multi-pin JST connectors. Additionally, we fabricated a\ndaughterboard that hosts an OLED display and joystick controls,\nserving as the console’s main input and output interface. These\ncomponents were positioned ergonomically to ensure comfortable\noperation. The multi-pin JST connectors linked the ESP8266 moth-\nerboard with the daughterboard.\nThe updated motherboard effectively handled high-frequency,\nreal-time data transmission, as demonstrated by the I 2C communi-\ncation at 100kbit/sbetween the microcontroller, the display, andFigure 11: Multi-iteration renewal on a single piece of FR-4. Each column presents the software-generated PCB comparison\nresult, the renewal process, the prototype assembly, and the amount of resources saved. Left column: correction of a mistaken\nconnection in the camera roller PCB. Middle column: trimming the board size and modifying part of the camera roller circuit\nfor the WiFi radio prototype. Right column: converting the WiFi radio circuit into the ESPboy motherboard, along with a\ndaughterboard to expand functionality.Yan, et al.\nthe GPIO extender that processed the button inputs. The renewal\nprocess is documented in the right column of Figure 11.\nWhile this iteration introduced an additional PCB, we still re-\nduced material waste by largely reusing the original PCB as the\nmotherboard of the ESPBoy game console. Specifically, we saved\n5608.24mgof FR-4 and 25.99kJof energy while consuming only\n98.91mgof silver epoxy, reducing material costs by 87.5%. The fab-\nrication time remains comparable to engraving a new FR-4 board,\nwith a difference of less than 5 min .\n7.4 Renewing an Outsourced PCB\nWhile previous examples showcased how PCB Renewal reduces\nmaterial waste for CNC-milled PCBs, its versatility extends to\nfactory-made PCBs, such as those ordered online or found in com-\nmercial electronic devices. In this example, we repurposed a digital\nLED watch PCB, manufactured as a double-layer board with a solder\nmask by a small-batch PCB producer, into a PCB for an interactive\ncat toy.\nWe began by outsourcing an open-source LED watch PCB [ 49]\nto a small-batch manufacturer. The PCB featured a standard double-\nlayer configuration, a black solder mask, and a round shape (top\nimage in the left column of Figure 12). Since the manufacturer\nrequires a minimum order quantity of five PCBs, we had several ex-\ntra boards remaining after successfully assembling the LED watch\n(shown in the bottom images of the left column in Figure 12). Typ-\nically, such boards are difficult to reuse in other projects due to\ntheir specific design. However, with the PCB Renewal approach,\nthese surplus PCBs can be easily repurposed. In this case, they were\nmodified to function as the controller for an interactive cat toy ball.\nThe renewal process for an outsourced, double-sided PCB is\nlargely identical to that of an in-house, CNC-milled PCB, with\ntwo exceptions: the removal of the solder mask in the area to be\nmodified, and the editing of vias, if necessary.\nRemoving the solder mask was based on the engraving profile\ngenerated from the KiCAD plug-in (top image in the right column of\nFigure 12). Specifically, the plug-in computed the areas of difference\nbetween the original LED watch PCB and the newly designed toy\nPCB for both sides. These differential areas were then sent to a\n𝐶𝑂2laser cutter, which selectively removed the solder mask and\nexposed the copper conductors using rastering mode (row two in\nthe right column of Figure 12). Alternatively, the solder mask can\nbe removed using either a 1064 nmwavelength fiber laser [ 69] or a\ndiode laser [53].\nIn the new toy ball circuit design, new trace areas required electri-\ncal connections between both sides of the PCB, while some existing\nvias from the original PCB needed to be removed. To achieve this,\nundesired vias were drilled out using a square end mill during the\nengraving process. New vias are created using the same process,\nfollowed by either manual soldering into these through-holes to\nestablish electrical connections or filling the entire via hole with\nconductive epoxy. The modified PCB is shown in row three of the\nright column in Figure 12.\nThe repurosed PCB was then assembled and installed into a\ncustom 3D printed housing to complete the final cat toy (row four\nin the right column of Figure 12). The renewal process significantly\nreduced material waste, manufacturing energy, and fabrication time.\nFigure 12: Renewal of an outsourced double-layer PCB: the\nleft column shows the production of LED watch PCBs, in-\ncluding assembly into a working watch. The right column\nillustrates the renewal process, showcasing PCB design com-\nparisons, solder mask removal, and board modifications for\na cat toy, and its final assembly. A radar chart highlights the\nresources saved through PCB Renewal versus creating a\nnew FR4 board.\nThe sustainability modeling results are presented at the bottom\nof right column in Figure 12. These estimates assume that the\nbenchmark board is made using a CNC FR-4 board and account for\nthe energy required for laser cutting during solder mask removal.\nPCB Renewal notably provides a much shorter turnaround time\ncompared to ordering new PCBs from a manufacturer. Furthermore,\nit eliminates shipping-related energy costs, making PCB Renewal\na more efficient and sustainable solution.8 DISCUSSION\nPCB Renewal enables multiple iterations on a single FR-4 substrate,\nboth within and across projects, promoting more sustainable PCB\nmaking practices. However, this approach also has its limitations.\nIn this section, we discuss these limitations and outline potential\nfuture research opportunities.\n8.1 Unpacking Sustainability Benefits and\nTrade-Offs\nAcross various examples and design iterations, we observed consis-\ntent savings in materials, costs, and energy, though time savings\nvaried. For example, in the iteration of the camera roller for the\nsame project, PCB Renewal saved up to 60% of the time by re-\nengraving only a small section of copper rather than engraving\nall traces on a fresh substrate. In other cases, such as the ESPboy,\nPCB Renewal required slightly more time than fabricating a new\nPCB due to the increased amount of editing required. From the\ntiming perspective, if a circuit design is straightforward to mill, the\nrenewal approach might not be time-efficient. This underscores\nthat the decision between creating a PCB from scratch and using\nPCB Renewal is case-dependent and dynamic. The sustainability\nmodel developed in Section 5, along with its implementation in the\nsoftware plug-in (Section 6), provides end-users a practical tool for\nmaking informed decisions by offering comprehensive comparison\ndata for each design iteration.\nHowever, our current sustainability model has its own limitations\nand can be further improved. For example, the time and energy costs\nassociated with the delivery of outsourced PCBs are not currently\nfactored in, even though delivery is often the most time-consuming\naspect of the PCB manufacturing process. In fact, if delivery time\nis considered, renewing a factory-made PCB is almost always more\ntime-efficient than ordering a new one.\nAdditionally, the current calculation of material savings is rudi-\nmentary, focusing solely on the total weight of the material in-\nvolved. Ideally, the model would be more precise and informative\nif it considered the carbon footprint of the FR-4 material saved in\ncomparison to the additional use of silver-epoxy. However, since\ncarbon footprint data for silver-epoxy is unavailable, total weight\nremains one of the few standardized metrics accessible for com-\nparing different materials. This limitation highlights the need for\na more open-data approach to LCA [ 19,47], particularly as new\nmaterials are developed and introduced to the market.\n8.2 Automating PCB Renewal\nWhile our current work has evaluated PCB Renewal in terms\nof material, time, and energy costs, other practical factors must\nbe considered, such as the increased likelihood of manual errors\nintroduced during the renewal process. For example, manually\ndepositing silver-epoxy may require skills and experience, while\ncuring the epoxy-filled PCB necessitates transferring the board\nto an additional heating device. Additionally, cutting new traces\non an existing board needs precise alignment, requiring users to\ncarefully position the board in the CNC machine. For some users,\nthese extra steps and the increased risk of manual mistakes are\nimportant trade-offs to consider when weighing PCB Renewal\nagainst the simplicity of creating a new board from scratch.We envision that a few simple upgrades to a desktop CNC ma-\nchine could reduce some of the labor effort, making PCB Renewal\nmore accessible. For example, epoxy deposition could be automated\nwith desktop CNC machines that support syringe extruders. In\naddition, the CNC cutting plate could be equipped with a heating\nelement (e.g., a 3D printer heating bed), allowing the curing pro-\ncess to be integrated into the automated workflow within the CNC\nmachine. Finally, alignment could also be automated, for example,\nthrough a camera-based calibration process. If these changes are\nimplemented, we can potentially transform an off-the-shelf CNC\nmachine designed for making PCBs into one that also supports\nthe remaking or renewal process, promoting more sustainable PCB\nmaking practices.\n8.3 Supported PCB Materials and Types\nThis paper focuses on the FR-4 PCB substrate, as it is the most\ncommonly used material for both in-house and outsourced PCB\nproduction. However, other more environmentally friendly PCB\nsubstrates, such as paper-based FR-1 or cellulose-based FR-3, might\nalso be compatible with the current workflow, though we have\nnot explored these options. We suspect that working with FR-1\nor FR-3 materials may require alternative conductive epoxies that\ncure through UV processes rather than heat, given these substrates’\nlower operating temperature. This suggests a potential future di-\nrection for comprehensively understanding the comparability of\ndifferent substrate materials and conductive epoxies.\nOur method supports single- and double-layer PCBs, whether\nmanufactured in-house or outsourced. While our example (Sec-\ntion 7.4) demonstrates the technical viability of renewing and up-\ndating externally manufactured PCBs, it depends on having access\nto the original circuit design. For commercial PCBs that are not\nopen source, this requirement poses a significant obstacle. To enable\nthe renewal process for commercial PCBs, reverse engineering tech-\nniques utilizing X-ray tomography [ 3,10] or computer vision [ 44]\nwould be necessary. However, integrating this approach into the\ncurrent workflow remains an open question and requires further\nresearch.\n8.4 Toward PCB Reuse in the Long Run\nOur work primarily explores the technical feasibility of PCB Re-\nnewal . However, achieving a long-term impact in sustainable mak-\ning requires understanding end-users’ willingness to adopt PCB\nRenewal , which necessitates deployment and active community\nengagement.\nAs a first step, we have open-sourced the PCB Renewal software\nplug-in (Section 6). Future deployment will allow us to explore\nintegrating PCB Renewal with other complementary methods that\nsupport PCB reuse. For example, the SolderlessPCB [ 68] method\nenables the reuse of electronic components without soldering, while\necoEDA [ 42] facilitates component reuse across multiple projects.\nIt would be interesting to explore whether a more integrated and\ncomprehensive PCB reuse system could influence end-users’ PCB\nmaking and usage practice over time.\nFinally, while this paper primarily considers PCB Renewal in\nthe context of individual PCB fabrication, it also holds potential for\nindustrial-scale recycling. For example, integrating a PCB layoutYan, et al.\nrecognition system into recycling facilities could potentially enable\ncentralized operations to adopt PCB Renewal , allowing useful PCBs\nto be repurposed before entering the waste stream. Investigating\nindustrial applications could uncover new opportunities for PCB\nRenewal on a larger scale.\n9 CONCLUSION\nIn this paper, we introduced PCB Renewal , a novel technique that\n“erases” and “reconfigures” existing circuit traces. We presented\nPCB Renewal workflow and evaluate its electrical performance\nand mechanical durability. We modeled the sustainability impact of\nPCB Renewal by calculating the material usage, cost, power, and\ntime consumption for renewing PCB versus using new substrates.\nWe implemented a custom EDA software plug-in that guides epoxy\ndeposition, generates updated profiles, and calculates resource use.\nWe showcased the effectiveness of PCB Renewal with a set of\nwalkthrough examples, and concluded the paper by discussing its\nlimitations and proposing future directions.\nACKNOWLEDGMENTS\nWe thank Sandbox, the Jagdeep Singh Family Makerspace, for pro-\nviding access to tools during the development and documentation\nof this project. We also thank our cat, Zhu Yuanzhang, for enjoying\nthe custom cat toy and serving as a great model for Figure 12. An\nLLM service was used exclusively for proofreading.",
      "metadata": {
        "filename": "PCB Renewal_ Iterative Reuse of PCB Substrates for Sustainable Electronic Making.pdf",
        "hotspot_name": "PCBA_Production",
        "title": "PCB Renewal: Iterative Reuse of PCB Substrates for Sustainable\n  Electronic Making",
        "published_date": "2025-02-18T19:29:09Z",
        "pdf_link": "http://arxiv.org/pdf/2502.13255v1",
        "query": "PCB assembly sustainability environmental assessment"
      }
    },
    "Recyclable vitrimer-based printed circuit board for circular electronics": {
      "full_text": "1 Recyclable vitrimer -based printed circuit board  \nfor circular electronics  \n \nZhihan Zhang1, Agni K. Biswal2, Ankush Nandi2, Kali Frost3, Jake A. Smith1,3,  \nBichlien H. Nguyen1,3, Shwetak Patel1, Aniruddh Vashisth2*, Vikram Iyer1* \n \n1 Paul G. Allen School of Computer Science & Engineering, University of Washington  \n2 Department of Mechanical Engineering, University of Washington  \n3 Microsoft Research  \n* Corresponding authors : vashisth @uw.edu  or vsiyer@uw.edu  \n  1 Abstract  \nElectronics are integral to modern life; however, at their end -of-life these devices produce \nenvironmentally hazardous electronic waste (e -waste). Recycling the ubiquitous printed circuit  \nboards (PCBs) that make up a substantial mass and volume fraction of e -waste is challenging due \nto their use of irreversibly cured thermoset epoxies. We present a PCB formulation using \ntransesterification vitrimers (vPCBs), and an end -to-end fabrication process compatible with \nstandard manufacturing ecosystems. We create functional prototypes of IoT devices transmitting \n2.4 GHz radio signals on vPCBs with electric al and mechanical properties meeting industry \nstandards. Fractures and holes in vPCBs can be repaired while retaining comparable performance \nover more than four repair cycles. We further demonstrate non -destructive decomposition of \ntransesterification vitr imer composites with solid inclusions and metal attachments by polymer \nswelling with small molecule solvents. We hypothesize that unlike traditional solvolysis recycling, \nswelling does not degrade the materials. Through dynamic mechanical analysis we find negligible \ncatalyst loss, minimal changes in storage modulus, and equivalent polymer backbone composition \nacross multiple recycling cycles. We achieve 98% polymer recovery, 100% fiber recovery, and \n91% solvent recovery which we reuse to create new vPCBs wi thout degraded performance. Our \ncradle -to-cradle life -cycle assessment shows substantial environmental impact reduction over \nconventional PCBs in 11 categories.   2 Main  \nElectronics have become an increasingly integral part of modern life in everything from c onsumer \ndevices like smartphones and laptops to state -of-the-art industrial, scientific and medical \ntechnologies. However, when electronics reach the end of their life cycle due to obsolescence or \ndamage, they become electronic waste (e -waste) and pose sig nificant environmental hazards. E -\nwaste contains a complex toxic mixture of various metals, silicon integrated circuits (ICs), glass \nfibers, thermoset polymers, flame retardants, and more, which can pollute the air, soil, and water \nposing significant hazar ds for the surrounding communities1–3. With over 53.6 million metric tons \n(Mt) generated in 2019 alone4, e-waste is one of the fastest -growing waste stream s globally5 and \na matter of global concern6. \nIn response to this issue, efforts are underway to reduce e -waste by transitioning to a \ncircular economy model in which electronics can b e transformed into secondary raw materials. A \nkey challenge in achieving a circular manufacturing cycle is the recycling of printed circuit boards \n(PCBs). PCBs are ubiquitous in electronics and rank within the world’s top 100 most traded \nproducts7. They form  the physical substrate upon which chips are mounted and connected with \nmetal traces patterned on their surface. PCBs are most commonly composite materials made of \nglass fiber weaves within a flame -retardant thermoset epoxy matrix (FR -4). The majority of p rior \nrecycling efforts have focused on the recovery of intact ICs and high -value minerals such as gold \nand copper8,9; however 70% of a PCB’s volume and mass is made up of die lectric substrates10.  \nTo maximize sustainability, we aim to create a PCB that can be repeatedly recycled to \nproduce brand -new circuits with the same performance. Achieving a scalable and closed -loop \ncircular manufacturing cycle presents several challeng es. First, PCBs must be capable of tolerating \nelectronics manufacturing processes which include chemical etching, electroplating, and elevated -3 temperatures for soldering. Prior water -soluble11–16 and healable materials17 sacrifice compatibility \nwith these methods to achieve recyclability which limits their potenti al to scale. Additionally, these \nmaterials do not meet the rigorous standards for moisture absorption, flammability, dielectric \nconstant, loss tangent, and other criteria needed to produce high -speed and RF circuits common in \nmodern electronics. Second, th e irreversibly cured thermosets in the dielectric substrates like FR -\n4 are formulated to be resistant to fire and chemicals, which makes it incredibly challenging to \nfully deconstruct them into raw materials for recycling. While substituting thermoplastics  in place \nof these thermosets is a potential solution, thermoplastic reprocessing damages the polymer \nchains18, which reduces their thermal and mechanical properties and prevents their reuse in new \nPCBs. Third, closed -loop recycling imposes the strict constraint of deconstructing composites into \nraw materials without damage . A number of methods have been expl ored to recycle these materials \nhowever they all result in substantial material damage19,20. Direct mechanical grinding and \ncrushing of waste PCBs are among the simplest, however it is incompatible  with recycling \ncomposite materials like FR -4. This process destroys the glass fiber and polymer, preventing them \nfrom being reused for making a new board. Mechanical recycling is used to homogenize the PCB \nso that it can be put into a furnace or subjec ted to a chemical leaching process for commercial \nmetal recovery, however, these methods also suffer from low efficiency in the separation of metals. \nIn contrast,  thermal decomposition  (pyrolysis), or chemical dissolution in strong solvents \n(solvolysis) h ave higher efficiency but require high processing temperatures which increases \nenergy costs21, or require nonreusable solvents21,22 thereby producing hazardous byproducts23. \nAdditionally, many of these methods are only able to recycle the glass fiber and dissolve the \npolymer chains completely22. Because thermosets lack the ability to reform bonds, the material 4 properties of the recycling outputs are degraded thereby limiting their application poten tial and \nreuse cycles.  \nIn this article, we present a different solution by introducing a novel PCB formulation using \ntransesterification vitrimers (vPCBs) that are specifically designed for circularity. We then develop \ntechniques for vPCB repair, and non -destructive recycling methods for vitrimer composites with \nsolid inclusions by polymer swelling using small molecule solvents accompanied by analyzing the \nunderlying mechanisms. Recent work has shown that dynamic covalent adaptive networks (CANs) \nwith assoc iative exchange reactions can undergo reversible crosslinking at elevated temperatures24. \nThese materials, termed vitrimers, exhibit dynamic covalent bond exchange reactions when heated. \nThis enables the unique healing capabilities of vitrimers25 and repeated reversal of fatigue -induced \ndamage to recover properties close to those o f the original material26, allowing the r ecovered \nmaterials to be reused repeatedly with minimal degradation. We specifically focus on vitrimers \nwith associative exchange reactions over dissociative mechanisms due to their ability to maintain \na high crosslink density over multiple recycling cycle s. Replacing the traditional thermoset epoxy \nmatrix with a vitrimer addresses the fundamental recycling challenge described above. vPCBs not \nonly enable the repair of PCBs but also open up the potential for high -value recycling by reusing \nthe polymer to cr eate a new, high -performance PCB. Further, these methods could be generalized \nto this entire class of vitrimer polymers whose backbones can be tuned to achieve desired material \nproperties.  \nFig. 1 illustrates our fully circular manufacturing and recycling pipeline which highlights \nour key contributions. First, we create glass fiber -reinforced vitrimer composites and develop an \nend-to-end PCB fabrication process compatible with current electronic manufacturing services \n(EMS) ecosystems including multi -layer copper lamination, chemical etching, laser structuring, 5 electroless plating, and soldering. We perform detailed characterization of our vPCBs \ndemonstrating they meet industry standards for peel strength of copper -clad laminates, flexural \nstrength, moisture  absorption, dielectric constant, and resistivity. We then use them to demonstrate \na fully functional IoT sensor capable of transmitting 2.4 GHz Bluetooth  signals. Second, we \nleverage the bond exchange capabilities of vitrimers to repair physical damage  to vPCBs.  We \nrepair fractures, holes and mechanical deformations, demonstrating that repaired vPCBs maintain \ntheir mechanical and electrical properties over more than four repair cycles. We verify the absence \nof microcracks and pores using scanning elect ron microscopy (SEM) and show that covalent \nbonding achieves higher joint strength than cyanoacrylate adhesives in she ar punch tests.   \nThird, we develop an end -to-end recycling process for our vPCBs. We observe that certain \nsolvents can cause the vitrimer  to swell,  and leverage this polymer swelling in small -molecule \nsolvents to deconstruct transesterification vitrimer composites with solid inclusions and metal \nattachments. We hypothesize that unlike traditional solvolysis recycling, swelling does not degr ade \nthe materials; however, this method raises a number of fundamental questions of whether the \nprocess removes non -covalently bonded catalyst critical to the transesterification vitrimer’s \nproperties, effects on the crosslink density, and integrity of the  polymer backbone. We develop a \nprocess to create recycled samples and comprehensively investigate these questions across \nmultiple recycling cycles. We perform dynamic mechanical analysis revealing negligible change \nin vitrimer transition temperature  (Tv) of ± 0.7 °C indicating no catalyst loss, and minimal change \nin storage modulus (3.1%) which depends on crosslink density. We confirm with Fourier -\ntransform infrared spectroscopy (FTIR) spectroscopy that the molecular structure remains \nunchanged, and verify  recycled vPCBs have mechanical and electrical properties on par with the \noriginal. Our recycling process achieves 98% recovery of the vitrimer polymer and 100% recovery 6 of glass fibers, and even a 91% recovery of the solvent which we successfully reuse. Fourth, we \nperform a comprehensive cradle -to-cradle life -cycle assessment (LCA) leveraging industrial scale \nmodels to compare the environmental impact of our remanufacturing and recycling systems with  \nconventional PCBs . Our results show a 47.9% improvement in global warming potential, 65.5% \nin ozone depletion potential, and 80.9% in human cancer toxicity emissions for our recycling \nprocess compared to conventional PCB end -of-life disposal scenarios.   7 Fig 1. Transesterif ication vitrimer -based fully recyclable PCB  \n \nConceptual diagram showing the closed -loop repair and recycling of vPCB. a, GFRV composite. \nb, Cross -sectional view of a three -layer vPCB. c, Photograph  of a vPCB -based recyclable IoT next \nto a server. The printed circuit board assembly (PCBA) can be repaired if warping or damage \noccurs in the base material. After the device is decommissioned, the substrate materials can be \nrecycled and the electronic com ponents could be reused. d, Recovered vitrimer from obsolete \nvPCB after  drying and pulverizing, which can be reused to fabricate new GFRVs.   \n8 Results  \nGlass fiber reinforced vitrimer composites  \nTo enable PCB circularity, we seek to disassemble the entire board into its constituent raw \nmaterials without damage for reuse. While the electronics and copper traces can be recovered using \nthermal exposure27 or the same chemical etchants used to patter n the circuits28, the dielectric \nsubstrate material remains. The most common material, FR -4 is a combination of a woven glass \nfiber impregnated with a thermoset epoxy resin. The covalent crosslinks of these thermosets give \nthe material desirable p roperties such as high structural integrity, chemical stability, and resistance \nto temperature. These properties, however,  present a trade -off common in sustainability: the \nmaterial’s robustness makes it very difficult to effectively separate the epoxy and  glass fibers for \nrecycling or reuse . \nIn this work, we take a different approach inspired by recent advancements in healable \nmaterials to re -engineer PCBs using polymers with dynamic covalent adaptable networks (CANs); \nvitrimers are a special class of CANs with a thermosetting macromolecular network with he aling \nabilities. This property also enables entirely new capabilities. First, it enables remanufacturing, in \nwhich holes in the material can be refilled and segments can be tiled together without transforming \nthem back into raw starting materials . Second, the ability to reuse vitrimers as secondary raw \nmaterials opens the possibility  for effective recycling if they can be non-destructively  separated \nfrom the glass fibers . Our vitrimer polymers are synthesized from a bifunctional epoxide (DGEBA) \nand an  acid (adipic acid) in the presence of a catalyst Triazabicyclodecene (TBD). The chemical \nstructures for the reaction are shown in Fig. 2a. In contrast to the prior attempts at integrating \nhealable polymers into circuits29, our vitrimer chemistry is optimized for similarity to conventional \nFR-4 by choosing a bisphenol -A epoxide which is typically used in PCBs30. The normalized stress -9 relaxations at temperatures ranging fro m 140 °C to 240 °C  and the characterized storage modulus \nand tan delta for vitrimer with a 5 mol % catalyst concentration are shown in Extended Data Fig. \n1. It is noteworthy that by tuning the specific components and functional groups (Fig. 2d), a wide \nvariety of different properties can be achieved31–33, such as a higher glass transition temperature \n(Tg) of 146 °C34. \nOur recyclable GFRV composites are created using a scalable fabrication process similar \nto conventional FR -4 fabrication shown in Fig . 2b (see Methods for additional details). Figure 2c \nschematically illustrates the structure of the resulting GFRV.  \nMoisture absorption of GFRV is characterized through immersion in water according to \nthe IPC PCB standard (see Methods for additional detai ls). Moisture can accelerate various failure \nmechanisms in PCBs by causing cracking, delamination, soldering issues, and changes in dielectric \nproperties. High moisture absorption is a common failure mode for many sustainable PCB \nmaterials, as moisture ing ress usually weakens polymer by hydrolytic cleavage in chains35. Fig. 2e \nshows our vitrimer is on par with common PCB dielectrics like polyimide and within 0.2% of th e \nFR-4 standard. We hypothesize this difference is due to the presence of the ester linkages between \nmonomers, and this value could be further reduced by modifying the vitrimer chemistry.  \nFlexural strength is also evaluated using a three -point flexural  test (see Methods) and the \nresults are visualized in Fig. 2f. The performance is within the range of FR4 -based materials and \nthese values could be tuned by changing the quality of the glass fiber weave .  10 Fig. 2: Glass fiber reinforced vitrimer composite.  \n \na, Reaction of transesterification vitrimer under heat and pressure. b, Schematic of the fabrication \nprocess of GFRV.  c, Schematic of four -layer glass fiber GFRV. d, Bond exchange via \ntransesterification in vitrimer networks enables healing and recyclabili ty when heated. Properties \ncan be tuned by changing the functional chains R 1 and R 2 in the epoxide and acid used. e, \nCharacterized moisture absorption of GFRV compared to the PCB standards ( N = 3, error bar = \n±𝜎). f, Characterized flexural strength of GFR V compared to the PCB standards of woven glass \nFR-4 and woven e -glass fabric FR -4.  \n11 Vitrimer -based PCB  \nWe create recyclable , reusable , and repairable  circuit boards by integrating these unique vitrimer \nmaterials into PCBs. The first step to creating a PCB from the raw GFRV is to add a conductive \ncopper layer for circuit traces. In traditional PCB manufacturing, a glass fiber weave is \nimpregnated with a  phenolic epoxy resin, and partially cured. The resulting material, known as \nprepreg, is then laminated with thin copper foil and can be patterned and stacked to create \nmultilayer circuits. To create the vPCBs, a similar lamination process is used as illus trated in Fig. \n3a. Sheets of copper foil are laminated on the raw GFRV in a heat press (see Methods for details). \nThis method can easily scale to roll -to-roll manufacturing for high -volume production.  \nTo assess the quality of vPCB copper adhesion, the peel strength for 16 copper -clad \nlaminates (CCLs) is systematically measured for heat press temperatures of 120 -160 °C and heat \npress time of 30 min -2 h. Empirical measurements were performed due to the complexities of \naccurately modeling the  adhesion bonds formed by heat and pressure on polymer surfaces. Our \nresults show that the interfacial adhesion between copper and vitrimer increases as the heat press \ntemperature and time increase (Fig. 3b). We note however that above 180 °C lower viscosi ty \ncauses the vitrimer to be squeezed out. The results follow the expected trend, whereby the increase \nin temperature and pressure results in dissociation in the covalent adaptive network, thereby \nincreasing the availability of functional reactive sites fo r adhesion. Bonding can be further \nimproved by adding a thin, partially cured vitrimer layer between GFRV and copper foil This \nsignificantly enhances peel strength after thermal stress, exceeding the PCB standard (Extended \nData Fig. 2), as the partially cu red vitrimer has more reactive sites available.  \nNext, circuit traces are patterned onto the raw copper -coated substrate. Chemical etching \nand laser structuring are the two commonly used processes in PCB manufacturing. Laser 12 structuring is often used for mo re intricate circuits due to its high precision, ability to cut through \na wider range of materials, and speed in small -scale manufacturing. Chemical etching is less \nprecise but is more suited to low -cost, large -scale production because it can etch many par ts or \nmultiple boards simultaneously. We demonstrate compatibility with both manufacturing processes. \nBriefly, the raw copper -coated GFRV laminate is placed in a laser micromachining system (LPKF \nProtolaser U4), where the laser is used to both drill holes and directly remove copper to create a \ncircuit. Alternatively, we can coat the copper with a thin mask layer and use the laser to remove \nthe mask in selective regions, then place the board in ferric chloride solution to etch away the \nexposed copper. This m ethod can be used to pattern individual layers of material that can then be \naligned and heat pressed to create a multi -layer laminate. The final step is applying an additional \nmask layer and using an electroless copper plating process to connect the layers  through vias (See \nMethods for additional details).  \nThe fabricated circuits are characterized to compare vPCBs to traditional FR4 -based \ncircuits. The dielectric constant of the PCB substrate material is crucial for achieving good high -\nfrequency electrical performance, as it affects the intensity of signal reflections and crosstalk, \nwhich ultimately impacts the overall signal integrity. The measured dielectric constant and loss \ntangent of the vPCB substrate compared to the PCB standards of various FR -4 types  are shown in \nFig. 3c, d, respectively (details on characterization procedures are provided in the Methods). The \nresults show that the dielectric constant for our vPCB is within the typical range of 3.5 to 5.5, \ndemonstrating it meets the required specifica tions. We note that this value can also be tuned to \nmeet the needs of the application by changing the glass fiber volume content, vitrimer chemistry, \nand polymer -free volume.  13 Low flammability is another crucial property required to protect PCBs from poten tial \nignition as many electronic components produce heat and contain flammable polymers. \nConventional PCB substrates such as FR -4 are formulated with flame retardants to self -extinguish \npotential fires. To evaluate the flammability of vPCB, a GFRV is rigor ously tested using a burner, \nand found to exhibit excellent fire -retardant properties. As shown in Fig. 3e, the fire was \nextinguished within 7 seconds after the removal of the flame source (Fig. 3e , Supplementary Video \n2), conforming to the 10 -second stand ard.  14 Fig. 3: Characterization of vPCB.  \n \na, Schematic of the scalable manufacturing process of vPCB. b, Curves of the peeling force per \nwidth of copper -clad versus displacement for laminates with various heat press temperatures and \ntimes ( N ≥ 3, shaded region indicates max and min values of trials). c, d, Characterized dielectric \nconstant ( c), and loss tang ent (d) of vPCB compared to the PCB standards of various FR -4 (N = 3, \nerror bar = ± 𝜎). e, Flammability test of vPCB. The sample is retracted after 0 s to move it away \nfrom the burner.  \n15 Platform evaluation  \nTo evaluate the potential of our recyclable PCBs  for real -world applications, we create a \nfunctional prototype of an IoT sensor to monitor environmental conditions such as temperature, \nhumidity, and pressure. Such sensors provide critical information for environmental and smart -\nbuilding monitoring, and this industry is projected to grow by billions of new devices in the coming \nyears which raises a pressing need to mitigate the environmental harms of the e -waste they will \ngenerate36. In addition to being a high -impact application, a complete IoT sensor requires meeting \nnumerous technical specificatio ns which highlight the versatility of our vPCBs. The platform must \nperform all the basic functions of a small computer, demonstrating the potential for using vPCBs \nin a variety of computing devices. Beyond the standard requirements of consumer electronics,  IoT \ndevices must also be capable of high -frequency signaling for transmitting and receiving radio \nsignals at GHz frequencies which are affected by PCB properties such as dielectric constant and \nloss tangent.  \nWe create an end -to-end prototype of a wireless environmental sensor with vPCB \naccording to the schematic shown in Fig. 4a and deploy it on a server rack as shown in Fig. 4b. \nOur device includes a microcontroller with an integrated Bluetooth radio, a digit al sensor, a coin \ncell battery, onboard chip antenna, and assorted passive components (see Methods for additional \ndetails). We fabricate the circuit on both vPCB and standard FR -4, and program both to wirelessly \ntransmit sensor measurements from a data cen ter setting for 16 hours. Figs. 4c -e show the resulting \ntemperature, pressure, and humidity. Both devices were able to successfully transmit Bluetooth \npackets and no gaps are observed in the data from device failure. The data show the sensors \nthemselves ar e closely correlated due to their proximity, but with small fixed offsets likely due to \nindividual sensor variation or their small physical separation.  16 To further qualitatively analyze the signal integrity of circuits on vPCB, we perform eye \ndiagram measur ements that reflect vital parameters for signal in digital transmissions, such as \nsignal -to-noise ratio (SNR) and clock timing jitter. A well -routed signal line is designed and \npatterned onto both vPBC and FR -4. A serial BERT (Keysight N4903A) is used to g enerate a 2.48 \nGb/s pseudorandom binary sequence (PRBS), and the eye diagram is measured by a wide -\nbandwidth oscilloscope (Keysight 86100D). Our vPCB has nearly identical performance as \nstandard FR -4 at a glance (Fig. 4f).   17 Fig. 4: Platform evaluation  \n \na, Circuit block diagram. ADC, analogue -to-digital converter; PKT GEN, packet generation; C1 –\nC2, capacitor 1 –capacitor 2; L1, inductor 1; X1, 32 MHz crystal oscillator 1; I2C, Inter -Integrated \nCircuit. b, Photograph of experimental set -up. c, d, e,  Data cent er temperature ( c), relative \nhumidity ( d), and pressure ( e) measurements from two platforms over 16 h showing successful \ncontinuous running and high -frequency signal transmission. f, Comparison of eye diagrams of FR -\n4 and vPCB .  \n18 Repair and remanufacturing  \nA unique property of our vPCBs is that the vitrimer’s dynamic covalent bonds enable reliable \nhealing of physical damage such as fractures and holes. This opens up the ability to repair and \nremanufacture PCBs to reduce their environmental impact and promote  a circular economy. A \nrecent study from Microsoft37 showed that repairing broken devices with replaceable parts resulted \nin a reduction of up to 85% in greenhouse gas (GHG) emissions and 90% in waste avoidance \ncompared to replacing the whole device. Once these devices are beyond repair, we  explore the \npotential of vPCBs to enable value cycling by repairing physically damaged base materials and \nremanufacturing them into new ones.  \nA repair method is developed that heals holes and fractures as shown in Fig. 5a. Copper is \nfirst removed through chemical etching. Subsequently, a small amount of fresh vitrimer material \nin liquid form is applied to fill the holes and follow the same heat press process shown in Fig. 2b \nto create a new CCL. The repaired GFRV is shown in Fig. 5a, wherein all the throug h holes have \nbeen healed with no discernable marks indicating the hole location. The joint strength of healed \nvia holes is also evaluated using a shear punch test (see Methods for details and Extended Data \nFig. 3). A comparison is made between the FR4 with  repaired via holes using cyanoacrylate glue \nand the results indicate that vitrimer creates a stronger interface at the hole boundary, avoiding \ncatastrophic failure as seen with the superglue bond that breaks. The healing capability of vitrimer \nchemistry e nables the creation of new covalent bonds between fresh vitrimer and GFRV. \nSpecifically, the transesterification reaction occurring in the vitrimer system leads to topological \nrearrangements while preserving the integrity of the molecular network26,38. This healing \nmechanism cannot be achieved by those of traditional thermoplastics or thermosetting plastics. 19 Using the same fabrication steps described in the previous section we can use the resulting bare \nGFRV to create new circuits.  \nTo evaluate our healed composite, the copper adhesion is first measured after four cycles \nof copper re -lamination as shown in Fig. 5b. Interestingly, we find that the interfacial adhesion \nimproves after each repair cycle. This is likely attributed to the micro -scale indentations on the \nGFRV surface caused by heat -pressed copper foil, as observed using SEM shown in Fig. 5c. These \ninden tations increase the surface roughness after each remanufacturing attempt , which is known \nto improve adhesion39. The dielectric constant and volume resistivity of the repaired material a re \nevaluated following and found to remain within the range of common FR -4 varying by a maximum \nof 6.5% even after four repair cycles as shown in Fig. 5d,e.  \nvPCBs also have shape -memory properties that can be used to reverse physical damage  \nthat can occur due to mechanical stress  such as GPU sagging . Mechanical deformation of the vPCB \nbelow its Tv results in a temporary shape change. Subsequent heat treatment at 100 °C (above both \nTg and Tv) for 1 min prompts the material to revert back to its original, pre -deformation \nconfiguration. Figure 5 f demonstrates a GFRV that has been deformed under an external force, \nthen successfully recovers to its thermodynamically favored original shape after heat treatment. \nThis shape -memory attribute not only underscores the inherent self -healing properties of vitrimers \nbut also further highlights the potential for the circular reuse of mechanically damaged vPCB \nmaterials.   20 Fig. 5: Repair and remanufac turing of vPCB  \n \na, Optical micrographs of a via on vPCB being healed. b, Average peeling force of copper -clad \nafter three repair cycles for various specimens with secondary vertical axes showing the \ncorresponding surface roughness  before each repair cycle  (N = 4, error bars = ± 𝜎). c, SEM images \nof GFRV surface after each repair cycle, showing orderly indentations caused by heat -press ed \ncopper foil. d, e, Characterized dielectric constants (d), and volume resistivities (e) of repaired \nGFRV after each repair cycle ( N ≥ 3 (d) and N = 1000 (r), error bars = ± 𝜎). Remanufactured vPCB \nexhibits nearly identical dielectric constants and volume resistivities. f, Thermadapt  shape -\nmemory behavior of GFRV. GFRV is deformed under external force, and is recovered to its \noriginal shape after being heated at 100 °C for 1 min. This allows for the repair of warped vPCBs \nat elevated temperatures, making it a more environmentally frie ndly option compared to traditional \nPCBs.   \n21 Closed -loop recycling  \nVitrimer -based PCBs can be repeatedly repaired through the process described above, however \nthese PCBs will eventually reach the end -of-life when a design becomes outdated  or if repair is no \nlonger economically feasible. To maximize circularity, we further develop a closed -loop recycling \nprocess for end -of-life vPCBs to nondestructively disassemble the fiber composites back into high-\nquality  raw materials for use in creating new, fully func tional circuit boards.  \nWe begin the vPCB recycling processes by removing the components, cleaning the surface, \nand then dissolving the copper using ferric chloride to isolate the GFRV substrate (see Methods \nfor details). Fig. 6a illustrates our process fo r deconstructing the GFRV into raw materials. We \nimmerse the GFRV sample in various polar aprotic solvents for the extraction of glass fibers. We \nexamined acetone, chloroform, dimethylformamide (DMF), and tetrahydrofuran (THF). We found \nthat DMF and THF do  not react with the GFRV materials but can ingress into the polymeric \nnetwork due to their small molecules, resulting in swelling (Extended Data Fig. 4, Supplementary \nVideo 1). This swelling enables the separation of the vitrimer matrix and glass fibers wi thout \nchemical degradation. We select THF for our recycling process, as it achieves similar swelling \nperformance but has a low boiling point of 66 °C and enables easy solvent removal to recover the \nvitrimer. In contrast, DMF has a higher boiling point of 1 53 °C and requires rotary evaporation \nwhich may result in the loss of the catalyst in the material  (see Supplementary 1.1 for additional \ndiscussion) .  \nThe vitrimer matrix is fully separated from the glass fiber layers after immersion in THF \nfor 96 hours  (shown in Fig. 6a, details are provided in the Methods). The recovered vitrimer and \nglass fiber are shown in Fig. 6a, b, respectively, and can be reused directly after drying. Vitrimer \npolymer can be easily recovered with ~98% isolated yield  after being precipitated out from the 22 THF, and 100% of the glass fiber is recovered. We also demonstrate a THF recovery efficiency of \napproximately 91% , with 1.5% being lost due to evaporation during testing and measurements, \nand 7.5% remaining in the swollen vitrimer matrix which was not attempted to recapture. The THF \nrecovery efficiency could be further optimized to over 97% by performing recycling in a properly \ndesigned and operated recovery system40. This is a marked improvement over previous solvolysis \nmethods, which necessitate additional adsorption or distillation procedures to remove the solvent \nand recover the polymer matrix.  \nWe create new GFRV samples using recycled materials b y grinding the recovered vitrimer \ninto a fine powder, mixing it with a 40 wt% fresh vitrimer matrix in viscous liquid form to fully \npenetrate the small interspaces in our dense glass fiber weave, and applying the same heat press \nprocess described above. We  note the required pressure has an inverse quartic relationship to pore \nsize according to the Hagen Poiseuille equation suggesting the potential for using 100% recycled \nvitrimer by adjusting the material or process parameters (see Supplementary  1.2 for add itional \ndiscussion ). The resulting reformed GFRV is shown next to a newly made GFRV sample in Fig. \n6c. The resulting composite showed no apparent differences except for slight discoloration on the \nsurface due to thermal oxidation.  \nWe hypothesize above that swelling does not degrade the materials; however, this method \nraises a number of fundamental questions of whether the process removes non -covalently bonded \ncatalyst critical to the transesterification vitrimer’s properties, whethe r it affects the crosslink \ndensity, or changes the polymer backbone. Next , we perform a series of experiments to investigate \nour hypothesis that small molecule solvent -based  swelling does not degrade the materials and \nunderstand the underlying mechanisms o f our recycling process. We first examine SEM images of \nreformed GFRV which show smooth surfaces without any noticeable damage in the form of 23 microcracks or pores (Fig. 6c). At a macro -scale, this confirms that the dynamic nature of the \npolymeric macromole cular network is preserved after every recycling as the polymer can flow and \nform a continuous sheet from pulverized recycled vitrimer powder without being dissociated by \nthe solvent.  \nThe mechanical behavior of the recycled vitrimer matrix is evaluated by  comparing the \nstorage modulus and tan delta values post -recycling (Extended Data Fig. 5a,b). We quantify the \nretention of storage modulus post -recycling, observing 96.9% after one recycle cycle and 94.4% \nafter two cycles (Extended Data Fig. 5c). Although we observe some narrowing and broadening \nof the tan delta peak between cycles (see Supplementa ry 1.3 for additional discussion), the high \nstorage modulus retention clearly attests that the reduction in crosslink density induced by our \nswelling recycling approach is negligible. To  evaluate the potential loss of non -covalently attached \ncatalyst durin g our recycling procedure, we leveraged Maxwell’s relation for viscoelastic materials \nto derive Arrhenius curves41 (Extended Data Fig. 5d). We determined a Tv of 79.6 °C f or pristine \nvitrimer. Upon recycling once, the Tv was slightly reduced to 78.3 °C and 79.4 after recycling \ntwice. The marginal shift in Tv implies no loss of the non -covalently bonded catalyst throughout \nour THF swelling recycling process across multiple c ycles, as a reduction in catalyst concentration \nwithin vitrimer would precipitate a substantial increase in Tv41. \nTo further confirm the  maintained  chemical composition of  the vitrimer polymer backbone \nafter the recycling process, samples at different recycling stages are analyzed using Fourier -\ntransform infrared (FTIR) spectroscopy and compared with a new GFRV sample in Fig. 6 e. We \nobserve the disappearance of 903 cm-1 for epoxy functional group after the vitrimer is cured, and \nthe appearance of sharp peaks at 1037, 1726 and broad intense peaks at 3359 cm-1 for -CO-O-, -\nC=O, and -OH groups respectively. This indicates the consumption of all epoxy end groups for 24 the form ation of ester linkages with crosslinked networks in vitrimers. The recovered vitrimer \nretained identical functional groups with characteristic peaks as the original, as shown in Fig. 6 d, \nwhich indicates that recycled vitrimers maintain their crosslink den sity and undergo exchange \nreactions without changing the chemical polymer backbone. Figure 6 e also indicates that the \nvitrimer can resist ferric chloride corrosion, which enables the production of vPBC through the \nconventional and scalable PCB manufacturin g technique of chemical etching. The properties of \nGFRV made of recycled vitrimer are also highly comparable to the original one (Extended Data \nFig. 6).  25 Fig. 6: Recycling of vPCB  \n \na, Photographs showing recycling procedures of GFRV from obsolete PCB  and recovered vitrimer \nmatri x after precipitation from the THF . b, Photograph of reclaimed glass fiber weaves and SEM \nimage showing the clean microstructure from recycling. c, The recovered vitrimer was reused to \nform new GFRVs. The appearance of reformed com posite maintains  nearly identical to the pristine , \nand SEM images show  smooth surfaces of reformed GFRV without any microcracks or pores . d, \nComparison of FTIR spectra of pristine  vitrimer , vitrimer after etching, and recovered vitrimer  \nfrom THF . e, Environmental impact comparison of conventional FR -4 PCB and vPCB in end -of-\nlife disposal scenarios across 11 different categories. vPCB is assumed either four cycles of \nremanufacturing or recycling, with a THF recovery efficiency of 97% in the recycling s cenario . \nAs a comparison baseline, FR -4 PCB is assumed to have 5 lifetimes with the best -case assumption \nfor end -of-life incineration. The vPCB results are normalized to the FR -4 PCB for each category \nrespectively.   \n26 Life-cycle assessment  \nWhen proposing ne w remanufacturing or recycling processes, it is important to assess their \nenvironmental impacts or benefits using tools such as LCA42. We perfor m an LCA to evaluate the \nenvironmental impact of our vPCB and compare our remanufacturing and recycling processes to \nconventional PCB fabrication and disposal at scale. Our cradle -to-cradle  LCA uses industrial \nmodels and encompasses material synthesis, CCL  manufacturing, freight transportation, and end -\nof-life PCB disposal or recycling.  \nNotably, the GFRV fabrication process produces cured  laminates. This is a key advantage \nover conventional prepreg materials which are only partially cured and have a limited shelf life of \n3-6 months32. Maximizing the lifetime of conventional prepreg materials requires refrigeration and \nlow-humidity environments, thereby increasing their environmental footprint. To quantify this \ndifference, we compared the environmental impact of vPCB freight with conventional FR -4 \nprepregs. The LCA results demonstrate a substantial reduction of 20.0% in global warming \npotential, 28.0% in ozone depletion potential, 14.0% in fossil depletion, and 26.0% in mineral and \nmetal use (Extended Data Fig. 7). \nIn order to compare the environmen tal impact of vPCBs with conventional PCBs in end -\nof-life scenarios, we use the business -as-usual standard of incineration as the baseline for the latter. \nOur model makes the best -case assumption for conventional PCBs, in which 89.3% of the waste \nheat and 11.1% of the waste electricity will be recovered and credited as energy production. Our \nremanufacturing scenario models the case where a product is repaired and remanufactured, with \ncertain parts replaced as necessary. Our LCA model looks specifically at t he environmental cost \nof remanufacturing the PCB using the method described in the section above. In the recycling 27 scenario, the product is broken down into its component raw materials, which are then reused in \ncreating new PCBs.  \nThe results, as shown  in Fig. 6 e, indicate that vPCBs have the potential to substantially \nreduce numerous environmental impacts including acidification, global warming potential, and \nozone depletion potential. Extended Data Fig. 8 shows a detailed breakdown of the carbon \nfootprint  of a conventional PCB indicating that raw materials make up 48.5% of the total \nenvironmental cost. We show however that leveraging the unique healing properties of our vPCB \nenables remanufacturing with significantly less material and energy and allows thi s process to be \nrepeated for multiple cycles to further reduce impact. Our LCA assessment indicates that this \nmethod would reduce all of the studied environmental impact metrics by over 65% with four \nremanufacturing cycles.  \nThe primary benefits of using v PCBs are reducing end -of-life byproducts, disassembling \nend-of-life vPCBs into raw materials and creating new, fully functional circuit boards. The \nenvironmental impact resulting from four recycling cycles shows a remarkable reduction in 11 \nmetrics of up t o 80.9%. This includes a 35.8% reduction in acidification, 47.9% in global warming \npotential impact, 59.1% in freshwater ecotoxicity, 61.8% in human non -cancer toxicity emissions, \n65.5% in ozone depletion potential, 38.0% in particulate matter, 45.9% in ph otochemical ozone, \n40.2% in fossils depletion, 79.2% in mineral and metals use, and a 28.1% reduction in water use \nover four recycling cycles . It is noteworthy that the use of vPCBs could lead to savings of up to \n80.9% in human cancer toxicity emissions.  \nIn this study, we have reported a recyclable PCB based on transesterification vitrimers and \na nondestructive, swelling -based separation for vitrimer composite recycling. Our design achieves \nthe following three key goals: 1) raw materials with strong adhesi on between the vitrimer material 28 and copper layers on par with FR -4 PCBs, 2) compatibility of vPCBs with the traditional, \ninexpensive PCB manufacturing processes to enable scalable production including multi -layer \ndesigns, 3) electrical and mechanical prop erties comparable to standard FR -4 PCBs required to \ncreate functional circuits.  Such recyclable PCBs with excellent electrical and mechanical \nproperties, chemical etchant resistance, repairability, and closed -loop recyclability, make them a \npromising an d straightforward alternative solution to conventional PCBs. Not only do they reduce \nthe toxic byproduct problems associated with conventional PCBs, but they also significantly \nimprove a broad range of environmental impact metrics. Additionally, the abunda nce and \nwidespread commercial availability of epoxies and catalysts with varying properties create \nnumerous opportunities to experiment with new vitrimer formulations optimized for specific \nproperties. Our approach of swelling PCBs could also be explored f or automated disassembly of \ncircuit components to simultaneously enhance value recovery from ICs while recycling raw \nmaterials. We hope this study will open up new directions for the development and application of \nemerging recyclable polymeric materials an d composites for the electronics and computer industry \nthat centers on environmental sustainability.   29 Data availability  \nAll data needed to evaluate the conclusions of the paper are available in the paper or in the \nExtended Data and Source Data.   30 Reference  \n1. Hibbert, K. & Ogunseitan, O. A. Risks of toxic ash from artisanal mining of discarded \ncellphones. J. Hazard. Mater.  278, 1–7 (2014).  \n2. Awasthi, A. K., Zeng, X. & Li, J. Environmental pollution of electronic waste recycling in \nIndia: A critical review. Environ. Pollut.  211, 259 –270 (2016).  \n3. Song, Q., Li, J. & Zeng, X. Minimizing the increasing solid waste through zero waste \nstrategy. J. Clean. Prod.  104, 199 –210 (2015 ). \n4. Forti, V., Balde, C. P., Kuehr, R. & Bel, G. The Global E -waste Monitor 2020: Quantities, \nflows and the circular economy potential . (United Nations University/United Nations Institute \nfor Training and Research, International Telecommunication Union, and International Solid \nWaste Association, 2020).  \n5. Abdelbasir, S. M., Hassan, S. S. M., Kamel, A. H. & El -Nasr, R. S. Status of electronic waste \nrecycling techniques: a review. Environ. Sci. Pollut. Res.  25, 16533 –16547 (2018).  \n6. Ogunseitan, O. A., Schoenung, J. M., Saphores, J. -D. M. & Shapiro, A. A. The Electronics \nRevolution: From E -Wonderland to E -Wasteland. Science  326, 670 –671 (2009).  \n7. Ogunseitan, O. A. et al.  Biobased materials for sustainable printed circuit boards. Nat. Rev. \nMater.  7, 749 –750 (2022).  \n8. Yang, C., Li, J., Tan, Q., Liu, L. & Dong, Q. Green Process of Metal Recycling: Coprocessing \nWaste Printed Circuit Boards and Spent Tin Stripping Solution. ACS Sustain. Chem. Eng.  5, \n3524 –3534 (2017).  \n9. Zeng, X., Mathew s, J. A. & Li, J. Urban Mining of E -Waste is Becoming More Cost -Effective \nThan Virgin Mining. Environ. Sci. Technol.  52, 4835 –4841 (2018).  31 10. Hsu, E., Durning, C. J., West, A. C. & Park, A. -H. A. Enhanced extraction of copper \nfrom electronic waste via ind uced morphological changes using supercritical CO2. Resour. \nConserv. Recycl.  168, 105296 (2021).  \n11. Kawahara, Y., Hodges, S., Cook, B. S., Zhang, C. & Abowd, G. D. Instant inkjet circuits: \nlab-based inkjet printing to support rapid prototyping of UbiComp devices. in Proceedings of \nthe 2013 ACM international joint conference on Pervasive and ubiquitous computing  363–\n372 (Association for Computing Machinery, 2013). doi:10.1145/2493432.2493486.  \n12. Siegel, A. C. et al.  Foldable Printed Circuit Boards on Paper  Substrates. Adv. Funct. \nMater.  20, 28–35 (2010).  \n13. Huang, X. et al.  Biodegradable Materials for Multilayer Transient Printed Circuit Boards. \nAdv. Mater.  26, 7371 –7377 (2014).  \n14. Cheng, T. et al.  Silver Tape: Inkjet -Printed Circuits Peeled -and-Transferr ed on Versatile \nSubstrates. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.  4, 6:1-6:17 (2020).  \n15. Arroyos, V. et al.  A Tale of Two Mice: Sustainable Electronics Design and Prototyping. \nin Extended Abstracts of the 2022 CHI Conference on Human Facto rs in Computing Systems  \n1–10 (Association for Computing Machinery, 2022). doi:10.1145/3491101.3519823.  \n16. Cheng, T. et al.  SwellSense: Creating 2.5D interactions with micro -capsule paper. in \nProceedings of the 2023 CHI Conference on Human Factors in Compu ting Systems  1–13 \n(Association for Computing Machinery, 2023). doi:10.1145/3544548.3581125.  \n17. Kuang, X., Mu, Q., Roach, D. J. & Qi, H. J. Shape -programmable and healable materials \nand devices using thermo - and photo -responsive vitrimer. Multifunct. Mater . 3, 045001 \n(2020).  32 18. Shojaeiarani, J., Bajwa, D. S., Rehovsky, C., Bajwa, S. G. & Vahidi, G. Deterioration in \nthe Physico -Mechanical and Thermal Properties of Biopolymers Due to Reprocessing. \nPolymers  11, 58 (2019).  \n19. Mir, S. & Dhawan, N. A comprehens ive review on the recycling of discarded printed \ncircuit boards for resource recovery. Resour. Conserv. Recycl.  178, 106027 (2022).  \n20. Rocchetti, L., Amato, A. & Beolchini, F. Printed circuit board recycling: A patent review. \nJ. Clean. Prod.  178, 814 –832 (2018).  \n21. Chen, Z. et al.  Recycling Waste Circuit Board Efficiently and Environmentally Friendly \nthrough Small -Molecule Assisted Dissolution. Sci. Rep.  9, 17902 (2019).  \n22. Khrustalev, D., Tirzhanov, A., Khrustaleva, A., Mustafin, M. & Yedrissov, A. A new \napproach to designing easily recyclable printed circuit boards. Sci. Rep.  12, 22199 (2022).  \n23. Beeler, B. & Bell, L. Plastic Recycling Schemes Generate High Volumes of Haza rdous \nWaste. IPEN  https://ipen.org/news/plastic -recycling -schemes -generate -high-volumes -\nhazardous -waste (2021).  \n24. Montarnal, D., Capelot, M., Tournilhac, F. & Leibler, L. Silica -Like Malleable Materials \nfrom Permanent Organic Networks. Science  334, 965 –968 (2011).  \n25. Zheng, N., Xu, Y., Zhao, Q. & Xie, T. Dynamic Covalent Polymer Networks: A \nMolecular Platform for Designing Functions beyond Chemical Recycling and Self -Healing. \nChem. Rev.  121, 1716 –1745 (2021).  \n26. Kamble, M. et al.  Reversing fatigue in carbon -fiber reinforced vitrimer composites. \nCarbon  187, 108 –114 (2022).  \n27. Park, S., Kim, S., Han, Y. & Park, J. Apparatus for electronic component disassembly \nfrom printed circuit board assembly in e -wastes. Int. J. Miner. Process.  144, 11–15 (2015).  33 28. Lee, M. -S., Ahn, J. -G. & Ahn, J. -W. Recovery of copper, tin and lead from the spent \nnitric etching solutions of printed circuit board and regeneration of the etching solution. \nHydrometallurgy  70, 23–29 (2003).  \n29. Zou, Z. et al.  Rehealable, fully recyclable, and malleable electronic skin enabled by \ndynamic covalent thermoset nanocomposite. Sci. Adv.  4, eaaq0508 (2018).  \n30. Kehong, F. High performance epoxy copper clad laminate. Circuit World  30, 16–19 \n(2004).  \n31. Guerre, M., Taplan, C., Winne, J. M. & Prez, F. E. D. Vitrimers: directing chemical \nreactivity to control material properties. Chem. Sci.  11, 4855 –4870 (2020).  \n32. Yang, Y., Xu, Y., Ji, Y. & Wei, Y. Functional epoxy vitrimers and composites. Prog. \nMater. Sci.  120, 100710 (202 1). \n33. Liu, T. et al.  A Self -Healable High Glass Transition Temperature Bioepoxy Material \nBased on Vitrimer Chemistry. Macromolecules  51, 5577 –5585 (2018).  \n34. Zhang, X. et al.  Novel Phosphazene -Based flame retardant polyimine vitrimers with \nMonomer -Recov ery and high performances. Chem. Eng. J.  440, 135806 (2022).  \n35. Borges, C. S. P. et al.  Effect of Water Ingress on the Mechanical and Chemical Properties \nof Polybutylene Terephthalate Reinforced with Glass Fibers. Materials  14, 1261 (2021).  \n36. Daepp, M. I. G. et al.  Eclipse: An End -to-End Platform for Low -Cost, Hyperlocal \nEnvironmental Sensing in Cities. in 2022 21st ACM/IEEE International Conference on \nInformation Processing in Sensor Networks (IPSN)  28–40 (2022). \ndoi:10.1109/IPSN54338.2022.00010.  \n37. Hollins, O. Executive Summary: An assessment of the greenhouse gas  emissions and \nwaste impacts from improving the repairability of Microsoft devices.  34 38. Wu, P., Liu, L. & Wu, Z. A transesterification -based epoxy vitrimer synthesis enabled \nhigh crack self -healing efficiency to fibrous composites. Compos. Part Appl. Sci. Manuf.  162, \n107170 (2022).  \n39. Zhang, D. & Huang, Y. Influence of surface roughness and bondline thickness on the \nbonding performance of epoxy adhesive joints on mild steel substrates. Prog. Org. Coat.  153, \n106135 (2021).  \n40. DuPont. Recovery of Tetrahydrofuran (THF).  \n41. Hubbard, A. M. et al.  Vitrimer Transition Temperature Identification: Coupling Various \nThermomechanical Methodologies. ACS Appl. Polym. Mater.  3, 1756 –1766 (2021).  \n42. Tian, X., Stranks, S. D. & You, F. Life cycle assessment of recycling strategies for \nperovskite photovoltaic modules. Nat. Sustain.  4, 821 –829 (2021).  \n43. IPC-7711C/7721C Rework, Modification and Repair of Electronic Assemblies. (2017).  \n44. IPC TM -650 Test Metho ds Manual. IPC International, Inc.  https://www.ipc.org/test -\nmethods.  \n  35 Acknowledgments  \nWe thank T. Cheng for discussion, Z. Englhardt for help with Bluetooth coding, B. Kuykendall \nfor the use of mechanical tester s, C. Li for feedback on the figures, K. Liao and M.Parker  for help \nwith flammability testing, and H. Wang for help with composite fabrication . We also thank D. \nBaker, F. Newman, and C. Toskey for help with sputter coating and copper plating. This research \nis funded by the Microsoft Climate Research Initiative.  \n \nAuthor Information  \nAuthors and Affiliations  \nPaul G. Allen School of Computer Science & Engineering, University of Washington, Seattle, \nWA, USA  \nZhihan Zhang, Jake A. Smith, Bichlien H. Nguyen, Shwetak Patel & Vikram Iyer  \n \nDepartment of Mechanical Engineering, University of Washington, Seattle, WA, USA  \nAgni K. Biswal, Ankush Nandi & Aniruddh Vashisth  \n \nMicrosoft Research, Redmond, WA, USA  \nKali Fros t, Jake A. Smith & Bichlien H. Nguyen  \nCorresponding author  \nCorrespondence to Aniruddh Vashisth or Vikram Iyer.  36 Contributions  \nZ.Z., B.H.N., A.V. and V.I. conceptualized, organized and structured the work. Z.Z., A.K.B. and \nA.N. fabricated glass fiber reinfor ced vitrimer composites. Z.Z. manufactured vitrimer -based PCB \nand conducted characterizations. Z.Z. designed the hardware system, experiments and evaluations. \nZ.Z., J.A.S., and B.H.N. designed the repair experiments and evaluations. Z.Z., A.K.B., J.A.S., \nB.H.N. and A.V. designed the recycling experiments and evaluations. Z.Z. and A.K.B conducted \nmaterial characterizations . K.F. conducted the life -cycle assessment analysis. Z.Z. and V.I. wrote \nthe manuscript. All authors contributed to the study concept and experimental methods, discussed \nthe results and edited the manuscript.  \n \nEthics declarations  \nCompeting interests  \nK.F., J.A.S., and B.H.N. are employees of Microsoft Corporation. S.P. is a Google employee. Z.Z., \nA.K.B., A.N., A.V. and V.I. declare no competi ng interests.   37 Extended data figures and tables  \nExtended Data Fig. 1 Dynamic mechanical analysis of pristine vitrimer and recycled vitrimer  \n \na, b, c, Normalized stress relaxation curves of pristine vitrimer (a), vitrimer after one recycling \ncycle (b), and vitrimer after two recycling cycles (c) at temperatures ranging from 140 °C to 240 °C . \nIn all cases increasing temperature results in faster stress r elaxation. d, e, f, Characterized storage \nmodulus, loss modulus, and tan delta of pristine vitrimer (d), vitrimer after one recycling cycle (e), \nand vitrimer after two recycling cycles (f).   \n38 Extended Data Fig. 2 Peel strength for laminates with a layer of partially cured vitrimer  \n \na, b, Curves of the peeling force per width of copper -clad versus displacement for laminates with \na layer of partially cured vitrimer after thermal stress ( a), and at 125 °C ( b) compar ed to the PCB \nstandard of FR -4.  \n39 Extended Data Fig. 3 Joint strength  of repaired via holes in GFRV  \n \na, Photograph showing the joint strength testing setup. Specimen is centered on a metal hollow \ncylinder support with a support span of 16 mm. b, Characterized shear stress of repaired via holes \nin GFRV compared to the repaired holes in FR -4 using super glue. c, Photogra ph of FR -4 after \nshear punch, showing cyanoacrylate glue bond  broke . d, Photograph of GFRV  after shear punch, \nshowing the repaired via hole was deformed into a funnel -shape under the force of  punch but \nremained intact, indicating a stronger interface at the hole boundary .  \n40 Extended Data Fig. 4 Solvents test for GFRV recycling  \n \nGFRV samples were cut into rectangular shapes and immersed in various solutions (Acetone, \nCHCl 3, DMF, THF); the top, middle and bottom photos were taken immediately af ter immersing, \nafter 48 hours, and after 96 hours, respectively.   \n41 Extended Data Fig. 5 Characterized storage modulus, tan delta, r etention of storage modulus , and \nvitrimer transition temperature  of recycled vitrimer  \n \na, Characterized  storage modulus temperature sweep results of vitrimer after one and two recycling \ncycles compared to pristine. The storage modulus shows a slight decrease after recycling. b, Tan \ndelta temperature sweep results of vitrimer after one and two recycling cycl es compared to pristine, \ntan delta broadens and the left shift of peaks is negligible after recycling. c, Retention of storage \nmodulus ( N = 3, error bars = ± 𝜎) of vitrimer after one and two recycling cycles compared to pristine.  \nd, Tv comparison of pristine vitrimer, vitrimer after one and two recycling cycles, indicating the \nshift of Tv is negligible after recycling. The Arrhenius plot is derived with a linear fit to the low -\ntemperature region (140 °C to 180 °C), and its intersecti on with where the stress -relaxation \nconstant is 10^6 indicates the Tv.   \n42 Extended Data Fig. 6 Characterized electrical and mechanical properties of reformed GFRV  \n \na, b, c, Characterized dielectric constant (a), flexural strength (b), volume resistivity (c), and loss \ntangent (d) of reformed GFRV compared to virgin composite ( N ≥ 3 (a, c, d) and N = 1000 (c), \nerror bars = ± 𝜎).  \n43 Extended Data Fig. 7 Environmental impact of vPCB freight  \nComparison of the environmental impact of vPCB freight versus conventional FR -4 prepreg \nfreight across 11 different categories.   \n44 Extended Data Fig. 8  Global warming potential impact breakdown of conventional FR -4 PCB  \n \nDetailed contributions in kg CO 2 equivalents to global warming potential for conventional FR -4 \nPCB.  \n",
      "metadata": {
        "filename": "Recyclable vitrimer-based printed circuit board for circular electronics.pdf",
        "hotspot_name": "PCBA_Production",
        "title": "Recyclable vitrimer-based printed circuit board for circular electronics",
        "published_date": "2023-08-24T01:34:50Z",
        "pdf_link": "http://arxiv.org/pdf/2308.12496v1",
        "query": "PCB assembly sustainability environmental assessment"
      }
    },
    "Sustainable bioplastics from amyloid fibril-biodegradable polymer blends": {
      "full_text": "  \n1  Sustainable bioplastics from amyloid fibril-biodegradable polymer blends   Mohammad Peydayesh, Massimo Bagnani, and Raffaele Mezzenga*  ETH Zurich, Department of Health Sciences and Technology, Schmelzbergstrasse 9, Zurich 8092, Switzerland   E-mail: raffaele.mezzenga@hest.ethz.ch    Keywords: bioplastics, amyloid fibrils, sustainability, waste management, circular economy   Plastic waste production is a global challenging problem since its accumulation in the environment is causing devastating effects on the planet's ecosystem. Sustainable and green solutions are urgently needed, and this pairs with increasingly stronger regulations combined with improved ecological awareness. This study proposes a simple, scalable and water-based process to produce free-standing, transparent and flexible bioplastic films by combining amyloid fibrils with biodegradable polymers as two main building blocks. Amyloid fibrils can be obtained through denaturation and self-assembly from a broad class of food proteins found in milk, soy, and egg, for example. Whey is used here as a model protein, since it is the major by-product of dairy industries, and its valorization creates a valuable opportunity to produce sustainable, biodegradable, and environmentally friendly bioplastics perfectly integrated within a circular economy. Against this background, we highlight the sustainability superiority of these bioplastics over common plastics and bioplastic via a detailed life cycle assessment, anticipating an important role of this new class of bioplastics in mitigating the pressing plastic pollution challenge.   1. Introduction Plastic is one of the most abundant man-made materials and, although its widespread use started only 70 years ago, an estimate of 8300 million metric tons (Mt) of plastic have been produced to date. The intensive utilization of single-use containers drastically accelerated plastic production, and packaging is now plastic's largest market[1]. Approximately 150 Mt of solid   \n2  plastic, corresponding to half of the annual global production is thrown away each year worldwide[2]. Most polymers used to produce plastics are derived from fossil hydrocarbons, posing a threat to fossil fuel sources. Additionally, most common plastics are not biodegradable and, if not permanently destroyed by proper thermal treatment, accumulate in the natural environment or landfills causing devastating effects to the planet's ecosystem. Up to 80% of the global plastic waste ends up contaminating the environment, and 4-12 Mt of plastic enters the oceans each year[3]. Plastic debris can be found in all the ocean basins[4] and are so abundant in the environment that it can be used as a geological indicator of the Anthropocene era[5]. Although recycling is suggested as a possible solution to mitigate some of these problems, this process remains limited to less than 9% of global plastic waste since it is costly, time-consuming, cannot be applied to many polymeric materials, and the quality of the polymers obtained is low[2]. Thanks to the growing awareness about the environmental issues related to the accumulation and disposal of traditional plastics, stringent regulations are being implemented worldwide, pushing the plastic market towards a transition to more sustainable products and processes. The production of bioplastic increased dramatically in the past few years, and it is expected to grow even more substantially in the future[6–8]. The European Bioplastic organization categorizes bioplastic into two major groups defined as biodegradable and bio-based plastics[9]. The latter are produced employing renewable resources such as cellulose, starch polylactide (PLA), or polyhydroxyalkanoates (PHAs) instead of fossil fuels; however, unfortunately, not all bio-based plastics are biodegradable, such as those produced by converting renewable resources like corn, sugarcane, and cassava into building blocks for polyethylene terephthalate (PET)[9,10]. The American Society for Testing and Materials (ASTM) defines biodegradable those plastics that degrade under the action of naturally occurring microorganisms such as bacteria, fungi, and algae[11]. Additionally, if a plastic degrades due to biological processes into biomass, carbon dioxide, water, and inorganic compounds without leaving toxic residues, it is defined as   \n3  compostable. So all compostable plastics are biodegradable; however, not all biodegradable plastics are compostable[11].  Protein-based bioplastic is attracting tremendous attention due to its broad availability, fast biodegradation rates, and food-grade nature resulting in films that can even be classified as edible[12–15]. The main drawbacks of protein-based bioplastic derive from the intrinsic nature of native protein monomers, which are often globular, hydrophilic, and water-soluble and result in difficulties to process films, show poor mechanical and barrier properties, and are very sensitive to water and humidity. In the dairy industries, for producing 1 kg of cheese, 8-9 kg of whey are produced as a by-product. It represents the main by-product of the dairy industries, where each year, approximately 120 million tons of whey are produced globally. Since only half of the whey produced is transformed into valuable products such as human or animal feed[16] (Figure 1), the disposal of surplus whey represents a crucial issue for the dairy industries and causes environmental concerns due to the high biological oxygen demand by-product[17–19]. In fact, whey contains a high load of organic matter, which is mainly composed of lactose (0.18-60 kg/m3), proteins (1.4-33.5 kg/m3), and fats (0.08-10.58 kg/m3)[18,20]. Whey is, therefore, a rich source of proteins whose the most abundant is β-lactoglobulin[21,22], a globular protein that can easily self-assembly into amyloid fibrils[23–25]. Amyloid fibrils play critical functional roles in various biological processes in multiple organisms, ranging from bacteria to humans. Thanks to their promising biophysical properties, mechanical and chemical stability, many applications have been proposed[26–29]. Amyloid fibrils can also self-assembly in vitro from various proteins in milk, egg, and soy through denaturation and hydrolysis under proper conditions, typically involving low pH and high temperatures[30–32]. A broad range of functionalities characterizes the fibrils obtained from these food proteins and, thanks to their remarkable properties that are far superior to those of single monomers,   \n4  such as high stiffness and aspect ratios, they have been used as building blocks for developing suspensions, emulsions, membranes, and gels with high performances[28,29,32,33].  This study focuses on developing bioplastic composed of amyloid fibrils and showcase them as ideal candidates to produce hybrid films. In particular, we show that amyloid fibrils can be used as building blocks for engineering novel bioplastics with targeted characteristics, further tuned by blending different functional additives such as bio-polymers and plasticizers to improve the performances of the resulting films. Other important film properties such as water stability, hydrophobicity, and antioxidant activity can be tuned by chemical treatment or coatings. These novel bioplastics are characterized by a wide range of properties that can be achieved and tuned without requiring the use of non-biodegradable or toxic compounds. Moreover, these bioplastics show great potential for commercialization with economic viability thanks to the meager cost of the protein used, which are mainly obtained by waste products of the food industry, and the cheap, scalable, and environmentally friendly water-based production protocol for film formation. Ultimately, producing bioplastic using food wastes helps industry in two ways: not only it improves their production in terms of sustainability, waste management, and valorization, but it also directly improves their circular economy.   2. Results and disscussion  2.1. Bioplastic films The bioplastic presented here can be obtained by a simple, scalable, and water-based protocol which is schematically described in Figure 1. Specifically, WPI (whey protein isolate) -here taken as a model protein available from food processing byproducts- is dispersed in pH 2 water together with a plasticizer (Glycerol) and a water-soluble polymer (such as poly (vinyl alcohol), PVA). The solution is then heated at 90°C for 5 h to solubilize all the components homogeneously and convert whey protein (mostly β-lactoglobulin) into self-assembled amyloid   \n5  fibrils. Free-standing homogeneous and transparent films (see Figure 1) can be obtained by casting the solution onto suitable substrates followed by solved evaporation.   \n Figure 1. Schematic representation of the bioplastic preparation protocol. Each year in Switzerland 1.3 Mt of whey are produced, of which only half of it is valorized efficiently[16]. Whey protein isolate, Glycerol, and PVA are dispersed in water, the solution is heated at 90°C for 5 hours to allow β- lactoglobulin monomers to self-assemble into amyloid fibrils. The solution is then cast to obtain bioplastic film through solvent evaporation.   2.2. Microstructure  To verify that amyloid fibrils self-assembly is not inhibited by the other compounds in solution (Glycerol and PVA and other compounds contained in WPI), atomic force microscopy (AFM) has been used to image the solution resulting after the heat treatment. Figure 2a shows amyloid fibrils obtained from pure β-lactoglobulin dispersed in pH 2 water. As a comparison, Figure \n  \n6  2b shows the amyloid fibrils formed from WPI and in the presence of PVA and Glycerol, confirming that the other compounds do not inhibit the self-assembly of β-lactoglobulin into amyloid fibrils in solution. The main difference between the fibrils obtained appears to be their contour length distribution that decreases when amyloids are produced in the other compounds' presence. This difference derives from the fact that a very high viscosity characterizes the solution used to produce bioplastic. To avoid gelation, vigorous stirring has to be applied during the heat treatment. In fact, these mechanical stresses are known to induce shortening of amyloid aggregates[34]. In Figure 2 c and d, the SEM images of the surface of the hybrid amyloid and monomer films are presented. As shown in the figure, the amyloid-based films' surface is smooth, homogenous, and without cracks. However, the surface of the films obtained with WPI monomers shows multiple cracks spanning several micrometers. These cracks increase the oxygen and vapor transport through the films, resulting in films that might not be suitable for food packaging. Whey has already been proposed as the right candidate to obtain edible films[35–37], thanks to the low cost and broad availability of this by-product. However, the film resulting from the whey protein in their native or partially hydrolyzed state results in low mechanical properties and water stability [35–37]. On the contrary, amyloid based bioplastic results in a film characterized by a highly homogeneous surface, thanks to the high degree of interactions between amyloid fibrils, the plasticizer (Glycerol), and the polymer chains (PVA), favored by the very high aspect ratio of amyloid fibrils combined with the numerous functional groups on their surface.     \n7   Figure 2. Characterization of amyloid fibrils and film surfaces. a) AFM of amyloid fibrils, b) AFM of the mixture of amyloid fibrils, Glycerol, and PVA, c) SEM of hybrid amyloid film top surface, and d) SEM of hybrid monomer film top surface.  2.3. Surface contact angle One of the most significant weaknesses of bioplastics is their inherent low water stability, especially compared to petrol-based plastics[14,38–40]. Producing films that are not degraded by vapor or water exposure is essential for their real-world application. We performed static water contact angle analysis to determine the hydrophobicity of amyloid-based films (Figure S1). As depicted in Figure 3a, the contact angle varies highly depending on the composition and the chemical treatment. In particular, the film's contact angle formed by native WPI monomers is the smallest one, confirming the hydrophilic nature of this species. However, the contact angle \n  \n8  measured on amyloid fibril-based films resulted in a more hydrophobic nature than the monomer-based ones. Most impoetantly, the chemical crosslinking treatment using FAS resulted in films with a surface characterized by a super-hydrophobic nature, with contact angles above 90°. The films containing CA, showed the lowest contact angle measured, even compared to protein monomers.    \n Figure 3. a) Water contact angle of the films after different crosslinking treatment. b) mass change, and c) water absorption.    \n  \n9  2.4. Water stability To further investigate the interaction of the films with water, we performed two additional experiments, the water absorption, and the mass release tests. As shown in Figure 3b, a substantial mass release of about 50% has been measured on all the films after 2 h of water immersion. However, after this first water immersion, the mass loss measured in films containing amyloid fibrils decreased drastically, leaving the mass essentially stable up to 24 h. The films' main component released in water is assumed to be primarily the plasticizer due to the high water solubility of this compound. This is further confirmed by the fact that the dried films appeared much more brittle than the same films before water immersion. At the same time, the films composed by WPI native monomers showed a continuous mass loss during all the time tested, up to complete film dissolution (not shown in the figure).  The water adsorption measurements resulted in very similar behavior in the films. As shown in Figure 3c, amyloid-based films absorbed around 225% of water after 24 h of immersion. The swelling, however, increased significantly in the films obtained by WPI monomers, reaching water adsorption values of around 325%. The low performances of monomer-based films are supposed to be related to the hydrophilic nature of the protein native state but also to the surface properties of these films. As already shown in Figures 2 and 3a, monomer-based films are characterized by a hydrophilic, inhomogeneous, and fractured surface, confirming the highest performances of amyloid fibrils compared to native monomers.  2.5. Mechanical properties The mechanical properties of the hybrid whey amyloid, monomer, amyloid FAS-coated, and CA-contained films are depicted in Figure 4. As observed in stress-strain curves (Figure 4a), fibrillization, combination, and coating directly affect the mechanical properties of the films. The maximum stress at break, ultimate elongation, and toughness values are presented in Figure 4b-d, respectively (for Young's modulus, see Figure S2). The maximum stress of pure   \n10  amyloid hybrid film is 17 MPa, similar to the value for monomer hybrid films. However, as shown in Figure 4c, the elongation of hybrid amyloid films is improved at least a factor two compared to hybrid monomer films. It was speculated that the better elongation of the amyloid hybrid films was attributed to the good alignment of nanofibril chains, allowing interfibrillar molecular rearrangement during deformation without undergoing fracture [41]. The excellent elongation to break of hybrid films is a very important property for food packaging applications. However, this property leads to a lower Young's modulus for hybrid amyloid fibrils films than monomer hybrid films (Figure S2). Furthermore, it appeared that although either coating or hybridizing the films with FAS or CA, improves the maximum stress and Young's modulus, it decreases the film strain, resulted in rigid and less flexible films. This phenomenon is due to the crosslinking effect and limited mobility of whey amyloid fibrils chains after the addition of CA or coating with FAS [42]. Finally, the toughness of different hybrid films was measured as the area under the stress-strain curves, and the results are summarized in Figure 4d. As observed, all the hybrid amyloid-based films are characterized by higher toughness, which are up to 2 times higher than hybrid monomer ones.   \n11   Figure 4. Mechanical properties of films. a) stress-strain plot, b) maximum stress, c) maximum strain, and d) toughness.  2.6. Optical properties The optical properties of the films were analyzed using UV-vis spectroscopy, the resulting transmittance values in the visible spectrum (660 nm), and the UV range (280 nm) are summarized in Table 2. All the films tested are characterized by a high degree of transparency in the visible spectra with transmittance values above 95%. The films showed good UV-screening ability in the UV range with transmittance values dropping below 60%. This property is desirable in food-packaging materials since UV irradiation increases the oxidation rates of food, even at low temperatures [43]. At the same time, high transparency in the visible spectra is an important physical property, providing a see-through on the wrapped items. Hence, hybrid amyloid films features ideal optical properties for packaging applications.  \n  \n12  Table 1. Optical properties of the prepared films.  % Transmittance (660 nm) % Transmittance (280 nm) Hybrid amyloid 95.9±0.4 57±7 Hybrid monomer 96.0±0.0 53±6 Hybrid amyloid FAS 97.4±0.1 58±2 Hybrid amyloid CA 97.2±0.1 56±2  2.7. Water vapor permeability The barrier properties of the films against water vapor have been measured along 24 h, and the results are summarized in Figure 5a. Amyloid-based films showed the best performances, and the WVP resulted stable over the 24 h sampling. The hybrid films produced with WPI monomers showed WVP values higher than the hybrid amyloid films, and WVP drastically increased with time. Interestingly, the amyloid-based film showed extremely low WVP, and close to zero mass change was measured during the first 4 h of the test. On average, the hybrid monomer films showed WVP value 2.05×10-6 \tg m m-2 day-1 Pa-1, 1.5 times higher than the hybrid amyloid ones (1.65×10-6 g m m-2 day-1 Pa-1).  2.8. Antioxidant activity  The antioxidant properties of the different hybrid films were analyzed with ABTS radical scavenging activity, and the results are presented in Figure 5b. The untreated hybrid amyloid films and FAS-treated amyloid films showed higher antioxidant performance when compared to hybrid WPI monomer films. Amyloid films treated with CA showed antioxidant properties such as those produced with WPI monomers. The antioxidant properties of the films are derived from the amino acids cysteine, tyrosine, tryptophan, and histidine, which are strong free radicals scavengers [44]. Importantly, this activity is enhanced for amyloid fibrils due to their significantly higher surface-to-volume ratio than protein monomers.    \n13  2.9. Food migration To evaluate the performance of films as food packaging materials, the migration of components from the films to food was tested using Tenax®, a standard dry food simulant. As shown in Figure 5c, in hybrid amyloid films, a migration intensity of 3.6 mg/dm2 is found, a value which is well below the limit (10 mg/dm2) set by European Union legislation. Hybrid WPI monomer films, however, resulted in a migrations rate of 35.7 mg/dm2, that is, well above the acceptable threshold.  \n Figure 5. Film characterization. a) water vapor permeability. b) antioxidant activity. c) food migration analysis with Tenax®.   \n  \n14  2.10. Generality, scalability, and benchmark to other plastics   To explore the generality of the approach for fabricating different hybrid amyloid fibrils, we replaced PVA with Methylcellulose (MC), methyl ether of cellulose. As observed in Figure 6a, the resulting HAm-MC film was smooth, flexible, and transparent. While both PVA and MC are biodegradable polymers, MC has the advantage of being from biosources and more sustainable. However, the water stability of native MC hybrid amyloid fibrils films was lower than the ones with PVA. Next, we demonstrate the scalability of the hybrid whey amyloid fibrils bioplastic by its production on a larger scale. As shown in Figure 6b, the bioplastic film with the size of 1.5 m×1.5 m can be produced successfully via a simple solution-casting process.  The mechanical properties of the hybrid amyloid fibrils films with PVA and MC were compared to different engineering, thermosets, rubbers and biodegradable plastics [45] (Figure 6c). Although both bioplastics in this work have similar elastic modulus, HAm-PVA has a higher elongation value compared to HAm-MC, reaching the value of 750%. This excellent strain value places the HAm-PVA film among the best elastic plastics ever produced. Even though the young modulus of bioplastics in this work was in the range of PTFE, PBS, PCL, LDPE, and EPDM, their elastic modulus was lower than the bioplastics such as PBS and PLA. However, precisely owing to such a high elastic modulus and a low elongation rate, it is not easy to make applicable bioplastics with PBS and PLA, and typically, they need to be blended with more flexible plastics for market applications[46]. This fact can be reflected in the toughness values shown in Figure 6d. Most of the biodegradable plastic suffer from low toughness values, resulting in products that are not ductile and are fragile. In case of amyloid based films, this property is significantly enhanced, resulting in toughness values several orders of magnitude superior to most of commonly used bioplastics.   \n15   Figure 6. Generality, scalability, and comparison to other plastics. a) Visual appearance of the hybrid bioplastic based on amyloid fibrils and methylcellulose. b) scalability example of hybrid amyloid fibrils bioplastic. c) mechanical properties of different hybrid amyloid fibrils bioplastic and other plastics[45]. Panel d) shows the toughness values of bioplastics based on starch[47,48], PHB[49], PHA[50], PLA[49], cellulose[51] and hybrid amyloid films.  2.11. Life cycle assessment LCA of HAm-PVA compared to PLA and PVF, including the exact amount of each category impact, is summarized in Table 2. Furthermore, the normalized environmental impact profile of production of the three plastics, comprising all 18 impacts, is presented in Figure 7a. Across all the impact categories, HAm-PVA bioplastic has the lowest environmental impact compared to PLA and PVF. This superiority is due to the HAm-PVA bioplastic greener and more straightforward production process. More importantly, the raw material for HAm-PVA \n  \n16  bioplastic production is a waste from the dairy industry, which adds additional value to sustainability and circular economy. Combined to their excellent properties and degradability, the bioplastic presented in this work could lead to further environmental benefits not included in this LCA. As observed, freshwater and marine ecotoxicities, human toxicity, and freshwater eutrophication have the highest relative contribution to the environmental impacts. The LCA results reveal that the main cause for the high values in these categories comes from the required energy for plastics production. To shed light on energy demand and correlate it with the other environmental impacts, we further performed CED analysis. The energy use breaks down into the categories of nonrenewable (fossil, nuclear, and biomass) and renewable (biomass, solar/wind/geothermal, and hydro) energy [52]. As shown in Figure 7b, PVF and PLA have 4 and 3 times higher energy consumption, respectively, than HAm-PVA bioplastic. Figure 7c shows the impact of the three plastics production on climate change. While the production of 1 kg of PVF and PLA approximately results in around 17 and 9 kg CO2 eq, HAm-PVA bioplastic only emits about 4 kg CO2 eq. Marine ecotoxicity is one of the most acute destructive effects of plastics on the environment since 10% of the plastic produced every year ends up in the oceans [53]. As observed in Figure 7d, the results for marine ecotoxicity showed the same trend as climate change impact, where HAm-PVA bioplastic showed minimal effect (0.02 kg 1,4-DB eq) compared to PLA (0.07 kg 1,4-DB eq), and PVF (0.16 kg 1,4-DB eq). Furthermore, as shown in Figure 7e, compared to the other two plastic, PLA has the highest impact on water resources, mainly due to water irrigation during crop production and high energy demand. Altogether, the LCA results demonstrate unambigously the superiority of HAm-PVA bioplastic in terms of sustainability and environmental compatibility compared to other typical plastics and bioplastics.     \n17  Table 2. LCA impact assessment based on ReCiPe Midpoint method. Impact category Unit HAm-PVA PLA PVF Climate change kg CO2 eq 4.260198 9.220808 16.65613 Ozone depletion kg CFC-11 eq 5.58E-07 4.76E-07 1.14E-06 Terrestrial acidification kg SO2 eq 0.020939 0.048895 0.078856 Freshwater eutrophication kg P eq 0.001091 0.004544 0.007002 Marine eutrophication kg N eq 0.001531 0.007004 0.002663 Human toxicity kg 1,4-DB eq 0.934583 2.914094 6.327723 Photochemical oxidant formation kg NMVOC 0.010556 0.027956 0.037809 Particulate matter formation kg PM10 eq 0.007353 0.029297 0.042293 Terrestrial ecotoxicity kg 1,4-DB eq 0.000881 0.008153 0.000708 Freshwater ecotoxicity kg 1,4-DB eq 0.03041 0.079411 0.174013 Marine ecotoxicity kg 1,4-DB eq 0.02145 0.070634 0.163069 Ionising radiation kBq U235 eq 0.065584 1.161086 1.593614 Agricultural land occupation m2a 0.071638 1.518946 0.576349 Urban land occupation m2a 0.009264 0.102883 0.103839 Natural land transformation m2 0.000828 0.001176 0.002372 Water depletion m3 0.070116 0.322912 0.21498 Metal depletion kg Fe eq 0.028109 0.115734 0.662357 Fossil depletion kg oil eq 1.468327 2.324439 3.864043    \n18   Figure 7. LCA of HAm-PVA compared to PLA and PVF. a) Normalized environmental impact profile of HAm-PVA compared to PLA and PVF, comprising all 18 impact categories of the ReciPe method (CC: climate change, OD: ozone depletion, TA: terrestrial acidification, FE: freshwater eutrophication, ME: marine eutrophication, HTOX: human toxicity, POF: photochemical oxidant formation, PMF: particulate matter formation, TTOX: terrestrial ecotoxicity, FTOX: freshwater ecotoxicity, MTOX: marine ecotoxicity, IR: ionizing radiation, ALO: agricultural land occupation, ULO: urban land occupation, NLT: natural land transformation, WAT: water depletion, MET: metal depletion, and FOS: fossil depletion). b) LCA results comparison based on cumulative energy demand impact. c) LCA results comparison based on climate change impact. d) LCA results comparison based on marine ecotoxicity. e) LCA results comparison based on water depletion impact. \n  \n19   3. Conclusion In this work we introduced amyloid fibrils as a suitable building block for developing hybrid bioplastics. To this end, in situ fibrillization of whey monomer, selected here as model protein from food processing waste, took place in the presence of a plasticizer and a biodegradable plastic such as PVA and MC. The resultant films were transparent, robust, flexible, tough and exhibited acceptable water stabilities, as well as good barrier properties for food packaging applications. The films can be prepared from low-priced bio-based and biodegradable sources, highlighting their affordability and environment friendliness for a broad range of applications. Accordingly, LCA performed on these new bioplastics and two biodegradable polymers as benchmarks revealed a superior performance of the present bioplastics in all the normalized environmental impact indicators. Additionally, this new class of bioplastic adds value to circular economy by valorizing whey as a by-product of the dairy industry. These results demonstrated the potential of hybrid whey amyloid fibrils bioplastic as an efficient, sustainable, and inexpensive solution for alleviating the global plastics production and pollution issue.  4. Experimental Section/Methods  Materials: Whey protein isolate (WPI) was supplied from Fonterra, New Zealand. Polyvinyl alcohol (PVA, fully hydrolyzed, Mw approx. 200000) and Hydrochloric acid (36%) were purchased from Merck. Methyl cellulose (MC) (viscosity: 400 cP), Glycerol (≥99.5%), Citric acid (CA) (≥99.5%), 1H,1H,2H,2H-Perfluorooctyltriethoxysilane (FAS), 2,2′-azinobis(ethyl-2,3-dihydrobenzothiazoline-6-sulfonic acid) diammonium salt (ABTS), and Tenax® porous polymer adsorbent (60-80 mesh) were provided from Sigma Aldrich. Bioplastic film formation: For fabricating hybrid amyloid fibril films, 4 g of WPI first were dissolved in 100 ml of water. Then the pH of the solution was adjusted to 2 and 3 g of Glycerol, as a plasticizer, were dispersed in the solution. For hybrid amyloid fibril films with PVA and MC, 3 g and 2 g of each compound, respectively were added to the solution. To convert the   \n20  WPI monomers to amyloid fibrils and dissolving the biodegradable polymer, the solution was stirred and heated at 90 °C for 5 h. After the incubation, the solution was quenched and cast immediately on a petri dish to dry at room temperature. For the study of the effect of CA in film properties, 1.5 g were added to solutions before the fibrilization process. Moreover, for fabricating more hydrophobic films, they were placed in an ethanol solution containing FAS (0.5 wt %) for 1 h. Subsequently, the films were dried under room temperature to obtain the FAS-coated hybrid amyloid films. The details of composition and treatment are listed in Table 3. Table 3. Bioplastic Films composition Bioplastics Whey amyloid Whey monomer PVA MC Gly FAS treatment Hybrid amyloid 40 wt.% - 30 wt.% - 30 wt.% No Hybrid monomer - 40 wt.% 30 wt.% - 30 wt.% No Hybrid amyloid-FAS 40 wt.% - 30 wt.% - 30 wt.% Yes Hybrid amyloid-CA 40 wt.% - 30 wt.% - 30 wt.% No Hybrid amyloid-MC 45 wt.% - - 23 wt.% 32 wt.% No  Characterization Atomic force microscopy: Atomic force microscopy (AFM) and scanning electron microscopy (SEM) were used to characterize the morphologies of amyloid fibrils and their hybrid films, respectively. For AFM, the solutions were dried onto cleaved mica and analyzed by applying the tapping mode. A Hitachi SU5000 scanning electron microscope characterized the structure and properties of hybrid bioplastic films. Small pieces of films were attached to stubs with paste and sputter-coated with 5 nm of platinum/palladium under planetary rotational movement (Safematic, CCU- 10, Switzerland) before imaging.  Mechanical properties: The mechanical properties of films were evaluated by measuring tensile strength and elongation using a Z010 (Zwick) equipped with a 100 N load cell. The stress (σ)-  \n21  strain (ε) curves were obtained at room temperature. The Young's modulus was calculated from the stress-strain curves.  Water contact angle: The water contact angle of the films was recorded by Nikon D300 digital camera at 25 ֯C and relative humidity of 50%. To shed light on the interaction of films with water, their weight loss, and water absorption after immersing in the water and at different time intervals were measured.  Water absorption: For the water absorption evaluation of films, approximately 150 mg of each film were immersed in water for 1 h, and the weight changes due to water absorption was measured before and after immersion.  Antioxidant activity: The antioxidant activity of film was determined by the spectrophotometric method described by Kusznierewicz et al.[54] Briefly, the stock solution of ABTS with the concentration of 7 mM was diluted with water to display the absorbance of 0.7 at 734 nm. Then 4.5 mL of ABTS solution was combined with a piece of film (10 mg). After 20 min reaction time, the film was removed, and the solution was transferred to a cuvette, and its absorbance was measured at 734 nm with the use of a UV−vis spectrophotometer (Cary 100, Agilent Technology). Finally, the amount of ABTS radicals scavenged by 1 g of the film was calculated based on the Beer-Lambert–Bouguer Law (equation 1): 𝑆𝑐!\"#$=%!\"#$×(!%(!&)×*+++,×-×.        (1) where, ScABTS is the amount of scavenged ABTS (μmol), VABTS is the volume of stock solution of ABTS added to the film (mL), A0 is the absorbance of the initial ABTS solution; Af is the absorbance of the radical solution after reaction time; ε is the ABTS molar extinction coefficient (16,000 M−1 cm−1 at 734 nm), l is the optical path of the cuvette (1 cm) and m is the film mass (g) [54]. Food-contact migration: The food-contact migration properties of the bioplastic films were evaluated based on EU technical guidelines for compliance testing in the framework of the   \n22  plastic FCM Regulation (EU) No 10/2011[55]. To assess the possible migration of molecules from the films to the food, Tenax® was used as a dry food simulant. In a clean glass petri dish, a square-shaped film with a dimension of 1 cm was placed between two layers of Tenax® powder (15 mg below and 15 mg above the sample) and stored in the oven for 2 h at 70 °C. The overall food migration was calculated by the mass difference of Tenax® before and after the treatment. Water vapor permeability: The water vapor permeability (WVP) of the different hybrid films has been characterized using the modified cup method[56]. Briefly, glass vials with an inner diameter of 1.5 cm and a height of 6 cm were filled with 15 mL of water. The films were mounted on top of the vial using Parafilm to block air leakages. The weights of the vials have been measured after 1, 2, 4, 8, and 24 hours and the WVP estimated for each time step using the following formula: WVP\t(g·..'/01\t30)\t=4·6!·7·∆3        (2) Where W (g) is the weight decrease, t (m) is the film thickness, A (m2) is the film surface area exposed to air, T (day) is the time, and ∆P is the difference in water vapor pressure between the inside and the outside of the vial (assumed 3173 Pa).  Life cycle assessment: The environmental impact of HAm-PVA bioplastic was compared via life cycle assessment (LCA) with one typical plastic film Polyvinyl fluoride (PVF), and one common bioplastic Polylactic acid (PLA). The LCA was an attributional and prospective LCA of an emerging product and performed according to the protocol of (ISO) 14040/44 standard [57,58]. The LCA assesses cradle-to-use life cycle impacts of producing 1 kg of these plastics. The Life cycle inventory (LCI) for all plastics is summarized in Table S1. For HAm-PVA bioplastic, the process data was provided by our laboratory experiments. For PLA, the inventory data for Ingeo® polylactide production technology from corn was used based on the LCA assessed by Suwanmanee et al[59]. Life cycle models were built using SimaPro v. 8.3 and based   \n23  on Ecoinvent 3 database. ReCiPe midpoint (H) method was used to evaluate the impact of the LCI over a broad range of impact categories, 18 in this case. Additionally, cumulative energy demand (CED) was used for energy use calculations.  Notes M.P and M.B. contributed equally to this work.   References [1] R. Geyer, J. R. Jambeck, K. L. Law, Production, Use, and Fate of All Plastics Ever Made, 2017. [2] J. M. Garcia, M. L. Robertson, Science (80-. ). 2017, 358, 870. [3] A. L. Brooks, S. Wang, J. R. Jambeck, Sci. Adv. 2018, 4, eaat0131. [4] D. K. A. Barnes, F. Galgani, R. C. Thompson, M. Barlaz, Philos. Trans. R. Soc. B Biol. Sci. 2009, 364, 1985. [5] J. Zalasiewicz, C. N. Waters, J. A. Ivar do Sul, P. L. Corcoran, A. D. Barnosky, A. Cearreta, M. Edgeworth, A. Gałuszka, C. Jeandel, R. Leinfelder, J. R. McNeill, W. Steffen, C. Summerhayes, M. Wagreich, M. Williams, A. P. Wolfe, Y. Yonan, Anthropocene 2016, 13, 4. [6] J. X. Chan, J. F. Wong, A. Hassan, Z. Zakaria “8 - Bioplastics from agricultural waste, Biopolymers and Biocomposites from Agro-Waste for Packaging Applications 2021, 141-169.  [7] A. Iles, A. N. Martin, J. Clean. Prod. 2013, 45, 38. [8] Ezgi Bezirhan Arikan, Havva Duygu Ozsoy, J. Civ. Eng. Archit. 2015, 9, 188. [9] P. Jariyasakoolroj, P. Leelaphiwat, N. Harnkarnsujarit, J. Sci. Food Agric. 2020, 100, 5032. [10] I. W. Sutherland, Bioplastic and Biopolymer Production, 2007. [11] ASTM C778, ASTM, Annu. B. ASTM Stand. 2009, 3.   \n24  [12] I. Arvanitoyannis, E. Psomiadou, A. Nakayama, S. Aiba, N. Yamamoto, Food Chem. 1997, 60, 593. [13] S. Guilbert, B. Cuq, N. Gontard, Food Addit. Contam. 1997, 14, 741. [14] A. Jerez, P. Partal, I. Martínez, C. Gallegos, A. Guerrero, Rheol. Acta 2007, 46, 711. [15] M. Pommet, A. Redl, M. H. Morel, S. Domenek, S. Guilbert, Macromol. Symp. 2003, 197, 207. [16] K. Kopf-Bolanz, W. Bisig, N. Jungbluth, C. Denkel, “Potential of whey as a food constituent in Switzerland,” 2015. [17] J. Nikodinovic-Runic, M. Guzik, S. T. Kenny, R. Babu, A. Werker, K. E. O’Connor, in Adv. Appl. Microbiol., Academic Press Inc., 2013, pp. 139–200. [18] A. R. Prazeres, F. Carvalho, J. Rivas, J. Environ. Manage. 2012, 110, 48. [19] T. M. M. M. Amaro, D. Rosa, G. Comi, L. Iacumin, Front. Microbiol. 2019, 10, DOI 10.3389/fmicb.2019.00992. [20] J. Chandrapala, M. C. Duke, S. R. Gray, M. Weeks, M. Palmer, T. Vasiljevic, Sep. Purif. Technol. 2016, 160, 18. [21] G. Kontopidis, C. Holt, L. Sawyer, J. Dairy Sci. 2004, 87, 785. [22] D. Rocha-Mendoza, E. Kosmerl, A. Krentz, L. Zhang, S. Badiger, G. Miyagusuku-Cruzado, A. Mayta-Apaza, M. Giusti, R. Jiménez-Flores, I. García-Cano, J. Dairy Sci. 2021, 104, 1262. [23] C. Lara, J. Adamcik, S. Jordens, R. Mezzenga, Biomacromolecules 2011, 12, 1868. [24] R. Mezzenga, P. Fischer, Reports Prog. Phys. 2013, 76, DOI 10.1088/0034-4885/76/4/046601. [25] J. Adamcik, R. Mezzenga, Macromolecules 2012, 45, 1137. [26] Y. Cao, R. Mezzenga, Adv. Colloid Interface Sci. 2019, 269, 334. [27] T. P. J. Knowles, R. Mezzenga, Adv. Mater. 2016, 28, 6546. [28] Y. Shen, L. Posavec, S. Bolisetty, F. M. Hilty, G. Nyström, J. Kohlbrecher, M. Hilbe,   \n25  A. Rossi, J. Baumgartner, M. B. Zimmermann, R. Mezzenga, Nat. Nanotechnol. 2017, 12, 642. [29] S. Bolisetty, R. Mezzenga, Nat. Nanotechnol. 2016, 11, 365. [30] C. Akkermans, A. J. Van Der Goot, P. Venema, H. Gruppen, J. M. Vereijken, E. Van Der Linden, R. M. Boom, J. Agric. Food Chem. 2007, 55, 9877. [31] G. M. Kavanagh, A. H. Clark, S. B. Ross-Murphy, Int. J. Biol. Macromol. 2000, 28, 41. [32] Y. Cao, R. Mezzenga, Adv. Colloid Interface Sci. 2019, 269, 334. [33] G. Nyström, M. P. Fernández-Ronco, S. Bolisetty, M. Mazzotti, R. Mezzenga, Adv. Mater. 2016, 28, 472. [34] M. Bagnani, G. Nyström, C. De Michele, R. Mezzenga, ACS Nano 2019, 13, 591. [35] S. F. Sabato, B. Ouattara, H. Yu, G. D’aprano, C. Le Tien, M. A. Mateescu, M. Lacroix, 2001, DOI 10.1021/jf0005925. [36] P. Cinelli, M. Schmid, E. Bugnicourt, J. Wildner, A. Bazzichi, I. Anguillesi, A. Lazzeri, Polym. Degrad. Stab. 2014, 108, 151. [37] R. Sothornvit, J. M. Krochta, J. Agric. Food Chem. 2000, 48, 3913. [38] F. Bilo, S. Pandini, L. Sartore, L. E. Depero, G. Gargiulo, A. Bonassi, S. Federici, E. Bontempi, J. Clean. Prod. 2018, 200, 357. [39] M. Liu, M. Arshadi, F. Javi, P. Lawrence, S. M. Davachi, A. Abbaspourrad, J. Clean. Prod. 2020, 276, 123353. [40] G. Trujillo-de Santiago, C. Rojas-de Gante, S. García-Lara, L. Verdolotti, E. Di Maio, S. Iannace, J. Polym. Environ. 2015, 23, 72. [41] F. Zhang, X. You, H. Dou, Z. Liu, B. Zuo, X. Zhang, ACS Appl. Mater. Interfaces 2015, 7, 3352. [42] K. Wilpiszewska, A. K. Antosik, M. Zdanowicz, J. Polym. Environ. 2019, 27, 1379. [43] A. Rahma Khoirunnisa, I. Made Joni, C. Panatarani, E. Rochima, D. Praseptiangga,   \n26  1927, 30041. [44] B. S. Berlett, E. R. Stadtman, J. Biol. Chem. 1997, 272, 20313. [45] F. Luzi, L. Torre, J. Kenny, D. Puglia, Materials (Basel). 2019, 12, 471. [46] R. Sindhu, B. Ammu, P. Binod, S. K. Deepthi, K. B. Ramachandran, C. R. Soccol, A. Pandey, Brazilian Arch. Biol. Technol. 2011, 54, 783. [47] T. Ghosh Dastidar, A. Netravali, ACS Sustain. Chem. Eng. 2013, 1, 1537. [48] A. M. Shi, L. J. Wang, D. Li, B. Adhikari, Carbohydr. Polym. 2013, 96, 593. [49] J. Chee Chuan Yeo, J. K. Muiruri, B. Hoon Tan, W. Thitsartarn, J. Kong, X. Zhang, Z. Li, C. He, 2018, DOI 10.1021/acssuschemeng.8b03978. [50] C. Thellen, M. Coyne, D. Froio, M. Auerbach, C. Wirsen, J. A. Ratto, J. Polym. Environ. 2008, 16, 1. [51] P. Wei, J. Huang, Y. Lu, Y. Zhong, Y. Men, L. Zhang, J. Cai, ACS Sustain. Chem. Eng. 2019, 7, 1707. [52] Q. Li, S. McGinnis, C. Sydnor, A. Wong, S. Renneckar, ACS Sustain. Chem. Eng. 2013, 1, 919. [53] E. Mendenhall, Mar. Policy 2018, 96, 291. [54] B. Kusznierewicz, H. Staroszczyk, E. Malinowska-Pańczyk, K. Parchem, A. Bartoszek, Food Packag. Shelf Life 2020, 24, 100478. [55] G. Perotto, L. Ceseracciu, R. Simonutti, U. C. Paul, S. Guzman-Puyol, T. N. Tran, I. S. Bayer, A. Athanassiou, Green Chem. 2018, 20, 894. [56] J. Han, S. H. Shin, K. M. Park, K. M. Kim, Food Sci. Biotechnol. 2015, 24, 939. [57] R. Arvidsson, D. Kushnir, B. A. Sandén, S. Molander, Environ. Sci. Technol. 2014, 48, 4529. [58] T. Walser, E. Demou, D. J. Lang, S. Hellweg, Environ. Sci. Technol 2011, 45, 4570. [59] U. Suwanmanee, V. Varabuntoonvit, P. Chaiwutthinan, M. Tajan, T. Mungcharoen, T. Leejarkpai, Int. J. Life Cycle Assess. 2013, 18, 401.   \n27   Supporting Information    Sustainable bioplastics from amyloid fibril-biodegradable polymer blends   Mohammad Peydayesh, Massimo Bagnani, and Raffaele Mezzenga*   Table S1. Life cycle inventory data Life cycle inventory data 1. HAm-PVA Process Inputs    Chemicals Quantity Units SimaPro Process Data base Notes Material Whey 800 g Liquid whey, from cheese production, at plant/NL Economic EcoInvent    Water 800 ml Water, process, surface EcoInvent    PVA 24 g Polyvinylchloride, bulk polymerised {GLO}| market for | Alloc Def, S EcoInvent Replacement for PVA  Glycerol 24 g Glycerin, at biodiesel plant/kg/RNA EcoInvent   HCl 4.4 g Hydrochloric acid, Mannheim process (30% HCl), at plant/RER Economic EcoInvent           Equipment Quantity Units SimaPro Process Data base Notes Electricity Heating and stirring plate 0.966 kW.h Electricity, high voltage {ASCC}| market for | Alloc Def, S EcoInvent  Process Outputs          Chemicals Quantity Units SimaPro Process Data base Notes   HAm-PVA 168 g N/A N/A          2. PLA Process Inputs [1]   Chemicals Quantity Units SimaPro Process Data base Notes Material Maize grain 1.54 kg Maize grain {GLO}| market for | Alloc Def, S EcoInvent    \n28             Equipment Quantity Units SimaPro Process Data base Notes Electricity Corn plantation, Diesel and chemicals 6.4 MJ Energy, from diesel burned in machinery/RER Economic EcoInvent    PLA pellet production 10.6 kW.h Electricity, high voltage {GLO}| market group for | Alloc Def, S EcoInvent           Process Outputs           Chemicals Quantity Units SimaPro Process Data base Notes   PLA 1 kg N/A N/A          3. PVF Process Inputs          Chemicals Quantity Units SimaPro Process Data base Notes Material PVF 1 kg Polyvinylfluoride {GLO}| market for | Alloc Rec, S EcoInvent            Process Outputs    Chemicals Quantity Units SimaPro Process Data base Notes   PVF 1 kg Polyvinylfluoride {GLO}| market for | Alloc Rec, S EcoInvent        \n29   Figure S1. The images show photographs of water droplets standing on the different films.   \n  \n30   Figure S2. Young's modulus of the different films.   \n  \n31  SI references [1] U. Suwanmanee, V. Varabuntoonvit, P. Chaiwutthinan, M. Tajan, T. Mungcharoen, T. Leejarkpai, Int. J. Life Cycle Assess. 2013, 18, 401.  ",
      "metadata": {
        "filename": "Sustainable bioplastics from amyloid fibril-biodegradable polymer blends.pdf",
        "hotspot_name": "PCBA_Production",
        "title": "Sustainable bioplastics from amyloid fibril-biodegradable polymer blends",
        "published_date": "2021-05-29T12:49:57Z",
        "pdf_link": "http://arxiv.org/pdf/2105.14287v1",
        "query": "PCB assembly sustainability environmental assessment"
      }
    },
    "475_C aging embrittlement of partially recrystallized FeCrAl ODS ferritic steels": {
      "full_text": "475 °C aging embrittlement of partially recrystallized FeCrAl ODS ferritic\nsteels after simulated tube process\nZhexian Zhanga,1,∗, Daniel Morrallb,1, Kiyohiro Yabuuchia\naInstitute of Advanced Energy, Kyoto University, Gokasho, Uji, 611-0011, Kyoto, Japan\nbGraduate School of Energy Science, Kyoto University, Yoshida-honmachi, Sakyo-ku, Kyoto, 606-8501, Kyoto, Japan\nAbstract\nTube processing and aging effects in FeCrAl ODS steels are investigated in four mechanical alloyed ferritic\nODS steels, Fe15Cr (SP2), Fe15Cr5Al (SP4), Fe15Cr7Al (SP7) and Fe18Cr7Al (SP11). These steels were\nmade into 0.3mm thick plates by simulated tube processing (STP). Strengthening after partial recrystal-\nlization was achieved after the last cold rolling and heat treatment step. However, the ductility reduced\nabout one third of the as-extruded steels. The STPed steels were aged at 475 °C in sealed vacuum tubes up\nto 2000 hrs and 10000 hrs, respectively. The yield stress and elongation were investigated by tensile tests.\nThe results revealed that all the STPed steels fractured in a ductile manner irrespective of aging conditions.\nAging hardening and ductility reduction in STPed steels are similar to as-extruded ones. The STPed ODS\nsteels showed similar ageing embrittlement resistance as as-extruded steels, but much higher than the non-\nODS steels. The aging hardening based on cut-through and bow-pass mechanisms were discussed. The time\ndependent hardening of overaged steel ( β′only) was analyzed as well.\nKeywords:\nFeCrAl ODS steel, ATF cladding, simulated tube processing, aging embrittlement, aging hardening\n1. Introduction\nFeCrAl ferritic alloys are considered as promising\ncandidate materials for accident tolerant fuel (ATF)\ncladding in the designs for suppressing hydrogen\ngeneration reactions with hot water in light water\nreactors (LWR) at extreme high temperatures[1–\n3]. However, the neutron penalty caused by larger\nabsorption section of Fe atoms, which will reduce\nthe neutron efficiency in comparison to conven-\ntional zircaloy system, has to be relieved by re-\nducing the FeCrAl cladding wall thickness and/or\nenhancing the uranium enrichment of the nuclear\nfuel. For example, the thickness of iron-based alloy\ncladdings was calculated to be limited to 300 µm\n∗corresponding author\nEmail address: zzhan124@utk.edu (Zhexian Zhang)\n1This work was done by Zhexian Zhang, and Daniel Mor-\nrall when they were staff of Institute of Advanced Energy,\nKyoto University and student of Graduate School of En-\nergy Science, Kyoto University, respectively. Zhexian Zhang\nis now a visiting researcher in the University of Tennessee\nKnoxville.with 4.78% U235 fuel enrichment to match the same\ncycle length of Zircaloy without changing other fuel\npin geometries[4]. In general, reducing the cladding\ntube thickness is preferred than enhancing uranium\nenrichment upon the technical feasibility and econ-\nomy preference[5].\nNevertheless, the structural integrity of claddings\nrequires a minimum thickness to meet the strength\ndemand for safety consideration. To compen-\nsate for the strength loss by the thickness reduc-\ntion of cladding wall and to enhance the strength\nat elevated temperatures, the strategy of oxide\ndispersion strengthening (ODS) was adopted in\nthe development of FeCrAl ATF cladding mate-\nrials. Based on this strategy, several Japan na-\ntional programs of FeCrAl ODS ferritic steels R&\nD were conducted[6, 7]. These programs are on\nthe bases of the knowledges accumulated by the\nJapanese leading programs of R& D of ODS ferritic\nmartensitic steels with[8][9, 10] and without[11] Al-\naddition for the applications to core components\nof Gen IV fast reactors. After decades of inves-\ntigations, the FeCrAl ODS ferritic steels devel-\nPreprint submitted to arXiv November 6, 2023arXiv:2310.13842v2  [cond-mat.mtrl-sci]  3 Nov 2023oped in Japan have demonstrated excellent prop-\nerties, particularly the creep resistance[12–14], ox-\nidation/corrosion resistances[15–19] and radiation\ntolerance[20–23].\nHowever, the increased ultra-high strength of Fe-\nCrAl ODS steels also brought about difficulty in the\nfabrication processes such as cold rolling and pil-\nger milling[24, 25]. This concern was proposed to\nbe overcome by recrystallization treatment, which\nreduces the yield strength of ODS steels during\ntubing[26]. According to the previous study on\nFeCr(Al) ODS ferritic steels[27], the typical recrys-\ntallization temperatures after hot extrusion ranges\nfrom 1050 °C to 1400 °C, depending on the alloy\ncompositions and pre-mechanical processing (forge,\nrolling, etc). The on-set recrystallization tempera-\nture of FeCrAl ODS ferritic steels could be signifi-\ncantly reduced by the degree of cold work[26]. The\nreduction of yield stress could reach to 25% 50%\nafter fully recrystallization, while the property of\ntotal tensile elongation would barely change[28].\nIn the tubing process, a four-cycle cold-rolling\n(CR) and heat-treatment (HT) process was first de-\nsigned in the R& D of Fe9Cr ODS alloy[29, 30].\nLater, this processing route was applied to general\ntubing of FeCrAl ODS steels[7][31]. The thickness\nreduction was about 50% in each CR step. How-\never, the temperatures of the HT vary in interme-\ndiate and last steps. The intermediate HT temper-\natures were designed to be a little below the recrys-\ntallization temperature to induce only recovery to\navoid cracking in further rolling[32]. The recrystal-\nlization was only designed in the last step to pro-\nduce equiaxed grains (however, most of the grains\nstill elongated to the rolling direction, but generally\nthe anisotropy was greatly reduced) and reduce the\noverall yielding strength. Although recrystalliza-\ntion can effectively reduce the yielding strength, it\nhas been found that repeating recrystallization will\nbe retard by the previous recrystallization[32–34].\nThis is the reason that recrystallization was only\ndesigned in the last step in tubing processing.\nDuring the service of FeCrAl tube claddings in\nreactors, the steels may suffer aging embrittlement\nunder the operation temperatures. This aging ef-\nfect is owing to the formation of various fine pre-\ncipitates which cause the deterioration of mechan-\nical properties. According to the recent publica-\ntions based on atom probe tomography (APT), age-\nhardening could be brought about by both Cr-rich\nα′phases[35–40] andor (Al, Ti)-rich β′phases[41–\n44] depending on the concentration of Cr and Al inthe FeCrAl ODS steels. As for the effect of Al addi-\ntion on age-hardening, Kobayashi and Takasugi[45]\ninvestigated the age-hardening of diffused FeCrAl\nalloys with multiple concentrations and showed\nthat the addition of Al shifted the Fe-Cr miscibil-\nity boundary to a higher Cr concentration, conse-\nquently hinders the α-α′phase separation. Particu-\nlarly in the study by Dou et al, when Al is >7wt%,\nnoα′phase was formed in Fe15Cr-(7,9)Al ODS\nsteels[43], while the Al additions will enhance the\nformation of Ti-Al enriched β′precipitates caus-\ning age-hardening without the occurrence of α-α′\nphase separation[41]. Sang et al[42] reported that\ntheβ′could form in early stage during thermal ag-\ning in high Al ODS steels. The age-hardening of\nas-extruded FeCrAl ODS steels has been investi-\ngated at 475 °C up to 9000 hrs[46], while the aged\nFeCrAl ODS steels after tubing process have been\ninvestigated by Yano et al[31]. The aging of high-Al\nFeCrAl ODS steels were also investigated by Maji\net al[47] where again no α′but Al enriched precipi-\ntates (mainly FeAl and Fe 3Al) were formed in these\nsteels.\nThe solution to the aging embrittlement is to\noptimize the composition of FeCrAl steels. The\nconcentrations of Cr and Al are required to sat-\nisfy both the oxidation resistance at extreme high\ntemperature, and low aging rate at reactor oper-\nation temperatures. To this end, the “SP” series\nof FeCrAl ODS steels were designed and produced,\nwith the Cr between 12 and 18wt%, and the Al of\n0, 5 9wt%. In this range of Cr concentration, α′\nwill form through a special nucleation and growth\nway (where the Cr concentration in precipitates and\nsize increase simultaneously)[35], while Al will hin-\nder this process[48]. These researches focusing on\nthe SP-series FeCrAl steels have been intensively\ncarried out and summarized in a recent review[49].\nThis study belongs to the bunch of the research\nwork on the “SP” series FeCrAl ODS steels. The\nobjective is to investigate the aging behavior of the\nselected FeCrAl steels after simulated tube fabri-\ncation processing (STP). Different to conventional\ndesign with recrystallization only in the last HT\nof tubing process, we tentatively designed the full\nrecrystallization in the 3rd HT and partially recrys-\ntallization in the last HT of the STP. This design\naims to reduce the rolling difficulty in the 4th CR\nand increase the final strength of the steels than\nconventional fully recrystallized FeCrAl tube. The\nSTPed steels were thermal aged at 475 °C up to 2000\nhrs and 10000 hrs. The mechanical properties were\n2Figure 1: The workflow of experimental procedure includes\nsimulated tube processing and thermal ageing together with\neach measurement and observation.\ninvestigated by means of uniaxial tensile test. A\ncomparison of the aging hardening behavior was\nmade between the as-extruded and STPed FeCrAl\nODS steels. The hardening rates were analyzed by\ncombination of a precipitation kinetics model and\ntwo-fold hardening mechanism. The (Cr, Al) con-\ncentration dependence on hardening was discussed\nas well.\n2. Experiments\n2.1. Materials\nThe starting materials used in this study are Fe-\nCrAl ODS ferritic steels (SP series) produced by\nKOBELCO, Ltd. The chemical compositions are\nshown in Table 1. There are four steels with differ-\nent Al and Cr concentration, which are named as\nFe15Cr (SP2), Fe15Cr5Al (SP4), Fe15Cr7Al (SP7)\nand Fe18Cr7Al (SP11). The Fe15Cr (SP2) steel\ncontains no Al but 2wt% W. All the steels were\nproduced by mechanical alloying method starting\nwith Ar-gas-atomized alloy powders, element pow-\nders and Y 2O3powders. The mechanically alloyed\npowders were encapsuled into a can to degas at\n400 °C in a vacuum of 0.1 Pa for 2 hrs and extruded\nat 1150 °C into a rod with 25 mm diameter. Therod was finally annealed at 1150 °C for 1 hr and\nsubjected to air cooling.\nThe workflow of the STP and aging experiments\nare illustrated in Figure 1. The simulated tube fab-\nrication process was started with the as-received\nODS bars which were cut from the extruded rod.\nThe bars were subjected to cold rolling and ther-\nmally annealing followed by furnace cooling at a\nrate of ∽150 Khr in average to achieve a full ferrite\nphase. The CR-HT was repeated four times. The\nrolling process was illustrated in Figure 2a. The\nrolling was performed at room temperature, with\neach single rolling pass yielding ∽1 to 2% thickness\nreduction. The rolling direction (RD) was parallel\nto the extrusion direction without being reversed.\nAs the specimens were very hard, cracks were eas-\nily generated at the head of the specimen during\nthe rolling. In this case, we cut off these cracked\nparts to prevent it from growing deeper in follow-\ning rolling. The thickness of specimens was ap-\nproximately 50% reduced in each rolling cycle, with\nsubsequent annealing treatment at 950 °C, 850 °C,\n1150 °C and 1150 °C for 1 hr, respectively. The total\nthickness reduction ηand strain ϵare defined as:\nη= 1−h/h0,η=ln(h0/h). The initial sample\nthickness, h 0, was 4 mm. The parameters of h, η\nandϵin cold rolling and subsequent annealing tem-\nperatures were displayed in Table 2. Tensile speci-\nmens of dog-bone shape were produced from STPed\nplates of 0.3 mm thickness. Isothermal aging was\nperformed on the dog-bone tensile specimens sealed\nin vacuum capsules at 475 °C for 2000 hrs and 10000\nhrs followed by iced water quenching.\n2.2. Microstructural observations\nTransmission electron microscopy (TEM) was\nperformed on JEOL 2200 field emission TEM to\ninvestigate the microstructures of grain and oxide\nmorphology. The foils for TEM observation were\nthinned by focused ion beam (FIB, Hitachi FB2200)\nto 250 nm, followed by flash polishing in a mixture\nof 5% HClO 4and 95% CH 3OH at a voltage of 30\nV and temperatures between -30 °C and -65 °C.\nElectron Back Scatter Diffraction (EBSD) was\napplied to investigate the grain morphology via an\nEDAX detector equipped on Zeiss Ultra-55 field-\nemission scanning electron microscopy (FE-SEM)\nwith acceleration voltage of 10 to 15eV. The over-\nall grain morphology was scanned on an area of\n150µmx 150µmwith a step of 1 µmvia con-\nventional EBSD to identify recrystallization occur-\nrence. Since grains in ODS steels are generally sub-\n3Figure 2: a) Schematic view of the cold rolling in the simulated tube processing, b) IPF of the grain morphologies of four\nFeCrAl ODS ferritic steels rolling surface after each cold rolling and heat treatment (CR-HT) cycle, and c) the transmitted\nEBSD (TKD) image of cross-section of the fine subgrains of Fe15Cr after ageing for 2000 hr. The upward of conventional\nEBSD IPF is parallel to the rolling direction. The TKD specimen is normal to the rolling direction.\nTable 1: Chemical compositions of FeCrAl ODS ferritic steels (wt%, Bal. Fe)\nID Cr Al W Ti Y C O N Ar\nFe15Cr SP2 14.24 <0.01 1.85 0.23 0.18 0.028 0.12 0.005 0.006\nFe15Cr5Al SP4 14.39 4.65 - 0.32 0.39 0.032 0.22 0.005 0.006\nFe15Cr7Al SP7 14.13 6.42 - 0.51 0.38 0.032 0.22 0.005 0.006\nFe18Cr7Al SP11 16.83 6.31 - 0.49 0.38 0.032 0.22 0.004 0.006\nmicron size with small misorientations, the Trans-\nmission Kikuchi Diffraction (TKD)[50] was per-\nformed as well with a step size of 50 nm to show\nthe mixture of fine grains and recrystallized grains\nafter partially recrystallization.\n2.3. Mechanical tests\nMicro-Vickers hardness was tested by HMV-2T\n(Shimadzu Corp.) with 2 kg load and 10 sec hold-\ning time at room temperature. The hardness was\nmeasured on the rolling surface of the plates after\neach cycle of CR-HT.\nTensile tests were performed on INTESCO 205X\ntensile assembly with a load cell of 5 kN. The dog-\nborn miniaturized tensile specimens were sampled\nfrom the STPed plates with the loading direction\nparallel to the rolling direction. The gage geometry\nof the tensile specimen was 5 mm in length, 1.2 mm\nin width and 0.3 mm in thickness. The tests were\ncarried out at a displacement rate of 0.2 mm/min,resulting in an initial strain rate of 6.67 ×10-4/s.\nThe yield stress (YS) was defined as 0.2% off-set\nflow stress. Two or three specimens were tested\nat each aging condition. All the tensile tests were\nperformed at room temperature.\n3. Results\n3.1. Simulated tubing processing\nAccording to our previous research[28], the grain\nmorphology of the FeCrAl ODS steels after hot ex-\ntrusion have a strong α-fiber texture (RD <110>)\nstructure parallel to the extrusion direction, and\nhave fine and isotropic grain shape on the cross-\nsectional surface. It was also shown that the fibers\ncontained very fine sub-grains with small crystal\nmisorientations. The hardness of the as-extruded\nsteels are available in our previous publication[46].\nThe evolution of grain morphology after each\ncold rolling and heat treatment (CR-HT) is shown\n4Figure 3: The dispersion morphology of nano-particles in FeCrAl ODS ferritic steels at the conditions of as-extruded, simulated\ntubing processing (STP) and 475 °C 2000hrs aging. The wight arrows indicate oxide particles. The grain morphology of STPed\nFe15Cr was shown as well. There are both elongated and equiaxed grains, indicating partially recrystallization in this material.\nTable 2: The parameters of simulated tubing process\nThickness, h (mm) Total thickness reduction, ηTotal strain, εAnnealing temperature, T ( °C)\nInitial 4 0 0 -\nCR-HT1 2 0.5 0.69 950\nCR-HT2 1 0.75 1.39 850\nCR-HT3 0.6 0.85 1.90 1150\nCR-HT4 0.3 0.925 2.59 1150\n5Figure 4: The Vickers hardness of FeCrAl ODS ferritic steels\nduring simulated tubing processing after each cold rolling\nand heat treatment (CR-HT) cycle. All the hardness were\nmeasured on the rolling surface.\nin Fig. 2(b). After the first cycle (CR-HT1),\nFe15Cr7Al (SP7) and Fe18Cr7Al (SP11) were sub-\njected to recrystallization. It is obvious that\nFe15Cr5Al (SP4) recrystallized after the 2nd cy-\ncle of CR-HT. In Fe15Cr (SP2), which is an Al-free\nferritic steel, the recrystallization didn’t occur until\nthe 3rd cycle of CR-HT. After the fourth cycle of\nCR-HT, there were still fine grains remaining, in-\ndicating that the grains were only partially recrys-\ntallized. Fig.2(c) is a cross-section image (normal\nto RD) of Fe15Cr aged for 2000 hrs. As the grain\nboundaries are considered stable at 475 °C, they\ncan represent for the grain sizes after the STP. In\nFig.2(c), both fine grains and coarse grains coex-\nisted in the same specimen. The high thermal sta-\nbility during STP of the grain structure in Fe15Cr\n(SP2) was interpreted in terms of fine oxide parti-\ncle dispersion with a very high number density in\nAl-free ODS steel. These oxides could hinder the\nmovement of grain boundaries, so that elevate the\non-set recrystallization temperature. Fig.3 summa-\nrizes the typical morphology of oxides in Al-free\nand Al-added ferritic ODS steels. In Fe15Cr7Al\nand Fe18Cr7Al, the oxides are Y-Al-O type with\nlarger size but smaller density than Y-Ti-O oxides\nin Fe15Cr. These results correspond to the fact\nthat adding Al will coarsen the size of oxides in\nODS steels during fabrication[51]. The growth of\noxides during STP could be ignored due to short\nannealing time[52].\nThe hardness change by recrystallization reflects\nthe change in grain sizes, ignoring the influence oftextures. Cold rolling will induce subgrain bound-\naries and multiply dislocations, while thermal an-\nnealing will relieve the residual stress and trigger\nnucleation and grain growth if recrystallization oc-\ncurred. The Vickers hardness after each CR-HT\ncycle of each ODS steel is shown in Fig.4. The\nFe15Cr5Al and Fe15Cr7Al showed a similar behav-\nior in Vickers hardness change. Fe15Cr showed a\ndramatic decrease after the CR-HT3. Combined\nwith the grain morphology in Fig. 2(b), Fig.4 in-\ndicates that all the steels heated at 1150 °C will en-\ndure significant recrystallization in HT3. Although\nthe recrystallization was designed to soften material\nfor further thinning, all the specimens in this work\nwere finally hardened after the entire STP proce-\ndure compared to the as-extruded steels.\nThe final hardening in CR-HT4 is because of the\nrepeating recrystallization retarded by intermedi-\nate recrystallization in HT3. According to Leng\net al[34], repeating recrystallization requires higher\non-set temperature and has higher hardness than\nfirst recrystallization at the same annealing temper-\nature. Explanation was made by means of experi-\nments of EBSD orientation density function (ODF)\nanalysis[25, 32, 33, 49] and the theory built on nu-\ncleation driving force and grain boundary migra-\ntion rate[53]. First, recrystallization will produce\n{111}and{110}textures on the rolling plane. Cold\nrolling on the recrystallized {111}<112>crys-\ntals will produce strong {100}<110>texture,\nwhich contains extremely low strain energy that\nthe driving force for recrystallization is very low,\ntoo. Second, the grain boundary migration rate de-\npends on both the misorientation between the re-\ncrystallized nuclei and the matrix[54], and the pin-\nning force of fine oxides. For Al-free steels, the ox-\nides are extremely small so that the pinning force\ncontrols the migration rate. For Al-added steels,\nthe pinning force will be smaller. The nucleus of\n{111}<112>has a misorientation below 45 °on\nrolling surface{112}<110>and{111}<110>\n[25] that they may grow faster during HT4.\nIn practice, cracks easily occurred in the steels\nduring CR2 and CR3, particularly for Fe15Cr,\nwhich has extremely high hardness during CR3.\nThere are no cracks generated in CR4 as recrys-\ntallization in HT3 greatly reduced the yielding\nstrength. The final hardness after HT4 is slightly\nhigher than the as-extruded, which means the steels\nare only partially recrystallized. Through the de-\nsigned STP, the aim to increase the final strength\nthan conventional fully recrystallized steels were\n6Figure 5: The engineering stress-strain curves of STPed Fe15Cr, Fe15Cr5Al, Fe15Cr7Al, Fe18Cr7Al ODS steels before and\nafter ageing at 475 °C to 2000 hrs and 10000 hrs. Note that the elastic component includes the elongation of machine assembly.\nachieved.\n3.2. Tensile properties\nThe engineering stress-strain curves of the STPed\nODS steels were shown in Fig.5 with a presentative\ntest at each aging condition. The age-hardening\nshowed dependence on the content concentration.\nBefore aging, the tensile strength of FeCr ODS is\nreduced by Al-addition, which is due to the reduc-\ntion of density of oxide particles. The yield strength\nincreases with increasing Al and Cr concentration\nbecause of solid solution strengthening[55]. As for\nthe aging effects, age-hardening is much more sig-\nnificant in Al-added ODS steels (SP4, 7, 11) than\nthat in Al-free steel (SP2). This behavior is similar\nto the as-extruded ODS steels in which (Al, Ti)-\nrichβ′-phases (as well as α′-phases) were induced\nby the aging[42–44].\nFig.6 shows the yield stress (YS), ultimate tensile\nstress (UTS), uniform elongation (UE) and total\nelongation (TE) of all the tested specimens. Gen-\nerally, the scattering of YS is larger than that of\nUTS because the ill-defined proof stress could be\neasily affected by the stiffness of tensile machine\nand/or load cell especially for thin specimens. Thescattering of TE in Fig.6c was affected by the de-\nformation after necking. A notable variation in\nds(e)/deoccurred after the UTS, which means that\nthe area of cross section of the specimen shrank\nrapidly after the necking occurrence. There is an\ninconsistency occurred in the yield stress and Vick-\ners hardness of STPed SP2. In Fig.4, the Vick-\ners hardness of STPed SP2 was similar to the as-\nreceived (as-extruded), however, in Fig.6a, the yield\nstress of STPed SP2 is much lower ( 100MPa) than\nas-extruded steel. This might be owing to the\nanisotropy grain morphology in the steels after hot-\nextrusion and STP. As for the aging effect on the\ntensile strength and tensile elongation, all the 2000\nhrs aged FeCrAl ODS steels showed YS hardening\nand TE reduction. The 10000 hrs aging, however,\nresults in “recovery” effect with reduced hardening\nand increased elongation compared to the 2000 hrs\naged ones.\nThe true stress-strain ( σ-ϵ) curves were estimated\nfrom the plastic deformation using the following\nequations:ϵ=ln(eP\ns+ 1),σ(ϵ) =s·(eP\ns+ 1) where\neP\nsis the plastic component of engineering strain\nof specimen, sis the engineering stress, ϵandσ(ϵ)\nstands for the true strain and true stress respec-\ntively. The true stress-strain curve mainly devi-\n7Figure 6: The a) yield strength (YS), b) ultimate tensile strength (UTS), c) total elongation (TE), d) uniform elongation (UE)\nof ODS ferritic steels as-extruded (hollow) and after STP (solid), and after aging for both the specimens as extruded and after\nSTP.\n8Figure 7: Deformation energy ( J·m−3) of as-extruded and STPed steels before and after 475ř Caging.\nFigure 8: The morphology of fracture surface of STPed Fe15Cr, Fe15Cr5Al, Fe15Cr7Al and Fe18Cr7Al before and after 2000hrs\naging.\n9ates 1) after necking where the cross area reduced\ndramatically and the materials experienced three-\ndimensional stress state, and 2) around the initial\nplastic deformation regions where eP\ns/eE\ns≪1. The\nempirical Ludwik relationship was evaluated by the\ndata between 0.5% true strain to true uniform elon-\ngation:\nσ(ϵ) =σ0+Kϵn(1)\nwhereσ0is the true yield stress, Kis the strength\ncoefficient, nis the strain hardening exponent\nThe estimated Kandnvalues from true stress-\nstrain curves are listed in Table 3. Kis the ampli-\ntude of the strain hardening term and nis applied\non the strain ϵdirectly. Note that as the strain\nis smaller than 1, lower nwill lead to higher strain\nhardening ratio. From Table 3 we can conclude that\nthe 2000 hrs aged specimens have the lowest Kand\nncompared to other conditions.\nThe aging embrittlement can be described by the\nreduction of total deformation energy, u DE, in ten-\nsile test by:\nuDE=/integraldisplayUE\n0σdϵ+/integraldisplayTE\nUEσdϵ (2)\nThe first term in equation (2) represents the en-\nergy applied by uniform deformation (UDE) until\ntensile stress reaches the UTS. The second term is\ndefined as the fracture energy (FE) which is ac-\ncompanied by necking. In practice, the integration\nwas calculated by trapezoidal method. In Fig.7,\nthe STPed steels exhibited significant decreases in\nuDEcompared to extruded steels irrespective of Al\nconcentration in each ODS steel. This reduction\nshould be owing to the micro-crack generation dur-\ning cold rolling. The recrystallized layered grains\nmay accelerate the growth of cracks due to weak\ngrain boundary cohesion.\nAs for the effect of aging on the DE, three types of\ntrends were showed: 1) for the Fe15Cr, the STPed\nsteels showed reduction in DE after 2000hrs aging,\nbut recovered at 10000 hrs aging, which was even\nhigher than the non-aged one. This behavior is dif-\nferent to the as-extruded steels, whose DE at 9000\nhrs aging was still lower than non-aged. 2) for the\nFe15Cr7Al, the DE of both STPed and as-extruded\nsteels decreases as aging time increases. 3) for the\nFe15Cr5Al and Fe18Cr7Al, the DE of STPed steels\nreduced at 2000 hrs but increased at 10000 hrs\naging, which are similar to the as-extruded ones.\nThe three types of behavior of DE correspond to\nthe three different modes of precipitates inducedby aging: 1) only α′in Fe15Cr and 2) only β′in\nFe15Cr7Al were generated, but 3) both α′andβ′\nprecipitates formed in Fe15Cr5Al and Fe18Cr7Al,\nwhich will be discussed in Section 4.2.\n3.3. Fractography\nAll the specimens before and after aging frac-\ntured in a ductile manner with plastic shearing in-\nduced dimples on the rupture surfaces. This phe-\nnomenon indicates that the materials still behave\nas ductile after 10000 hrs aging in tensile test at\nambient temperature. This behavior is different\nto the recent research which showed a typical brit-\ntle fracture manner of Fe15Cr7Al ODS steel after\n15000 hrs aging with tensile loaded in the tube hoop\ndirection[31].\nFig.9 shows two characteristic features on the\nfracture surfaces. The first one is large precipi-\ntates located inside the dimples as shown in Fig.9a.\nThese precipitates are enriched in Al according to\nEDS spectrum analysis and can be deduced as alu-\nmina. These particles can function as the initial\nseparation sites because of dislocation pile-ups and\nthe weak adhesion of the interface between the par-\nticle and the matrix. The second one is the long sec-\nondary cracks as shown in Fig. 9b. These cracks are\nformed on the layered and elongated grain bound-\naries, which are parallel to the rolling surface. This\ndelamination is a typical phenomenon in laminated\nODS steels[56]. The bamboo-like grains in the\nSTPed steels will help the propagation of the sec-\nondary cracks[57]. The secondary cracks might be\nthe reason for the elongation reduction compared\nto the as-extruded ODS steels.\n4. Discussion\n4.1. The aging effect on total elongation\nThe fabrication of FeCrAl ODS cladding tubes\nrequires recovery and recrystallization as a stan-\ndard routine. Occurrence of recrystallization de-\npends on the cold rolling degree and annealing tem-\nperatures which determines stored energy, namely,\ndriving force of recrystallization. While Ha[27] re-\nported that a full recrystallization would not cause\nsever loss of elongation in extruded ODS steels, the\nresults from Yano et al[31] and Sakamoto et al[7]\nshowed a rather large ductility loss in pilger rolled\nrecrystallized FeCrAl ODS steels. There is approx-\nimately 30% reduction of total elongation in steels\nafter full tubing routine compared to as-extruded\n10Table 3: The strength coefficient Kand strain hardening opponent nin Ludwik relationship\n0 hr 2000 hrs 10000 hrs\nn K n K n K\nFe15Cr 0 .355±0.024 540.489±66.991 0.313±0.043 469.947±44.958 0.305±0.019 574.742±66.108\nFe15Cr5Al 0 .454±0.069 872.585±78.248 0.341±0.040 559.785±33.790 0.356±0.052 749.861±60.764\nFe15Cr7Al 0 .376±0.010 764.343±53.914 0.345±0.053 615.251±30.515 0.418±0.034 728.033±44.502\nFe18Cr7Al 0 .345±0.052 674.347±27.706 0.266±0.033 434.258±27.903 0.359±0.028 663.538±65.669\nones[31]. In this study, the STPed steels in Fig.6\nshowed a similar trend as those after full tubing\nroutine with a significant loss of elongation. Only\na small loss of elongation after the recrystallization\nin the work by Ha[27] is probably due to no tubing\nprocess put on, suggesting that the loss of elonga-\ntion is closely associated with cold rolling process.\nOne of the supreme behaviors in the plastic defor-\nmation of FeCrAl ODS steels is their smaller loss\nof elongation by thermal aging compared to non-\nODS steels[58]. To illustrate this phenomenon, the\naging-hardening in terms of the change in YS with\nrespect to the loss of elongation of all the steels were\nshown in Fig.10. The Fe15Cr-melt, Fe15CrC-melt\nand Fe15CrXs-melt are non-ODS steels produced\nby arc-melting method. The Fe15Cr and Fe12Cr\nare ODS steels produced by mechanical alloying\nmethod[59]. A commercial SUS430 which contains\n16% Cr without Al was compared as well. It should\nbe noted that the aging effect were quite stable af-\nter approximately 5000 hrs aging according to the\nprevious experiment results[60].\nFig. 10 shows that the loss of elongation by ag-\ning is rather smaller in ODS ferritic steels (red and\ngreen) than in arc-melted (non-ODS) alloys (hol-\nlow black) with respect to the same amount of age-\nhardening. This behavior is consistent with our\nprevious works[58, 60]. As for the comparison be-\ntween as-extruded (red) and STPed (green) condi-\ntions, the loss of elongation in these steels are quite\nsimilar (<0.03). This result indicates that the duc-\ntility loss by aging is less sensitive to the nature of\nODS steels (concentration, grain morphology, pre-\ncipitates, etc).\nThe rupture of ductile materials could be ex-\nplained by micro-voids forming along the center of\nthe necked region. The voids could form around\nprecipitates where dislocation pile up and stress\nconcentration occurs. Thus, non-deformable large\nparticles, such as core-shelled Y-Ti-O in Al-free\nODS steels, and β′and Y-Al-O in Al-added ODSsteels, could act as initial nucleation sites of micro-\nvoids. The theory to evaluate the ductile elongation\ndeveloped by McClintock[61]:\nϵf=(1−n)ln(l0/2b0)\nsinh/bracketleftbig\n(1−n) (σa+σb)//parenleftbig\n2σ/√\n3/parenrightbig/bracketrightbig (3)\nWhereϵfis the strain to fracture, nis the harden-\ning exponent in Ludwik relationship, l0is the mean\nspacing of micro-voids, b0is the initial radius of\nholes,σaandσbare stresses related to different di-\nrection of holes, σis the true flow stress.\nIt shows that the higher spacing of precipitates\n(micro-voids) and the higher strain-hardening ex-\nponent n in equation (1) will yield higher ductility.\nThe latter is true that the lowest n at 2000 hrs\naging in Table 3 also yields the lowest total elonga-\ntion. The former is related to the density of various\nprecipitates. As the total volume fraction of pre-\ncipitates is insensitive to aging period at over-aging\ncondition[48], density of precipitates will decrease\nas the radius increases, thus the fracture elongation\nwill increase after the peak aging hardening.\nThe excellent resistance to aging ductility loss in\nODS steels is owing to the smaller grain size in com-\nparison to the non-ODS steels. As recrystallization\noccurs, the density of grain boundaries and triple\njunctions will be greatly reduced. These defects\nare effective dislocation sources at which the stress\nconcentration will emit dislocations at initial plastic\nstrain. The deformation at vicinity of grain bound-\naries is easier than grain interior due to higher num-\nber of mobile dislocations which might avoid the\npile up at obstacles such as aging precipitates in\ncenter of grains. Therefore, the increased mobile\ndislocations emitted from triple junction and grain\nboundaries will contribute to the deformation of\nspecimen during tensile thus showing enhanced re-\nsistance in elongation reduction by aging.\nHowever, it should be also noted that grain\nboundaries could play a negative role in creep re-\nsistance at elevated temperatures. Under low stress\n11Figure 9: The SEM images of fracture surfaces: a) Al-containing particles in dimples and b) secondary cracks penetrating\nalong elongated grains.\nFigure 10: Relationships between age-hardening and loss of elongation at the conditions of as-extruded (red), after STP (green),\nand arc-melted (non-ODS) alloys (hollow). The ageing temperature was 475 °C for all the specimens and the ageing period\nwas 9000hrs and 10000hrs for as-extruded and STP, respectively.\n12that is less than the dislocation moving in center of\ngrains, the deformation was mainly based on grain\nboundary sliding[62]. Further, the emitted disloca-\ntions from triple junction could enhance the disloca-\ntion climb inside grains. It seems a trade-off existed\nin the grain size effect between aging resistance and\ncreep resistance.\n4.2. The time dependent aging hardening\nAs aforementioned, the aging embrittlement was\nmainly ascribed to the formation of α′andβ′pre-\ncipitates. Fig. ??illustrated the α′precipitation\ndomain in the Fe-Cr-Al ternary phase diagram.\nThe Fe15Cr (SP2) is in the inner region of α′\nsurrounded by the K-T curve. APT works have\ndemonstrated the α′precipitates formed since early\nstages of the aging[42]. The Fe15Cr5Al (SP4) and\nthe Fe18Cr7Al (SP11) are located close to the K-T\ncurve. The APT work showed the precipitates in\nFe15Cr5Al contain both α′andβ′and core-shell\nstructured oxides[43]. Currently there was no APT\nwork for Fe18Cr7Al, but it could be speculated\nthat the precipitates were similar to Fe15Cr5Al.\nThe Fe15Cr7Al (SP7) is located off-shore of the α′\nprecipitation region. The APT work also demon-\nstrated that no α′but a number of β′precipitation\nformed after aging in this steel[43, 44].\nDislocation interaction with precipitates are di-\nvided into two types 1) cutting-through and 2) bow-\npass[63, 64]. In the cutting-through mechanism, the\ncoherent precipitates are deformable. The harden-\ning of cutting-through could be expressed as:\n\n\n∆σchs=M/parenleftbig12\nπ/parenrightbig1\n2γ3\n2s/parenleftig\nf\nGb/parenrightig1\n21\nr\n∆σcohs=Mα(εG)3\n2/parenleftig\n2rf\nGb/parenrightig1\n2\n∆σms= 0.0055M(Gp−G)3\n2/parenleftig\n2f\nG/parenrightig1\n2/parenleftbigr\nb/parenrightbig0.275\n∆σcut−through = ∆σchs+ ∆σcohs+ ∆σms≈∆σms\n(4)\nWhere ∆σchsis the chemical strengthening re-\nlated to the surface energy of precipitates, ∆ σcohs\nis the coherency strengthening, ∆ σmsis the mod-\nulus strengthening, M= 3.06 is the Taylor factor,\nγsis the interfacial energy between precipitates and\nmatrix,G≈80 GPa is the shear modulus of typical\nFeCrAl ODS steels, Gp= 115 GPa is the modulus\nofα′,εis the misfit of the interfaces.\nThe non-deformable hardening mechanism can\nbe illustrated by the dispersoid barrier hardening\nmodel. A simplified Orowan type equation is used\nto describe this behavior:∆σbox−pass= 0.1Gbf1\n2\nrlnr\nb(5)\nWhereGis the shear modulus of matrix, bis\nthe Burgers vector of mobile dislocations, fis the\nvolume fraction of precipitates, ris the radius of\nprecipitates.\nA hypothesis was made in the following discus-\nsion that no supersaturation remained in the ma-\ntrix, which means the volume fraction fis close to a\nconstant. This hypothesis is considered valid when\naging over 2000 hrs.\nWhenfis constant and ris larger than 1 nm,\nthe derivative shows∂∆σcut−through\n∂r/vextendsingle/vextendsingle/vextendsingle/vextendsingle\nf>0, which\nmeans the aging hardening is monotone increasing.\nSimilarly,∂∆σbow−pass\n∂r/vextendsingle/vextendsingle/vextendsingle/vextendsingle\nf<0, which means hard-\nening by non-deformable precipitates will decrease\nwith aging time (when r>1nm).\nThe critical radius of deformable α′precipitates\nis around 4 nm, calculated by equations 4 and 5.\nAbove this critical radius, the cutting-through force\nbecomes higher than bow-pass-by, that the α′will\nbecome non-deformable. The actual critical radius\nmay vary in different steels.\nThe hardening with precipitate radius and vol-\nume fraction by cut-through and bow-pass mecha-\nnism were shown in Fig.12. The hardening of cut-\nthrough mechanism is insensitive to particle radius.\nTherefore, the hardening in α′-only steels should be\nmainly induced by increasing of precipitates vol-\nume. This could explain the sever increase of hard-\nening in as-extruded Fe12Cr and Fe15Cr in the first\n2000 aging hours. After that, the radius of α′grad-\nually grew beyond the critical, the hardening mech-\nanism converted to bow-pass. As the volume frac-\ntion remained nearly constant, the hardening de-\ncreased after the mechanism converting point. This\ncould be supported by 5000 hrs aged as-extruded\nsteels[46] and the 10000hrs aged STPed Fe15Cr in\nthis study, whose hardening are lower than 2000hrs\naging, as shown in Fig.6a.\nThe hardening by bow-pass mechanism is sensi-\ntive to both precipitate radius and volume fraction.\nIn the early stage of aging ( 300hrs), β′started to\nappear with a rather large radius (2.58nm). The\naging hardening went to maximum around 700 hrs,\nthen started to decrease according to the Vickers\nhardness test[46]. This indicates the volume frac-\ntion reached saturation in the coarsening (overag-\n13Figure 11: The ternary diagram of Fe15Cr (SP2), Fe15Cr5Al (SP4), Fe15Cr7Al (SP7), Fe18Cr7Al (SP11). The Cr and Al\nconcentrations are normalized atom ratios (Fe% +Cr% +Al% =1). The superimposed green curve is the concentration boundary\nofα′precipitation developed by Kobayashi and Takasugi (K-T).\nFigure 12: The precipitate hardening by cut-through and bow-pass mechanism respectively.\n14Figure 13: The fitted aging hardening of Fe15Cr7Al (SP7).\ning) stage after 700hrs aging. The decreasing of\nhardening ascribes to the decreased number den-\nsity ofβ′in Al-added ODS steels.\nThe coarsening process of precipitation may sub-\nject to the following kinetic equation[65]:\n⟨R(t)⟩m−⟨R(0)⟩m=Krt (6)\nWhere⟨R(t)⟩is the mean size of precipitates at\ntimet,⟨R(0)⟩is the initial mean size, Kris the\nkinetic constant. In diffusion-limited coarsening, m\nequals 3, and in source/sink-limited coarsening, m\nequals 2[66].\nIn the analysis in PM2000[67], α′were found fol-\nlows the LSW theory[68, 69], where the radii of pre-\ncipitates,r, could be expressed by a simplified tem-\nporal power law:\nr=Krt1\n3 (7)\nHowever, the kinetic constant Krand the\nexponent of time showed recrystallization\ndependence[67]. Here we take the exponent\nof time as 1 /3 for both α′andβ′. For simplicity,\nthe change of oxides was ignored as well.\nWhen only non-deformable precipitate exists, the\ncombination of precipitation kinetics and hardening\nmechanism of equation 5 and 7 yields:\n∆σ= 0.1Gbf1\n2\nKrt−1\n3lnKrt1\n3\nb(8)\nHence the hardening is only dependent on aging\ntime. Rewrite equation 8 we get:\n∆σ·t1\n3=a+klnt (9)Wherea= 0.1Gbf1/2/Kr·ln(Krb) andk=\n(1/30)Gbf1/2/Kr. The constant Krcan be eval-\nuated byKr=b·exp(a/3k).\nThe fitting results from equation 9 were shown\nin Fig.13. The blue dash line is linear fitted by the\nVickers hardness tests on aged bulk extruded mate-\nrials, using the conversion equation YS= 2.76VH.\nThe STPed FeCrAl ODS steel has a higher harden-\ning than as-extruded. This may be owing to the re-\ncrystallization which eliminated grain boundaries,\nwhere Ti segregates. Thus, the volume fraction\nofβ′in STPed steel should be larger than in as-\nextruded, as predicted in Fig.12.\nThe derivate of equation 9 with respect to t\nyields:\n˙∆σ·t1\n3+ ∆σ·1\n3t−1\n3=k\nt(10)\nSet ˙∆σ= 0, combined with the equation 9, the\nrest terms in equation 10 can be written as:\ntmax=exp(3−a/k) (11)\nThis is the method to evaluate the over-aging\ntime where maximum age hardening occurred for\nthe non-deformable precipitates. The estimated ki-\nnetic constant Krand max hardening time tmax\nare listed in Table 4.\n5. Conclusion\nFour FeCrAl ferritic ODS steels, Fe15Cr (SP2),\nFe15Cr5Al (SP4), Fe15Cr7Al (SP7) and Fe18Cr7Al\n15Table 4: The parameters Krandtmaxof aged FeCrAl ODS\nsteels\nFe15Cr7Al Kr(×−3)tmax(hrs)\nSTPed 7.385 691\nAs-extruded 5.515 513\n(SP11), were fabricated by simulated tube process-\ning (STP) to plates with 0.3 mm thickness. The\nplates were aged at 475 °C for 2000 hrs and 10000\nhrs in vacuum. Uniaxial tensile tests were per-\nformed to investigate the aging embrittlement in\ndifferent steels. The obtained results are summa-\nrized as below:\n1. The four cycles of CR-HT STP with recrystal-\nlization in HT3 and repeating temperature in\nHT4 yielded partially recrystallization in the\nlast STP step.\n2. Tensile tests to the rolling direction showed\nthat yield stress returned similar to the as-\nextruded ones after STP, except SP2 whose YS\nreduced 100MPa. The TE reduction of STPed\nsteels is 1/3 of the extruded steels. All the\nSTPed steels showed reduction of deformation\nenergy in tensile tests compared to extruded\nsteels.\n3. The deformation energy change after aging\ncould be divided into three types, correspond-\ning to the precipitation types of α′andβ′pre-\ncipitates formation.\n4. For Al-free ODS steels after STP, the aging\nhardening is smaller than as-extruded ones.\nFor Al-added ODS steels after STP, the ag-\ning hardening haviour is similar to as-extruded.\nRecovered effect in hardening and elongation\nreduction appeared after 10000 hrs aging.\n5. All the specimens/materials after aging frac-\ntured in a ductile manner. There were two\ncharacteristic features on the tensile fracture\nsurface: 1) Al-containing particles were ob-\nserved in the dimples and 2) secondary crack-\ning along elongated layered grain.\n6. The loss of elongation by ageing is rather\nsmaller in ODS ferritic steels than in non-ODS\nalloys with respect to the same amount of age-\nhardening.\n7. Over-aging occurred before 10000 hrs anneal-\ning in all the STPed steels. The analysis com-\nbining Orowan hardening equation and LSW\ntheory showed that the STPed SP7 has a\nhigherβ′growth rate constant.6. Acknowledgement\nZXZ would like to thank Prof. Akihiko Kimura\nin Kyoto University for the supporting of the ex-\nperiment and the research communications.",
      "metadata": {
        "filename": "475_C aging embrittlement of partially recrystallized FeCrAl ODS ferritic steels.pdf",
        "hotspot_name": "Steel_Punching_and_Bending",
        "title": "475°C aging embrittlement of partially recrystallized FeCrAl ODS\n  ferritic steels after simulated tube process",
        "published_date": "2023-10-20T22:31:18Z",
        "pdf_link": "http://arxiv.org/pdf/2310.13842v2",
        "query": "steel punching bending LCA reduction"
      }
    },
    "Anisotropic behaviour law for sheets used in stamping_ A comparative study of st": {
      "full_text": " \n 1 \nJournal home page :http://www.sciencedirect.com/science/journal/16310721  \n Anisotropic behaviour law for sheets used in stam ping: A comparative study of steel and aluminium \nComptes Rendus Mecanique , Volume 331, Issue 1 , January 2003 , Pages 33-40  \nJean-Jacques Sinou and Bruno Macquaire \n \n  \nANISOTROPIC BEHAVIOUR LAW FOR SHEETS USED IN STAMPING - COMPARATIVE STUDY \nOF STEEL AND ALUMINIUM \n \nLOI DE COMPORTEMENT ANISOTROPE POUR TOLES UTILISEES EN EMBOUTISSAGE - \nETUDE COMPARATIVE DE L'ACIER ET L'ALUMINIUM \n  \nJean-Jacques SINOU\n+, Bruno MACQUAIRE* \n \n \nMECANIQUE, Mécanique des solides et des structures/ Mechanics of  solids and structures (M6). \n \n \n+Laboratoire de Tribologie et Dyna mique des Systèmes UMR CNRS 5513, \nEcole Centrale de Lyon, 36 avenue Guy de Collongue, 69134 Ecully, FRANCE. \n \n*RENAULT S.A, \nManager BIW Advanced Design & new Materials, \nTechnocentre Renault, \n1 avenue du golf, 78288 Guyancourt Cedex, FRANCE. \n \n \nAbstract  - For a car manufacturer, unweighting vehicles  is obvious. Replacing steel by aluminium moves \ntowards that goal. Unfortunately, aluminium’s stamping numerical simulation results are not yet as reliable \nas those of steel. Punch-strength and spring-back phenomena are not correctly described. This study on \naluminium validates the behaviour law Hill 48 quadratic  yield criterion with both isotropic and kinematic \nhardening. It is based on the yield surface and on associ ated experimental tests (uniaxial test, plane tensile \ntest, plane compression and tensile shearing). \nSolids and Structures / stamping of sheets / yield surface / mixed hardening . \n \nRésumé  - Réduire le poids des véhicules est l’un des pr incipaux objectifs des constructeurs automobiles. \nLe remplacement de l’acier par l’aluminium va dans  ce sens. Malheureusement, la simulation numérique \nde l’emboutissage des tôles d’aluminium n’a pas encore  atteint le niveau de fiabilité de l’acier : des \nproblèmes tels que l’effet poinçon ou le retour élasti que n’y sont pas encore correctement décrits. L’étude \nconsiste donc à valider la loi de comportement du critère quadratique HILL 48 avec écrouissage mixte \n(écrouissage isotrope et cinématique) à partir de la su rface de charge et des essais expérimentaux classiques \n(essais de traction simple, traction plane, compression plane et cisaillement).  \nSolides et structures / Emboutissage des tôles /  surface de charge / écrouissage mixte. \n \nVersion française abrégée   \n 2Actuellement, l’un des principaux objectifs des c onstructeurs automobiles est l’allégement des \nvéhicules. Or la tendance actuelle ne va pas dans ce se ns : équipement standard de plus en plus complet, \nconfort optimisé, assistance électronique et renforts  de structure omniprésents. Cela contribue à une \naugmentation du poids des véhicules. Se trouvant, de plus, confronté aux règlements imposant une \nréduction de la consommation des véhicules, le sect eur automobile se doit d’axer ses recherches sur \nl’allégement des structures. L’aluminium se positionne donc aujourd’hui comme un concurrent potentiel de l’acier : on peut espérer un gain de poids de 30-40%  par rapport à l’acier. La diversité des matériaux \nimplique que chacun d’entre eux soit employé de façon optimale. Ainsi, la mise en forme des matériaux \nmétalliques de l’automobile passe par la maîtrise de l’ emboutissage. En effet, parmi les procédés de mise \nen forme, l’emboutissage des tôles minces est l’un de s plus répandus dans l’industrie automobile. L’enjeu \ndes prochaines années est de concevoir et valid er des outils d’emboutissage grâce à la simulation \nnumérique, sans avoir recours aux essais de valida tion actuels qui nécessitent la construction d’outils \nd’essai coûteux. Cependant, la simulation numérique ne peut valider des calculs sur la faisabilité de pièces \nde carrosserie qu’à la condition que les paramètres caractérisant les matériaux soient les plus justes possible et reflètent au mieux la réalité. C’est pour cette raison que les codes de calcul nécessitent des modèles de comportement réalistes et des données fiables sur les matériaux. \nL’objectif de ce travail est donc de trouver et valider une loi de comportement pour les tôles d'acier XES \net d'aluminium 6016 à partir d’essais expérimentaux simples (essais de traction simple, traction plane, \ncompression plane et cisaillement). \n Dans un premier temps, nous recherchons à modé liser le comportement des matériaux étudiés \n(aluminium et acier); nous prenons le critère quadr atique  HILL 48 non centré, avec une loi d’écrouissage \nmixte, qui permet de tenir compte de “ l’histoire  du matériau ” provenant de son élaboration. Les \nexpressions théoriques des coefficients de Lankford associés r\n0 et r 90 , ainsi que les lois gouvernant \nl'évolution du centre de la surface de charge, caractéris ant l'écrouissage cinématique, et l'évolution de la \nvariable d'écrouissage isotrope sont établies. \nDans un second temps, nous recherchons, à partir d' essais expérimentaux, à identifier la surface de \ncharge décrite par le critère quadratique HILL 48 non cen tré. Les essais de cisaillement et de compression \nplane permettent d’avoir des points expérimentaux, pour  l’identification de la surface de charge, dans des \nrégions non exploitées par les essais de traction simple et plane et l'identification du comportement s'effectue alors par la caractérisation de la surface de charge (figure 1). De plus , afin d’identifier la loi \nd’écrouissage mixte, comportant la loi d’écrouissage ci nématique et la loi d’écrouissage isotrope (Lemaître \n& Chaboche [4]), nous déterminons, à partir d’essais expérimentaux, les surfaces de charges pour des prédéformations uniaxiales (sens long 0°) de 8% et 14% . Ainsi, à partir des évolutions des surfaces de \ncharges obtenues (figure 2 pour l’acier  et figure 3 pour l’aluminium), nous en déduisons les coefficients \ncaractéristiques des lois d’écrouissage mixte. Nous observons alors une bonne corrélation entre les divers \nessais expérimentaux et les surfaces de charges obte nues en appliquant le modèle quadratique HILL 48 non \ncentré (figure 2 et figure 3). De même, les comparai sons expérimentales et théoriques sur un essai de \ntraction simple (figure 4), dans le cas de l’aluminium, nous permettent de valider la loi de comportement HILL 48 quadratique non centrée avec écrouissage mixte et l’ identification des divers coefficients associés. \nDans le cas de l’aluminium (figure 3), nous remar quons que la surface de charge initiale est décentrée \net que cette dernière se translate progressivement et tend vers une position peu évolutive pour les grandes déformations : même si l’écrouissage cinématique rest e faible par rapport à l’écrouissage isotrope, omettre \nce dernier peut nous conduire à une identification a pproximative voir fausse du comportement des alliages \nd'aluminium. En revanche, le critère quadratique HILL 48 avec un écrouissage isot rope peut suffire pour \nidentifier correctement le comportement de l’acier (figure 2).  \n1 Introduction \nThe lightening of vehicles is obvious ly one of the many goals of cars manufacturers. However, this is \nnot the current trend, which is toward more and more complete standard equipment, optimum convenience,  \n 3electronic assistance, and omnipresent strengthening stru ctures. All these transformations contribute to the \nincreasing weight of the vehicles. The use of alumin ium in automotive/transport application is primarily \ndriven by its high strength to weight ratio characteristics.  This characteristic contributes to the efficiency in \nfuel consumption because of the reduction in weight. Aluminium is a serious challenge to steel : 30-40% \nreduction weight can be expected in using aluminium instead of steel. \nThe large range of available materials implies that th e use of each one has to be optimised. In this way, \nthe imposition of the metallic material of a car needs the stamping control. The stamping of thin aluminium \nsheets is one of the most widely used shaping pro cesses in the car industry. The validation of stamping \ntools by numerical simulation needs models having realistic behaviour and reliable data concerning \nmaterial. The goals are to find an “aluminium behavi our law” for numerical simulation of the stamping \nprocess and to validate the constitutive equations (plas ticity and hardening criterions) based on very simple \nexperimental tests (uniaxial tensile test, plane tensile  test, plane compression test and monotonic and cyclic \ntensile shearing tests). The behaviour identification is  carried out by characterisation of the yield surface \nand the determination of the elastic limit of each material.   \nThe tensile shearing and plane compression tests give additional information for the identification of the \nyield surface and for both the kinematic  and isotropic hardening. These experiments allowed us to obtain \nexperimental plots in areas not studied previously, as illustrated in figure 1. Next, we can check the validity \nof the quadratic or non-quadratic criterion. \nIn this study, we are considering aluminium 6016 (Alloy 6000 series (AlMgSi). Composition in % mass: \n0.9-1.5% Si., 0.4% max. Fe, 0.2% max. Cu, 0.2% ma x. Mn, 0.3-0.6% Mg, 0.1% max. Cr, 0.2% max. Zn, \n0.15% max. Ti, Al remaining,. Heat treatment T4 ) and steel XES (Composition in % mass: 0.08%C max., \n0.03% P max., 0.4% Mn max.). \n \n2 Theoretical approach \n2.1 Constitutive equations \nWhatever the mechanical tests and the associated lo ading area, the later is identified in relation to a \nbehaviour model. Such a model could be defined as  follow [1]: a linear isot ropic elastic behaviour \n(Young’s modulus, Poisson’s coefficient) and a plastic be haviour identified from a plastic criterion with an \nassociated flow rule (quadratic or not) identifie d by the initial yield surface and an hardening model \n(isotropic, kinematic or both) identified by the devel opment of the yield surface or the cyclic test. The \nproblem is to find the constitutive equation resulting from all the realized experimental tests and \ncorresponding to a possible physical description of  all the phenomena observed during the shape up-\nmaking and the use of materials. \nDuring the elaboration and the transformation of th e metals or semi-products, the steel sheets are \nflattened. This flattening diminishes their thickness and gives them particular physical qualities (skin-pass \nprocess : hardening passing on rolling mills which give s an elongation of 0.5 to 2.5 %). Under the same \nconditions, the aluminium sheets are subject to a plan age going from coils to plane sheets. These various \nprocesses concerning the shape up-making of the mate rials show a coupling between the plastic criterion \nand the hardening model (Macquaire [2]) as being a result of the “material story” (with the kinematic \nhardening which is very important at low strains). For metal sheets possessing or thotropy, Hill’s (1948) [3] \nyield criterion has received the most attention and favor. This quadratic non-centered criterion imposes a \nmixed hardening law (kinematic and isotropic). Moreover,  this criterion allows one to take into account the \n“material his story” resulting from its processing. So , we define the following model, called quadratic non \ncentered Hill’s (1948) yield criterion : \n \n222 2\n11 11 22 22 22 22 33 33 33 33 11 11 23 23\n22\n31 31 12 12H( X X ) F( X X ) G( X X ) 2L( X )1f( ,X ) R0\n22M( X ) 2N( X )σ − −σ + + σ − −σ + + σ − −σ + + σ −\nσ= −=\n+σ − +σ −⎛ ⎞\n⎜ ⎟⎜ ⎟⎝ ⎠ (1)  \n 4 \nwhere σ, X, R, are the strain tensor , the tensor con cerning the translation of the yield locus and the \nisotropic hardening variable, respectiv ely. F, G, H, L, M, N define material parameters characterizing the \nanisotropy. Moreover, the mixed hardening law (kinematic and isotropic [4]) can be expressed as follow:  \np\n0 dX C .d .X.d=ε − γ λ   and  Rs a t dR C (R R).dp= −     (2) \n \nwhere 0C,γ are the material parameters characterizing the kinematic hardening. C R, R sat are the material \nparameters characterizing the isotropic hardening. pdε, dλ and dp define the incremental strains, the \nconstant parameter and the equivalent plastic deforma tion, respectively. The equivalent plastic strain can \nbe expressed as follow (Lemaître & Chaboche [4])  \npp pp p p p p 222 2 2 p2\n33 22 11 33 22 11 23 31 12\n2F(Hd Gd ) G(Fd Hd ) H(Gd Fd ) d d ddp 2 2 2 2\nNLM (GH FG HF)ε− ε + ε− ε + ε−ε ε ε ε=+ + +\n++⎛ ⎞\n⎜ ⎟⎜ ⎟⎝ ⎠  (3) \n \n2.2 The mixed hardening law  \nWe note that yield functions provide information on the properties of metals such as the orientation \ndependence of plastic strain ratio, the uniaxial tensile yield stress and the principal direction of strain-rate \ntensor. But it is not sufficient to obtain all the parame ters characterizing the mixed hardening law. This is \nwhy we considered also the evolution of the Lankfor d coefficient, initial and subsequent yield surfaces \nduring complex loading path, to determine all the para meters of the mixed hardening law. It was  known \nthat the directional 0° and 90° plastic strain ratios are given by p p\n0 23rd d=εε  and p p\n90 13rd d=ε ε . The \nassociated plasticity condition imposes p\n1 1df . dε=∂ ∂σ λ , p\n2 2df . dε=∂ ∂σ λ  and p\n3 3df . dε= ∂∂ σ λ . By \nsubstitutions, we obtain : \n \n12 1 23\n0\n31 1 23H(X X ) F(X X )\nG( X X ) F(X X )r−− σ − −=\n−+− σ + −      and  22 1 3\n90\n13 223H( X ) 2X GXr\nG(X X ) F( X X )σ− + −=\n−− + σ − −    (4) \nThe objective is to obtain the four material  parameters of the mixed hardening law ( C0,γ,CR, Rsat). Here, \nwe considered the evolution of the yield locus X and the isotropic hardening parameter R. To determine the \nevolution of the yield locus, we use the Lemaître & Chaboche model [4]: \n \np pp\n00 1dX C .d .X.d C .d .X. (F, G, H, j).d=ε − γ λ =ε − γ ψ ε     (5) \n \nwhere 22 2\n2F( H( j 1) Gj) G(F H( j 1)) H(Gj F)(F,G,H, j) 2.\n(GH FG HF)−+ − + +++ −ψ=\n++      and p\n2\np\n1dj\ndε=\nε. \n \nSo, the  three equations governing the yield locus evolution can be deduced :  \n()p p\n11 0 0\n11CX1 e X e−γψε −γψε=− +\nγψ;()p p\n11 0 0\n22C. jX1 e X e−γψε −γψε=−+\nγψ; ()p p\n11 0 0\n33C. ( j 1 )X1 e X e−γψε −γψε −+=− +\nγψ (6) \nWe applied the same process for the determination of the evolution of the scalar R. We decide to note R as \n(Lemaître & Chaboche [4]) Rs a t dR C (R R).dp=−  (with () dp f R .d d=∂∂ − λ =λ ). Then, we obtain \np\nRs a t 1dR C (R R). (F, G, H, j).d=− ψ ε . The equation governing the evolution of the scalar R can be deduced :  \n  \n 5()p p\nR1 R1Cd Cd\nsat 0 RR 1 e R e−ψε − ψε=− +      ( 7 )  \n \nFinally, we could obtain the analytical expressions of the initial and subsequent yield surface, of the \nevolutions of the yield locus and the scalar R duri ng complex determined loading path. They allow the \nidentification of the material parameter for the obtention of the constitutive equation. \n3 Yield surface: experimental and theoretical results  \nThe first step is to obtain the initial yield locus. We need to identify the material parameters characterizing \nthe anisotropy, the initial yield locus (ijX for i,j=1 to 3) and the scalar  coefficient R. Because of the \nelaboration and transformation of metals (metal sheets possessing orthotropy, skin-pass…) and the \ndefinition of the Lemaître & Chaboche model, we only need four experimental te sts for the identification \nof the initial yield locus. We deci de to use the two uniaxial tensile tests (0° and 90°) and the two plane \ntensile tests (0° and 90°). The others tests (plane compression tests and linear and cyclic tensile shearing \ntests) are only used to validate definitively the in itial yield surface. By considering the quadratic non \ncentered Hill’s yield criterion (1), the associated pl asticity condition and the specific conditions for each \ntensile test (uniaxial 0° tensile test: 23 0 σ= σ= ; uniaxial 90° tensile test:13 0 σ=σ = ; plane strain 0° tensile \ntest: p\n2d0ε=  and 30σ= ; plane strain 90° tensile test:p\n1d0ε= and 30σ=), we obtain the expressions: \n \n() ()2 22 2\n32 32 2 3 2 3uniaxial\n11(GX HX ) GX HX 2 F H X (G F)X 2FX X 2R\nX\n2−++ + −+ + + − −\nσ= +⎡ ⎤⎣ ⎦    (8) \n \n()2 22 2\n13 13 1 3 1 3uniaxial\n22(HX FX ) HX FX (F H) 2X (G F)X 2GX X 2R\nX\nFH−++ + − + + + − −\nσ= +\n+⎡ ⎤⎣ ⎦    (9) \n \n() () ( )( )\n()2 22\n33 3plane\n1 122\n2X G HF/(F G) X G HF/(F G) 2 H (F H) X G F F (F H) 2R\nX\n2H ( FH )//\n/−++ + ++ − − + + −+ −\nσ= +\n−+⎡ ⎤⎣ ⎦ (10) \n \n()() ( )( )\n()2 22 2\n33 3plane\n2 222\n2X HG 2F X GH 2F 4 H F H X G F G 2R\nX\n2F H H/2 /2\n/2− + + + − − ++ +− −\nσ= +\n+−⎡ ⎤⎣ ⎦ (11) \nBy using equations (4), (8), (9), (10) and (11), we obtain the expression of the quadratic non centered \nHill’s (1948) yield surface. Figure 2 and Figure 3 pres ent the different results obtained for steel and \naluminium, respectively. One can observe a perfect correlation between the experimental tests and the yield surface. Hence, we note that the yield locus for al uminium does not remain centered, as illustrated in \nFigure 3 : this implies the need of kinematic law. \nIn order to obtain the constitutive law of steel and aluminium, it is necessary to have subsequent \nexperimental yield surfaces during complex loading. We carry out predeformations on steels (8% and 14% \nuniaxial deformations 0°), which were then cut to perf orm classical experimental tests for the yield surface \n(uniaxial tensile tests, plane tensile test, etc...). We decide to use two uniaxial tensile tests (0° and 90°),  \ntwo plane strain tensile tests (0° and 90°) to determin e the subsequent yield surface. The other data (plane \ncompression tests, a cyclic tensile shearing test and the evolutions of the Lankford coefficients) are only \nused to validate definitively the yield surfaces. Figure 2 and Figure 3 present the results obtained for steel \nand aluminium, respectively. One can observe a perfect  correlation between all the experimental tests and \nthe yield surface. Hence, we note that the evolu tion of the yield surface during the complex imposed \nloading path has a predominant isotropic form. Howeve r, in the case of aluminium, we observe that the  \n 6initial yield center is different from zero and we have a evolution of this yield center for the subsequent \nyield surfaces. \n4 Determination of the mixed hardening law \nIn this part, we consider the mixed nonlinear hardening law (Lemaître & Chaboche) defined previously. \nWe intend to identify the four coefficients of this law ( C0,γ,CR, Rsat). We previously determined the initial \nyield surfaces and the evolution of the yield surfaces during a complex loading path. Using the evolution of \nthe yield surfaces, it is easier to determine the evolution of the yield locus and the evolution of the scalar R. \nTherefore, we could use a lot of tests to identify the f our coefficients and to validate them definitively. At \nthe beginning, we introduce the evolution of the yield locus and the evolution of the scalar R. We obtain also the four coefficients of the mixed non-linear hardening law “Lemaître & Chaboche non-linear”. The \nvalues of the coefficients for aluminium and steel  are given in Table 1. Secondly, we have a good \nagreement between the experimental uniaxial 0° tensile test and the theoretical expression associated, as \nillustrated in figure 4.  \n5 Conclusion \nIn this study, the “Hill 48 quadratic yield criterion w ith both isotropic and kinematic hardening “. By \ncomparing both experimentally measured and calculated da ta based on this criterion, it is demonstrated that \nthis criterion leads to a good description of the phenomen a. The use of the characterisation of yield surface \nis necessary to have a good representation of the cons titutive equation because of the sensitivity of models \nto plastic strain ratios. In this paper, we explain th at the determination of the elastic limit of each material \nand the use of all mechanical tests (uniaxial tensile test, plane tensile test, plane compression test, linear and cyclic tensile shearing tests) are used to c onfirm the “Hill 48 quadratic yield criterion with both \nisotropic and kinematic hardening” behaviour law.  \n ",
      "metadata": {
        "filename": "Anisotropic behaviour law for sheets used in stamping_ A comparative study of st.pdf",
        "hotspot_name": "Steel_Punching_and_Bending",
        "title": "Anisotropic behaviour law for sheets used in stamping: A comparative\n  study of steel and aluminium",
        "published_date": "2008-01-19T07:48:14Z",
        "pdf_link": "http://arxiv.org/pdf/0801.3018v1",
        "query": "steel punching bending LCA reduction"
      }
    },
    "Fast Privacy-Preserving Punch Cards": {
      "full_text": "arXiv:2006.06079v3  [cs.CR]  19 Feb 2021Fast Privacy-Preserving Punch Cards\nSaba Eskandarian\nStanford University\nsaba@cs.stanford.edu\nAbstract —Loyalty programs in the form of punch cards that\ncan be redeemed for beneﬁts have long been a ubiquitous\nelement of the consumer landscape. However, their increasi ngly\npopular digital equivalents, while providing more conveni ence\nand better bookkeeping, pose a considerable privacy risk. T his\npaper introduces a privacy-preserving punch card protocol that\nallows ﬁrms to digitize their loyalty programs without forc ing\ncustomers to submit to corporate surveillance. We also pres ent\na number of extensions that allow our scheme to provide other\nprivacy-preserving customer loyalty features.\nCompared to the best prior work, we achieve a 14×reduction\nin the computation and a 11×reduction in the communication\nrequired to perform a “hole punch,” a 55×reduction in the\ncommunication required to redeem a punch card, and a 128×\nreduction in the computation time required to redeem a card.\nMuch of our performance improvement can be attributed to\nremoving the reliance on pairings or range proofs present in\nprior work, which has only addressed this problem in the cont ext\nof more general loyalty systems. By tailoring our scheme to\npunch cards and related loyalty systems, we demonstrate tha t\nwe can reduce communication and computation costs by orders\nof magnitude.\nI. I NTRODUCTION\nPunch cards that can be redeemed for rewards after a\nnumber of purchases are a widely-used incentive for custome r\nloyalty. Although these time-tested loyalty schemes remai n\npopular, they are increasingly being replaced with digital\nequivalents that reside in mobile apps instead of physical w al-\nlets. The beneﬁts of going digital for business owners inclu de\nstronger defenses against counterfeit cards, a more conven ient\ncustomer experience, and better bookkeeping around the pop -\nularity and efﬁcacy of their loyalty programs [ 33], [11].\nUnfortunately, digital loyalty programs also introduce my r-\niad new opportunities for customers’ privacy to be vio-\nlated [ 11], [40], e.g., by linking customer behavior across\ntransactions. This kind of tracking can be conducted by the\nbusiness itself, a third-party loyalty service, or a malici ous\nactor who gains access in a data breach. Thus any ﬁrm who\nwants to protect customer privacy should attempt to ensure\nthat its digital loyalty program does not collect unnecessa ry\ndata. But is it possible to digitize the traditional punch ca rd\nwithout damaging customer privacy?\nOne solution to this problem is to rely on classical tech-\nniques such as blind signatures, anonymous credentials, or\nEcash [ 20], [21], [14], [15], [13], [2]. As a simple example,\nconsider a scheme where, to give a customer a hole punch, the\nserver produces a blind signature on a secret chosen by the\ncustomer. After the customer has accrued enough signatures , it\nreveals all of them to the server, who keeps a database of usedsecrets to prevent double spending. While this kind of schem e\nworks, it requires storage linear in the number of punches\nper card on both the client and server, and card redemption\nrequires time linear in the number of punches as well.\nA recent line of work, beginning with the Black Box\nAccumulation (BBA) of Jager and Rupp [ 37], addresses this\nproblem by building schemes which keep track of a cus-\ntomer’s “balance” of punches on a card in constant space.\nUnfortunately, although individual hole punches are unlin kable\nin the original BBA scheme, the processes of issuing and\nredeeming a punch card are not. This shortcoming is rectiﬁed\nin a series of follow-up works [ 34], [5], [6], [36], [35], all of\nwhich additionally extend the idea of black box accumulatio n\nto support a broader set of functionalities, usually via som e\ncombination of blind signatures and zero-knowledge proofs\nthat previously signed values satisfy certain relationshi ps, e.g.,\nthat a balance was correctly updated.\nThis work introduces new protocols speciﬁcally designed to\nsupport privacy-preserving digital punch cards. By focusi ng\nspeciﬁcally on the requirements of punch cards and similar\npoints-based loyalty programs, we are able to make both\nqualitative and quantitative improvements over prior work .\nUnlike the works listed above, our main protocol does not rel y\non pairings or range proofs, enabling signiﬁcant performan ce\nimprovements. Moreover, by stepping away from previous\nabstractions used for punch cards, we can handle punch card\nissuance non-interactively , meaning that a customer can gen-\nerate a new, unpunched card without any interaction with the\nserver. As an ancillary beneﬁt, this removes a potential den ial\nof service opportunity in prior systems, where a customer\ncould register many punch cards without actually needing to\nearn any punches.\nIn terms of performance, our scheme reduces the client\nside computation required to generate a new punch card by\n171×compared to prior work (in addition to not requiring\ninteraction with the server), reduces the total client and s erver\ncomputation times to perform a card punch by 14×, and\nreduces the time to redeem a card by 128×. Communication\ncosts to punch and redeem a card are also reduced by 11×and\n55×, respectively. We outperform the blind signature-based\nscheme sketched above even for punch cards that require a\nsingle punch before redemption.\nOur core protocol is quite simple. To generate a punch card,\na client picks a random secret and hashes it to a point in an\nelliptic curve group using a hash function modeled as a rando m\noracle [ 26], [3]. To receive a hole punch, the client masks this\ngroup element and sends it to the server, who sends it backraised to a server-side secret value, along with a proof that this\nwas done honestly. Finally, after several punches, the clie nt\nredeems the card by sending the unmasked version along with\nthe initial random secret to the server. The server checks th at\nthe group element submitted matches the hash of the random\nsecret raised to the appropriate exponent. It also checks th at\nthe punch card being redeemed has not been redeemed before.\nSince the server is not involved in card issuance and only\never sees separately masked versions of the card, it cannot\nlink a redeemed card to any past transaction. We prove, in\nthe Algebraic Group Model (AGM) [ 28], that a malicious\ncustomer cannot successfully claim more rewards than it is\nentitled to redeem.\nWe also present a number of extensions to our main scheme\nthat allow us to handle variations on the typical punch card.\nFor example, we can handle special promotions where users\nget multiple punches, programs where purchases receive a\nﬁxed number of points instead of a single punch, and even\nprivate ticketing systems. Our most involved extension all ows\ncustomers to merge the points on two punch cards without\nrevealing anything to the server about the individual punch\ncards being merged. This extension uses pairings, but it sti ll\nmaintains the other advantages of our protocol and outper-\nforms prior work, albeit by a smaller margin.\nOur schemes are implemented in Rust with an An-\ndroid wrapper for testing on mobile devices, and all\nour code and raw performance data are open source at\nhttps://github.com/SabaEskandarian/PunchCard .\nII. D ESIGN GOALS\nThe goal of a digital punch card scheme is to allow\ncustomers to have a “punch card” that resides on their mobile\ndevice and is punched, e.g., via NFC, at the time a purchase\nis made, instead of a physical card that must be handed to a\nstore employee and physically punched with a hole puncher.\nThis section begins by motivating privacy-preserving punc h\ncards and discussing different deployment scenarios. Then we\ngive security deﬁnitions and contrast the goals of our work\nwith those of closely related works.\nA. Motivation and Deployment Scenarios\nGiven that digital loyalty programs are so often used to\ncollect data on individual customers, we begin by reviewing\nthe potential beneﬁts of a privacy-preserving punch card\nscheme. Digital punch cards provide additional security an d\nconvenience to both the customer and the store.\nFirst, there is the convenience for customers, who can carry\nfewer cards when the punch card programs they participate\nin are digital and can enjoy a smoother checkout experience.\nAlthough this indirectly beneﬁts businesses, there are als o\ndirect beneﬁts to having a digital punch card program, even i f\nit does not track individual users. Aggregate statistics su ch as\nthe number of cards punched or redeemed in a given day can\nbe time-consuming and error-prone statistics for employee s to\nrecord, especially during busy business hours, but a digita l app\ncan make keeping this information effortless. Digital solu tionsalso save the time employees would otherwise spend punching\ncards, speeding up lines. Moreover, many physical punch car ds\nare punched with hole punches that can easily be bought at\nan ofﬁce supply store or on specialty stores online, meaning\nthat counterfeiting is trivial for any motivated adversary . Going\ndigital removes this possibility.\nOur scheme constitutes only part of a privacy-preserving\nsolution to mobile apps for businesses, and the app in which\nit is deployed must also be designed to protect privacy. This is\ntrue of any privacy-preserving cryptographic scheme deplo yed\nwithin a larger product. We envision this deployment could\neither be a feature in a larger app that a business already\nprovides for its customers, or there could be a stand-alone a pp\nthat handles the punch card program for multiple businesses .\nThe latter approach has the beneﬁt that auditing the one\napp can sufﬁce for assuring the privacy of punch cards at\nseveral businesses as opposed to auditing each implementat ion\nseparately.\nWe note that it may be the case that a customer’s payment\nmethod already identiﬁes them, e.g., if they use a credit\ncard instead of cash, but having a private loyalty system is\nstill important despite this. In particular, there are far m ore\nregulations on what payment networks like Visa/MasterCard\ncan do with a customer’s card information than what an\narbitrary loyalty app that collects user data can do, so ther e\nis additional privacy harm in a loyalty app getting more\ncustomer information. Moreover, new components added to\nthe customer experience should be designed in a privacy-ﬁrs t\nway, so that the loyalty program does not become a barrier to\nprivacy in the transaction process later.\nB. Functionality Goals\nA punch card scheme consists of three components. First, a\nclient running on a customer’s phone should be able to create\na new punch card. Next, the client and a server running a\nloyalty program can interact in order for the server to give t he\nclient a “hole punch.” Finally, a client can submit a complet ed\npunch card to the server for veriﬁcation, and the server will\naccept valid punch cards that have not already been redeemed .\nThe server keeps a database DBof previously redeemed cards\nto make sure a client doesn’t redeem the same card multiple\ntimes. After verifying a card, the server can give the client\nsome out-of-band reward. In general, each of these steps can\nbe a multi-round interactive protocol between the two parti es.\nHowever, since all our protocols involve exactly one round, we\npresent the syntax of a punch card scheme below as consisting\nof individual algorithms instead of interactive protocols .\nApunch card scheme deﬁned with respect to a security\nparameter λis deﬁned as follows.\n•ServerSetup (1λ)→sk,pk,DB: On input a security pa-\nrameterλ, the initial server setup produces server public\nand secret keys, as well as an empty database to record\npreviously redeemed punch cards.\n•Issue(1λ)→psk,p: On input a security parameter λ, the\nIssue algorithm generates new punch card pand a punch\ncard secret psk.\n2•ServerPunch (sk,pk,p)→p′,π: On input the server keys\nand a punch card p,ServerPunch outputs an updated\npunch card p′and a proof πthat the punch card pwas\nupdated correctly.\n•ClientPunch (pk,psk,p,p′,π)→psk′,p′′or⊥: Given the\npublic key, a punch card secret psk, the accompanying\npunch card p, a server-updated punch card value p′, and\na proofπ,ClientPunch outputs an updated secret psk′\nand card p′′if the proof πis accepted and⊥otherwise.\n•ClientRedeem (psk,p)→psk′,p′: Given a punch\ncard secret psk and the corresponding punch card p,\nClientRedeem outputs an updated secret psk′and card\np′that are ready to be sent to the server for redemption.\n•ServerVerify (sk,pk,DB,psk,p,n)→1/0,DB′: on input\nthe server keys, redeemed card database, a punch card,\nthe accompanying secret, and an integer n∈Zdeter-\nmining the required number of punches for redemption,\nServerVerify outputs a bit determining whether or not the\npunch card is accepted and an updated database DB′.\nCorrectness for a punch card scheme is deﬁned in a\nstraightforward way. An honestly generated punch card that\nhas received npunches should be accepted by an honest server.\nThis should hold true even after many punch cards have been\ngenerated and redeemed.\nDeﬁnition II.1 (Correctness) .We say that a punch card\nscheme is correct if for\nsk,pk,DB0←ServerSetup (1λ)\nand any n∈Z, the following set of operations, repeated\nsequentially N=poly(λ)times, results in bj= 1 for all\nj∈[N]with all but negligible probability in λ.\n(psk0,p0)←Issue(1λ)\nfori∈[n] :\np′\ni,πi←ServerPunch (sk,pk,pi)\npski+1,pi+1←ClientPunch (pk,pski,pi,p′\ni,πi)\npsk,p←ClientRedeem (pskn,pn)\nbj,DBj+1←ServerVerify (sk,pk,DBj,psk,p,n)\nThe functionality we desire from our punch cards is at a\nhigh level similar to that offered by black box accumulation\n(BBA) [ 37]. Although we offer a similar functionality, we\nwill do so with stronger security guarantees and signiﬁcant ly\nimproved performance. On the other hand, other follow-up\nworks to BBA [ 34], [5], [6], [36] offer additional features\nthat might be useful in other kinds of loyalty programs, such\nas reducing balances and partially spending accrued reward s.\nThese features enable other applications, but, as describe d\nin Section I, they render the solutions less effective for the\noriginal punch card problem. Bobolz et al. [ 6] introduce the\npossibility of recovering from a partially completed spend that\ngets interrupted mid-protocol, e.g., due to a communicatio n\nor hardware fault. Our scheme avoids the potential for thisproblem entirely because redemption only requires a single\nmessage from the client to the server.\nOne way in which our setting differs fundamentally from\nprior work is the way in which we prevent a punch card\nfrom being redeemed more than once. In our setting, the\nserver has access to a database of all previously redeemed\ncards when deciding whether or not to accept a new punch\ncard submitted for veriﬁcation. Prior works consider an ofﬂine\ndouble spending scenario where the server may not have\naccess to such a database but must be able to identify clients\nwho have double spent punch cards after the fact. We do not\npursue this goal for three reasons, listed in order of increa sing\nimportance below.\n1) Not necessary: point-of-sale terminals often require an in-\nternet connection to work, so synchronizing spent punch\ncards between different locations of a ﬁrm with multiple\nbranches can happen online with less performance cost\nthan an ofﬂine veriﬁcation approach.\n2) Prohibitively expensive: the performance cost of checki ng\nwhether a punch card was double spent in prior work\nis often prohibitive, requiring an exponentiation for each\npreviously redeemed punch card. This would be about 8\norders of magnitude slower than the hash table lookup\nrequired in our setting (as measured on our evaluation\nsetup).\n3) Requires real-world identity: identifying the human use r\nwho double spent a punch card in a way that the person\ncan be penalized requires some notion of real-world\nidentity tied to the punch card client. This means that any\nloyalty system providing such a feature would require a\nuser’s real-world identity in order to operate. This violat es\nour original goal of making a punch card loyalty program\ndigital with no damage to user privacy.\nC. Security Goals\nAt a high level, a punch card scheme must provide two kinds\nof security guarantees. First, it must protect client priva cy\nsuch that the server cannot link messages sent by the same\nclient. Second, it must be sound in that no client can redeem\nmore rewards than it has honestly accrued through valid hole\npunches authorized by the server.\nWe formally deﬁne privacy using a simulation-based deﬁni-\ntion. This means that in order for privacy to be satisﬁed, the re\nmust exist a simulator algorithm that can generate the view\nof the punch card server without access to client-side secre ts.\nInformally, if the server can’t distinguish between the out put\nof the simulator and a real client, then it surely can’t learn\nanything from interacting with a real client because it coul d\nhave received the same information by running the simulator\non its own. Thus nothing is leaked that links one interaction\nwith the server with other interactions. For simplicity, we do\nnot prove security in the UC framework [ 17], [18], but we do\nnot anticipate any signiﬁcant obstacles to composability i n our\nprotocols.\nDeﬁning privacy for hole punches is rather straightforward .\nWe simply require that the punch cards output by successful\n3REALPRIV (λ,A):\n1.T←{}\n2.c←0\n3.(sk,pk)←A1(λ)\n4.b←AOissue,Opunch,Oredeem\n2 (λ)\n5. Output b\nThe experiment REALPRIV (λ,A)makes use of the fol-\nlowing oracles, which all have access to the shared state\nTkeeping track of issued punch cards and the public key\npk, subject to the restriction that Oredeem is only called\nonce on each input id.\nOissue(λ):\n1.psk,p←Issue(1λ)\n2.T[c]←(psk,p)\n3.c←c+1\n4. Output c,p\nOpunch(id,p′,π):\n1. ifid/∈T, output⊥\n2.(psk,p)←T[id]\n3.(psk′,p′′)←ClientPunch (pk,psk,p,p′,π)\n4. if(psk′,p′′)/ne}ationslash=⊥, thenT[id]←(psk′,p′′)\n5. Output p′′\nOredeem(id):\n1. ifid/∈T, output⊥\n2.(psk,p)←T[id]\n3.psk′,p′←ClientRedeem (psk,p)\n4. Output psk′,p′IDEALPRIV (λ,A,S):\n1.T←{}\n2.c←0\n3.(sk,pk)←A1(λ)\n4.b←AOissue,Opunch,Oredeem\n2 (λ)\n5. Output b\nThe experiment IDEALPRIV (λ,A,S)makes use of the\nfollowing oracles, which all have access to the shared\nstateTkeeping track of issued punch cards and the public\nkeypk, subject to the restriction that Oredeem is only\ncalled once on each input id.\nOissue(λ):\n1.psk,p←Issue(1λ)\n2.T[c]←(0,psk,p)\n3.c←c+1\n4. Output c,p\nOpunch(id,p′,π):\n1. ifid/∈T, output⊥\n2.(cid,psk,p)←T[id]\n3.(psk′,p′′)←ClientPunch (pk,psk,p,p′,π)\n4. if(psk′,p′′)/ne}ationslash=⊥, thenT[id]←(cid+1,psk′,p′′)\n5. Output p′′\nOredeem(id):\n1. ifid/∈T, output⊥\n2.(cid,psk,p)←T[id]\n3.psk′,p′←S(sk,cid)\n4. Output psk′,p′\nFig. 1: Real and ideal privacy experiments\ncalls to Issue andClientPunch can always be simulated\nregardless of the functions’ inputs. We need not consider ca ses\nwhere the output is ⊥because in this case the client in a\ndeployment of the punch card scheme would not return to the\nserver with the updated punch card, so there is no possibilit y\nof later interactions being linked.\nHandling redemption is a little more involved because\nthe view of the server during redemption may depend on\nthe number of earlier punches on the same card. For this\nreason, our privacy deﬁnition for redemption deﬁnes real\nand ideal privacy experiments, both of which begin with\nthe challenger initializing an empty table Tmapping unique\ninteger identiﬁers to punch cards and a counter c←0that\nis incremented each time a new punch card is issued. The\nadversary is allowed to pick server secret and public keys\n(sk,pk), and then it is allowed to interact with oracles Oissue,\nOpunch , andOredeem which play the role of the client in the\npunch card scheme. In the real privacy experiment, these\noracles act as wrappers around the Issue ,ClientPunch , and\nClientRedeem functions, simply calling the functions on the\nrequested punch card (identiﬁed by an idnumber chosen at\nissuance) and performing bookkeeping when punch cards areissued, updated, or redeemed. The ideal privacy experiment\nhas identicalOissue andOpunch oracles, except it additionally\nkeeps track of how many punches each card has received.\nThe idealOredeem oracle, instead of calling ClientRedeem ,\ncalls the simulator algorithm S, which is given no information\nabout the card being redeemed except the number of punches\nit has previously received. At the end of each experiment,\nthe adversary outputs a distinguishing bit b. This deﬁnition\nimplies that no adversary can distinguish between redeemed\npunch cards that have the same number of punches on them.\nTaken together with the simulators for punches, this implie s\nthat the server can simulate all interactions it has with cli ents.\nDeﬁnition II.2 (Privacy) .LetΠbe a punch card scheme. We\nsay thatΠhasprivacy if for security parameter λ, and for\nevery adversaryA= (Apunch,Aredeem), withAredeem further\nsubdivided intoA1,A2, there exists a negliglible function\nnegl(·)and simulator algorithms Spunch andSredeem such that\nthe following conditions hold.\n•For any punch card p/ne}ationslash=⊥output by Issue(1λ),\n/vextendsingle/vextendsingle/vextendsinglePr/bracketleftbig\nApunch(p)/bracketrightbig\n−Pr/bracketleftbig\nApunch(Spunch(1λ))/bracketrightbig/vextendsingle/vextendsingle/vextendsingle<negl(λ).\n4SOUND(λ,A):\n1.sk,pk,DB←ServerSetup (1λ)\n2.T←{}\n3.c←0\n4.cpunch←0\n5.credeem←0\n6.AOpunch,Oredeem,OhonIssue,OhonPunch,OhonRedeem,Ocorrupt(λ,pk)\n7. ifcredeem> cpunch , output 1. Otherwise, output 0.\nThe experiment SOUND(λ,A)makes use of the follow-\ning oracles, which all have access to the shared state\nT,c,cpunch,credeem,sk,pk,DB.\nOpunch(p):\n1.p′,π←ServerPunch (sk,pk,p)\n2.cpunch←cpunch+1\n3. Output (p′,π)\nOredeem(psk,p,n):\n1.b,DB′←ServerVerify (sk,pk,DB,psk,p,n)\n2. ifb= 1:\n3.credeem←credeem+n\n4.DB←DB′\n5. Output b\nOhonIssue():\n1.psk,p←Issue(1λ)\n2.T[c]←(0,psk,p)\n3.c←c+1\n4. Output p\nOhonPunch(id):\n1. ifid/∈T, output⊥\n2.(cid,psk,p)←T[id]\n3.p′,π←ServerPunch (sk,pk,p)\n4.(psk′,p′′)←ClientPunch (pk,psk,p,p′,π)\n5. if(psk′,p′′)/ne}ationslash=⊥, thenT[id]←(cid+1,psk′,p′′)\n6. Output p′,π,p′′\nOhonRedeem(id):\n1. ifid/∈T, output⊥\n2.(cid,psk,p)←T[id]\n3.psk′,p′←ClientRedeem (psk,p)\n4.b,DB′←ServerVerify (sk,pk,DB,psk′,p′,n)\n5. ifb= 1, thenDB←DB′\n6. Output psk′,p′,b\nOcorrupt(id):\n1. ifid/∈T, output⊥\n2.(cid,psk,p)←T[id]\n3. delete idfromT\n4.cpunch←cpunch+cid\n5. Output (cid,psk,p)\nFig. 2: Soundness experiment•For any punch card p′′/ne}ationslash=⊥output by ClientPunch (pk,\npsk,p,p′,π)for any choice of (pk,psk,p,p′,π),\n/vextendsingle/vextendsingle/vextendsinglePr/bracketleftbig\nApunch(p′′)/bracketrightbig\n−Pr/bracketleftbig\nApunch(Spunch(1λ))/bracketrightbig/vextendsingle/vextendsingle/vextendsingle<negl(λ).\n•It holds that\n/vextendsingle/vextendsingle/vextendsinglePr/bracketleftbig\nREALPRIV (λ,Aredeem) = 1/bracketrightbig\n−Pr/bracketleftbig\nIDEALPRIV (λ,Aredeem,Sredeem) = 1/bracketrightbig/vextendsingle/vextendsingle/vextendsingle<negl(λ),\nwhere the experiments REALPRIV andIDEALPRIV are\ndeﬁned as in Figure 1.\nThe starting point for our soundness deﬁnition resembles\nthat of BBA [ 37], which requires that a malicious client\ncan only redeem as many punches as it has accrued. Aside\nfrom modifying the syntax of the deﬁnition to match our\nown, we have also modiﬁed it to allow the adversary to\ninterleave hole punches and redemptions instead of requiri ng\nthat all redemptions occur at the end of the protocol. More\nimportantly, our deﬁnition also requires that an adversary\ncannot steal punches from honest users. BBA only requires\nthat the total number of punches redeemed does not exceed\nthe total number of punches given, which does not rule out\nthe possibility of a malicious user stealing punches from\nhonest users. Our soundness deﬁnition also models adaptive\ncorruptions, where the adversary can corrupt a previously\nhonest user at any time during the security experiment. The\ndeﬁnition gives the adversary the ability to get hole punche s\nand redeem cards, but it also allows the adversary to ask for\nhonest users to generate, punch, and redeem cards, with the\nadversary eavesdropping on all communications and having\nthe ability to corrupt honest users at any point, acquiring t heir\npunches and secrets.\nDeﬁnition II.3 (Soundness) .LetΠbe a punch card scheme.\nThen for a security parameter λand adversaryA, we deﬁne\nthe soundness experiment SOUND(λ,A)in Figure 2. We say\nthat a punch card scheme Πsatisﬁes soundness if there exists a\nnegligible function negl(·)such that for any efﬁcient adversary\nA, we have\nPr[SOUND(λ,A) = 1]<negl(λ).\nAs in BBA, this deﬁnition does not capture whether or not\na client can transfer value from one punch card to another\nor merge separate, partially ﬁlled punch cards to redeem a\nsingle, larger card. In fact, it is not entirely clear if this kind\nof card merging is a malicious behavior to be avoided or a\nbeneﬁcial feature to be desired. This kind of merging appear s\nto be difﬁcult to do in our main construction, but we show how\nto extend our scheme to allow a limited degree of merging in\nSection IV.\nIII. P RIVACY -PRESERVING PUNCH CARDS\nThis section describes our main punch card scheme. In\naddition to its quantitative improvements over prior work,\n5which we measure in Section V, our scheme has a number\nof other desirable properties:\n•Whereas most prior works make use of pairings, ei-\nther because they rely on Groth-Sahai proofs [ 31] or\nPointcheval-Saunders signatures [ 44], our punch card\nscheme does not require pairings.\n•We require no communication at all to issue a new punch\ncard – a client can do this on its own without server\ninvolvement. This removes a potential denial of service\nopportunity present in prior work, where a client could\ninitiate a number of punch cards without making any\npurchases, thereby making the server incur unnecessary\nstorage and computation at no cost to the malicious client.\n•Our redemption process involves a client sending a\nsingle message to the server, so there is no potential\nfor the process to be interrupted mid-protocol and no\nneed for a recovery process of the form proposed by\nBobolz et al. [ 6].\nA. Main Construction\nA basic scheme . We will begin with a bare-bones version of\nour scheme that provides neither privacy nor soundness. Fro m\nthis starting point, we will gradually build up to our actual\nscheme. Throughout, we will work in a group Gof prime\norderq.\nTo set up the initial scheme, the server chooses a secret\nsk∈Zq, and a client chooses a group element p0←RGto\nrepresent the punch card. To receive a hole punch, the client\nsendspito the server, who returns pi+1←psk\ni. To redeem a\ncard after npunches, the client submits p0andpnto the server,\nwho accepts if pn=pskn\n0andp0has not been previously used\nin a redeemed card. A nice feature of this approach, which\nwe will keep in our ﬁnal construction, is that punching a card\nrequires no modiﬁcation of server-side state.\nAdding privacy . The scheme above clearly provides no\nprivacy because the server can link the different times it se es\na punch card. We can make punches made on the same card\nunlinkable by only sending the server masked versions of the\npunch card, in a way reminiscent of standard oblivious PRF\nconstructions [ 43], [27]. The punch card is always masked\nwith a fresh value m←RZqbefore being sent to the server,\nso the server only sees p′←pm, notpitself. The mask\nmis removed (via exponentiation by 1/m) before the next\nmask is applied. This means that the server sees a different\nrandom group element each time it punches a card. Moreover,\nan honest server only sees a random group element p∈Gand\npsknat redemption time.\nUnfortunately, this does not actually sufﬁce to provide pri -\nvacy against an actively malicious server. Consider a malic ious\nserver who always follows the scheme above, but during\none hole punch (for a client it later wishes to re-identify)\nit uses a different secret sk′←RZqso thatsk/ne}ationslash=sk′except\nwith negligible probability. Then when an unsuspecting cli ent\nattempts to redeem its punch card, instead of submittingp,pskn, it really submits p,pskn−1sk′, allowing the server to\nidentify it.\nWe can handle the attack above by having the server give\na zero knowledge proof of knowledge that it has honestly\npunched a card. To facilitate this, we require the server set up\nto also output a public key pk←gsk, for some publicly\nknown generator g∈G. Then the server can prove at\npunching time that it is returning a punch card p′such that\np′=psk, i.e., that p,pk,p′form a DDH tuple [ 25]. This can be\nproven efﬁciently with a generic Chaum-Pedersen proof [ 22]\nmade non-interactive in the random oracle model [ 26], [3].\nThe server generates the proof πand sends it to the client\nalong with the punched card pi+1. The client rejects the\nupdated card if the proof does not verify. We denote proofs\nusing the notation of Camenisch and Stadler [ 16], where\nZKPK{(sk),pk=gsk,p′=psk}represents the Chaum-\nPedersen proof, and require the standard zero knowledge and\nexistential soundness properties [ 7].\nAdding soundness . The two modiﬁcations above ensure that\nthe scheme provides privacy, and they even protect against\ntheft of honest clients’ secrets by eavesdropping maliciou s\nclients because all the messages sent by honest clients to\nthe server are uniformly random until the point where the\npunch card is redeemed (and then it’s too late for the card\nto be stolen). Unfortunately, the scheme still fails to prov ide\nsoundness, as a malicious client can redeem more points than\nit has received punches. Consider a client who at ﬁrst honest ly\nfollows the protocol and redeems a punch card by submitting\np0,pn. Next, it submits a masked pnfor another punch and\ngets back pn+1. Finally, it submits p1,pn+1as another valid\npunch card. According to the scheme described thus far, the\nserver would accept this punch card redemption, meaning tha t\nthe malicious client can redeem 2npunches even though it\nonly received n+1punches.\nThe attack above works because the client can choose\nany group element it wants as p0. We modify our scheme\nto provide soundness by forcing clients to generate p0as\nthe output of a hash function modeled as a random oracle\nH:{0,1}λ→G. In particular, instead of choosing a\nrandomp0, the client chooses a random u← {0,1}λand\nsetsp0←H(u). When redeeming a punch card, instead of\nsendingp0,pn, the client sends u,pn, and the server checks\nthatpn=H(u)skn. Since the hash function is modeled as a\nrandom function, a malicious client cannot ﬁnd the preimage\nof a group element under H, eliminating the attack.\nWith this defense, our scheme now provides both privacy\nand soundness. We formalize our construction as follows.\nConstruction III.1 (Punch Card Scheme) .LetGbe a group\nof prime order qwith generator g∈G, and let Hbe a hash\nfunctionH:{0,1}∗→G, modeled as a random oracle.\nWe construct our punch card scheme as follows:\n•ServerSetup (1λ)→sk,pk,DB: Select random sk←RZq\nand setpk←gsk∈G. Initialize DBas an empty hash\ntable, and return sk,pk, andDB.\n6•Issue(1λ)→psk,p: First, select a random secret u←R\n{0,1}λand a random masking value m←RZq. Then\ncompute p←H(u)m∈G. Letpsk←(u,m). Return\npsk,p.\n•ServerPunch (sk,pk,p)→p′,π: Compute p′←pskas\nwell as the proof of knowledge π←ZKPK{(sk),pk=\ngsk,p′=psk}. Outputp′,π.\n•ClientPunch (pk,psk,p,p′,π)→psk′,p′′or⊥: First,\nverify the proof π. If veriﬁcation fails, output ⊥. Oth-\nerwise, begin by interpreting pskas(u,m). Then sample\na new random masking value m′←RZqand compute\np′′←(p′)m′/m. Setpsk′←(u,m′), and output psk′,p′′.\n•ClientRedeem (psk,p)→psk′,p′: Begin by interpreting\npsk as(u,m)withu∈ {0,1}λandm∈Zq. Then\ncompute p′←p1/m∈G. Returnu(aspsk′) andp′.\n•ServerVerify (sk,pk,DB,psk,p,n)→1/0,DB′: Check\nwhetherp=H(psk)sknand whether psk∈DB. If the\nﬁrst check returns true and the second returns false, insert\npskintoDBand return 1,DB. Otherwise, return 0,DB.\nObserve that the asymptotic complexity of almost every\noperation in our punch card scheme depends only on the\nsecurity parameter λ, with two exceptions. The ﬁrst exception\nis that operations on DB have amortized time complexity\nO(λ), but in the worst case a read/write to DBcould depend\non the number of previously redeemed punch cards. The other\nexception is the exponentiation sknperformed in ServerVerify ,\nwhereO(logn)group operations are required. However, since\nthe same nis often used for every punch card in practice,\nthe server could precompute sknto remove the logarithmic\ndependence on n.\nB. Security\nWe now discuss the security of our constructions. We begin\nby proving the privacy of our punch card scheme.\nTheorem III.2. Assuming the existential soundness of the\nChaum-Pedersen proof system, our punch card scheme has\nprivacy (Deﬁnition II.2) in the random oracle model.\nProof. We begin by describing the simulator S=\n(Spunch,Sredeem).\n•Spunch(1λ)→p: This simulator samples and outputs a\nrandom group element p←RG.\n•Sredeem(sk,cid)→psk′,p′: This simulator samples a\nrandom string psk′←R{0,1}λand computes p′←\nH(psk′)skcid. It outputs psk′,p′.\nThe ﬁrst two conditions of privacy are clearly met by Spunch\nbecause the punch card output by a successful call to Issue or\nClientPunch is always a group element raised to a randomly\nchosen mask m(orm′), which will be distributed identically\nto a randomly chosen group element p←RG.\nNext, we show through a short series of hybrids that\nREALPRIV (λ,A)≈cIDEALPRIV (λ,A,Sredeem)for our\npunch card scheme.\nH0: This hybrid is the real privacy experiment\nREALPRIV (λ,A).H1: In this hybrid, we add an abort condition to the execution\nof the experiment. The experiment aborts and outputs 0\nifClientPunch outputsp′′/ne}ationslash=⊥(i.e., it accepts the proof\nπ) but it is not the case that pk=gsk∧p′=psk.\nThis hybrid is indistinguishable from H0by the sound-\nness of the Chaum-Pedersen proof system. In particular,\nan adversaryAwho can distinguish between H0andH1\ncan be used by an algorithm Bto break the soundness\nof the proof system as follows. Bplays the role of\nthe adversary in the soundness game for the Chaum-\nPedersen proof, and plays the role of the challenger\ntoAin either H0orH1with probability 1/2each.\nWheneverAcauses experiment H1to abort due to the\ncheck introduced in this hybrid, Bsubmits the proof π\nand the statement pk=gsk∧p′=pskto the soundness\nchallenger. Otherwise, Boutputs⊥.\nThe algorithmBdescribed above breaks the soundness\nof the Chaum-Pedersen proof with the same advantage\nthatAdistinguishes between H0andH1. To see why,\nobserve that the only difference in the view of Abetween\nH0andH1occurs when H1aborts. ThusAmust cause\nthe experiment to abort with probability at least equal to\nits distinguishing advantage between H0andH1. But\nwhenever H1aborts,Bhas a statement and proof that\nviolate the soundness of the Chaum-Pedersen proof, so it\nwins the soundness game with the same advantage. This\nargument must be repeated for each proof given to the\nadversary.\nH2: In this hybrid, the challenger switches to record-keeping\nin the table Tin the way IDEALPRIV does and replaces\ncalls toClientRedeem with calls toSredeem .\nThis hybrid is indistinguishable from H1because the\ndistribution of the adversary A’s view is identical in\nthe two hybrids. The outputs of Oissue andOpunch are\nunchanged by the bookkeeping change between the two\nhybrids, so we need only consider Oredeem .\n–Oredeem(id)→psk′,p′: InH1, this oracle returns the\nsecretuused to generate the punch card stored at\nT[id]as well as the value of that punch card pafter\nremoving the last mask mto getp′←p1/m. The value\nofuis distributed uniformly at random in {0,1}λ.\nThe value of p′is equal to H(u)raised to the server\nsecretskas many times as there was a successful call\ntoOpunch(id,·,·)– that is, a call whose output was\nnot⊥. This is the case because in each such call, the\npunch card value stored in Tis raised to skand its\nmask is replaced with a new one. The ﬁnal unmasking\noperation p′←p1/mresults in a punch card value\np′=H(u)skn, wherenis the number of successful\ncalls toOpunch(id,·,·).\nInH2,uclearly has the same distribution as in H1be-\ncause inSredeem it is sampled directly as u←R{0,1}λ.\nThe value p′also has the same distribution as in H1\nbecause the table Tkeeps count of the number cid\nof successful calls to Opunch(id,·,·), soSredeem can\ncompute p′←RH(u)skciddirectly.\n7H3: This hybrid is identical to H2except the abort condi-\ntion introduced in H1is removed. As was the case in\nH1, this hybrid is indistinguishable from the preceding\nhybrid by the soundness of the Chaum-Pedersen proof\nsystem. It also corresponds to the ideal privacy game\nIDEALPRIV (λ,A,S), completing the proof.\nHaving proven privacy, we now turn to soundness. We prove\nthe soundness of our scheme in the algebraic group model\n(AGM) [ 28], where for every group element the adversary\nproduces, it must also give a representation of that group\nelement in terms of elements it has already seen. This is a\nstrictly weaker model (in the sense that it puts fewer restri c-\ntions on the adversary) than the widely-used generic group\nmodel [ 49], in which some of the prior works on privacy-\npreserving loyalty programs have been proven secure [ 5],\n[6]. Our proof relies on the q-discrete log assumption, which\nassumes the computational hardness of winning the followin g\ngame.\nDeﬁnition III.3 (q-discrete log game) .Theq-discrete log\ngame for a group Gof prime order pis played between a\nchallengerCand an adversaryA. The challengerCsamples\nx←RZpand sends gx,gx2,...,gxqtoA. The adversaryA\nresponds with a value z∈Zp, and the challenger outputs 1\niffz=x.\nDepending on the concrete group in which the assumption is\nmade, the q-discrete log game could be vulnerable to Brown-\nGallant-Cheon attacks [ 10], [23], which reduce the security of\nthe assumption by a factor of√q. Fortunately this attack only\nnegligibly affects the security of the scheme, as qis at most\na polynomial in the security parameter λ.\nWe now state and prove our soundness theorem.\nTheorem III.4. Assuming the zero-knowledge property of\nthe Chaum-Pedersen proof system and the q-discrete log\nassumption in G, our punch card scheme has soundness\n(Deﬁnition II.3) in the algebraic group model with random\noracles.\nProof. Sinceqalready refers to the order of the group G, we\nwill refer to the N-discrete log assumption throughout this\nproof. The high-level idea of the proof is to program random\noracle queries with re-randomizations of powers of gxgiven\nby theN-discrete log challenger. Then, whenever a punch card\nis given by the adversary, the algebraic adversary must also\ngive a representation of the punch card pin terms of group\nelements it has seen before. As such, the challenger can pick\nout thegxicomponent and replace it with gxi+1in its response.\nThen a punch card that is accepted before receiving npunches\nmust include a second representation of gxn, allowing us to\nsolve for x.\nWe now formalize the proof idea sketched above. Our proof\nproceeds through a series of hybrids.\nH0: This hybrid is the soundness experiment SOUND(λ,A).H1: In this hybrid, we replace each proof πoutput byOpunch\nandOhonPunch with a simulated proof.\nThe the zero-knowledge property of the Chaum-Pedersen\nproof guarantees that the proofs can be simulated. Since\nhybridsH0andH1are identical save for the real proofs\ninH0and the simulated proofs in H1, the output of an\nadversaryAwho distinguishes between H0andH1can\nalso be used to distinguish between a real and simulated\nproof with the same advantage. This argument must be\nrepeated for each proof given to the adversary.\nH2: In this hybrid, we add an abort condition to the execution\nof the experiment. The experiment aborts and outputs 0\nif theOredeem(psk,p,n)oracle ever outputs b= 1 when\nit receives a value of psk that the adversary has not\npreviously queried from the random oracle Hor been\ngiven as the result of a query to Ocorrupt .\nThis hybrid is indistinguishable from H1because the\nprobability of an adversary successfully triggering this\nabort condition is negligible in λand there are no\nother differences between H1andH2. In order for the\nOredeem oracle to output b= 1 , it must be the case\nthatServerVerify (sk,pk,DB,psk,p,n)outputs 1, which\nmeans that p=H(psk)skn.\nBut since His modeled as a random function and\n(psk,H(psk))is not known to the adversary, its output\nappears uniformly random in G. This is the case even\nfor the punch cards of uncorrupted honest users because\nthe messages the adversary sees from their interactions\nwith the server are always uniformly random group\nelements (because of the masks, see privacy proof) and\nsimulated proofs from the server. But then H(psk)skn\nis also distributed uniformly at random in G, and the\nprobability Pr[p=H(psk)skn]≤negl(λ).\nH3: In this hybrid, we modify how the challenger computes\nthe output of ServerPunch (sk,pk,p)and of the random\noracleH. Recall that since Ais an algebraic adversary,\nevery group element it sends is accompanied by a rep-\nresentation in terms of the previous group elements it\nhas seen: the generator g, returned punch cards p′\n1,...,p′\nQ\nfor theQqueries it has made to the Opunch oracle, and\nrandom oracle outputs H1,...,HQ′for theQ′random\noracle queries it has made.\nLetgi=gskifori∈Z. Whenever the adversary A\nor oracleOhonIssue()makes a call to the oracle Hon\na previously unqueried point u, the challenger samples\nr←RZqand sets H(u)←gr\n1. Sinceris distributed\nuniformly at random in Zq, so isH(u).\nNext, wheneverAmakes a call to the oracles Opunch(p)\norOhonPunch(id), instead of setting p′←psk, the chal-\nlenger looks at the algebraic representation of pand\nreplaces each occurrence of giwithgi+1, including\nreplacing gwithg1. Since the only elements Ahas seen\nareg, random oracle outputs, and the previous results of\nOpunch , the challenger can keep track of which elements\ncontain which gias it sends them to A. The outputs\nofOpunch(p)inH3are identical to the outputs in H2,\n8because the process described here results in the same\ngroup element p′that would be represented by psk.\nWe now additionally have the challenger keep track of al-\ngebraic representations of the group elements it produces\nitself (e.g., in honest users’ punch cards). This makes no\nchanges to the adversary’s view in the experiment.\nSince all the changes in H3result in identically dis-\ntributed outputs as in H2, the two hybrids are indistin-\nguishable.\nFromH3, we can prove that any algebraic adversary A\nwho wins the soundness game can be used by an algorithm\nB, described below, to break the N-discrete log assumption\ninG. AlgorithmBplays the role of the adversary in the N-\ndiscrete log game while simultaneously playing the role of\nthe challenger in H3. AlgorithmBsimulates H3exactly to\nA, except that it uses the N-discrete log challenge messages\ngx,gx2,...,gxNas the values of gi. That is, gi=gxi.\nMoreover, it sets pk←g1in the setup phase. Observe that\nthegiare distributed identically as in H3, so this is a perfect\nsimulation of H3withxplaying the role of sk. The value\nofNrequired in the assumption depends on the maximum\nnumber of sequential punches Arequests on the same group\nelement.\nNow, ifAwins the soundness game, it means that credeem>\ncpunch . This, in turn, implies that there was some successful\npunch card redemption ServerVerify where the accepted value\nofphad not been previously punched ntimes, i.e., the\nrepresentation of pdoes not contain gn+1. But since successful\nveriﬁcation requires that p=H(psk)xn= (gr\n1)xn=gr\nn+1, and\nthe algebraic adversary Amust give a representation of p,\nwe now have two different representations of gn+1=gxn+1,\nwhich together yield a degree- n+ 1 equation in x. This\nequation can be solved for xusing standard techniques [ 50],\nallowingBto recover xand win the N-discrete log game.\nIV. M ERGING PUNCH CARDS\nHaving described our main construction, we now consider\nanother feature sometimes enjoyed by physical punch cards\nthat we may want to reproduce digitally: merging partially-\nﬁlled cards. Just as in real life, it is possible to “merge” tw o\npunch cards by redeeming them separately and taking into\naccount the sum of the number of punches across the two\ncards. However, this process reveals the number of punches\nheld by each card at redemption time, information that the\ncustomer may want to hide. We can hide the value of the two\ncards being merged by resorting to pairings.\nDeﬁnition IV .1 (Pairings [ 7]).LetG0,G1,GTbe three cyclic\ngroups of prime order qwhereg0∈G0andg1∈G1are\ngenerators. A pairing is an efﬁciently computable function\ne:G0×G1→GTsatisfying the following properties:\n•Bilinear: for all u,u′∈G0andv,v′∈G1we have\ne(u·u′,v) =e(u,v)·e(u′,v)\nand\ne(u,v·v′) =e(u,v)·e(u,v′)•Non-degenerate: gT←e(g0,g1)is a generator of GT.\nWhenG0=G1, we say that the pairing is a symmetric\npairing . We refer to G0andG1as the pairing groups and\nrefer toGTas the target group .\nUsing a symmetric pairing, we can quite simply merge two\npunch cards without revealing the number of punches on each.\nBefore redeeming punch cards p0andp1which have iandj\npunches, respectively, with i+j=n, the client computes\np←e(p0,p1). To redeem a merged card, the client sends\nthe server the merged punch card palong with u0andu1,\nthe secrets for the two punch cards merged into p. The server\nchecks that p=e(H(u0)skn,H(u1)). The bilinear property of\nthe pairing ensures that e/parenleftbig\np0,p1/parenrightbig\n=e/parenleftbig\nH(u0)ski,H(u1)skj/parenrightbig\n=\ne/parenleftbig\nH(u0)skn,H(u1)/parenrightbig\n. We can even hide whether or not a\nredeemed punch card is merged by generating a fresh punch\ncard before redemption and merging a complete card with it.\nThe performance of symmetric pairings is far worse than\nthat of asymmetric pairings, so we would like to have a scheme\nthat works for asymmetric pairings as well. Unfortunately,\ndirectly converting the idea above to asymmetric pairings\nmeets with some difﬁculties. Since each punch card must\nbelong to either G0orG1, we can only merge pairs of cards\nwherep0∈G0andp1∈G1. But this is a decision that must\nbe made when a card is ﬁrst issued, restricting punch cards\nto being merged with cards that belong to the other pairing\ngroup.\nWe resolve this problem by splitting each punch card into\ntwo components, one in each pairing group. Each component\nbehaves as a punch card in the original scheme. Generating a\npunch card is similar to the original scheme, but the secret u←R\n{0,1}λis hashed by two different functions H0:{0,1}λ→\nG0andH1:{0,1}λ→G1. Each hole punch repeats the\npunch protocol of the original scheme twice, once in G0and\nonce inG1. Redeeming a card requires merging the G0and\nG1components of the two cards with each other as above,\nand since the client has a version of each punch card in both\ngroups, it can merge them as before.\nWe formalize this sketch of a solution below. We replace\ntheClientRedeem algorithm from our punch card syntax with\na newClientMergeRedeem algorithm that merges two punch\ncards before redeeming them.\nConstruction IV .2 (Mergeable Punch Card Scheme) .Let\nG0,G1,GTbe groups of prime order qwith generators\ng0∈G0,g1∈G1, and let H0,H1be hash functions\nH0:{0,1}∗→G0,H1:{0,1}∗→G1, modeled as random\noracles. We construct our punch card scheme as follows:\n•ServerSetup (1λ)→sk,pk,DB: Select random sk←RZq\nand setpk0←gsk\n0∈G0,pk1←gsk\n1∈G1. Initialize DB\nas an empty hash table, and return sk,pk= (pk0,pk1),\nandDB.\n•Issue(1λ)→psk,p: First, select a random secret u←R\n{0,1}λand random masking values m0←RZq,m1←R\nZq. Then compute p0←H0(u)m0∈G0,p1←\nH1(u)m1∈G1. Letpsk←(u,m0,m1). Returnpsk,p=\n(p0,p1).\n9•ServerPunch (sk,pk,p)→p′,π: First, interpret pk\nas(pk0,pk1)andpas(p0,p1). Compute p′\n0←\npsk\n0,p′\n1←psk\n1as well as the proofs of knowledge\nπ0←ZKPK{(sk),pk0=gsk\n0,p′\n0=psk\n0}andπ1←\nZKPK{(sk),pk1=gsk\n1,p′\n1=psk\n1}. Output p′=\n(p′\n0,p′\n1),π= (π0,π1).\n•ClientPunch (pk,psk,p,p′,π)→psk′,p′′or⊥: First, in-\nterpretpskas(u,m0,m1),pas(p0,p1),p′as(p′\n0,p′\n1),\nandπas(π0,π1). Next, verify the proofs π0and\nπ1. If either veriﬁcation fails, output ⊥. Then sample\nnew random masking values m′\n0←RZq,m′\n1←RZqand\ncompute p′′\n0←(p′\n0)m′\n0/m0,p′′\n1←(p′m′\n1/m1\n1). Finally,\noutputpsk′= (u,m′\n0,m′\n1),p′′= (p′′\n0,p′′\n1).\n•ClientMergeRedeem (psk,p,psk′,p′)→psk′′,p′′: Begin\nby interpreting pskas(u,m0,m1),psk′as(u′,m′\n0,m′\n1),\npas(p0,p1), andp′as(p′\n0,p′\n1). Then compute p′′←\ne(p1/m0\n0,(p′\n1)1/m′\n1)∈GT. Returnpsk′′= (u,u′)andp′′.\n•ServerVerify (sk,pk,DB,psk,p,n)→1/0,DB′: Begin by\ninterpreting pskas(u,u′). Then perform the following\nchecks:\n1)p=e(H0(u)skn,H1(u′))\n2)u∈DB\n3)u′∈DB\nIf the ﬁrst check returns true and the other checks\nreturn false, insert uandu′intoDBand return 1,DB.\nOtherwise, return 0,DB.\nAlthough not included in our formal construction, our\nscheme could be extended to allow more punches to occur on\na merged card so long as the client indicates that it is a merge d\ncard being punched and the punch/proof occur over elements\ninGT. Note that this scheme only allows for two punch cards\nto merged. Our general strategy for merging punch cards coul d\nbe extended to more than two cards using multilinear maps [ 8],\n[29], [24], but a construction that allows merging of more than\ntwo cards while only relying on efﬁcient standard primitive s\nwould require new techniques. This is an interesting proble m\nfor future work to address.\nWe now state and prove our security theorems for the\nmergeable punch card scheme. The only change required in the\nsecurity games to account for the change from ClientRedeem\ntoClientMergeRedeem is that the redeem oracle in the privacy\ngame takes in two ids instead of just one and passes both\ncorresponding punch cards to ClientMergeRedeem .\nTheorem IV .3. Assuming the existential soundness of the\nChaum-Pedersen proof system, our mergeable punch card\nscheme has privacy in the random oracle model.\nProof (sketch). We begin by describing the simulator S=\n(Spunch,Sredeem).\n•Spunch(1λ)→p: This simulator samples and outputs two\nrandom group elements p0←RG0andp1←RG1.\n•Sredeem(sk,cid,cid′)→psk′,p′: This simulator samples\ntwo random strings u←R{0,1}λ,u′←R{0,1}λand\ncomputes p′←e(H0(u)skcid,H1(u′)skcid′). It outputs\npsk←(u,u′)andp′.The ﬁrst two conditions of privacy are clearly met by Spunch\nbecause the punch card output by a successful call to Issue or\nClientPunch is always two group elements raised to randomly\nchosen masks m0andm1(orm′\n0andm′\n1), which will be\ndistributed identically to randomly chosen group elements\np0←RG0,p1←RG1.\nNext, we show through a short series of hybrids that\nREALPRIV (λ,A)≈cIDEALPRIV (λ,A,Sredeem)for our\nmergeable punch card scheme. The rest of the proof of this\ntheorem is very similar to that of Theorem III.2. The main\ndifference is that the soundness of the Chaum-Pedersen proo f\nsystem needs to be invoked separately in each of G0andG1.\nThus we only sketch the steps of the hybrid argument below.\nH0: This hybrid is the real privacy experiment\nREALPRIV (λ,A).\nH1: In this hybrid, we add an abort condition to the execution\nof the experiment. The experiment aborts and outputs 0\nifClientPunch outputsp′′/ne}ationslash=⊥(i.e., it accepts the proofs\nπ0andπ1) but it is not the case that pk0=gsk\n0∧p′\n0=psk\n0.\nH2: In this hybrid, we add an abort condition to the execution\nof the experiment. The experiment aborts and outputs 0\nifClientPunch outputsp′′/ne}ationslash=⊥(i.e., it accepts the proofs\nπ0andπ1) but it is not the case that pk1=gsk\n1∧p′\n1=psk\n1.\nH3: In this hybrid, the challenger switches to record-keeping\nin the table Tin the way IDEALPRIV does and replaces\ncalls toClientMergeRedeem with calls toSredeem .\nH4: This hybrid is identical to H3except the abort condition\nintroduced in H2is removed.\nH5: This hybrid is identical to H4except the abort condition\nintroduced in H1is removed. It also corresponds to the\nideal privacy game IDEALPRIV (λ,A,S), completing the\nproof.\nNext, we prove the soundness of our scheme. Since our\nnew scheme uses pairings, we use an asymmetric q-discrete\nlog assumption, which assumes the computational hardness o f\nwinning the following game.\nDeﬁnition IV .4 (asymmetric q-discrete log game) .Theq-\ndiscrete log game for groups G0,G1of prime order pis played\nbetween a challenger Cand an adversaryA. The challenger\nCsamplesx←RZpand sends gx\n0,gx2\n0,...,gxq\n0,gx\n1,gx2\n1,...,gxq\n1\ntoA. The adversaryAresponds with a value z∈Zp, and the\nchallenger outputs 1 iff z=x.\nTheorem IV .5. Assuming the zero-knowledge property of the\nChaum-Pedersen proof system and the asymmetric q-discrete\nlog assumption in G0andG1, our mergeable punch card\nscheme has soundness (Deﬁnition II.3) in the algebraic group\nmodel with random oracles.\nProof (sketch). Sinceqalready refers to the order of the\ngroupsG0,G1,GT, we will refer to the asymmetric N-\ndiscrete log assumption throughout this proof. The majorit y of\nproof of this theorem is very similar to that of Theorem III.4.\nThe main difference in the hybrids is that several hybrids ne ed\n10to be repeated to account for each punch card being made up\nof two group elements instead of one. Thus we only sketch\nthe steps of the hybrid argument below and focus on the last\nstep of the argument.\nH0: This hybrid is the soundness experiment SOUND(λ,A).\nH1: In this hybrid, we replace the proofs π0output byOpunch\nandOhonPunch with simulated proofs.\nH2: In this hybrid, we replace the proofs π1output byOpunch\nandOhonPunch with simulated proofs.\nH3: In this hybrid, we add an abort condition to the execution\nof the experiment. The experiment aborts and outputs 0\nif theOredeem(psk,p,n)oracle ever outputs b= 1 when\nit receives a value of uoru′that the adversary has not\npreviously queried from both random oracles H0andH1\nor been given as the result of a query to Ocorrupt .\nH4: In this hybrid, we modify how the challenger computes\nthe output of ServerPunch (sk,pk,p)and of the random\noracleH. Letg0,i=gski\n0andg1,i=gski\n1fori∈Z.\nRecall that sinceAis an algebraic adversary, every group\nelement it sends is accompanied by a representation in\nterms of the previous group elements it has seen. Just as\nwe did in the proof of Theorem III.4, instead of punching\ncards by raising psk\n0andpsk\n1, we examine the algebraic\nrepresentation of p0,p1submitted by the adversary and\nreplace each instance of g0,iorg1,iwithg0,i+1org1,i+1,\nrespectively. Also, whenever the adversary Amakes a\ncall to the oracles Hj(forj∈0,1) on a previously\nunqueried point u, the challenger samples r←RZqand\nsetsHj(u)←gr\nj,1.\nFromH4, we can prove that any algebraic adversary Awho\nwins the soundness game can be used by an algorithm B, de-\nscribed below, to break the N-discrete log assumption in either\nG0orG1. AlgorithmBplays the role of the adversary in the\nasymmetric N-discrete log game while simultaneously playing\nthe role of the challenger in H4. AlgorithmBsimulates H4\nexactly toA, except that it uses the asymmetric N-discrete\nlog challenge messages gx\n0,gx2\n0,...,gxN\n0,gx\n1,gx2\n1,...,gxN\n1as the\nvalues of g0,iandg1,i. That is, g0,i=gxi\n0andg1,i=gxi\n1.\nMoreover, it sets pk←(g0,1,g1,1)in the setup phase. Observe\nthat allg0,iandg1,iare distributed identically as in H4, so\nthis is a perfect simulation of H4withxplaying the role of\nsk. The value of Nrequired in the assumption depends on the\nmaximum number of sequential punches Arequests on the\nsame group element.\nNow, ifAwins the soundness game, it means that credeem>\ncpunch . This, in turn, implies that there was some successful\npunch card redemption ServerVerify where the accepted value\nofphad not been previously punched ntimes between the\ntwo merged cards. Let aandbthe number of times each of\nthe two merged cards had been punched before redemption,\nso we have a+b < n .\nThe successful veriﬁcation requires that\np=e(H1(u)xn,H2(u′)) =e(grxn\n0,1,gr′\n1,1) =e(grxa′\n0,1,gr′xb′\n1,1)\nfor anya′,b′wherea′+b′=n, and the algebraic adversary\nmust give a representation of p(it can include a pairing inthis representation). It must be true that one of a′orb′is\ngreater than aorb, respectively, because a+b < n . Thus the\nrepresentation of pmust not include either gr\n0,a′+1orgr′\n1,b′+1\nbecause one of those values will not have been given to A.\nThis means we now have two different representations of one\nof these elements, which together yield a degree a′+ 1 or\nb′+1equation in x. This equation can be solved for xusing\nstandard techniques [ 50], allowingBto recover xand win the\nasymmetric N-discrete log game.\nV. I MPLEMENTATION AND EVALUATION\nWe implemented our main punch card scheme from Sec-\ntion IIIas well as the mergeable punch card scheme from\nSection IV. Our implementation is written in Rust with a\nJava wrapper to run the Rust code on Android devices.\nThe implementation of the main punch card scheme re-\nlies on the curve25519-dalek [41] crate which im-\nplements curve25519 [ 4], and the mergeable punch card\nscheme uses the pairing-plus [30] crate, which pro-\nvides an implementation of BLS12-381 curves [ 9]. Our\nimplementation and raw evaluation data are available at\nhttps://github.com/SabaEskandarian/PunchCard .\nWe carried out our evaluation with the client running on a\nGoogle Pixel (ﬁrst generation) phone and the server running\non a laptop with an Intel i5-8265U processor @ 1.60GHz.\nAll data reported on our scheme comes from an average of\nat least 100 trials. ServerVerify was run with n= 10 punches\non each redeemed punch card and an empty database DBof\nused cards. We repeated the test of the main scheme with\na database of 1,000,000 used cards and saw no signiﬁcant\ndifference between that and the test with an empty database,\nleading us to conclude that the hash table lookup does not\ndominate the cost of the ServerVerify algorithm.\nFigure Ishows the running time of each of the algorithms\nin our main punch card construction as well as the amount of\nin-protocol data sent by the party running the algorithm in a\npunch card system. The data sent in ServerSetup refers to the\nsize of the public key which must be communicated to clients.\nObserve that we do not require the client to communicate\nany data in order for a new punch card to be issued. The\nmost costly operation, punching a card, requires less than\n5ms between the client and server combined, and all other\noperations require less than 1ms. Communication is under 20 0\nBytes for all operations.\nFigure IIshows the same information for the mergeable\npunch card scheme. The mergeable scheme runs consider-\nably more slowly than the main scheme. This is the result\nof 1) more work being required in the mergeable scheme,\n2) group operations being more costly in pairing groups,\nand 3) the heavily optimized library used for curve25519\n(curve25519-dalek ) in the implementation of the main\nscheme. Group elements in pairing groups are also larger tha n\nin curve25519. In curve25519, the size of a group element\ng∈Gis 32 Bytes, but in the BLS12-381 curves, g0∈G0\nrequires 48 Bytes, g1∈G1requires 96 Bytes, and gT∈GT\nrequires 576 Bytes.\n11ServerSetup Issue ServerPunch ClientPunch ClientRedeem Se rverVerify\nComputation Time (ms) 0.019 0.304 0.134 4.314 0.890 0.064\nData Sent (Bytes) 32 0 128 32 64 0\nTABLE I: Computation and communication costs for our main pu nch card scheme.\nServerSetup Issue ServerPunch ClientPunch ClientMergeRed eem ServerVerify\nComputation Time (ms) 1.09 34.97 4.33 137.79 36.43 4.00\nData Sent (Bytes) 144 0 496 144 640 0\nTABLE II: Computation and communication costs for our merge able punch card scheme using pairings.\nComparison to Blind Signatures . Before presenting our\nempirical comparison to prior work, we will begin with a\nrough comparison of our approach to the blind signature-\nbased scheme sketched in Section I. Since exponentiations\nare the most expensive operation in both protocols, we will\nuse them to roughly compare performance. Assuming the\nblind signature scheme is instantiated with blind Schnorr\nsignatures [ 47], [48], it incurs only a few exponentiations\nper hole punch, comparable to our scheme. However, the\nclient and server storage, as well as redemption time, will b e\nproportional to the number of hole punches required to redee m\na card. Whereas our scheme can be implemented with only one\nexponentiation required for a server to verify a punch card, the\nblind signature scheme would require two exponentiations p er\nhole punch to verify each Schnorr signature. This means that\nour scheme outperforms the blind signature approach during\nredemption, even when only one hole punch is required to\nredeem a card. Our communication costs for redemption are\nalso equivalent to the size of a single Schnorr signature, so\ncommunicating a single hole punch for redemption in the blin d\nsignature scheme, which involves sending a Schnorr signatu re\nand corresponding client-chosen secret, would already inc ur\nmore communication than our scheme does regardless of the\nnumber of punches on a card.\nMoreover, during card redemption in the blind signature-\nbased scheme, the server checks that all the secrets present ed\nby the client have been signed and then adds each one to a\ndatabase of previously redeemed secrets, checking for doub le-\nspending as it goes. As such, a card that requires 10 punches t o\nbe redeemed would introduce an order of magnitude differenc e\nbetween the two approaches, and this gap would increase\nlinearly as the number of punches required grows.\nThe situation changes somewhat when comparing with our\nmergeable scheme, where the blind signature scheme does\nnot require any modiﬁcation because it is trivially mergeab le.\nIn this setting, the server-side database storage costs of o ur\nmergeable scheme are still smaller as long as a punch card\nrequires more than two punches to be redeemed. On the other\nhand, hole punches will be noticeably slower because each\npunch operation in the mergeable scheme requires twice as\nmany exponentiations as the main scheme. The fact that the\nmergeable scheme also requires using groups G0andG1thatsupport a pairing means that each exponentiation will also b e\nslower. In our experiments, G0exponentiations were about 5×\nslower and G1exponentiations were about 16×slower than\nin curve25519.\nRedemption in the mergeable scheme also requires a pair-\ning, which in our experiments were about 40×slower than\nan exponentiation in curve25519. Despite exponentiations and\npairings being signiﬁcantly slower than those used in the\nblind signature scheme, we estimate, based on the relative\nperformance of exponentiations and pairings in the differe nt\ncurves, that redemption in our mergeable scheme will still b e\nfaster on the server when a punch card requires 23or more\npunches to redeem, and will require less communication when\na card requires 8or more punches to redeem.\nComparison to prior work . We now empirically compare our\npunch card scheme to the recent loyalty systems BBA+ [ 34],\nUACS [ 5], Bobolz et al. [ 6], and BBW [ 36]. We do not com-\npare to the original BBA work [ 37] because its performance\nis strictly worse than the works to which we do compare.\nWe use the performance numbers reported by each prior\nwork to which we compare. Performance numbers for UACS\nand Bobolz et al., were also recorded with a Google Pixel\nphone but used a computer with a stronger i7 processor. BBA+\nonly reports the client-side cost of each of its protocols an d\nuses a OnePlus 3 phone. BBW uses an unspeciﬁed smartphone\nwith a Snapdragon 845 processor (used, e.g., in the Pixel 3). In\norder to better capture the total cost of using each approach , we\ncombine client and server costs to give the overall computat ion\ncost of each scheme. However, the distribution of cost betwe en\nthe client and server is similar for all works, with the mobil e\ndevice incurring most of the computation cost. Since BBW\nmeasured its server-side costs on the same smartphone inste ad\nof an external server, we only count their client-side costs\nin our evaluation to ensure fairness. Moreover, BBW reports\ndifferent conﬁgurations to achieve the best computation an d\ncommunication costs, so we use the best reported number for\neach comparison.\nFigure IIIcompares the performance numbers of our\nschemes against those of prior work. Our main scheme issues a\ncard171.1×faster, punches a card 13.9×faster, and redeems\na card127.9×faster than the best prior work, BBW. The\nperformance improvement comes from removing the reliance\n12Issuing a Card Punching a Card Redeeming a Card\nBBA+ scheme 115.27 385.61 375.73\nUACS scheme 86 127 454\nBobolz et al. scheme 130 64 1254\nBBW Scheme 52 62 122\nOur main scheme 0.304 (171.1 ×faster) 4.448 (13.9 ×faster) 0.954 (127.9 ×faster)\nOur mergeable scheme 34.97 (1.5 ×faster) 142.12 40.43 (3.0 ×faster)\nTABLE III: Computation time (in milliseconds) for our schem es and prior work. Speedups shown in\nparentheses refer to improvement over best prior work.\nIssuing a Card Punching a Card Redeeming a Card\nBBA+ scheme 992 4048 3984\nBBW scheme 1005 1745 3502\nOur main scheme 0 160 (10.9 ×reduction) 64 (54.7 ×reduction)\nOur mergeable scheme 0 640 (2.7 ×reduction) 640 (5.5 ×reduction)\nTABLE IV: Communication (in Bytes) in our schemes and prior w orks that report communication costs.\nOur schemes incur no communication to issue a new card and ach ieve order of magnitude improvements for\nother operations. The pairing-based scheme requires more c ommunication because pairing group elements\nare larger and punching a card requires twice as many element s communicated.\non pairings (in all works except BBW) and signiﬁcantly\nreducing the number and complexity of zero knowledge proofs\nrequired in each operation compared to all prior works. We\nare able to do this by tailoring our solution more narrowly\nto supporting punch cards. Prior works require heavier zero -\nknowledge proofs to handle broader use cases, e.g, adding\nnegative points, so focusing on punch cards allows us to do\naway with much of the complexity of prior solutions.\nOur mergeable punch card scheme outperforms prior work\nin almost every category despite using pairings. The BBW\nscheme punches cards 2.2×faster than our scheme, but in\nreturn our scheme issues cards 1.5×faster and redeems cards\n3×faster. An important difference to point out between our\nimplementation and most prior work is that while our imple-\nmentation is done with BLS12-381 curves, which provide 128\nbits of security, all prior works except BBW use BN curves [ 1],\n[38] which only provide 100 bits of security [ 39], [46].\nFigure IVcompares the communication costs of our\nschemes with BBA+ and BBW, the only prior works to report\nthe communication costs incurred by their implementation.\nUnlike all prior work, our scheme requires no communication\nto issue a new card, and card punching and redemption require\n10.9×and54.7×less communication, respectively, than the\nbest prior work. For the mergeable scheme, the improvements\nare reduced to 2.7×and5.5×, but even this scheme requires\nsigniﬁcantly less communication.\nVI. E XTENSIONS\nWe now brieﬂy discuss extensions to our main punch card\nscheme that can allow it to be used in a wider variety of\napplications.Multi-punches . Some loyalty programs sometimes offer extra\npunches on their punch cards as a special promotion. Others\ndon’t use a punch card at all, opting instead for a system\nwhere different transactions earn varying numbers of point s.\nOur punch card scheme can easily be extended to handle these\nsituations by having the server raise ptoskt, wheretis the\nnumber of points being awarded for a given transaction.\nUnfortunately, this kind of multi-punch raises a new securi ty\nquestion. Most punch card schemes offer a ﬁxed value nat\nwhich point a card can be redeemed for some beneﬁt, or\nperhaps a few values at which different kinds of rewards are\nunlocked. But the possibility of gaining more than one punch\nwith a given transaction introduces the potential for a clie nt to\n“overshoot” the required number of points. This does not pos e\nan issue for functionality, because the client can just rede em\na card with n′> n punches and perhaps even get a new card\nwith the remaining balance. However, this might introduce a\nprivacy issue because the redemption reveals the total numb er\nof punches on a card, which is no longer always the exact\nsame value for all clients. One way to eliminate this problem\nis to have the server send all possible values psk,psk2,...,pskt\nwhen punching a card. This works well for settings where tis\nsmall, e.g., a double-punch promotion. We leave the problem\nof an efﬁcient solution for large tfor future work.\nManaging used card database size . Our punch card scheme\nrequires keeping a database DBof used punch card secrets u,\nstored in a hash table in our implementation. While this does\nnot pose a performance problem because of the amortized\nconstant time lookup in the hash table, the storage cost\nincreases over time. Although at 128 bits per secret, it woul d\ntake a long time for storage costs to become prohibitive, a\nhigh-volume punch card program may wish for a plan to\n13eventually remove old punch cards from the database without\nallowing double spending.\nOne way to help reduce the long-term storage requirement\nis by adding extra information into the secret u. Sinceuis\nultimately passed through a hash function modeled as a ran-\ndom oracle, adding structured information before the rando m\nbits makes no difference in the security of the scheme (unles s\nthe structured information itself leaks something). Clien ts can\nbe required to add an expiration date to the beginning of u.\nThen the card redemption would check whether the card being\nused is expired or not. To encourage clients to pick reasonab le\nexpiration dates, cards with expiration dates too far in the\nfuture could be rejected as well. Expiration dates used in u\ncould be standardized, e.g., to the ﬁrst day of a given year,\nto prevent the date itself from leaking too much information\nabout an individual customer’s shopping habits.\nProof of Redemption . Our scheme can easily be modiﬁed\nto allow servers to prove that a punch card has previously\nbeen redeemed, e.g., when declining to accept a customer’s\notherwise valid card. This can be achieved by having the card\nsecretuitself be derived as the hash of another redemption\nsecretrsgenerated by the client during issuance and sent to\nthe server at the end of redemption. To prove that a card has\npreviously been redeemed, the server sends back rsto the\nclient before this last step. Including this functionality adds\nan additional hash to card issuance and redemption as well\nas an additional communication round during redemption, bu t\nit may be desirable in situations where clients have reason t o\nbelieve that a server may try to dishonestly reject valid pun ch\ncards.\nPrivate ticketing . Our punch card scheme can also be viewed\nas a scheme for private ticketing, or, more generally, as a\none-time use keyed-veriﬁcation anonymous credential [ 19]. To\nissue a ticket, the client generates a new punch card, and the\nserver punches it. A ticket can reﬂect additional informati on\n(e.g., if a train ticket is ﬁrst class or coach, which transit zones\na ticket is valid for, etc.) by the number of punches added to t he\nticket. To record multiple pieces of information on the same\nticket, the random oracle Hcan be used to generate multiple\ngroup elements from u, each of which can hold a different\nnumber of punches. Since the punches cannot be linked to\ntheir redemption, a client can later present the ticket with out\nlinking it to the issuance process.\nVII. R ELATED WORK\nIn principle, the problem of privacy-preserving punch card s\ncan be approached with a number of different techniques. One\napproach to this problem is via standard anonymous credenti al\ntechniques [ 21], [14], [15]. Ecash systems [ 13], [2] or even\nthe uCentive system [ 42], which is speciﬁcally designed for\nloyalty programs, can be used to give a customer an unlinkabl e\ntoken for each purchase. However, storage and computation\ncosts to hold and redeem a token in these systems must be\nlinear in the number of “hole punches” a customer acquires.\nCryptographic techniques that could be used to accumulateconstant-sized tokens do exist (e.g., [ 32], [45]), but they rely\non pairings and incur costs higher than the protocols presen ted\nin this work.\nThe line of work which this paper follows begins with the\nBlack Box Accumulation of Jager and Rupp [ 37]. This work\nintroduced the notion of a constant-sized wallet that could\naccumulate points without being linkable, but used heavier\ntools than more recent work and offered weaker security. In\nparticular, although accumulation of individual points wa s un-\nlinkable in this scheme, the processes of issuing and removi ng\npoints were linkable.\nThis limitation was removed in subsequent works\nBBA+ [ 34] and UACS [ 34], which both strengthened security\ndeﬁnitions and improved performance compared to BBA.\nThese systems also added new features, such as partial spend -\ning of points, which enables new applications not covered by\nthe original BBA model. Bobolz et al. [ 6] further reﬁne UACS,\ndramatically reducing the time to add points by pushing most\ncostly operations into the redemption phase. Moreover, the ir\nscheme can recover from failures that occur mid-protocol. O ur\nscheme trivially satisﬁes this requirement because the pro tocol\nonly requires one message from each party and improves the\nperformance of issuing, punching, and redeeming compared\nto prior work.\nBBW [ 36], much like this work, improves on the perfor-\nmance of BBA+ by removing the reliance on pairings and\nusing zero-knowledge range proofs, instantiated with Bull et-\nproofs [ 12], instead. Our work focuses more narrowly on the\npunch card scenario, without supporting addition of negati ve\npoints, but it shows how we can achieve order of magnitude\nimprovements even over BBW and dispense with all but the\nsimplest of proofs in this setting.\nVIII. C ONCLUSION\nWe have presented a new scheme for punch card loyalty\nprograms that signiﬁcantly outperforms all prior work both\nquantitatively and qualitatively. Our scheme does not requ ire\nany server interaction for a client to receive a punch card,\ndoes not require pairings, and outperforms prior work in\ncard issuance, punching, and redemption by 171×,14×, and\n128×respectively, strictly dominating the performance of all\nprior solutions to this problem. We have also shown several\nextensions to our main scheme, including a modiﬁed protocol\nthat allows merging punch cards (using pairings) that still out-\nperforms prior work. Our implementation is open source and\navailable at https://github.com/SabaEskandarian/PunchCard .\nACKNOWLEDGMENTS\nI would like to thank Dan Boneh for several helpful conver-\nsations, as well as my shepherd Dan Roche and the anonymous\nreviewers for helpful suggestions that improved the ﬁnal dr aft\nof this paper. This work was funded by NSF, DARPA, a grant\nfrom ONR, and the Simons Foundation. Opinions, ﬁndings and\nconclusions or recommendations expressed in this material are\nthose of the authors and do not necessarily reﬂect the views\nof DARPA.\n14",
      "metadata": {
        "filename": "Fast Privacy-Preserving Punch Cards.pdf",
        "hotspot_name": "Steel_Punching_and_Bending",
        "title": "Fast Privacy-Preserving Punch Cards",
        "published_date": "2020-06-10T21:49:15Z",
        "pdf_link": "http://arxiv.org/pdf/2006.06079v3",
        "query": "steel punching bending LCA reduction"
      }
    }
  },
  "query_mapping": {
    "search_queries": {
      "PBT Injection Molding": "PBT injection molding LCA environmental impact",
      "Aluminium Die Casting": "aluminium die casting energy efficiency optimization",
      "PCBA Production": "PCB assembly sustainability environmental assessment",
      "Steel Punching and Bending": "steel punching bending LCA reduction"
    },
    "downloaded_papers": {
      "PBT Injection Molding": [
        {
          "title": "Recy-ctronics: Designing Fully Recyclable Electronics With Varied Form\n  Factors",
          "hotspot_name": "PBT Injection Molding",
          "pdf_link": "http://arxiv.org/pdf/2406.09611v1",
          "published_date": "2024-06-13T22:43:47Z",
          "query": "PBT injection molding LCA environmental impact",
          "filename": "Recy-ctronics_ Designing Fully Recyclable Electronics With Varied Form Factors.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=pbt%20injection%20molding%20lca%20environmental%20impact&start=0&max_results=3"
        },
        {
          "title": "The environmental value of transport infrastructure in the UK: an\n  EXIOBASE analysis",
          "hotspot_name": "PBT Injection Molding",
          "pdf_link": "http://arxiv.org/pdf/2504.20098v1",
          "published_date": "2025-04-26T11:33:03Z",
          "query": "PBT injection molding LCA environmental impact",
          "filename": "The environmental value of transport infrastructure in the UK_ an EXIOBASE analy.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=pbt%20injection%20molding%20lca%20environmental%20impact&start=0&max_results=3"
        },
        {
          "title": "Space debris through the prism of the environmental performance of space\n  systems: the case of Sentinel-3 redesigned mission",
          "hotspot_name": "PBT Injection Molding",
          "pdf_link": "http://arxiv.org/pdf/2207.06306v1",
          "published_date": "2022-07-13T15:58:25Z",
          "query": "PBT injection molding LCA environmental impact",
          "filename": "Space debris through the prism of the environmental performance of space systems.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=pbt%20injection%20molding%20lca%20environmental%20impact&start=0&max_results=3"
        }
      ],
      "Aluminium Die Casting": [
        {
          "title": "MEDPNet: Achieving High-Precision Adaptive Registration for Complex Die\n  Castings",
          "hotspot_name": "Aluminium Die Casting",
          "pdf_link": "http://arxiv.org/pdf/2403.09996v1",
          "published_date": "2024-03-15T03:42:38Z",
          "query": "aluminium die casting energy efficiency optimization",
          "filename": "MEDPNet_ Achieving High-Precision Adaptive Registration for Complex Die Castings.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=aluminium%20die%20casting%20energy%20efficiency%20optimization&start=0&max_results=3"
        },
        {
          "title": "An Innovative Line Balancing for the Aluminium Melting Process",
          "hotspot_name": "Aluminium Die Casting",
          "pdf_link": "http://arxiv.org/pdf/2504.02857v1",
          "published_date": "2025-03-29T14:31:33Z",
          "query": "aluminium die casting energy efficiency optimization",
          "filename": "An Innovative Line Balancing for the Aluminium Melting Process.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=aluminium%20die%20casting%20energy%20efficiency%20optimization&start=0&max_results=3"
        },
        {
          "title": "Optimization of Solidification in Die Casting using Numerical\n  Simulations and Machine Learning",
          "hotspot_name": "Aluminium Die Casting",
          "pdf_link": "http://arxiv.org/pdf/1901.02364v2",
          "published_date": "2019-01-08T15:28:33Z",
          "query": "aluminium die casting energy efficiency optimization",
          "filename": "Optimization of Solidification in Die Casting using Numerical Simulations and Ma.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=aluminium%20die%20casting%20energy%20efficiency%20optimization&start=0&max_results=3"
        }
      ],
      "PCBA Production": [
        {
          "title": "Recyclable vitrimer-based printed circuit board for circular electronics",
          "hotspot_name": "PCBA Production",
          "pdf_link": "http://arxiv.org/pdf/2308.12496v1",
          "published_date": "2023-08-24T01:34:50Z",
          "query": "PCB assembly sustainability environmental assessment",
          "filename": "Recyclable vitrimer-based printed circuit board for circular electronics.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=pcb%20assembly%20sustainability%20environmental%20assessment&start=0&max_results=3"
        },
        {
          "title": "PCB Renewal: Iterative Reuse of PCB Substrates for Sustainable\n  Electronic Making",
          "hotspot_name": "PCBA Production",
          "pdf_link": "http://arxiv.org/pdf/2502.13255v1",
          "published_date": "2025-02-18T19:29:09Z",
          "query": "PCB assembly sustainability environmental assessment",
          "filename": "PCB Renewal_ Iterative Reuse of PCB Substrates for Sustainable Electronic Making.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=pcb%20assembly%20sustainability%20environmental%20assessment&start=0&max_results=3"
        },
        {
          "title": "Sustainable bioplastics from amyloid fibril-biodegradable polymer blends",
          "hotspot_name": "PCBA Production",
          "pdf_link": "http://arxiv.org/pdf/2105.14287v1",
          "published_date": "2021-05-29T12:49:57Z",
          "query": "PCB assembly sustainability environmental assessment",
          "filename": "Sustainable bioplastics from amyloid fibril-biodegradable polymer blends.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=pcb%20assembly%20sustainability%20environmental%20assessment&start=0&max_results=3"
        }
      ],
      "Steel Punching and Bending": [
        {
          "title": "Fast Privacy-Preserving Punch Cards",
          "hotspot_name": "Steel Punching and Bending",
          "pdf_link": "http://arxiv.org/pdf/2006.06079v3",
          "published_date": "2020-06-10T21:49:15Z",
          "query": "steel punching bending LCA reduction",
          "filename": "Fast Privacy-Preserving Punch Cards.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=steel%20punching%20bending%20lca%20reduction&start=0&max_results=3"
        },
        {
          "title": "Anisotropic behaviour law for sheets used in stamping: A comparative\n  study of steel and aluminium",
          "hotspot_name": "Steel Punching and Bending",
          "pdf_link": "http://arxiv.org/pdf/0801.3018v1",
          "published_date": "2008-01-19T07:48:14Z",
          "query": "steel punching bending LCA reduction",
          "filename": "Anisotropic behaviour law for sheets used in stamping_ A comparative study of st.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=steel%20punching%20bending%20lca%20reduction&start=0&max_results=3"
        },
        {
          "title": "475°C aging embrittlement of partially recrystallized FeCrAl ODS\n  ferritic steels after simulated tube process",
          "hotspot_name": "Steel Punching and Bending",
          "pdf_link": "http://arxiv.org/pdf/2310.13842v2",
          "published_date": "2023-10-20T22:31:18Z",
          "query": "steel punching bending LCA reduction",
          "filename": "475_C aging embrittlement of partially recrystallized FeCrAl ODS ferritic steels.pdf",
          "api_link": "http://export.arxiv.org/api/query?search_query=steel%20punching%20bending%20lca%20reduction&start=0&max_results=3"
        }
      ]
    },
    "total_papers": 12,
    "download_timestamp": "2025-06-24 22:38:49"
  },
  "processing_summary": {
    "total_processed": 12,
    "processing_timestamp": "2025-06-24 22:39:00.703404"
  }
}